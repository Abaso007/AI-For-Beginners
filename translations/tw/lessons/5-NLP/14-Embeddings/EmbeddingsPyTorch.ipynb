{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入\n",
    "\n",
    "在我們之前的例子中，我們操作的是長度為 `vocab_size` 的高維度詞袋向量，並且我們明確地將低維度的位置表示向量轉換為稀疏的獨熱表示（one-hot representation）。這種獨熱表示並不具備記憶效率，此外，每個詞彙彼此之間是獨立處理的，也就是說，獨熱編碼的向量無法表達詞彙之間的任何語義相似性。\n",
    "\n",
    "在本單元中，我們將繼續探索 **News AG** 數據集。首先，讓我們載入數據並從之前的筆記本中獲取一些定義。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什麼是嵌入？\n",
    "\n",
    "**嵌入**的概念是用低維度的密集向量來表示詞語，這些向量以某種方式反映詞語的語義。我們稍後會討論如何構建有意義的詞嵌入，但目前可以將嵌入理解為降低詞向量維度的一種方法。\n",
    "\n",
    "嵌入層會將一個詞作為輸入，並生成指定的 `embedding_size` 的輸出向量。從某種意義上說，它與 `Linear` 層非常相似，但嵌入層不需要接收獨熱編碼向量，而是可以直接接收詞的編號作為輸入。\n",
    "\n",
    "通過將嵌入層作為我們網絡的第一層，我們可以從詞袋模型切換到 **嵌入袋** 模型。在嵌入袋模型中，我們首先將文本中的每個詞轉換為相應的嵌入，然後對所有這些嵌入計算某種聚合函數，例如 `sum`、`average` 或 `max`。\n",
    "\n",
    "![展示五個序列詞嵌入分類器的圖片。](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)\n",
    "\n",
    "我們的分類器神經網絡將以嵌入層開始，接著是聚合層，最後是線性分類器：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 處理變數序列長度\n",
    "\n",
    "由於這種架構的特性，我們需要以特定的方式來建立傳遞給網路的迷你批次。在前一單元中，使用詞袋模型（bag-of-words, BoW）時，迷你批次中的所有 BoW 張量都具有相同的大小 `vocab_size`，無論文本序列的實際長度如何。然而，當我們轉向使用詞嵌入（word embeddings）時，每個文本樣本中的單詞數量會有所不同，而在將這些樣本組合成迷你批次時，我們需要進行一些填充操作。\n",
    "\n",
    "這可以通過為數據源提供 `collate_fn` 函數的方式來完成：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練嵌入分類器\n",
    "\n",
    "現在我們已經定義了合適的資料加載器，可以使用上一單元中定義的訓練函數來訓練模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意**：我們在這裡僅訓練 25k 筆記錄（少於一個完整的 epoch）以節省時間，但您可以繼續訓練，撰寫一個函數來訓練多個 epoch，並嘗試調整學習率參數以獲得更高的準確率。您應該能夠達到約 90% 的準確率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBag 層與可變長度序列表示法\n",
    "\n",
    "在之前的架構中，我們需要將所有序列填充至相同的長度，以便將它們放入小批量中。這並不是表示可變長度序列的最有效方式——另一種方法是使用 **偏移量** 向量，該向量保存所有序列在一個大型向量中的偏移位置。\n",
    "\n",
    "![顯示偏移序列表示法的圖片](../../../../../lessons/5-NLP/14-Embeddings/images/offset-sequence-representation.png)\n",
    "\n",
    "> **Note**: 在上圖中，我們展示的是字符序列，但在我們的例子中，我們處理的是單詞序列。然而，使用偏移量向量表示序列的基本原理是相同的。\n",
    "\n",
    "為了使用偏移量表示法，我們使用 [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html) 層。它類似於 `Embedding`，但它以內容向量和偏移量向量作為輸入，並且還包含一個平均層，可以是 `mean`、`sum` 或 `max`。\n",
    "\n",
    "以下是使用 `EmbeddingBag` 的修改後的網絡：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要準備用於訓練的數據集，我們需要提供一個轉換函數來準備偏移向量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，與之前所有的例子不同，我們的網絡現在接受兩個參數：數據向量和偏移向量，它們的大小不同。同樣地，我們的數據加載器也提供了3個值而不是2個：文本和偏移向量都作為特徵提供。因此，我們需要稍微調整我們的訓練函數來處理這一點：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 語意嵌入：Word2Vec\n",
    "\n",
    "在之前的例子中，模型的嵌入層學會了將文字映射到向量表示，但這種表示並沒有太多語意上的意義。如果能學習到一種向量表示，使得相似的詞或同義詞在某種向量距離（例如歐幾里得距離）上彼此接近，那就更好了。\n",
    "\n",
    "為了達到這個目標，我們需要以特定的方式在大量文本上預訓練嵌入模型。最早的語意嵌入訓練方法之一被稱為 [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)。它基於兩種主要架構，用於生成詞的分佈式表示：\n",
    "\n",
    " - **連續詞袋模型** (CBoW) — 在這種架構中，我們訓練模型根據周圍的上下文來預測一個詞。給定 ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$，模型的目標是從 $(W_{-2},W_{-1},W_1,W_2)$ 預測 $W_0$。\n",
    " - **連續跳元模型** (Skip-Gram) 則與 CBoW 相反。模型使用周圍窗口的上下文詞來預測當前詞。\n",
    "\n",
    "CBoW 的速度較快，而 Skip-Gram 的速度較慢，但在表示不常見詞方面表現更好。\n",
    "\n",
    "![展示 CBoW 和 Skip-Gram 算法如何將詞轉換為向量的圖片。](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)\n",
    "\n",
    "要嘗試使用 Google News 數據集預訓練的 Word2Vec 嵌入，我們可以使用 **gensim** 庫。以下是找到與「neural」最相似的詞的示例：\n",
    "\n",
    "> **注意:** 當你第一次創建詞向量時，下載它們可能需要一些時間！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們也可以從詞中計算向量嵌入，用於訓練分類模型（為了清楚起見，我們僅顯示向量的前20個組件）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語義嵌入的優點在於可以操控向量編碼來改變語義。例如，我們可以要求找到一個詞，其向量表示盡可能接近詞 *king* 和 *woman*，並且盡可能遠離詞 *man*：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBoW 和 Skip-Grams 都是「預測型」嵌入，因為它們只考慮局部上下文。Word2Vec 並未利用全局上下文。\n",
    "\n",
    "**FastText** 基於 Word2Vec，通過為每個單詞以及單詞內的字符 n-grams 學習向量表示來進一步擴展。在每次訓練步驟中，這些表示的值會被平均成一個向量。雖然這增加了預訓練的計算量，但它使得詞嵌入能夠編碼子詞信息。\n",
    "\n",
    "另一種方法，**GloVe**，利用共現矩陣的概念，使用神經方法將共現矩陣分解為更具表達性和非線性的詞向量。\n",
    "\n",
    "你可以通過將嵌入模型切換為 FastText 和 GloVe 來試驗這些例子，因為 gensim 支援多種不同的詞嵌入模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 PyTorch 中使用預訓練的嵌入\n",
    "\n",
    "我們可以修改上述範例，將嵌入層中的矩陣預先填入語義嵌入，例如 Word2Vec。我們需要考慮到，預訓練嵌入的詞彙表與我們文本語料庫的詞彙表可能不匹配，因此我們將用隨機值初始化缺失詞彙的權重：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在讓我們訓練模型。請注意，由於嵌入層的大小更大，因此參數的數量也大幅增加，訓練模型所需的時間比前一個例子顯著增加。此外，正因如此，如果我們想避免過擬合，可能需要在更多的例子上訓練模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我們的情況中，並未看到準確率有顯著提升，這可能是由於詞彙差異較大所致。  \n",
    "為了解決詞彙差異的問題，我們可以採用以下解決方案之一：  \n",
    "* 重新訓練 word2vec 模型以適應我們的詞彙  \n",
    "* 使用預訓練的 word2vec 模型的詞彙來載入我們的數據集。在載入數據集時，可以指定使用的詞彙。  \n",
    "\n",
    "後者的方法似乎更簡單，尤其是因為 PyTorch 的 `torchtext` 框架內建了對嵌入的支持。例如，我們可以以下列方式實例化基於 GloVe 的詞彙：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已載入的詞彙表具有以下基本操作：\n",
    "* `vocab.stoi` 字典允許我們將單詞轉換為其在字典中的索引\n",
    "* `vocab.itos` 則執行相反的操作——將數字轉換為單詞\n",
    "* `vocab.vectors` 是嵌入向量的數組，因此要獲取單詞 `s` 的嵌入，我們需要使用 `vocab.vectors[vocab.stoi[s]]`\n",
    "\n",
    "以下是一個操作嵌入的範例，用來展示方程式 **kind-man+woman = queen**（我稍微調整了一下係數以使其生效）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使用這些嵌入來訓練分類器，我們首先需要使用GloVe詞彙表對我們的數據集進行編碼：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所述，所有向量嵌入都存儲在 `vocab.vectors` 矩陣中。這使得通過簡單的複製將這些權重加載到嵌入層的權重中變得非常容易：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在讓我們訓練模型，看看是否能獲得更好的結果：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們未看到準確性顯著提高的原因之一是因為我們的數據集中的某些詞語在預訓練的GloVe詞彙表中缺失，因此它們基本上被忽略了。為了克服這一問題，我們可以在我們的數據集上訓練自己的嵌入。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 語境嵌入\n",
    "\n",
    "傳統預訓練嵌入表示（例如 Word2Vec）的一個主要限制是詞義消歧的問題。雖然預訓練嵌入可以捕捉到一些單詞在語境中的含義，但每個單詞的所有可能含義都被編碼到同一個嵌入中。這可能會在下游模型中引發問題，因為許多單詞（例如 \"play\"）的含義會根據使用的語境而有所不同。\n",
    "\n",
    "例如，單詞 \"play\" 在以下兩個句子中的含義就完全不同：\n",
    "- 我去劇院看了一場**戲劇**。\n",
    "- 約翰想和他的朋友們一起**玩**。\n",
    "\n",
    "上述的預訓練嵌入將 \"play\" 的這兩種含義表示為相同的嵌入。為了克服這一限制，我們需要基於**語言模型**來構建嵌入，該模型是在大規模文本語料庫上訓練的，並且*知道*單詞如何在不同語境中組合使用。討論語境嵌入超出了本教程的範圍，但我們會在下一單元討論語言模型時回到這個主題。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責聲明**：  \n本文件使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。應以原文文件作為權威來源。對於關鍵資訊，建議尋求專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤讀概不負責。\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-31T10:54:45+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "tw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}