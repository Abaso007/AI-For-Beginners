<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-24T21:13:38+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ja"
}
-->
# 深層強化学習

強化学習（RL）は、教師あり学習や教師なし学習と並ぶ基本的な機械学習のパラダイムの一つとされています。教師あり学習では既知の結果を持つデータセットに依存しますが、RLは**実際に行動することで学ぶ**ことに基づいています。例えば、初めてコンピュータゲームを見たとき、ルールを知らなくてもプレイを始め、プレイしながら行動を調整することでスキルを向上させることができます。

## [講義前のクイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RLを行うためには以下が必要です：

* **環境**または**シミュレーター**：ゲームのルールを設定するもの。シミュレーター内で実験を行い、その結果を観察できる必要があります。
* **報酬関数**：実験がどれだけ成功したかを示すもの。コンピュータゲームを学ぶ場合、報酬は最終スコアになります。

報酬関数に基づいて行動を調整し、スキルを向上させることで次回のプレイをより良いものにすることができます。他の機械学習とRLの主な違いは、RLではゲームが終了するまで勝敗が分からないことです。そのため、特定の動きが良いかどうかを単独で判断することはできず、ゲーム終了時に初めて報酬を受け取ります。

RLでは通常、多くの実験を行います。各実験では、これまで学んだ最適な戦略を追求する（**活用**）と、新しい可能性を探索する（**探索**）のバランスを取る必要があります。

## OpenAI Gym

RLにとって非常に便利なツールが[OpenAI Gym](https://gym.openai.com/)です。これは**シミュレーション環境**であり、Atariゲームからポールバランスの物理学まで、さまざまな環境をシミュレートできます。OpenAI Gymは強化学習アルゴリズムをトレーニングするための最も人気のあるシミュレーション環境の一つであり、[OpenAI](https://openai.com/)によって維持されています。

> **Note**: OpenAI Gymで利用可能なすべての環境は[こちら](https://gym.openai.com/envs/#classic_control)で確認できます。

## CartPoleバランス

現代のバランスデバイス、例えば*セグウェイ*や*ジャイロスクーター*を見たことがあるでしょう。これらは加速度計やジャイロスコープからの信号に応じて車輪を調整することで自動的にバランスを取ることができます。このセクションでは、似たような問題であるポールバランスを解決する方法を学びます。これはサーカスのパフォーマーが手の上でポールをバランスさせる状況に似ていますが、ポールバランスは1次元でのみ発生します。

バランスの簡略化されたバージョンは**CartPole**問題として知られています。CartPoleの世界では、左右に動く水平スライダーがあり、その上に垂直のポールをバランスさせることが目標です。

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

この環境を作成して使用するには、いくつかのPythonコードが必要です：

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

各環境は以下のようにアクセスできます：
* `env.reset`：新しい実験を開始します。
* `env.step`：シミュレーションステップを実行します。**アクションスペース**からの**アクション**を受け取り、**観測**（観測スペースから）、報酬、および終了フラグを返します。

上記の例では、各ステップでランダムなアクションを実行しているため、実験の寿命は非常に短いです：

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RLアルゴリズムの目標は、モデル、つまり**ポリシー**πをトレーニングすることです。このポリシーは、特定の状態に応じてアクションを返します。また、ポリシーを確率的に考えることもできます。例えば、任意の状態*s*とアクション*a*に対して、π(*a*|*s*)は状態*s*でアクション*a*を取るべき確率を返します。

## ポリシー勾配アルゴリズム

ポリシーをモデル化する最も明白な方法は、状態を入力として受け取り、対応するアクション（またはすべてのアクションの確率）を返すニューラルネットワークを作成することです。ある意味では通常の分類タスクに似ていますが、大きな違いは各ステップでどのアクションを取るべきか事前に分からないことです。

ここでのアイデアは、その確率を推定することです。実験の各ステップでの総報酬を示す**累積報酬**のベクトルを構築します。また、**報酬割引**を適用し、初期の報酬の役割を減少させるために係数γ=0.99を掛けます。そして、より大きな報酬をもたらす実験経路のステップを強化します。

> ポリシー勾配アルゴリズムについて詳しく学び、実際の動作を確認するには[例のノートブック](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)をご覧ください。

## アクター・クリティックアルゴリズム

ポリシー勾配アプローチの改良版は**アクター・クリティック**と呼ばれます。その主なアイデアは、ニューラルネットワークが以下の2つを返すようにトレーニングされることです：

* ポリシー：どのアクションを取るべきかを決定します。この部分は**アクター**と呼ばれます。
* 状態で得られると予想される総報酬の推定値。この部分は**クリティック**と呼ばれます。

ある意味では、このアーキテクチャは[GAN](../../4-ComputerVision/10-GANs/README.md)に似ています。GANでは2つのネットワークが互いに競い合いながらトレーニングされますが、アクター・クリティックモデルではアクターが取るべきアクションを提案し、クリティックがその結果を批判的に評価します。ただし、目標はこれらのネットワークを統一してトレーニングすることです。

実験中に実際の累積報酬とクリティックが返す結果の両方を知っているため、それらの差を最小化する損失関数を構築するのは比較的簡単です。これにより**クリティック損失**が得られます。**アクター損失**はポリシー勾配アルゴリズムと同じアプローチを使用して計算できます。

これらのアルゴリズムのいずれかを実行した後、CartPoleは次のように動作することが期待されます：

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ 演習：ポリシー勾配とアクター・クリティックRL

以下のノートブックで学習を続けてください：

* [TensorFlowでのRL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorchでのRL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## その他のRLタスク

現在、強化学習は急速に成長している研究分野です。強化学習の興味深い例として以下があります：

* **Atariゲーム**をコンピュータに教えること。この問題の難しい部分は、単純なベクトルとして表現される状態ではなく、スクリーンショットであることです。そのため、CNNを使用してスクリーン画像を特徴ベクトルに変換したり、報酬情報を抽出する必要があります。AtariゲームはGymで利用可能です。
* チェスや囲碁などのボードゲームをコンピュータに教えること。最近では、**Alpha Zero**のような最先端プログラムが、2つのエージェントが互いに対戦しながらゼロから学習し、各ステップで改善することでトレーニングされました。
* 産業界では、シミュレーションから制御システムを作成するためにRLが使用されています。[Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste)というサービスはそのために特化されています。

## 結論

ゲームの望ましい状態を定義する報酬関数を提供し、探索空間を知的に探索する機会を与えることで、エージェントをトレーニングして良い結果を達成する方法を学びました。2つのアルゴリズムを試し、比較的短期間で良い結果を得ることができました。しかし、これはRLへの旅の始まりに過ぎません。さらに深く掘り下げたい場合は、別のコースを受講することを検討してください。

## 🚀 チャレンジ

「その他のRLタスク」セクションに記載されているアプリケーションを探索し、1つを実装してみてください！

## [講義後のクイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## 復習と自己学習

[初心者向け機械学習カリキュラム](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)で古典的な強化学習についてさらに学びましょう。

コンピュータがスーパーマリオを学ぶ方法について話している[この素晴らしい動画](https://www.youtube.com/watch?v=qv6UVOQ0F44)を視聴してください。

## 課題：[マウンテンカーをトレーニングする](lab/README.md)

この課題では、別のGym環境である[Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)をトレーニングすることが目標です。

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知おきください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。