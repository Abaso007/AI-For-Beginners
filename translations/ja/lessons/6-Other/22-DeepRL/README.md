<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T13:05:56+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ja"
}
-->
# 深層強化学習

強化学習（Reinforcement Learning, RL）は、教師あり学習や教師なし学習と並ぶ基本的な機械学習のパラダイムの一つとされています。教師あり学習では既知の結果を含むデータセットに依存しますが、RLは**実践を通じて学ぶ**ことに基づいています。例えば、初めてコンピュータゲームを見たとき、ルールを知らなくてもプレイを始め、プレイを繰り返しながら行動を調整することでスキルを向上させることができます。

## [講義前クイズ](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RLを実行するには以下が必要です：

* **環境**または**シミュレータ**：ゲームのルールを設定するもの。シミュレータ内で実験を実行し、その結果を観察できる必要があります。
* **報酬関数**：実験がどれだけ成功したかを示すもの。例えば、コンピュータゲームを学ぶ場合、報酬は最終スコアになります。

報酬関数に基づいて行動を調整しスキルを向上させることで、次回のプレイでより良い結果を得られるようになります。他の機械学習とRLの主な違いは、RLでは通常、ゲームが終了するまで勝敗が分からない点です。そのため、特定の動きが良いかどうかを単独で判断することはできず、ゲーム終了時に初めて報酬を受け取ります。

RLでは通常、多くの実験を行います。各実験では、これまでに学んだ最適な戦略を活用する（**活用**）と、新しい可能性を探る（**探索**）のバランスを取る必要があります。

## OpenAI Gym

RLにとって非常に便利なツールが[OpenAI Gym](https://gym.openai.com/)です。これは**シミュレーション環境**で、Atariゲームからポールバランスの物理学まで、さまざまな環境をシミュレートできます。OpenAI Gymは、強化学習アルゴリズムをトレーニングするための最も人気のあるシミュレーション環境の一つであり、[OpenAI](https://openai.com/)によって維持されています。

> **Note**: OpenAI Gymで利用可能なすべての環境は[こちら](https://gym.openai.com/envs/#classic_control)で確認できます。

## カートポールのバランス

皆さんはおそらく、*セグウェイ*や*ジャイロスクーター*のような現代のバランスデバイスを見たことがあるでしょう。これらは、加速度計やジャイロスコープからの信号に応じて車輪を調整することで自動的にバランスを取ることができます。このセクションでは、似たような問題、つまりポールのバランスを取る方法を学びます。これは、サーカスのパフォーマーが手の上でポールをバランスさせる状況に似ていますが、ここでは1次元でのバランスに限定されます。

簡略化されたバランス問題は**カートポール**問題として知られています。カートポールの世界では、左右に動く水平スライダーがあり、その上に垂直のポールをバランスさせるのが目標です。

<img alt="カートポール" src="images/cartpole.png" width="200"/>

この環境を作成して使用するには、以下のような数行のPythonコードが必要です：

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

各環境は以下のようにアクセスできます：
* `env.reset`：新しい実験を開始
* `env.step`：シミュレーションステップを実行。**アクションスペース**からの**アクション**を受け取り、**観測**（観測スペースから）、報酬、および終了フラグを返します。

上記の例では、各ステップでランダムなアクションを実行しているため、実験の寿命は非常に短くなります：

![バランスを取れないカートポール](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RLアルゴリズムの目標は、モデル、つまり**ポリシー** &pi; をトレーニングすることです。このポリシーは、特定の状態に応じてアクションを返します。また、ポリシーを確率的とみなすこともできます。つまり、任意の状態 *s* とアクション *a* に対して、状態 *s* で *a* を取るべき確率 &pi;(*a*|*s*) を返します。

## ポリシー勾配アルゴリズム

ポリシーをモデル化する最も明白な方法は、状態を入力として受け取り、対応するアクション（またはすべてのアクションの確率）を返すニューラルネットワークを作成することです。ある意味では、通常の分類タスクに似ていますが、大きな違いは、各ステップでどのアクションを取るべきかを事前に知ることができない点です。

ここでのアイデアは、それらの確率を推定することです。実験の各ステップでの**累積報酬**のベクトルを構築し、実験中の各ステップでの合計報酬を示します。また、**報酬の割引**を適用し、初期の報酬の役割を減少させるために係数 &gamma;=0.99 を掛けます。その後、より大きな報酬をもたらす実験経路のステップを強化します。

> ポリシー勾配アルゴリズムについてさらに学び、その実例を[こちらのノートブック](CartPole-RL-TF.ipynb)で確認してください。

## アクター・クリティックアルゴリズム

ポリシー勾配アプローチの改良版が**アクター・クリティック**と呼ばれるものです。このアプローチの主なアイデアは、ニューラルネットワークが以下の2つを返すようにトレーニングされることです：

* どのアクションを取るべきかを決定するポリシー（**アクター**と呼ばれる部分）
* この状態で得られると予想される合計報酬の推定値（**クリティック**と呼ばれる部分）

ある意味では、このアーキテクチャは[GAN](../../4-ComputerVision/10-GANs/README.md)に似ています。GANでは2つのネットワークが互いに競い合いながらトレーニングされますが、アクター・クリティックモデルでは、アクターが取るべきアクションを提案し、クリティックがその結果を批判的に評価します。ただし、私たちの目標はこれらのネットワークを協調してトレーニングすることです。

実験中に得られる実際の累積報酬とクリティックが返す結果の両方を知っているため、それらの差を最小化する損失関数を構築するのは比較的簡単です。これにより**クリティック損失**を得ることができます。**アクター損失**は、ポリシー勾配アルゴリズムと同じアプローチを使用して計算できます。

これらのアルゴリズムのいずれかを実行した後、カートポールは次のように振る舞うことが期待されます：

![バランスを取るカートポール](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ 演習: ポリシー勾配とアクター・クリティックRL

以下のノートブックで学習を続けてください：

* [TensorFlowでのRL](CartPole-RL-TF.ipynb)
* [PyTorchでのRL](CartPole-RL-PyTorch.ipynb)

## その他のRLタスク

現在、強化学習は急速に成長している研究分野です。強化学習の興味深い例として以下があります：

* **Atariゲーム**をコンピュータに教えること。この問題の課題は、単純な状態がベクトルとして表現されるのではなく、スクリーンショットとして表現される点です。このスクリーン画像を特徴ベクトルに変換するか、報酬情報を抽出するためにCNNを使用する必要があります。AtariゲームはGymで利用可能です。
* チェスや囲碁のようなボードゲームをコンピュータに教えること。最近では、**Alpha Zero**のような最先端プログラムが、2つのエージェントが互いに対戦しながらゼロからトレーニングされ、各ステップで改善されました。
* 産業界では、シミュレーションから制御システムを作成するためにRLが使用されています。[Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste)というサービスは、特にそのために設計されています。

## 結論

私たちは、ゲームの望ましい状態を定義する報酬関数を提供し、探索空間を知的に探索する機会を与えることで、エージェントをトレーニングして良い結果を達成する方法を学びました。2つのアルゴリズムを試し、比較的短期間で良い結果を得ることができました。しかし、これはRLへの旅の始まりに過ぎません。さらに深く掘り下げたい場合は、別のコースを受講することを検討してください。

## 🚀 チャレンジ

「その他のRLタスク」セクションに記載されているアプリケーションを探索し、1つを実装してみてください！

## [講義後クイズ](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## 復習と自己学習

[初心者向け機械学習カリキュラム](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)で、古典的な強化学習についてさらに学びましょう。

コンピュータがスーパーマリオをプレイする方法を学ぶ[この素晴らしい動画](https://www.youtube.com/watch?v=qv6UVOQ0F44)を視聴してください。

## 課題: [マウンテンカーをトレーニングする](lab/README.md)

この課題では、別のGym環境である[Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)をトレーニングすることが目標です。

---

