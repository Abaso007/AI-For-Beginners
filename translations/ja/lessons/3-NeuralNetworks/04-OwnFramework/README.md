<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-24T21:14:52+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "ja"
}
-->
# ニューラルネットワーク入門：多層パーセプトロン

前のセクションでは、最もシンプルなニューラルネットワークモデルである1層パーセプトロン（線形2クラス分類モデル）について学びました。

このセクションでは、このモデルをより柔軟なフレームワークに拡張し、以下を可能にします：

* **多クラス分類**を2クラス分類に加えて実行する
* **回帰問題**を分類に加えて解く
* 線形分離不可能なクラスを分離する

また、Pythonで独自のモジュール型フレームワークを開発し、さまざまなニューラルネットワークアーキテクチャを構築できるようにします。

## [講義前のクイズ](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## 機械学習の形式化

まず、機械学習の問題を形式化することから始めましょう。トレーニングデータセット**X**とラベル**Y**があると仮定し、最も正確な予測を行うモデル*f*を構築する必要があります。予測の品質は**損失関数**ℒによって測定されます。以下の損失関数がよく使用されます：

* 数値を予測する必要がある回帰問題の場合、**絶対誤差** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| または **二乗誤差** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> を使用できます。
* 分類の場合、**0-1損失**（モデルの**精度**とほぼ同じ）や**ロジスティック損失**を使用します。

1層パーセプトロンの場合、関数*f*は線形関数*f(x)=wx+b*として定義されます（ここで*w*は重み行列、*x*は入力特徴ベクトル、*b*はバイアスベクトル）。異なるニューラルネットワークアーキテクチャでは、この関数はより複雑な形を取ることができます。

> 分類の場合、ネットワークの出力として対応するクラスの確率を得ることが望ましいことがよくあります。任意の数値を確率に変換するため（つまり出力を正規化するため）、**ソフトマックス**関数σをよく使用します。この場合、関数*f*は*f(x)=σ(wx+b)*となります。

上記の*f*の定義において、*w*と*b*は**パラメータ**θ=⟨*w,b*⟩と呼ばれます。データセット⟨**X**,**Y**⟩が与えられると、データセット全体の誤差をパラメータθの関数として計算できます。

> ✅ **ニューラルネットワークのトレーニングの目的は、パラメータθを変化させて誤差を最小化することです**

## 勾配降下法による最適化

関数最適化のよく知られた方法として**勾配降下法**があります。この方法のアイデアは、損失関数のパラメータに対する微分（多次元の場合は**勾配**と呼ばれる）を計算し、誤差が減少するようにパラメータを変化させることです。これを以下のように形式化できます：

* 初期パラメータをランダムな値で設定する w<sup>(0)</sup>, b<sup>(0)</sup>
* 以下のステップを何度も繰り返す：
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

トレーニング中、最適化ステップはデータセット全体を考慮して計算されるべきです（損失はすべてのトレーニングサンプルを通じて合計として計算されることを思い出してください）。しかし実際には、データセットの小さな部分を**ミニバッチ**と呼び、それに基づいて勾配を計算します。この部分は毎回ランダムに選ばれるため、この方法は**確率的勾配降下法**（SGD）と呼ばれます。

## 多層パーセプトロンとバックプロパゲーション

1層ネットワークは、前述のように線形分離可能なクラスを分類することができます。より豊かなモデルを構築するために、ネットワークの複数の層を組み合わせることができます。数学的には、関数*f*がより複雑な形を取り、以下のように複数のステップで計算されます：
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

ここで、αは**非線形活性化関数**、σはソフトマックス関数、パラメータθ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>です。

勾配降下アルゴリズムは同じままですが、勾配を計算するのがより困難になります。連鎖微分法則を用いることで、以下のように微分を計算できます：

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ 損失関数のパラメータに対する微分を計算するために連鎖微分法則が使用されます。

これらの式の左端部分はすべて同じであるため、損失関数から始めて計算グラフを「逆方向」にたどることで効率的に微分を計算できます。このため、多層パーセプトロンのトレーニング方法は**バックプロパゲーション**（バックプロップ）と呼ばれます。

<img alt="計算グラフ" src="images/ComputeGraphGrad.png"/>

> TODO: 画像の引用

> ✅ バックプロップについては、ノートブックの例でさらに詳しく説明します。

## 結論

このレッスンでは、独自のニューラルネットワークライブラリを構築し、それを使用して簡単な2次元分類タスクを実行しました。

## 🚀 チャレンジ

付属のノートブックでは、多層パーセプトロンを構築およびトレーニングするための独自のフレームワークを実装します。これにより、現代のニューラルネットワークがどのように動作するかを詳細に確認できます。

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb)ノートブックに進み、作業を進めてください。

## [講義後のクイズ](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## 復習と自己学習

バックプロパゲーションはAIや機械学習でよく使用されるアルゴリズムであり、[さらに詳しく](https://wikipedia.org/wiki/Backpropagation)学ぶ価値があります。

## [課題](lab/README.md)

このラボでは、このレッスンで構築したフレームワークを使用して、MNIST手書き数字分類を解決することが求められます。

* [指示](lab/README.md)
* [ノートブック](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知おきください。元の言語で記載された文書が公式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤認について、当方は一切の責任を負いません。