<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-24T21:15:22+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "ja"
}
-->
# ニューラルネットワークフレームワーク

すでに学んだように、ニューラルネットワークを効率的にトレーニングするためには、以下の2つのことを行う必要があります：

* テンソルを操作すること（例：乗算、加算、シグモイドやソフトマックスなどの関数を計算すること）
* 勾配降下法による最適化を行うために、すべての式の勾配を計算すること

## [事前クイズ](https://ff-quizzes.netlify.app/en/ai/quiz/9)

`numpy`ライブラリは最初の部分を実行できますが、勾配を計算する仕組みが必要です。[前のセクション](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb)で開発したフレームワークでは、`backward`メソッド内で逆伝播を行うためにすべての微分関数を手動でプログラムする必要がありました。理想的には、フレームワークは定義可能な*任意の式*の勾配を計算する機能を提供すべきです。

もう一つ重要なのは、GPUやその他の専門的な計算ユニット（例：[TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)）で計算を実行できることです。深層ニューラルネットワークのトレーニングには*膨大な計算*が必要であり、これらの計算をGPU上で並列化することが非常に重要です。

> ✅ 「並列化」という用語は、計算を複数のデバイスに分散することを意味します。

現在、最も人気のあるニューラルネットワークフレームワークは、[TensorFlow](http://TensorFlow.org)と[PyTorch](https://pytorch.org/)です。どちらもCPUとGPUでテンソルを操作するための低レベルAPIを提供しています。さらに、これらの低レベルAPIの上には、高レベルAPIである[Keras](https://keras.io/)や[PyTorch Lightning](https://pytorchlightning.ai/)があります。

低レベルAPI | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------|-------------------------------------|--------------------------------
高レベルAPI | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**低レベルAPI**は、いわゆる**計算グラフ**を構築することを可能にします。このグラフは、入力パラメータを基に出力（通常は損失関数）を計算する方法を定義し、GPUが利用可能であればその計算をGPUに送ることができます。この計算グラフを微分して勾配を計算する関数があり、それを使ってモデルパラメータを最適化することができます。

**高レベルAPI**は、ニューラルネットワークを**層のシーケンス**として扱い、ほとんどのニューラルネットワークの構築を非常に簡単にします。モデルのトレーニングは通常、データを準備し、`fit`関数を呼び出すことで行われます。

高レベルAPIは、典型的なニューラルネットワークを迅速に構築することを可能にし、多くの詳細を気にする必要がありません。一方で、低レベルAPIはトレーニングプロセスをより細かく制御することができるため、新しいニューラルネットワークアーキテクチャを扱う研究でよく使用されます。

また、両方のAPIを組み合わせて使用することも可能です。例えば、低レベルAPIを使用して独自のネットワーク層アーキテクチャを開発し、それを高レベルAPIで構築・トレーニングされた大規模なネットワーク内で使用することができます。または、高レベルAPIを使用して層のシーケンスとしてネットワークを定義し、独自の低レベルトレーニングループを使用して最適化を行うこともできます。両方のAPIは同じ基本的な概念を使用しており、互いにうまく連携するように設計されています。

## 学習

このコースでは、PyTorchとTensorFlowの両方に対応したコンテンツを提供しています。好みのフレームワークを選び、それに対応するノートブックだけを進めてください。どちらを選ぶべきか迷った場合は、**PyTorch vs. TensorFlow**に関するインターネット上の議論を読んでみてください。また、両方のフレームワークを見て理解を深めるのも良いでしょう。

可能な限り、高レベルAPIを使用して簡単に進めます。ただし、ニューラルネットワークがどのように動作するかを基礎から理解することが重要だと考えているため、最初は低レベルAPIとテンソルを使用して作業を始めます。ただし、詳細を学ぶのに時間をかけたくない場合は、それらをスキップして高レベルAPIノートブックに直接進むこともできます。

## ✍️ 演習: フレームワーク

以下のノートブックで学習を続けてください：

低レベルAPI | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
------------|-------------------------------------|--------------------------------
高レベルAPI | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

フレームワークを習得した後、過学習の概念を復習しましょう。

# 過学習

過学習は機械学習において非常に重要な概念であり、正しく理解することが非常に重要です！

以下の5つの点（グラフ上の`x`で表される）を近似する問題を考えてみましょう：

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.ja.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.ja.jpg)
-------------------------|--------------------------
**線形モデル、2つのパラメータ** | **非線形モデル、7つのパラメータ**
トレーニング誤差 = 5.3 | トレーニング誤差 = 0
検証誤差 = 5.1 | 検証誤差 = 20

* 左側では、良好な直線近似が見られます。パラメータ数が適切であるため、モデルは点の分布の背後にあるアイデアを正しく捉えています。
* 右側では、モデルが強力すぎます。点が5つしかないのにモデルには7つのパラメータがあるため、すべての点を通過するように調整することができます。その結果、トレーニング誤差は0になります。しかし、これによりデータの正しいパターンを理解することが妨げられ、検証誤差が非常に高くなります。

モデルの豊かさ（パラメータ数）とトレーニングサンプル数の間で正しいバランスを取ることが非常に重要です。

## 過学習が発生する理由

  * トレーニングデータが不足している
  * モデルが強力すぎる
  * 入力データにノイズが多すぎる

## 過学習の検出方法

上記のグラフからわかるように、過学習は非常に低いトレーニング誤差と高い検証誤差によって検出できます。通常、トレーニング中はトレーニング誤差と検証誤差の両方が減少し始めますが、ある時点で検証誤差が減少を止めて上昇し始めることがあります。これが過学習の兆候であり、この時点でトレーニングを停止するべき（または少なくともモデルのスナップショットを作成するべき）という指標になります。

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.ja.png)

## 過学習を防ぐ方法

過学習が発生している場合、以下のいずれかを行うことができます：

 * トレーニングデータを増やす
 * モデルの複雑さを減らす
 * [正則化技術](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md)を使用する（例：[Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout)）。これについては後で詳しく説明します。

## 過学習とバイアス-分散トレードオフ

過学習は、統計学におけるより一般的な問題である[バイアス-分散トレードオフ](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)の一例です。モデルの誤差の原因を考えると、以下の2種類の誤差があることがわかります：

* **バイアス誤差**は、アルゴリズムがトレーニングデータ間の関係を正しく捉えられないことによって引き起こされます。これは、モデルが十分に強力でないこと（**アンダーフィッティング**）に起因する可能性があります。
* **分散誤差**は、モデルが入力データのノイズを意味のある関係ではなく近似することによって引き起こされます（**過学習**）。

トレーニング中、バイアス誤差は減少し（モデルがデータを近似することを学ぶため）、分散誤差は増加します。過学習を防ぐためには、トレーニングを手動で（過学習を検出した場合）または自動で（正則化を導入することで）停止することが重要です。

## 結論

このレッスンでは、TensorFlowとPyTorchという2つの人気のあるAIフレームワークのさまざまなAPIの違いについて学びました。また、非常に重要なトピックである過学習についても学びました。

## 🚀 チャレンジ

付属のノートブックには「タスク」が記載されています。ノートブックを進めてタスクを完了してください。

## [事後クイズ](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## 復習と自己学習

以下のトピックについて調査してください：

- TensorFlow
- PyTorch
- 過学習

以下の質問を自問してください：

- TensorFlowとPyTorchの違いは何ですか？
- 過学習とアンダーフィッティングの違いは何ですか？

## [課題](lab/README.md)

このラボでは、PyTorchまたはTensorFlowを使用して、単層および多層の全結合ネットワークを用いた2つの分類問題を解決することが求められます。

* [指示](lab/README.md)
* [ノートブック](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知おきください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。本翻訳の使用に起因する誤解や誤認について、当方は一切の責任を負いません。