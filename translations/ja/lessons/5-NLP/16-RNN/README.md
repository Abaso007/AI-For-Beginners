<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-24T21:03:42+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "ja"
}
-->
# リカレントニューラルネットワーク (RNN)

## [事前クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

これまでのセクションでは、テキストのリッチなセマンティック表現と、その上に単純な線形分類器を使用してきました。このアーキテクチャは、文中の単語の集約された意味を捉えることができますが、埋め込みの上で行われる集約操作によって元のテキストからこの情報が失われるため、単語の**順序**を考慮することはできません。このため、これらのモデルは単語の順序をモデル化することができず、テキスト生成や質問応答のようなより複雑または曖昧なタスクを解くことができません。

テキストシーケンスの意味を捉えるためには、**リカレントニューラルネットワーク**（RNN）と呼ばれる別のニューラルネットワークアーキテクチャを使用する必要があります。RNNでは、文を1つのシンボルずつネットワークに通し、ネットワークはある**状態**を生成します。この状態を次のシンボルとともに再びネットワークに渡します。

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.ja.png)

> 著者による画像

トークンの入力シーケンス X<sub>0</sub>,...,X<sub>n</sub> が与えられると、RNNはニューラルネットワークブロックのシーケンスを作成し、このシーケンスをバックプロパゲーションを使用してエンドツーエンドでトレーニングします。各ネットワークブロックは (X<sub>i</sub>,S<sub>i</sub>) のペアを入力として受け取り、結果として S<sub>i+1</sub> を生成します。最終状態 S<sub>n</sub> または (出力 Y<sub>n</sub>) は線形分類器に渡され、結果を生成します。すべてのネットワークブロックは同じ重みを共有し、1回のバックプロパゲーションパスでエンドツーエンドでトレーニングされます。

状態ベクトル S<sub>0</sub>,...,S<sub>n</sub> がネットワークを通じて渡されるため、単語間のシーケンシャルな依存関係を学習することができます。例えば、シーケンス内のどこかに単語 *not* が現れると、状態ベクトル内の特定の要素を否定することを学習し、否定を表現することができます。

> ✅ 上記の図では、すべてのRNNブロックの重みが共有されているため、同じ図を1つのブロック（右側）として表現し、リカレントフィードバックループを持たせてネットワークの出力状態を入力に戻すことができます。

## RNNセルの構造

単純なRNNセルがどのように構成されているかを見てみましょう。RNNセルは、前の状態 S<sub>i-1</sub> と現在のシンボル X<sub>i</sub> を入力として受け取り、出力状態 S<sub>i</sub> を生成します（場合によっては、生成ネットワークのように他の出力 Y<sub>i</sub> にも関心があります）。

単純なRNNセルには2つの重み行列があります。1つは入力シンボルを変換するもので（これをWと呼びます）、もう1つは入力状態を変換するものです（これをHと呼びます）。この場合、ネットワークの出力は σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b) として計算されます。ここで、σは活性化関数、bは追加のバイアスです。

<img alt="RNNセルの構造" src="images/rnn-anatomy.png" width="50%"/>

> 著者による画像

多くの場合、入力トークンはRNNに入る前に埋め込み層を通過して次元を下げます。この場合、入力ベクトルの次元が *emb_size*、状態ベクトルが *hid_size* の場合、Wのサイズは *emb_size*×*hid_size*、Hのサイズは *hid_size*×*hid_size* となります。

## 長短期記憶 (LSTM)

古典的なRNNの主な問題の1つは、いわゆる**勾配消失**問題です。RNNは1回のバックプロパゲーションパスでエンドツーエンドでトレーニングされるため、ネットワークの最初の層に誤差を伝播させるのが難しくなり、遠く離れたトークン間の関係を学習することができません。この問題を回避する1つの方法は、**ゲート**を使用して**明示的な状態管理**を導入することです。この種のよく知られたアーキテクチャには、**長短期記憶**（LSTM）と**ゲート付きリレー単位**（GRU）があります。

![長短期記憶セルの例を示す画像](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> 画像ソース TBD

LSTMネットワークはRNNと似た構造を持っていますが、レイヤー間で渡される2つの状態（実際の状態Cと隠れベクトルH）があります。各ユニットでは、隠れベクトル H<sub>i</sub> が入力 X<sub>i</sub> と連結され、**ゲート**を介して状態Cに何が起こるかを制御します。各ゲートはシグモイド活性化（出力範囲[0,1]）を持つニューラルネットワークであり、状態ベクトルと乗算されるとビットマスクのように考えることができます。以下のゲートがあります（上記の図で左から右へ）：

* **忘却ゲート**は隠れベクトルを受け取り、ベクトルCのどの成分を忘れるべきか、どの成分を通過させるべきかを決定します。
* **入力ゲート**は入力と隠れベクトルから情報を取り出し、状態に挿入します。
* **出力ゲート**は状態を*tanh*活性化を持つ線形層を介して変換し、隠れベクトル H<sub>i</sub> を使用してその一部を選択し、新しい状態 C<sub>i+1</sub> を生成します。

状態Cの成分は、オンまたはオフに切り替えられるフラグのように考えることができます。例えば、シーケンス内で名前 *Alice* に出会ったとき、それが女性キャラクターを指していると仮定し、文中に女性名詞があるというフラグを状態に立てることができます。その後、*and Tom* というフレーズに出会ったとき、複数名詞があるというフラグを立てることができます。このようにして、状態を操作することで文の文法的特性を追跡することができます。

> ✅ LSTMの内部を理解するための優れたリソースとして、Christopher Olahによる[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)という素晴らしい記事があります。

## 双方向および多層RNN

これまで、シーケンスの始まりから終わりまで1方向に動作するリカレントネットワークについて説明しました。これは、私たちが読む方法や音声を聞く方法に似ているため自然に見えます。しかし、多くの実用的なケースでは入力シーケンスにランダムアクセスできるため、リカレント計算を両方向で実行する方が理にかなっている場合があります。このようなネットワークは**双方向**RNNと呼ばれます。双方向ネットワークを扱う場合、各方向に1つずつ、2つの隠れ状態ベクトルが必要です。

リカレントネットワーク（1方向または双方向）は、シーケンス内の特定のパターンを捉え、それを状態ベクトルに保存するか、出力に渡すことができます。畳み込みネットワークと同様に、最初の層で抽出された低レベルのパターンから高レベルのパターンを捉えるために、最初の層の上に別のリカレント層を構築することができます。これにより、**多層RNN**の概念が生まれます。これは、2つ以上のリカレントネットワークで構成され、前の層の出力が次の層の入力として渡されます。

![多層LSTM-RNNを示す画像](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.ja.jpg)

*Fernando Lópezによる[この素晴らしい投稿](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3)からの画像*

## ✍️ 演習: 埋め込み

以下のノートブックで学習を続けてください：

* [PyTorchでのRNN](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlowでのRNN](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## 結論

このユニットでは、RNNがシーケンス分類に使用できることを見てきましたが、実際にはテキスト生成、機械翻訳など、さらに多くのタスクを処理できます。これらのタスクについては次のユニットで検討します。

## 🚀 チャレンジ

LSTMに関する文献を読み、その応用について考えてみてください：

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [事後クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## 復習と自己学習

- Christopher Olahによる[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。

## [課題: ノートブック](assignment.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。