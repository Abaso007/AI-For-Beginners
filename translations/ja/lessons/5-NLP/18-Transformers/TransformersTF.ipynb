{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意メカニズムとトランスフォーマー\n",
    "\n",
    "リカレントネットワークの主な欠点の一つは、シーケンス内のすべての単語が結果に同じ影響を与えることです。このため、名前付きエンティティ認識や機械翻訳などのシーケンス間タスクにおいて、標準的なLSTMエンコーダーデコーダーモデルでは性能が最適化されません。実際には、入力シーケンス内の特定の単語が他の単語よりも順序出力に大きな影響を与えることがよくあります。\n",
    "\n",
    "機械翻訳のようなシーケンス間モデルを考えてみましょう。このモデルは2つのリカレントネットワークによって実装されます。一つのネットワーク（**エンコーダー**）が入力シーケンスを隠れ状態に圧縮し、もう一つのネットワーク（**デコーダー**）がその隠れ状態を展開して翻訳結果を生成します。このアプローチの問題点は、ネットワークの最終状態が文の冒頭を記憶するのが難しくなるため、長い文に対してモデルの品質が低下することです。\n",
    "\n",
    "**注意メカニズム**は、RNNの各出力予測に対する各入力ベクトルの文脈的影響を重み付けする手段を提供します。このメカニズムは、入力RNNの中間状態と出力RNNの間にショートカットを作成することで実装されます。この方法では、出力記号$y_t$を生成する際に、異なる重み係数$\\alpha_{t,i}$を用いてすべての入力隠れ状態$h_i$を考慮します。\n",
    "\n",
    "![エンコーダー/デコーダーモデルと加法型注意層を示す画像](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.ja.png)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)の加法型注意メカニズムを備えたエンコーダーデコーダーモデル。画像は[このブログ記事](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)から引用。]*\n",
    "\n",
    "注意行列$\\{\\alpha_{i,j}\\}$は、出力シーケンス内の特定の単語の生成において、入力単語がどの程度影響を与えるかを表します。以下はそのような行列の例です：\n",
    "\n",
    "![Bahdanau - arviz.orgから引用されたRNNsearch-50によるサンプルアラインメントを示す画像](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.ja.png)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)（図3）から引用された図]*\n",
    "\n",
    "注意メカニズムは、現在またはほぼ現在の自然言語処理の最先端技術の多くを支えています。しかし、注意を追加することでモデルのパラメータ数が大幅に増加し、RNNのスケーリング問題を引き起こしました。RNNのスケーリングにおける重要な制約は、モデルのリカレント性がバッチ処理やトレーニングの並列化を困難にすることです。RNNではシーケンスの各要素を順序通りに処理する必要があるため、簡単に並列化することができません。\n",
    "\n",
    "注意メカニズムの採用とこの制約が組み合わさり、現在私たちが使用している最先端のトランスフォーマーモデル（BERTやOpenGPT3など）が誕生しました。\n",
    "\n",
    "## トランスフォーマーモデル\n",
    "\n",
    "各予測の文脈を次の評価ステップに渡す代わりに、**トランスフォーマーモデル**は**位置エンコーディング**と**注意**を使用して、指定されたテキストウィンドウ内で与えられた入力の文脈を捉えます。以下の画像は、位置エンコーディングと注意がどのようにして指定されたウィンドウ内の文脈を捉えるかを示しています。\n",
    "\n",
    "![トランスフォーマーモデルで評価がどのように行われるかを示すアニメーションGIF](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "各入力位置が独立して各出力位置にマッピングされるため、トランスフォーマーはRNNよりも並列化が容易であり、より大規模で表現力豊かな言語モデルを可能にします。各注意ヘッドは、単語間の異なる関係を学習するために使用され、自然言語処理の下流タスクを改善します。\n",
    "\n",
    "## シンプルなトランスフォーマーモデルの構築\n",
    "\n",
    "Kerasには組み込みのトランスフォーマーレイヤーは含まれていませんが、自分で構築することができます。これまでと同様に、AG Newsデータセットのテキスト分類に焦点を当てますが、トランスフォーマーモデルはより難しいNLPタスクで最良の結果を示すことに注意する価値があります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kerasの新しいレイヤーは`Layer`クラスを継承し、`call`メソッドを実装する必要があります。まずは**Positional Embedding**レイヤーから始めましょう。[公式のKerasドキュメント](https://keras.io/examples/nlp/text_classification_with_transformer/)のコードを使用します。すべての入力シーケンスを長さ`maxlen`にパディングすることを前提とします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このレイヤーは2つの`Embedding`レイヤーで構成されています。1つはトークンを埋め込むためのもので（前に説明した方法で行います）、もう1つはトークンの位置を埋め込むためのものです。トークンの位置は、`tf.range`を使用して0から`maxlen`までの自然数のシーケンスとして作成され、それを埋め込みレイヤーに渡します。この2つの埋め込みベクトルを加算することで、入力の位置情報を埋め込んだ表現が得られます。この表現の形状は`maxlen`$\\times$`embed_dim`となります。\n",
    "\n",
    "次に、トランスフォーマーブロックを実装してみましょう。これは、先ほど定義した埋め込みレイヤーの出力を受け取ります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformerは、位置エンコードされた入力に対して`MultiHeadAttention`を適用し、`maxlen`$\\times$`embed_dim`の次元を持つアテンションベクトルを生成します。このベクトルは入力と混合され、`LayerNormalization`を使用して正規化されます。\n",
    "\n",
    "> **Note**: `LayerNormalization`は、この学習パスの*コンピュータビジョン*の部分で説明した`BatchNormalization`に似ていますが、各トレーニングサンプルの前の層の出力を独立して正規化し、[-1..1]の範囲に収めます。\n",
    "\n",
    "この層の出力は次に`Dense`ネットワーク（この場合、2層のパーセプトロン）を通過し、その結果が最終出力に加えられます（最終出力も再び正規化されます）。\n",
    "\n",
    "さて、完全なTransformerモデルを定義する準備が整いました：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT トランスフォーマーモデル\n",
    "\n",
    "**BERT**（Bidirectional Encoder Representations from Transformers）は、非常に大規模な多層トランスフォーマーネットワークで、*BERT-base* では12層、*BERT-large* では24層の構造を持っています。このモデルは、まず大規模なテキストデータ（Wikipedia + 書籍）を使用して、教師なし学習（文中のマスクされた単語を予測する）で事前学習されます。事前学習の過程で、モデルは言語理解の高度な知識を吸収し、その後、他のデータセットで微調整を行うことで活用できます。このプロセスは**転移学習**と呼ばれます。\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ からの画像](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.ja.png)\n",
    "\n",
    "BERT、DistilBERT、BigBird、OpenGPT3 など、微調整可能なトランスフォーマーアーキテクチャには多くのバリエーションがあります。\n",
    "\n",
    "では、事前学習済みの BERT モデルを使用して、従来のシーケンス分類問題を解決する方法を見てみましょう。[公式ドキュメント](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)からアイデアと一部のコードを拝借します。\n",
    "\n",
    "事前学習済みモデルをロードするには、**Tensorflow hub** を使用します。まず、BERT 専用のベクトライザーをロードしましょう:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元のネットワークが訓練された際に使用されたものと同じベクトライザーを使用することが重要です。また、BERTベクトライザーは以下の3つのコンポーネントを返します：\n",
    "* `input_word_ids`：入力文のトークン番号のシーケンス\n",
    "* `input_mask`：シーケンスのどの部分が実際の入力で、どの部分がパディングであるかを示します。これは、`Masking`レイヤーによって生成されるマスクに似ています\n",
    "* `input_type_ids`：言語モデリングタスクで使用され、1つのシーケンス内で2つの入力文を指定することができます。\n",
    "\n",
    "次に、BERT特徴抽出器をインスタンス化します：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT層は以下のような有用な結果を返します：\n",
    "* `pooled_output` は、シーケンス内のすべてのトークンを平均化した結果です。これをネットワーク全体の知的な意味的埋め込みと見なすことができます。これは、以前のモデルで使用した `GlobalAveragePooling1D` 層の出力に相当します。\n",
    "* `sequence_output` は、最後のトランスフォーマーレイヤーの出力です（上記のモデルで使用した `TransformerBlock` の出力に対応します）。\n",
    "* `encoder_outputs` は、すべてのトランスフォーマーレイヤーの出力です。4層のBERTモデルをロードしているため（名前に `4_H` が含まれていることから推測できるように）、4つのテンソルがあります。最後のテンソルは `sequence_output` と同じです。\n",
    "\n",
    "次に、エンドツーエンドの分類モデルを定義します。*関数型モデル定義* を使用し、モデルの入力を定義した後、一連の式を用いてその出力を計算します。また、BERTモデルの重みを学習可能にせず、最終的な分類器のみを学習させます：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練可能なパラメータが少ないにもかかわらず、プロセスはかなり遅いです。これは、BERTの特徴抽出が計算負荷が高いためです。訓練不足やモデルパラメータの不足が原因で、十分な精度を達成できなかったようです。\n",
    "\n",
    "BERTの重みをアンフリーズして、それも訓練してみましょう。これには非常に小さい学習率が必要であり、**ウォームアップ**を伴うより慎重な訓練戦略が必要です。また、**AdamW**オプティマイザーを使用します。オプティマイザーを作成するために`tf-models-official`パッケージを使用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングはかなりゆっくり進むようですが、いくつかのエポック（5〜10）でモデルをトレーニングしてみて、以前使用したアプローチと比較して最良の結果が得られるかどうか試してみると良いでしょう。\n",
    "\n",
    "## Huggingface Transformersライブラリ\n",
    "\n",
    "Transformerモデルを使用するもう一つの非常に一般的で少し簡単な方法は、[HuggingFaceパッケージ](https://github.com/huggingface/)を利用することです。このパッケージは、さまざまなNLPタスクのためのシンプルな構築ブロックを提供します。TensorflowとPyTorchの両方で利用可能で、PyTorchはもう一つの非常に人気のあるニューラルネットワークフレームワークです。\n",
    "\n",
    "> **Note**: Transformersライブラリの動作に興味がない場合は、このノートブックの最後までスキップしても構いません。これまで行った内容と本質的に異なるものは見られないからです。BERTモデルを異なるライブラリとかなり大きなモデルを使用してトレーニングする同じ手順を繰り返します。そのため、プロセスにはかなり長いトレーニングが含まれるので、コードをざっと見るだけでも良いでしょう。\n",
    "\n",
    "では、[Huggingface Transformers](http://huggingface.co)を使用してこの問題をどのように解決できるか見てみましょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず最初に使用するモデルを選ぶ必要があります。組み込みモデルに加えて、Huggingfaceには[オンラインモデルリポジトリ](https://huggingface.co/models)があり、コミュニティによって提供された多くの事前学習済みモデルを見つけることができます。これらのモデルはすべて、モデル名を指定するだけでロードして使用することができます。必要なバイナリファイルは自動的にダウンロードされます。\n",
    "\n",
    "場合によっては、自分自身のモデルをロードする必要があることもあります。その場合、トークナイザーのパラメータ、モデルパラメータを含む`config.json`ファイル、バイナリウェイトなど、関連するすべてのファイルが含まれるディレクトリを指定することができます。\n",
    "\n",
    "モデル名から、モデルとトークナイザーの両方をインスタンス化することができます。まずはトークナイザーから始めましょう:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer`オブジェクトには、テキストを直接エンコードするために使用できる`encode`関数が含まれています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シーケンスをモデルに渡すのに適した方法でエンコードするために、トークナイザーを使用することもできます。例えば、`token_ids`、`input_mask`フィールドなどを含めることができます。また、`return_tensors='tf'`引数を指定することで、TensorFlowのテンソルを取得することもできます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私たちの場合、事前学習済みのBERTモデルである`bert-base-uncased`を使用します。*Uncased*は、このモデルが大文字小文字を区別しないことを示しています。\n",
    "\n",
    "モデルを訓練する際には、トークン化されたシーケンスを入力として提供する必要があるため、データ処理パイプラインを設計します。`tokenizer.encode`はPythonの関数であるため、前のユニットで使用した方法と同様に、`py_function`を使用して呼び出します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度は、`BertForSequenceClassification`パッケージを使用して実際のモデルをロードできます。これにより、最終的な分類器を含む、分類に必要なアーキテクチャがすでにモデルに備わっていることが保証されます。最終的な分類器の重みが初期化されていないため、モデルには事前トレーニングが必要であるという警告メッセージが表示されますが、それは完全に問題ありません。なぜなら、まさにこれからそれを行うからです！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary()`からわかるように、このモデルにはほぼ1億1000万のパラメーターが含まれています！おそらく、比較的小さなデータセットで単純な分類タスクを行いたい場合、BERTのベース層を訓練したくないでしょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さあ、トレーニングを始めましょう！\n",
    "\n",
    "> **Note**: フルスケールのBERTモデルをトレーニングするには非常に時間がかかる可能性があります！そのため、最初の32バッチだけトレーニングを行います。これはモデルのトレーニング設定を示すためのものです。もしフルスケールのトレーニングを試したい場合は、`steps_per_epoch`と`validation_steps`のパラメータを削除して、時間がかかる準備をしてください！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もし、イテレーションの回数を増やし、十分に待機し、さらに複数のエポックでトレーニングを行えば、BERTによる分類が最高の精度をもたらすことが期待できます！これは、BERTがすでに言語の構造を非常によく理解しており、最終的な分類器を微調整するだけで済むからです。しかし、BERTは大規模なモデルであるため、トレーニング全体のプロセスには時間がかかり、かなりの計算能力（GPU、できれば複数）が必要です。\n",
    "\n",
    "> **Note:** この例では、最も小さい事前学習済みのBERTモデルの1つを使用しています。より大きなモデルを使用すれば、さらに良い結果が得られる可能性があります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このユニットでは、**トランスフォーマー**に基づいた非常に新しいモデルアーキテクチャについて学びました。これらをテキスト分類タスクに適用しましたが、同様にBERTモデルはエンティティ抽出、質問応答、その他のNLPタスクにも使用できます。\n",
    "\n",
    "トランスフォーマーモデルは、現在のNLPにおける最先端技術を代表しており、ほとんどの場合、カスタムNLPソリューションを実装する際に最初に試すべきアプローチです。しかし、より高度なニューラルモデルを構築したい場合、このモジュールで説明したリカレントニューラルネットワークの基本的な原理を理解することは非常に重要です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責事項**:  \nこの文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書を正式な情報源としてお考えください。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は責任を負いません。\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-30T10:21:03+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "ja"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}