<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-24T21:49:43+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "ja"
}
-->
# 注意メカニズムとトランスフォーマー

## [事前講義クイズ](https://ff-quizzes.netlify.app/en/ai/quiz/35)

NLP分野で最も重要な問題の1つは**機械翻訳**であり、Google翻訳のようなツールの基盤となる重要なタスクです。このセクションでは、機械翻訳、またはより一般的には*シーケンス間変換*タスク（**文変換**とも呼ばれる）に焦点を当てます。

RNNを使用すると、シーケンス間変換は2つのリカレントネットワークによって実装されます。1つのネットワークである**エンコーダ**は入力シーケンスを隠れ状態に圧縮し、もう1つのネットワークである**デコーダ**はこの隠れ状態を展開して翻訳結果を生成します。このアプローチにはいくつかの問題があります：

* エンコーダネットワークの最終状態が文の冒頭を記憶するのが難しく、長い文ではモデルの品質が低下する
* シーケンス内のすべての単語が結果に同じ影響を与える。しかし実際には、入力シーケンス内の特定の単語が他の単語よりも出力に大きな影響を与えることが多い。

**注意メカニズム**は、RNNの各出力予測に対する各入力ベクトルの文脈的影響を重み付けする手段を提供します。これを実現する方法は、入力RNNの中間状態と出力RNNの間にショートカットを作成することです。この方法では、出力記号y<sub>t</sub>を生成する際に、異なる重み係数α<sub>t,i</sub>を用いてすべての入力隠れ状態h<sub>i</sub>を考慮します。

![エンコーダ/デコーダモデルと加法型注意層を示す画像](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.ja.png)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)の加法型注意メカニズムを持つエンコーダ-デコーダモデル、[このブログ記事](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)から引用

注意行列{α<sub>i,j</sub>}は、出力シーケンス内の特定の単語の生成において、入力単語が果たす役割の程度を表します。以下はそのような行列の例です：

![Bahdanau - arviz.orgから引用されたRNNsearch-50によるサンプルアラインメントを示す画像](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.ja.png)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)から引用

注意メカニズムは、現在またはほぼ現在のNLPの最先端技術の多くを支えています。しかし注意を追加するとモデルパラメータの数が大幅に増加し、RNNのスケーリング問題を引き起こしました。RNNのスケーリングにおける重要な制約は、モデルのリカレント性がトレーニングのバッチ化と並列化を困難にすることです。RNNではシーケンスの各要素を順序通りに処理する必要があり、簡単に並列化することができません。

![注意付きエンコーダデコーダ](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> [Googleのブログ](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)から引用

注意メカニズムの採用とこの制約が組み合わさり、現在私たちが使用している最先端のトランスフォーマーモデル（BERTやOpen-GPT3など）が誕生しました。

## トランスフォーマーモデル

トランスフォーマーの背後にある主なアイデアの1つは、RNNの逐次性を回避し、トレーニング中に並列化可能なモデルを作成することです。これを実現するために、以下の2つのアイデアが実装されています：

* 位置エンコーディング
* RNN（またはCNN）の代わりに自己注意メカニズムを使用してパターンを捉える（そのため、トランスフォーマーを紹介する論文は*「Attention is all you need」*と呼ばれています）

### 位置エンコーディング/埋め込み

位置エンコーディングのアイデアは以下の通りです：
1. RNNを使用する場合、トークンの相対位置はステップ数によって表されるため、明示的に表現する必要はありません。
2. しかし、注意に切り替えると、シーケンス内のトークンの相対位置を知る必要があります。
3. 位置エンコーディングを得るために、トークンのシーケンスにシーケンス内のトークン位置（つまり、0,1,...の数列）を追加します。
4. 次に、トークン位置をトークン埋め込みベクトルと混ぜ合わせます。位置（整数）をベクトルに変換するために、以下のような異なるアプローチを使用できます：

* トークン埋め込みに似た学習可能な埋め込み。このアプローチをここで検討します。トークンとその位置の両方に埋め込み層を適用し、同じ次元の埋め込みベクトルを生成し、それらを加算します。
* 元の論文で提案された固定位置エンコーディング関数。

<img src="images/pos-embedding.png" width="50%"/>

> 著者による画像

位置埋め込みを使用することで、元のトークンとそのシーケンス内の位置の両方を埋め込む結果が得られます。

### マルチヘッド自己注意

次に、シーケンス内のパターンを捉える必要があります。これを行うために、トランスフォーマーは**自己注意**メカニズムを使用します。これは、入力と出力として同じシーケンスに適用される注意です。自己注意を適用することで、文内の**文脈**を考慮し、どの単語が相互に関連しているかを確認できます。例えば、*it*のような共参照が指す単語を確認し、文脈を考慮することができます：

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.ja.png)

> [Googleのブログ](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)から引用

トランスフォーマーでは、**マルチヘッド注意**を使用して、ネットワークが異なる種類の依存関係（例：長期的 vs 短期的な単語関係、共参照 vs その他）を捉える能力を与えます。

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)には、トランスフォーマーレイヤーの実装に関する詳細が含まれています。

### エンコーダ-デコーダ注意

トランスフォーマーでは、注意は以下の2つの場所で使用されます：

* 自己注意を使用して入力テキスト内のパターンを捉える
* シーケンス翻訳を実行する - これはエンコーダとデコーダ間の注意層です。

エンコーダ-デコーダ注意は、セクションの冒頭で説明したRNNで使用される注意メカニズムと非常に似ています。このアニメーション図は、エンコーダ-デコーダ注意の役割を説明しています。

![トランスフォーマーモデルで評価がどのように行われるかを示すアニメーションGIF](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

各入力位置が各出力位置に独立してマッピングされるため、トランスフォーマーはRNNよりも並列化が容易であり、より大規模で表現力豊かな言語モデルを可能にします。各注意ヘッドは、単語間の異なる関係を学習するために使用され、下流の自然言語処理タスクを改善します。

## BERT

**BERT**（Bidirectional Encoder Representations from Transformers）は、非常に大規模な多層トランスフォーマーネットワークであり、*BERT-base*では12層、*BERT-large*では24層を持ちます。このモデルは、まず大規模なテキストデータ（Wikipedia + 書籍）を使用して教師なし学習（文中のマスクされた単語を予測する）で事前学習されます。事前学習中にモデルは言語理解の重要なレベルを吸収し、その後他のデータセットで微調整することで活用できます。このプロセスは**転移学習**と呼ばれます。

![http://jalammar.github.io/illustrated-bert/からの画像](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.ja.png)

> [画像の出典](http://jalammar.github.io/illustrated-bert/)

## ✍️ 演習: トランスフォーマー

以下のノートブックで学習を続けてください：

* [PyTorchでのトランスフォーマー](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [TensorFlowでのトランスフォーマー](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## 結論

このレッスンでは、トランスフォーマーと注意メカニズムについて学びました。これらはNLPツールボックスの中で重要なツールです。トランスフォーマーアーキテクチャには、BERT、DistilBERT、BigBird、OpenGPT3など多くのバリエーションがあり、微調整が可能です。[HuggingFaceパッケージ](https://github.com/huggingface/)は、PyTorchとTensorFlowの両方でこれらのアーキテクチャをトレーニングするためのリポジトリを提供しています。

## 🚀 チャレンジ

## [事後講義クイズ](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## レビューと自己学習

* トランスフォーマーに関する古典的な論文[Attention is all you need](https://arxiv.org/abs/1706.03762)を説明する[ブログ記事](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/)
* トランスフォーマーのアーキテクチャを詳細に説明する[一連のブログ記事](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)

## [課題](assignment.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な表現が含まれる可能性があることをご承知おきください。元の言語で記載された文書が公式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。