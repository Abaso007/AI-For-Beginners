<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-24T21:04:19+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "ja"
}
-->
# 埋め込み

## [事前講義クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

BoWやTF/IDFに基づく分類器をトレーニングする際、高次元の`vocab_size`長のバッグオブワードベクトルを操作し、低次元の位置表現ベクトルからスパースなワンホット表現に明示的に変換していました。しかし、このワンホット表現はメモリ効率が良くありません。また、各単語は互いに独立して扱われ、ワンホットエンコードされたベクトルは単語間の意味的な類似性を表現しません。

**埋め込み**のアイデアは、単語を低次元の密なベクトルで表現し、単語の意味的な意味を何らかの形で反映させることです。後ほど意味のある単語埋め込みを構築する方法について議論しますが、今は埋め込みを単語ベクトルの次元を下げる方法として考えてみましょう。

埋め込み層は単語を入力として受け取り、指定された`embedding_size`の出力ベクトルを生成します。ある意味では、`Linear`層に非常に似ていますが、ワンホットエンコードされたベクトルを受け取る代わりに、単語番号を入力として受け取ることができ、大きなワンホットエンコードされたベクトルを作成する必要がなくなります。

分類器ネットワークの最初の層として埋め込み層を使用することで、バッグオブワードから**埋め込みバッグ**モデルに切り替えることができます。このモデルでは、テキスト内の各単語を対応する埋め込みに変換し、それらすべての埋め込みに対して`sum`、`average`、`max`などの集約関数を計算します。

![5つのシーケンス単語に対する埋め込み分類器を示す画像](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ja.png)

> 著者による画像

## ✍️ 演習: 埋め込み

以下のノートブックで学習を続けましょう:
* [PyTorchでの埋め込み](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlowでの埋め込み](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## 意味的埋め込み: Word2Vec

埋め込み層は単語をベクトル表現にマッピングすることを学習しましたが、この表現には必ずしも意味的な意味が含まれているわけではありません。類似した単語や同義語が、あるベクトル距離（例: ユークリッド距離）において互いに近いベクトルに対応するようなベクトル表現を学習できると良いでしょう。

これを実現するには、大量のテキストコーパスを特定の方法で事前トレーニングする必要があります。意味的埋め込みをトレーニングする1つの方法として[Word2Vec](https://en.wikipedia.org/wiki/Word2vec)があります。これは、単語の分散表現を生成するために使用される2つの主要なアーキテクチャに基づいています:

- **連続バッグオブワード** (CBoW) — このアーキテクチャでは、周囲の文脈から単語を予測するようにモデルをトレーニングします。ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$が与えられた場合、モデルの目標は$(W_{-2},W_{-1},W_1,W_2)$から$W_0$を予測することです。
- **連続スキップグラム** — CBoWとは逆で、モデルは周囲の文脈単語のウィンドウを使用して現在の単語を予測します。

CBoWは高速ですが、スキップグラムは遅いものの、頻度の低い単語をより良く表現します。

![単語をベクトルに変換するためのCBoWとスキップグラムアルゴリズムを示す画像](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ja.png)

> [この論文](https://arxiv.org/pdf/1301.3781.pdf)からの画像

Word2Vecの事前トレーニング済み埋め込み（およびGloVeのような他の類似モデル）は、ニューラルネットワークの埋め込み層の代わりとしても使用できます。ただし、語彙の問題に対処する必要があります。Word2Vec/GloVeを事前トレーニングする際に使用された語彙は、私たちのテキストコーパスの語彙と異なる可能性が高いからです。この問題を解決する方法については、上記のノートブックを参照してください。

## 文脈的埋め込み

Word2Vecのような従来の事前トレーニング済み埋め込み表現の主な制限の1つは、単語の意味の曖昧性の問題です。事前トレーニング済み埋め込みは、文脈における単語の意味をある程度捉えることができますが、単語のすべての可能な意味が同じ埋め込みにエンコードされます。これにより、例えば「play」のように文脈によって異なる意味を持つ単語では、下流のモデルで問題が発生する可能性があります。

例えば、「play」という単語は以下の2つの文で全く異なる意味を持ちます:

- 劇場で**play**を観ました。
- ジョンは友達と**play**したいと思っています。

上記の事前トレーニング済み埋め込みでは、「play」のこれら2つの意味が同じ埋め込みに表現されています。この制限を克服するには、大量のテキストコーパスでトレーニングされた**言語モデル**に基づいて埋め込みを構築する必要があります。このモデルは、異なる文脈で単語がどのように組み合わされるかを「理解」しています。文脈的埋め込みについての議論はこのチュートリアルの範囲外ですが、コースの後半で言語モデルについて話す際に再び取り上げます。

## 結論

このレッスンでは、TensorFlowとPyTorchを使用して埋め込み層を構築し、単語の意味的な意味をよりよく反映させる方法を学びました。

## 🚀 チャレンジ

Word2Vecは、歌詞や詩の生成など、興味深いアプリケーションに使用されています。[この記事](https://www.politetype.com/blog/word2vec-color-poems)では、著者がWord2Vecを使用して詩を生成する方法を説明しています。また、[Dan Shiffmannによるこの動画](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain)も視聴して、この技術の別の説明を発見してください。その後、Kaggleなどから取得した独自のテキストコーパスにこれらの技術を適用してみてください。

## [事後講義クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## 復習と自己学習

Word2Vecに関するこの論文を読んでみてください: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [課題: ノートブック](assignment.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を期すよう努めておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書が公式な情報源と見なされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤認について、当方は一切の責任を負いません。