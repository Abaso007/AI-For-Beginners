{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treniranje RL-a za balansiranje Cartpolea\n",
    "\n",
    "Ova bilježnica dio je [kurikuluma AI za početnike](http://aka.ms/ai-beginners). Inspirirana je [službenim PyTorch vodičem](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) i [ovom PyTorch implementacijom za Cartpole](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "U ovom primjeru koristit ćemo RL za treniranje modela koji balansira štap na kolicima koja se mogu kretati lijevo i desno na horizontalnoj skali. Koristit ćemo okruženje [OpenAI Gym](https://www.gymlibrary.ml/) za simulaciju štapa.\n",
    "\n",
    "> **Napomena**: Kod ove lekcije možete pokrenuti lokalno (npr. iz Visual Studio Code-a), u kojem će se slučaju simulacija otvoriti u novom prozoru. Ako kod pokrećete online, možda ćete trebati napraviti neke prilagodbe koda, kako je opisano [ovdje](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Započet ćemo tako da provjerimo je li Gym instaliran:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ćemo stvoriti CartPole okruženje i vidjeti kako njime upravljati. Okruženje ima sljedeće značajke:\n",
    "\n",
    "* **Prostor akcija** je skup mogućih akcija koje možemo izvršiti u svakom koraku simulacije\n",
    "* **Prostor opažanja** je prostor opažanja koje možemo napraviti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogledajmo kako simulacija funkcionira. Sljedeća petlja pokreće simulaciju dok `env.step` ne vrati zastavicu za završetak `done`. Akcije ćemo nasumično birati pomoću `env.action_space.sample()`, što znači da će eksperiment vjerojatno vrlo brzo propasti (okruženje CartPole završava kada brzina CartPole-a, njegova pozicija ili kut budu izvan određenih granica).\n",
    "\n",
    "> Simulacija će se otvoriti u novom prozoru. Kod možete pokrenuti nekoliko puta i vidjeti kako se ponaša.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Možete primijetiti da opažanja sadrže 4 broja. Oni su:\n",
    "- Položaj kolica\n",
    "- Brzina kolica\n",
    "- Kut stupa\n",
    "- Brzina rotacije stupa\n",
    "\n",
    "`rew` je nagrada koju primamo pri svakom koraku. Možete vidjeti da u CartPole okruženju dobivate 1 bod za svaki korak simulacije, a cilj je maksimizirati ukupnu nagradu, tj. vrijeme tijekom kojeg CartPole može održavati ravnotežu bez pada.\n",
    "\n",
    "Tijekom učenja pojačanjem, naš cilj je trenirati **politiku** $\\pi$, koja će za svako stanje $s$ odrediti koju akciju $a$ trebamo poduzeti, dakle u suštini $a = \\pi(s)$.\n",
    "\n",
    "Ako želite probabilističko rješenje, možete razmišljati o politici kao o vraćanju skupa vjerojatnosti za svaku akciju, tj. $\\pi(a|s)$ bi značilo vjerojatnost da trebamo poduzeti akciju $a$ u stanju $s$.\n",
    "\n",
    "## Metoda gradijenta politike\n",
    "\n",
    "U najjednostavnijem algoritmu za učenje pojačanjem, nazvanom **Gradijent politike**, trenirat ćemo neuronsku mrežu da predvidi sljedeću akciju.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirat ćemo mrežu izvođenjem mnogih eksperimenata i ažuriranjem naše mreže nakon svakog izvođenja. Definirajmo funkciju koja će izvršiti eksperiment i vratiti rezultate (tzv. **trag**) - sva stanja, akcije (i njihove preporučene vjerojatnosti) i nagrade:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Možete pokrenuti jednu epizodu s neuvježbanom mrežom i primijetiti da je ukupna nagrada (poznata i kao duljina epizode) vrlo niska:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedan od izazovnih aspekata algoritma gradijenta politike je korištenje **diskontiranih nagrada**. Ideja je da izračunamo vektor ukupnih nagrada u svakom koraku igre, a tijekom tog procesa diskontiramo rane nagrade koristeći neki koeficijent $gamma$. Također normaliziramo dobiveni vektor, jer ćemo ga koristiti kao težinu za utjecaj na naše treniranje:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada krenimo s pravim treningom! Izvest ćemo 300 epizoda, a u svakoj epizodi napravit ćemo sljedeće:\n",
    "\n",
    "1. Provesti eksperiment i prikupiti trag.\n",
    "2. Izračunati razliku (`gradients`) između poduzetih akcija i predviđenih vjerojatnosti. Što je razlika manja, to smo sigurniji da smo poduzeli ispravnu akciju.\n",
    "3. Izračunati diskontirane nagrade i pomnožiti gradijente s diskontiranim nagradama - to će osigurati da koraci s višim nagradama imaju veći utjecaj na konačni rezultat od onih s nižim nagradama.\n",
    "4. Očekivane ciljne akcije za našu neuronsku mrežu djelomično će se uzimati iz predviđenih vjerojatnosti tijekom izvođenja, a djelomično iz izračunatih gradijenata. Koristit ćemo parametar `alpha` kako bismo odredili u kojoj mjeri se uzimaju u obzir gradijenti i nagrade - to se naziva *stopa učenja* algoritma za pojačanje.\n",
    "5. Na kraju, treniramo našu mrežu na stanjima i očekivanim akcijama te ponavljamo proces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada pokrenimo epizodu s renderiranjem kako bismo vidjeli rezultat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nadamo se da sada možete vidjeti da se štap prilično dobro može balansirati!\n",
    "\n",
    "## Model Akter-Kritičar\n",
    "\n",
    "Model Akter-Kritičar je daljnji razvoj politika gradijenata, u kojem gradimo neuronsku mrežu kako bismo naučili i politiku i procijenjene nagrade. Mreža će imati dva izlaza (ili to možete gledati kao dvije odvojene mreže):\n",
    "* **Akter** će preporučiti akciju koju treba poduzeti dajući nam distribuciju vjerojatnosti stanja, kao u modelu politika gradijenata.\n",
    "* **Kritičar** bi procijenio kakva bi nagrada bila od tih akcija. Vraća ukupne procijenjene nagrade u budućnosti za dano stanje.\n",
    "\n",
    "Definirajmo takav model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morali bismo malo izmijeniti naše funkcije `discounted_rewards` i `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ćemo pokrenuti glavnu petlju za treniranje. Koristit ćemo ručni proces treniranja mreže izračunavanjem odgovarajućih funkcija gubitka i ažuriranjem parametara mreže:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ključne točke\n",
    "\n",
    "Vidjeli smo dva algoritma za pojačano učenje u ovom demo prikazu: jednostavni policy gradient i sofisticiraniji actor-critic. Možete primijetiti da ti algoritmi rade s apstraktnim pojmovima stanja, akcije i nagrade - zbog čega se mogu primijeniti na vrlo različite okruženja.\n",
    "\n",
    "Pojačano učenje omogućuje nam da naučimo najbolju strategiju za rješavanje problema samo promatranjem konačne nagrade. Činjenica da nam nisu potrebni označeni skupovi podataka omogućuje nam da više puta ponavljamo simulacije kako bismo optimizirali naše modele. Ipak, još uvijek postoje mnogi izazovi u pojačanom učenju, koje možete istražiti ako odlučite posvetiti više pažnje ovom zanimljivom području umjetne inteligencije.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Odricanje od odgovornosti**:  \nOvaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogrešne interpretacije koje proizlaze iz korištenja ovog prijevoda.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-30T07:11:55+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "hr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}