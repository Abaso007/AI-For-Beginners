{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ugrađivanja\n",
    "\n",
    "U našem prethodnom primjeru radili smo s vektorima vreće riječi visoke dimenzionalnosti duljine `vocab_size`, te smo eksplicitno pretvarali vektore niskodimenzionalne pozicijske reprezentacije u rijetku one-hot reprezentaciju. Ova one-hot reprezentacija nije memorijski učinkovita, a osim toga, svaka se riječ tretira neovisno o drugima, tj. one-hot kodirani vektori ne izražavaju nikakvu semantičku sličnost između riječi.\n",
    "\n",
    "U ovoj jedinici nastavit ćemo istraživati **News AG** skup podataka. Za početak, učitajmo podatke i preuzmimo neke definicije iz prethodne bilježnice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Što je ugradnja?\n",
    "\n",
    "Ideja **ugradnje** je predstavljati riječi pomoću nižedimenzionalnih gustih vektora, koji na neki način odražavaju semantičko značenje riječi. Kasnije ćemo raspraviti kako izgraditi smislene ugradnje riječi, ali za sada razmotrimo ugradnje kao način smanjenja dimenzionalnosti vektora riječi.\n",
    "\n",
    "Dakle, sloj za ugradnju uzima riječ kao ulaz i proizvodi izlazni vektor određene veličine `embedding_size`. Na neki način, to je vrlo slično sloju `Linear`, ali umjesto da koristi vektor kodiran kao one-hot, može uzeti broj riječi kao ulaz.\n",
    "\n",
    "Korištenjem sloja za ugradnju kao prvog sloja u našoj mreži, možemo prijeći s modela vreće riječi na model **vreće ugradnji**, gdje prvo pretvaramo svaku riječ u našem tekstu u odgovarajuću ugradnju, a zatim izračunavamo neku agregatnu funkciju nad svim tim ugradnjama, poput `sum`, `average` ili `max`.\n",
    "\n",
    "![Slika koja prikazuje klasifikator s ugradnjom za pet riječi u nizu.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.hr.png)\n",
    "\n",
    "Naša neuronska mreža za klasifikaciju započet će slojem za ugradnju, zatim slojem za agregaciju, i linearnim klasifikatorom na vrhu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rad s promjenjivom veličinom sekvenci varijabli\n",
    "\n",
    "Zbog ove arhitekture, minibatchovi za našu mrežu moraju se kreirati na određeni način. U prethodnoj jedinici, kada smo koristili vreću riječi (bag-of-words), svi BoW tenzori u minibatchu imali su jednaku veličinu `vocab_size`, bez obzira na stvarnu duljinu naše tekstualne sekvence. Kada prijeđemo na ugrađivanje riječi (word embeddings), završit ćemo s promjenjivim brojem riječi u svakom uzorku teksta, a prilikom kombiniranja tih uzoraka u minibatcheve morat ćemo primijeniti neko popunjavanje (padding).\n",
    "\n",
    "To se može učiniti korištenjem iste tehnike pružanja funkcije `collate_fn` izvoru podataka:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treniranje klasifikatora ugrađivanja\n",
    "\n",
    "Sada kada smo definirali odgovarajući dataloader, možemo trenirati model koristeći funkciju za treniranje koju smo definirali u prethodnoj jedinici:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Napomena**: Ovdje treniramo samo za 25 tisuća zapisa (manje od jednog punog epoha) radi uštede vremena, ali možete nastaviti s treniranjem, napisati funkciju za treniranje kroz nekoliko epoha i eksperimentirati s parametrom stope učenja kako biste postigli veću točnost. Trebali biste moći postići točnost od oko 90%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sloj EmbeddingBag i prikaz sekvenci promjenjive duljine\n",
    "\n",
    "U prethodnoj arhitekturi morali smo popuniti sve sekvence na istu duljinu kako bismo ih uklopili u minibatch. Ovo nije najučinkovitiji način za prikazivanje sekvenci promjenjive duljine - drugi pristup bio bi korištenje **offset** vektora, koji bi sadržavao pomake svih sekvenci pohranjenih u jednom velikom vektoru.\n",
    "\n",
    "![Slika koja prikazuje prikaz sekvenci pomoću offseta](../../../../../translated_images/offset-sequence-representation.eb73fcefb29b46eecfbe74466077cfeb7c0f93a4f254850538a2efbc63517479.hr.png)\n",
    "\n",
    "> **Napomena**: Na slici iznad prikazana je sekvenca znakova, ali u našem primjeru radimo sa sekvencama riječi. Međutim, osnovni princip prikazivanja sekvenci pomoću offset vektora ostaje isti.\n",
    "\n",
    "Za rad s prikazom pomoću offseta koristimo sloj [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html). Sličan je sloju `Embedding`, ali kao ulaz uzima sadržajni vektor i offset vektor, a također uključuje sloj za prosjek koji može biti `mean`, `sum` ili `max`.\n",
    "\n",
    "Evo izmijenjene mreže koja koristi `EmbeddingBag`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da bismo pripremili skup podataka za treniranje, moramo osigurati funkciju za konverziju koja će pripremiti vektor pomaka:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imajte na umu da, za razliku od svih prethodnih primjera, naša mreža sada prihvaća dva parametra: vektor podataka i vektor pomaka, koji su različitih veličina. Slično tome, naš učitavač podataka također nam pruža 3 vrijednosti umjesto 2: i tekstualni i vektori pomaka pružaju se kao značajke. Stoga trebamo malo prilagoditi našu funkciju treniranja kako bismo to uzeli u obzir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantičke Ugradnje: Word2Vec\n",
    "\n",
    "U našem prethodnom primjeru, sloj za ugradnju modela naučio je mapirati riječi u vektorsku reprezentaciju, no ta reprezentacija nije imala puno semantičkog značenja. Bilo bi korisno naučiti takvu vektorsku reprezentaciju gdje bi slične riječi ili sinonimi odgovarali vektorima koji su blizu jedni drugima prema nekoj metričkoj udaljenosti (npr. euklidskoj udaljenosti).\n",
    "\n",
    "Da bismo to postigli, potrebno je unaprijed obučiti naš model za ugradnju na velikoj zbirci teksta na specifičan način. Jedan od prvih pristupa za obuku semantičkih ugradnji naziva se [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Temelji se na dvije glavne arhitekture koje se koriste za stvaranje distribuirane reprezentacije riječi:\n",
    "\n",
    " - **Kontinuirana vreća riječi** (CBoW) — u ovoj arhitekturi model se obučava da predvidi riječ iz okolnog konteksta. Za zadani ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, cilj modela je predvidjeti $W_0$ na temelju $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Kontinuirani skip-gram** je suprotan CBoW-u. Model koristi okolni prozor kontekstualnih riječi kako bi predvidio trenutnu riječ.\n",
    "\n",
    "CBoW je brži, dok je skip-gram sporiji, ali bolje predstavlja riječi koje se rjeđe pojavljuju.\n",
    "\n",
    "![Slika koja prikazuje algoritme CBoW i Skip-Gram za pretvaranje riječi u vektore.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.hr.png)\n",
    "\n",
    "Za eksperimentiranje s Word2Vec ugradnjom unaprijed obučenom na Google News skupu podataka, možemo koristiti biblioteku **gensim**. Ispod nalazimo riječi najsličnije riječi 'neural':\n",
    "\n",
    "> **Napomena:** Kada prvi put kreirate vektore riječi, njihovo preuzimanje može potrajati!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Također možemo izračunati vektorske ugradnje iz riječi, koje će se koristiti za treniranje modela klasifikacije (prikazujemo samo prvih 20 komponenti vektora radi jasnoće):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sjajna stvar kod semantičkih ugradnji je da možete manipulirati vektorskim kodiranjem kako biste promijenili semantiku. Na primjer, možemo tražiti riječ čija bi vektorska reprezentacija bila što bliža riječima *kralj* i *žena*, a što udaljenija od riječi *muškarac*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oba, CBoW i Skip-Grams, su \"prediktivni\" ugrađeni modeli, jer uzimaju u obzir samo lokalne kontekste. Word2Vec ne koristi globalni kontekst.\n",
    "\n",
    "**FastText** se nadovezuje na Word2Vec tako što uči vektorske reprezentacije za svaku riječ i znakove n-grama pronađene unutar svake riječi. Vrijednosti tih reprezentacija se zatim prosječno izračunavaju u jedan vektor pri svakom koraku treniranja. Iako ovo dodaje puno dodatnih izračuna tijekom pred-treniranja, omogućuje ugrađenim modelima riječi da kodiraju informacije o podriječima.\n",
    "\n",
    "Druga metoda, **GloVe**, koristi ideju matrice su-pojavljivanja i primjenjuje neuronske metode za dekompoziciju matrice su-pojavljivanja u izraženije i nelinearne vektore riječi.\n",
    "\n",
    "Možete se poigrati s primjerom mijenjajući ugrađene modele na FastText i GloVe, budući da gensim podržava nekoliko različitih modela za ugrađivanje riječi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korištenje unaprijed istreniranih ugradnji u PyTorchu\n",
    "\n",
    "Možemo prilagoditi gornji primjer kako bismo unaprijed popunili matricu u našem sloju za ugradnju semantičkim ugradnjama, poput Word2Vec-a. Trebamo uzeti u obzir da se rječnici unaprijed istreniranih ugradnji i našeg tekstualnog korpusa vjerojatno neće podudarati, pa ćemo inicijalizirati težine za nedostajuće riječi nasumičnim vrijednostima:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada trenirajmo naš model. Imajte na umu da je vrijeme potrebno za treniranje modela znatno duže nego u prethodnom primjeru, zbog veće veličine sloja za ugradnju, a time i znatno većeg broja parametara. Također, zbog toga ćemo možda morati trenirati naš model na više primjera ako želimo izbjeći prenaučenost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U našem slučaju ne vidimo značajan porast točnosti, što je vjerojatno zbog prilično različitih vokabulara.  \n",
    "Kako bismo prevladali problem različitih vokabulara, možemo koristiti jedno od sljedećih rješenja:  \n",
    "* Ponovno trenirati word2vec model na našem vokabularu  \n",
    "* Učitati naš skup podataka s vokabularom iz unaprijed treniranog word2vec modela. Vokabular koji se koristi za učitavanje skupa podataka može se specificirati tijekom učitavanja.  \n",
    "\n",
    "Drugi pristup čini se jednostavnijim, posebno zato što PyTorch `torchtext` okvir sadrži ugrađenu podršku za ugrađivanja. Na primjer, možemo instancirati vokabular temeljen na GloVe-u na sljedeći način:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Učitani rječnik ima sljedeće osnovne operacije:\n",
    "* `vocab.stoi` rječnik omogućuje pretvaranje riječi u njezin indeks u rječniku\n",
    "* `vocab.itos` radi suprotno - pretvara broj u riječ\n",
    "* `vocab.vectors` je niz vektora ugrađivanja, pa da bismo dobili ugrađivanje riječi `s`, trebamo koristiti `vocab.vectors[vocab.stoi[s]]`\n",
    "\n",
    "Evo primjera manipulacije ugrađivanjima kako bismo demonstrirali jednadžbu **kind-man+woman = queen** (morao sam malo prilagoditi koeficijent da bi radilo):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da bismo obučili klasifikator koristeći te ugrađene vektore, prvo moramo kodirati naš skup podataka koristeći GloVe vokabular:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kao što smo vidjeli gore, svi vektorski ugrađaji pohranjeni su u matrici `vocab.vectors`. To čini izuzetno jednostavnim učitavanje tih težina u težine sloja za ugrađivanje pomoću jednostavnog kopiranja:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada obučimo naš model i provjerimo hoćemo li dobiti bolje rezultate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedan od razloga zašto ne vidimo značajno povećanje točnosti je činjenica da neke riječi iz našeg skupa podataka nedostaju u unaprijed uvježbanom GloVe rječniku, te se stoga u biti ignoriraju. Kako bismo prevladali ovu činjenicu, možemo uvježbati vlastite ugradnje na našem skupu podataka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontekstualni Ugrađeni Vektori\n",
    "\n",
    "Jedno od ključnih ograničenja tradicionalnih unaprijed istreniranih reprezentacija ugrađenih vektora poput Word2Vec-a je problem razlučivanja značenja riječi. Iako unaprijed istrenirani ugrađeni vektori mogu uhvatiti dio značenja riječi u kontekstu, svako moguće značenje riječi kodirano je u isti vektor. To može uzrokovati probleme u modelima koji dolaze nakon, budući da mnoge riječi, poput riječi 'play', imaju različita značenja ovisno o kontekstu u kojem se koriste.\n",
    "\n",
    "Na primjer, riječ 'play' u ove dvije rečenice ima prilično različita značenja:\n",
    "- Otišao sam na **predstavu** u kazalištu.\n",
    "- John želi **igrati** se s prijateljima.\n",
    "\n",
    "Unaprijed istrenirani ugrađeni vektori gore predstavljaju oba ova značenja riječi 'play' u istom vektoru. Kako bismo prevladali ovo ograničenje, trebamo izgraditi ugrađene vektore temeljene na **jezičnom modelu**, koji je istreniran na velikom korpusu teksta i *zna* kako se riječi mogu kombinirati u različitim kontekstima. Rasprava o kontekstualnim ugrađenim vektorima izlazi izvan okvira ovog vodiča, ali ćemo im se vratiti kada budemo govorili o jezičnim modelima u sljedećoj jedinici.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Odricanje od odgovornosti**:  \nOvaj dokument je preveden korištenjem AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane stručnjaka. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogrešne interpretacije proizašle iz korištenja ovog prijevoda.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-30T08:15:10+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "hr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}