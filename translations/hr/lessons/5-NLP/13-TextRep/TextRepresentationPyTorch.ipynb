{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadatak klasifikacije teksta\n",
    "\n",
    "Kao što smo spomenuli, fokusirat ćemo se na jednostavan zadatak klasifikacije teksta temeljen na **AG_NEWS** skupu podataka, gdje je cilj klasificirati naslove vijesti u jednu od 4 kategorije: Svijet, Sport, Poslovanje i Znanost/Tehnologija.\n",
    "\n",
    "## Skup podataka\n",
    "\n",
    "Ovaj skup podataka ugrađen je u modul [`torchtext`](https://github.com/pytorch/text), tako da mu možemo lako pristupiti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovdje, `train_dataset` i `test_dataset` sadrže kolekcije koje vraćaju parove oznake (broj klase) i teksta, na primjer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dakle, ispisat ćemo prvih 10 novih naslova iz našeg skupa podataka:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budući da su skupovi podataka iteratori, ako želimo koristiti podatke više puta, moramo ih pretvoriti u popis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacija\n",
    "\n",
    "Sada trebamo pretvoriti tekst u **brojeve** koji se mogu predstaviti kao tenzori. Ako želimo reprezentaciju na razini riječi, trebamo učiniti dvije stvari:\n",
    "* koristiti **tokenizator** za razdvajanje teksta na **tokene**\n",
    "* izgraditi **vokabular** tih tokena.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korištenjem rječnika, možemo lako kodirati naš tokenizirani niz u skup brojeva:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predstavljanje teksta pomoću vreće riječi\n",
    "\n",
    "Budući da riječi nose značenje, ponekad možemo shvatiti značenje teksta samo promatrajući pojedinačne riječi, bez obzira na njihov redoslijed u rečenici. Na primjer, prilikom klasifikacije vijesti, riječi poput *vrijeme*, *snijeg* vjerojatno ukazuju na *vremensku prognozu*, dok bi riječi poput *dionice*, *dolar* upućivale na *financijske vijesti*.\n",
    "\n",
    "**Vreća riječi** (BoW) vektorsko predstavljanje najčešće je korišteno tradicionalno vektorsko predstavljanje. Svaka riječ povezana je s indeksom vektora, a element vektora sadrži broj pojavljivanja riječi u određenom dokumentu.\n",
    "\n",
    "![Slika koja prikazuje kako je vektorsko predstavljanje vreće riječi prikazano u memoriji.](../../../../../translated_images/bag-of-words-example.606fc1738f1d7ba98a9d693e3bcd706c6e83fa7bf8221e6e90d1a206d82f2ea4.hr.png) \n",
    "\n",
    "> **Napomena**: Možete također razmišljati o BoW kao o zbroju svih one-hot kodiranih vektora za pojedinačne riječi u tekstu.\n",
    "\n",
    "Ispod je primjer kako generirati predstavljanje vreće riječi koristeći Scikit Learn python biblioteku:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za izračunavanje vektora vreće riječi iz vektorske reprezentacije našeg AG_NEWS skupa podataka, možemo koristiti sljedeću funkciju:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Napomena:** Ovdje koristimo globalnu varijablu `vocab_size` za određivanje zadane veličine rječnika. Budući da je veličina rječnika često prilično velika, možemo ograničiti veličinu rječnika na najčešće riječi. Pokušajte smanjiti vrijednost `vocab_size` i pokrenuti donji kod te vidjeti kako to utječe na točnost. Možete očekivati određeni pad točnosti, ali ne dramatičan, u zamjenu za bolje performanse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treniranje BoW klasifikatora\n",
    "\n",
    "Sada kada smo naučili kako izgraditi Bag-of-Words reprezentaciju našeg teksta, trenirajmo klasifikator na temelju nje. Prvo, trebamo prilagoditi naš skup podataka za treniranje na način da se sve pozicijske vektorske reprezentacije pretvore u Bag-of-Words reprezentaciju. To se može postići prosljeđivanjem funkcije `bowify` kao parametra `collate_fn` standardnom torch `DataLoader`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada definirajmo jednostavnu neuronsku mrežu klasifikatora koja sadrži jedan linearni sloj. Veličina ulaznog vektora jednaka je `vocab_size`, a izlazna veličina odgovara broju klasa (4). Budući da rješavamo zadatak klasifikacije, konačna aktivacijska funkcija je `LogSoftmax()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ćemo definirati standardnu PyTorch petlju za treniranje. Budući da je naš skup podataka prilično velik, za potrebe podučavanja trenirat ćemo samo jedan epoch, a ponekad čak i manje od jednog epocha (navođenjem parametra `epoch_size` možemo ograničiti treniranje). Također ćemo izvještavati o akumuliranoj točnosti treniranja tijekom treniranja; učestalost izvještavanja određuje se pomoću parametra `report_freq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiGrami, TriGrami i N-Grami\n",
    "\n",
    "Jedno ograničenje pristupa vreće riječi je da su neke riječi dio višerječnih izraza, na primjer, riječ 'hot dog' ima potpuno drugačije značenje od riječi 'hot' i 'dog' u drugim kontekstima. Ako uvijek predstavljamo riječi 'hot' i 'dog' istim vektorima, to može zbuniti naš model.\n",
    "\n",
    "Kako bismo to riješili, **N-gram reprezentacije** često se koriste u metodama klasifikacije dokumenata, gdje je učestalost svake riječi, dvostruke riječi ili trostruke riječi korisna značajka za treniranje klasifikatora. U bigram reprezentaciji, na primjer, dodajemo sve parove riječi u vokabular, uz originalne riječi.\n",
    "\n",
    "Ispod je primjer kako generirati bigram reprezentaciju vreće riječi koristeći Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glavni nedostatak N-gram pristupa je što veličina rječnika počinje rasti izuzetno brzo. U praksi, potrebno je kombinirati N-gram reprezentaciju s nekim tehnikama smanjenja dimenzionalnosti, poput *ugrađivanja* (embeddings), o čemu ćemo raspravljati u sljedećoj jedinici.\n",
    "\n",
    "Kako bismo koristili N-gram reprezentaciju u našem **AG News** skupu podataka, trebamo izgraditi poseban ngram rječnik:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mogli bismo koristiti isti kod kao gore za treniranje klasifikatora, no to bi bilo vrlo neučinkovito u pogledu memorije. U sljedećoj jedinici, trenirat ćemo bigram klasifikator koristeći ugradnje.\n",
    "\n",
    "> **Napomena:** Možete zadržati samo one n-grame koji se pojavljuju u tekstu više od određenog broja puta. Ovo će osigurati da se rijetki bigrami izostave i značajno smanjiti dimenzionalnost. Da biste to postigli, postavite parametar `min_freq` na višu vrijednost i promatrajte kako se duljina rječnika mijenja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frekvencija termina i inverzna frekvencija dokumenata (TF-IDF)\n",
    "\n",
    "U BoW (Bag of Words) reprezentaciji, pojavljivanja riječi imaju jednaku težinu, bez obzira na samu riječ. Međutim, jasno je da su učestale riječi, poput *a*, *u*, itd., mnogo manje važne za klasifikaciju od specijaliziranih pojmova. Zapravo, u većini NLP zadataka neke su riječi relevantnije od drugih.\n",
    "\n",
    "**TF-IDF** označava **frekvenciju termina – inverznu frekvenciju dokumenata**. To je varijacija metode vreće riječi, gdje se umjesto binarne vrijednosti 0/1, koja označava pojavljivanje riječi u dokumentu, koristi vrijednost s pomičnim zarezom, koja je povezana s učestalošću pojavljivanja riječi u korpusu.\n",
    "\n",
    "Formalnije, težina $w_{ij}$ riječi $i$ u dokumentu $j$ definirana je kao:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "gdje su:\n",
    "* $tf_{ij}$ broj pojavljivanja riječi $i$ u dokumentu $j$, tj. BoW vrijednost koju smo ranije vidjeli\n",
    "* $N$ broj dokumenata u zbirci\n",
    "* $df_i$ broj dokumenata koji sadrže riječ $i$ u cijeloj zbirci\n",
    "\n",
    "TF-IDF vrijednost $w_{ij}$ raste proporcionalno broju pojavljivanja riječi u dokumentu, ali se smanjuje u odnosu na broj dokumenata u korpusu koji sadrže tu riječ, što pomaže u prilagodbi činjenici da se neke riječi pojavljuju češće od drugih. Na primjer, ako se riječ pojavljuje u *svakom* dokumentu u zbirci, tada je $df_i=N$, i $w_{ij}=0$, te bi ti termini bili potpuno zanemareni.\n",
    "\n",
    "TF-IDF vektorizaciju teksta možete jednostavno kreirati pomoću Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaključak\n",
    "\n",
    "Iako TF-IDF reprezentacije dodjeljuju težinu učestalosti različitim riječima, one nisu sposobne prikazati značenje ili redoslijed. Kao što je poznati lingvist J. R. Firth rekao 1935. godine: \"Potpuno značenje riječi uvijek je kontekstualno, i nijedno proučavanje značenja izvan konteksta ne može se smatrati ozbiljnim.\" Kasnije u ovom tečaju naučit ćemo kako izvući kontekstualne informacije iz teksta koristeći jezično modeliranje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Odricanje od odgovornosti**:  \nOvaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane stručnjaka. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogrešne interpretacije proizašle iz korištenja ovog prijevoda.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-30T08:17:08+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "hr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}