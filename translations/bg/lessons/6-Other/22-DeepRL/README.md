<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T14:25:45+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "bg"
}
-->
# Дълбоко Укрепващо Обучение

Укрепващото обучение (RL) се счита за един от основните парадигми на машинното обучение, наред с контролираното и неконтролираното обучение. Докато при контролираното обучение разчитаме на набор от данни с известни резултати, RL се основава на **учене чрез действие**. Например, когато за първи път видим компютърна игра, започваме да играем, дори без да знаем правилата, и скоро успяваме да подобрим уменията си просто чрез процеса на игра и адаптиране на поведението си.

## [Тест преди лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/43)

За да изпълним RL, ни трябват:

* **Среда** или **симулатор**, който задава правилата на играта. Трябва да можем да провеждаме експерименти в симулатора и да наблюдаваме резултатите.
* Някаква **функция за награда**, която показва колко успешен е бил експериментът ни. В случая на учене да играем компютърна игра, наградата би била нашият краен резултат.

Въз основа на функцията за награда трябва да можем да адаптираме поведението си и да подобрим уменията си, така че следващия път да играем по-добре. Основната разлика между другите видове машинно обучение и RL е, че при RL обикновено не знаем дали печелим или губим, докато не завършим играта. Следователно, не можем да кажем дали даден ход сам по себе си е добър или не - получаваме награда само в края на играта.

По време на RL обикновено провеждаме много експерименти. По време на всеки експеримент трябва да балансираме между следването на оптималната стратегия, която сме научили досега (**експлоатация**), и изследването на нови възможни състояния (**изследване**).

## OpenAI Gym

Отличен инструмент за RL е [OpenAI Gym](https://gym.openai.com/) - **среда за симулация**, която може да симулира много различни среди, започвайки от игри на Atari до физиката зад балансирането на прът. Това е една от най-популярните среди за симулация за обучение на алгоритми за укрепващо обучение и се поддържа от [OpenAI](https://openai.com/).

> **Note**: Можете да видите всички налични среди от OpenAI Gym [тук](https://gym.openai.com/envs/#classic_control).

## Балансиране на CartPole

Вероятно всички сте виждали модерни устройства за балансиране като *Segway* или *Gyroscooters*. Те могат автоматично да се балансират, като регулират колелата си в отговор на сигнал от акселерометър или жироскоп. В този раздел ще научим как да решим подобен проблем - балансиране на прът. Това е подобно на ситуация, когато цирков артист трябва да балансира прът на ръката си - но това балансиране на прът се случва само в 1D.

Опростена версия на балансирането е известна като **CartPole** проблем. В света на CartPole имаме хоризонтален плъзгач, който може да се движи наляво или надясно, а целта е да балансираме вертикален прът върху плъзгача, докато той се движи.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

За да създадем и използваме тази среда, ни трябват няколко реда Python код:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Всяка среда може да бъде достъпна по точно същия начин:
* `env.reset` започва нов експеримент
* `env.step` изпълнява стъпка на симулация. Получава **действие** от **пространството на действията** и връща **наблюдение** (от пространството на наблюденията), както и награда и флаг за прекратяване.

В горния пример изпълняваме случайно действие на всяка стъпка, поради което животът на експеримента е много кратък:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Целта на RL алгоритъм е да обучи модел - така наречената **политика** &pi; - която ще връща действие в отговор на дадено състояние. Можем също да разглеждаме политиката като вероятностна, например за всяко състояние *s* и действие *a* тя ще връща вероятността &pi;(*a*|*s*), че трябва да предприемем *a* в състояние *s*.

## Алгоритъм за Градиенти на Политиката

Най-очевидният начин за моделиране на политика е чрез създаване на невронна мрежа, която ще приема състояния като вход и ще връща съответните действия (или по-скоро вероятностите за всички действия). В известен смисъл, това би било подобно на нормална задача за класификация, с основна разлика - не знаем предварително кои действия трябва да предприемем на всяка от стъпките.

Идеята тук е да се оценят тези вероятности. Създаваме вектор от **кумулативни награди**, който показва общата ни награда на всяка стъпка от експеримента. Прилагаме също **дисконтиране на наградите**, като умножаваме по-ранните награди с някакъв коефициент &gamma;=0.99, за да намалим ролята на по-ранните награди. След това подсилваме онези стъпки по пътя на експеримента, които водят до по-големи награди.

> Научете повече за алгоритъма за градиенти на политиката и го вижте в действие в [примерния тетрадка](CartPole-RL-TF.ipynb).

## Алгоритъм Actor-Critic

Подобрена версия на подхода за градиенти на политиката се нарича **Actor-Critic**. Основната идея зад него е, че невронната мрежа ще бъде обучена да връща две неща:

* Политиката, която определя кое действие да предприемем. Тази част се нарича **actor**.
* Оценката на общата награда, която можем да очакваме да получим в това състояние - тази част се нарича **critic**.

В известен смисъл, тази архитектура наподобява [GAN](../../4-ComputerVision/10-GANs/README.md), където имаме две мрежи, които се обучават една срещу друга. В модела actor-critic, актьорът предлага действието, което трябва да предприемем, а критикът се опитва да бъде критичен и да оцени резултата. Въпреки това, целта ни е да обучим тези мрежи в унисон.

Тъй като знаем както реалните кумулативни награди, така и резултатите, върнати от критика по време на експеримента, е сравнително лесно да изградим функция за загуба, която ще минимизира разликата между тях. Това би ни дало **загуба на критика**. Можем да изчислим **загуба на актьора**, използвайки същия подход като в алгоритъма за градиенти на политиката.

След изпълнение на един от тези алгоритми можем да очакваме нашият CartPole да се държи така:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Упражнения: Градиенти на Политиката и Actor-Critic RL

Продължете обучението си в следните тетрадки:

* [RL в TensorFlow](CartPole-RL-TF.ipynb)
* [RL в PyTorch](CartPole-RL-PyTorch.ipynb)

## Други RL задачи

Укрепващото обучение днес е бързо развиваща се област на изследване. Някои от интересните примери за укрепващо обучение са:

* Обучение на компютър да играе **игри на Atari**. Предизвикателната част в този проблем е, че нямаме просто състояние, представено като вектор, а по-скоро екранна снимка - и трябва да използваме CNN, за да конвертираме този екранен образ в вектор от характеристики или да извлечем информация за наградата. Игри на Atari са налични в Gym.
* Обучение на компютър да играе настолни игри, като шах и Go. Наскоро програми като **Alpha Zero** бяха обучени от нулата чрез двама агенти, които играят един срещу друг и се подобряват на всяка стъпка.
* В индустрията RL се използва за създаване на системи за управление от симулация. Услуга, наречена [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste), е специално създадена за това.

## Заключение

Сега научихме как да обучаваме агенти да постигат добри резултати, просто като им предоставим функция за награда, която определя желаното състояние на играта, и като им дадем възможност интелигентно да изследват пространството за търсене. Успешно опитахме два алгоритъма и постигнахме добър резултат за сравнително кратък период от време. Въпреки това, това е само началото на вашето пътешествие в RL, и определено трябва да обмислите вземането на отделен курс, ако искате да се задълбочите.

## 🚀 Предизвикателство

Разгледайте приложенията, изброени в раздела "Други RL задачи", и опитайте да реализирате едно от тях!

## [Тест след лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Преглед и Самостоятелно Обучение

Научете повече за класическото укрепващо обучение в нашата [Учебна програма за машинно обучение за начинаещи](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Гледайте [това страхотно видео](https://www.youtube.com/watch?v=qv6UVOQ0F44), което разказва как компютър може да се научи да играе Super Mario.

## Задание: [Обучете Mountain Car](lab/README.md)

Вашата цел по време на това задание ще бъде да обучите различна среда от Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

