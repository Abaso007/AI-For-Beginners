<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-25T23:31:48+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "bg"
}
-->
# Дълбоко обучение чрез подсилване

Обучението чрез подсилване (Reinforcement Learning, RL) се счита за един от основните парадигми в машинното обучение, наред с обучението с учител и без учител. Докато при обучението с учител разчитаме на набор от данни с известни резултати, RL се основава на **учене чрез правене**. Например, когато за първи път видим компютърна игра, започваме да играем, дори без да знаем правилата, и скоро успяваме да подобрим уменията си просто чрез процеса на игра и адаптиране на поведението си.

## [Тест преди лекцията](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

За да изпълним RL, ни трябват:

* **Среда** или **симулатор**, който задава правилата на играта. Трябва да можем да провеждаме експерименти в симулатора и да наблюдаваме резултатите.
* **Функция за награда**, която показва колко успешен е бил нашият експеримент. В случая на обучение за игра на компютърна игра, наградата би била нашият краен резултат.

Въз основа на функцията за награда трябва да можем да адаптираме поведението си и да подобрим уменията си, така че следващия път да играем по-добре. Основната разлика между другите видове машинно обучение и RL е, че при RL обикновено не знаем дали печелим или губим, докато не завършим играта. Следователно, не можем да кажем дали даден ход сам по себе си е добър или не - получаваме награда едва в края на играта.

По време на RL обикновено провеждаме много експерименти. По време на всеки експеримент трябва да балансираме между следването на оптималната стратегия, която сме научили досега (**експлоатация**), и изследването на нови възможни състояния (**изследване**).

## OpenAI Gym

Отличен инструмент за RL е [OpenAI Gym](https://gym.openai.com/) - **среда за симулация**, която може да симулира много различни среди, започвайки от игри на Atari до физиката зад балансирането на прът. Това е една от най-популярните среди за симулация за обучение на алгоритми за подсилване и се поддържа от [OpenAI](https://openai.com/).

> **Note**: Можете да видите всички налични среди от OpenAI Gym [тук](https://gym.openai.com/envs/#classic_control).

## Балансиране на CartPole

Вероятно всички сте виждали модерни устройства за балансиране като *Segway* или *Гироскутери*. Те могат автоматично да се балансират, като регулират колелата си в отговор на сигнал от акселерометър или жироскоп. В тази секция ще научим как да решим подобен проблем - балансиране на прът. Това е подобно на ситуация, в която цирков артист трябва да балансира прът на ръката си - но това балансиране на прът се случва само в 1D.

Опростена версия на балансирането е известна като проблема **CartPole**. В света на CartPole имаме хоризонтален плъзгач, който може да се движи наляво или надясно, а целта е да се балансира вертикален прът върху плъзгача, докато той се движи.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

За да създадем и използваме тази среда, са ни нужни няколко реда Python код:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Всяка среда може да бъде достъпна по един и същ начин:
* `env.reset` стартира нов експеримент
* `env.step` изпълнява стъпка на симулация. Тя получава **действие** от **пространството на действията** и връща **наблюдение** (от пространството на наблюденията), както и награда и флаг за прекратяване.

В горния пример изпълняваме случайно действие на всяка стъпка, поради което животът на експеримента е много кратък:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Целта на RL алгоритъма е да обучи модел - така наречената **политика** π - която ще връща действие в отговор на дадено състояние. Можем също да разглеждаме политиката като вероятностна, например за всяко състояние *s* и действие *a* тя ще връща вероятността π(*a*|*s*), че трябва да предприемем *a* в състояние *s*.

## Алгоритъм за градиенти на политиката

Най-очевидният начин за моделиране на политика е чрез създаване на невронна мрежа, която ще приема състояния като вход и ще връща съответните действия (или по-скоро вероятностите за всички действия). В известен смисъл това би било подобно на нормална задача за класификация, с основна разлика - не знаем предварително кои действия трябва да предприемем на всяка от стъпките.

Идеята тук е да оценим тези вероятности. Създаваме вектор на **кумулативни награди**, който показва общата ни награда на всяка стъпка от експеримента. Също така прилагаме **дисконтиране на наградите**, като умножаваме по-ранните награди с коефициент γ=0.99, за да намалим ролята на по-ранните награди. След това подсилваме онези стъпки по пътя на експеримента, които водят до по-големи награди.

> Научете повече за алгоритъма за градиенти на политиката и го вижте в действие в [примерния ноутбук](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Алгоритъм Actor-Critic

Подобрена версия на подхода с градиенти на политиката се нарича **Actor-Critic**. Основната идея зад него е, че невронната мрежа ще бъде обучена да връща две неща:

* Политиката, която определя кое действие да предприемем. Тази част се нарича **actor**.
* Оценка на общата награда, която можем да очакваме да получим в това състояние - тази част се нарича **critic**.

В известен смисъл тази архитектура наподобява [GAN](../../4-ComputerVision/10-GANs/README.md), където имаме две мрежи, които се обучават една срещу друга. В модела actor-critic, актьорът предлага действието, което трябва да предприемем, а критикът се опитва да бъде критичен и да оцени резултата. Въпреки това, нашата цел е да обучим тези мрежи в унисон.

Тъй като знаем както реалните кумулативни награди, така и резултатите, върнати от критика по време на експеримента, е сравнително лесно да изградим функция за загуба, която ще минимизира разликата между тях. Това би ни дало **загуба на критика**. Можем да изчислим **загубата на актьора**, използвайки същия подход като в алгоритъма за градиенти на политиката.

След изпълнението на един от тези алгоритми можем да очакваме нашият CartPole да се държи така:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Упражнения: Градиенти на политиката и Actor-Critic RL

Продължете обучението си в следните ноутбуци:

* [RL в TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL в PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Други задачи за RL

Обучението чрез подсилване днес е бързо развиваща се област на изследване. Някои от интересните примери за обучение чрез подсилване са:

* Обучение на компютър да играе **игри на Atari**. Предизвикателната част в този проблем е, че нямаме просто състояние, представено като вектор, а по-скоро екранна снимка - и трябва да използваме CNN, за да преобразуваме това изображение вектор на характеристики или да извлечем информация за наградата. Игри на Atari са налични в Gym.
* Обучение на компютър да играе настолни игри, като шах и Го. Наскоро програми като **Alpha Zero** постигнаха върхови резултати, като се обучаваха от нулата чрез два агента, които играят един срещу друг и се подобряват на всяка стъпка.
* В индустрията RL се използва за създаване на системи за управление от симулация. Услуга като [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) е специално създадена за това.

## Заключение

Сега научихме как да обучаваме агенти да постигат добри резултати, като просто им предоставяме функция за награда, която определя желаното състояние на играта, и им даваме възможност интелигентно да изследват пространството за търсене. Успешно изпробвахме два алгоритъма и постигнахме добър резултат за сравнително кратък период от време. Въпреки това, това е само началото на вашето пътешествие в RL, и определено трябва да обмислите да вземете отделен курс, ако искате да се задълбочите.

## 🚀 Предизвикателство

Разгледайте приложенията, изброени в секцията „Други задачи за RL“, и опитайте да имплементирате едно от тях!

## [Тест след лекцията](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Преглед и самостоятелно обучение

Научете повече за класическото обучение чрез подсилване в нашата [Учебна програма за машинно обучение за начинаещи](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Гледайте [това страхотно видео](https://www.youtube.com/watch?v=qv6UVOQ0F44), което разказва как компютър може да се научи да играе Super Mario.

## Задание: [Обучение на Mountain Car](lab/README.md)

Вашата цел по време на това задание ще бъде да обучите различна среда от Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.