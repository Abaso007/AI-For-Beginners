{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение на RL за балансиране на Cartpole\n",
    "\n",
    "Този ноутбук е част от [Учебната програма за начинаещи в ИИ](http://aka.ms/ai-beginners). Той е вдъхновен от [официалния урок на PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) и [тази имплементация на Cartpole с PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "В този пример ще използваме RL, за да обучим модел да балансира прът върху количка, която може да се движи наляво и надясно по хоризонтална ос. Ще използваме средата [OpenAI Gym](https://www.gymlibrary.ml/), за да симулираме пръта.\n",
    "\n",
    "> **Забележка**: Можете да изпълните кода от този урок локално (например чрез Visual Studio Code), като в този случай симулацията ще се отвори в нов прозорец. Ако изпълнявате кода онлайн, може да се наложи да направите някои промени в кода, както е описано [тук](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Ще започнем, като се уверим, че Gym е инсталиран:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега нека създадем средата CartPole и да видим как да работим с нея. Една среда има следните свойства:\n",
    "\n",
    "* **Action space** е наборът от възможни действия, които можем да изпълним на всяка стъпка от симулацията\n",
    "* **Observation space** е пространството на наблюденията, които можем да направим\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека видим как работи симулацията. Следният цикъл изпълнява симулацията, докато `env.step` не върне флага за прекратяване `done`. Ще избираме действия на случаен принцип, използвайки `env.action_space.sample()`, което означава, че експериментът вероятно ще се провали много бързо (средата CartPole приключва, когато скоростта на CartPole, неговата позиция или ъгъл излязат извън определени граници).\n",
    "\n",
    "> Симулацията ще се отвори в нов прозорец. Можете да изпълните кода няколко пъти и да наблюдавате как се държи.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можете да забележите, че наблюденията съдържат 4 числа. Те са:\n",
    "- Позиция на количката\n",
    "- Скорост на количката\n",
    "- Ъгъл на пръта\n",
    "- Скорост на въртене на пръта\n",
    "\n",
    "`rew` е наградата, която получаваме на всяка стъпка. В средата CartPole можете да видите, че получавате 1 точка за всяка стъпка от симулацията, а целта е да максимизирате общата награда, т.е. времето, през което CartPole успява да се балансира, без да падне.\n",
    "\n",
    "По време на обучението чрез подсилване, нашата цел е да обучим **политика** $\\pi$, която за всяко състояние $s$ ще ни казва какво действие $a$ да предприемем, т.е. $a = \\pi(s)$.\n",
    "\n",
    "Ако искате вероятностно решение, можете да мислите за политиката като връщаща набор от вероятности за всяко действие, т.е. $\\pi(a|s)$ би означавало вероятността да предприемем действие $a$ в състояние $s$.\n",
    "\n",
    "## Метод на градиента на политиката\n",
    "\n",
    "В най-простия алгоритъм за обучение чрез подсилване, наречен **Градиент на политиката**, ще обучим невронна мрежа да предсказва следващото действие.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще обучим мрежата, като проведем много експерименти и актуализираме мрежата след всяко изпълнение. Нека дефинираме функция, която ще проведе експеримента и ще върне резултатите (т.нар. **следа**) - всички състояния, действия (и препоръчаните им вероятности) и награди:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можете да изпълните един епизод с нетренирана мрежа и да наблюдавате, че общата награда (или дължината на епизода) е много ниска:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Един от трудните аспекти на алгоритъма за градиент на политика е използването на **дисконтни награди**. Идеята е, че изчисляваме вектора на общите награди на всяка стъпка от играта и по време на този процес дисконтраме ранните награди, използвайки някакъв коефициент $gamma$. Също така нормализираме получения вектор, защото ще го използваме като тегло, за да повлияем на нашето обучение:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега да започнем истинското обучение! Ще проведем 300 епизода, като при всеки епизод ще изпълним следното:\n",
    "\n",
    "1. Провеждаме експеримента и събираме следата.\n",
    "1. Изчисляваме разликата (`gradients`) между предприетите действия и предсказаните вероятности. Колкото по-малка е разликата, толкова по-уверени сме, че сме избрали правилното действие.\n",
    "1. Изчисляваме дисконтирани награди и умножаваме градиентите по дисконтираните награди - това гарантира, че стъпките с по-високи награди ще имат по-голямо влияние върху крайния резултат в сравнение с тези с по-ниски награди.\n",
    "1. Очакваните целеви действия за нашата невронна мрежа ще бъдат частично взети от предсказаните вероятности по време на изпълнението и частично от изчислените градиенти. Ще използваме параметъра `alpha`, за да определим до каква степен градиентите и наградите се вземат предвид - това се нарича *скорост на учене* на алгоритъма за подсилване.\n",
    "1. Накрая обучаваме мрежата върху състоянията и очакваните действия и повтаряме процеса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега нека стартираме епизода с рендериране, за да видим резултата:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надявам се, че вече виждате, че пръчката може да се балансира доста добре!\n",
    "\n",
    "## Модел Actor-Critic\n",
    "\n",
    "Моделът Actor-Critic е по-нататъшно развитие на градиентите на политиката, при което изграждаме невронна мрежа, за да научим както политиката, така и оценените награди. Мрежата ще има два изхода (или можете да я разглеждате като две отделни мрежи):\n",
    "* **Actor** ще препоръчва действието, което да се предприеме, като ни предоставя разпределението на вероятностите за състоянието, както в модела с градиенти на политиката.\n",
    "* **Critic** ще оценява каква би била наградата от тези действия. Той връща общите оценени награди в бъдеще за даденото състояние.\n",
    "\n",
    "Нека дефинираме такъв модел:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще трябва леко да модифицираме функциите `discounted_rewards` и `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега ще изпълним основния цикъл за обучение. Ще използваме ръчен процес на обучение на мрежата чрез изчисляване на подходящи функции за загуба и актуализиране на параметрите на мрежата:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основни моменти\n",
    "\n",
    "В тази демонстрация разгледахме два алгоритъма за подсилено обучение: прост градиент на политика и по-сложния актьор-критик. Можете да видите, че тези алгоритми работят с абстрактни понятия като състояние, действие и награда - което ги прави приложими в много различни среди.\n",
    "\n",
    "Подсиленото обучение ни позволява да научим най-добрата стратегия за решаване на даден проблем, като се основаваме единствено на крайната награда. Фактът, че не се нуждаем от етикетирани набори от данни, ни позволява да повтаряме симулациите многократно, за да оптимизираме нашите модели. Въпреки това, все още има много предизвикателства в подсиленото обучение, които можете да изучите, ако решите да се задълбочите в тази интересна област на изкуствения интелект.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от отговорност**:  \nТози документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T22:59:31+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "bg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}