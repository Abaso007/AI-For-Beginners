<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "437c988596e751072e41a5aad3fcc5d9",
  "translation_date": "2025-08-25T21:25:05+00:00",
  "source_file": "lessons/7-Ethics/README.md",
  "language_code": "bg"
}
-->
# Етична и отговорна AI

Вие почти завършихте този курс и се надявам, че до този момент ясно виждате, че AI се основава на редица формални математически методи, които ни позволяват да откриваме връзки в данните и да обучаваме модели, които да възпроизвеждат някои аспекти на човешкото поведение. В този момент от историята разглеждаме AI като много мощен инструмент за извличане на модели от данни и за прилагането на тези модели за решаване на нови проблеми.

## [Тест преди лекцията](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5/)

Въпреки това, в научната фантастика често виждаме истории, в които AI представлява опасност за човечеството. Обикновено тези истории се въртят около някакъв вид бунт на AI, когато AI решава да се противопостави на хората. Това предполага, че AI има някакви емоции или може да взема решения, които не са предвидени от неговите разработчици.

Видът AI, който изучихме в този курс, не е нищо повече от голяма матрична аритметика. Това е много мощен инструмент, който ни помага да решаваме нашите проблеми, и както всеки друг мощен инструмент - той може да бъде използван за добри и лоши цели. Важно е да се отбележи, че той може да бъде *злоупотребен*.

## Принципи на отговорна AI

За да избегнем случайна или умишлена злоупотреба с AI, Microsoft определя важните [Принципи на отговорна AI](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-77998-cacaste). Следните концепции са основа на тези принципи:

* **Справедливост** е свързана с важния проблем на *пристрастията в модела*, които могат да бъдат причинени от използването на пристрастни данни за обучение. Например, когато се опитваме да предвидим вероятността даден човек да получи работа като разработчик на софтуер, моделът вероятно ще даде по-голямо предпочитание на мъжете - просто защото обучаващият набор от данни вероятно е бил пристрастен към мъжка аудитория. Трябва внимателно да балансираме обучаващите данни и да изследваме модела, за да избегнем пристрастия и да се уверим, че моделът взема предвид по-релевантни характеристики.
* **Надеждност и безопасност**. По своята природа AI моделите могат да правят грешки. Невронната мрежа връща вероятности и трябва да ги вземем предвид при вземането на решения. Всеки модел има определена точност и припомняне, и трябва да разберем това, за да предотвратим вредата, която може да причини грешен съвет.
* **Поверителност и сигурност** имат специфични за AI импликации. Например, когато използваме някои данни за обучение на модел, тези данни стават по някакъв начин "интегрирани" в модела. От една страна, това увеличава сигурността и поверителността, от друга - трябва да помним кои данни са били използвани за обучение на модела.
* **Инклузивност** означава, че не изграждаме AI, за да заменим хората, а по-скоро да ги допълним и да направим работата ни по-креативна. Това също е свързано със справедливостта, защото когато работим с недостатъчно представени общности, повечето от данните, които събираме, вероятно са пристрастни, и трябва да се уверим, че тези общности са включени и правилно обработени от AI.
* **Прозрачност**. Това включва да се уверим, че винаги сме ясни относно използването на AI. Също така, където е възможно, искаме да използваме AI системи, които са *интерпретируеми*.
* **Отговорност**. Когато AI моделите вземат някакви решения, не винаги е ясно кой е отговорен за тези решения. Трябва да се уверим, че разбираме къде лежи отговорността за решенията на AI. В повечето случаи бихме искали да включим хора в процеса на вземане на важни решения, така че реални хора да бъдат държани отговорни.

## Инструменти за отговорна AI

Microsoft е разработил [Инструментариум за отговорна AI](https://github.com/microsoft/responsible-ai-toolbox), който съдържа набор от инструменти:

* Табло за интерпретируемост (InterpretML)
* Табло за справедливост (FairLearn)
* Табло за анализ на грешки
* Табло за отговорна AI, което включва:

   - EconML - инструмент за причинно-следствен анализ, който се фокусира върху въпроси "какво ако"
   - DiCE - инструмент за контрафактуален анализ, който ви позволява да видите кои характеристики трябва да бъдат променени, за да се повлияе на решението на модела

За повече информация относно етиката на AI, моля посетете [този урок](https://github.com/microsoft/ML-For-Beginners/tree/main/1-Introduction/3-fairness?WT.mc_id=academic-77998-cacaste) в учебната програма за машинно обучение, който включва задачи.

## Преглед и самостоятелно обучение

Вземете този [учебен път](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77998-cacaste), за да научите повече за отговорната AI.

## [Тест след лекцията](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6/)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.