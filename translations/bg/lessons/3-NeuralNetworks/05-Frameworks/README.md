<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-25T23:51:46+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "bg"
}
-->
# Фреймворкове за невронни мрежи

Както вече научихме, за да можем ефективно да обучаваме невронни мрежи, трябва да направим две неща:

* Да работим с тензори, например да умножаваме, събираме и изчисляваме функции като сигмоид или softmax
* Да изчисляваме градиенти на всички изрази, за да извършваме оптимизация чрез градиентен спуск

## [Тест преди лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Докато библиотеката `numpy` може да изпълнява първата част, ни е необходим механизъм за изчисляване на градиенти. В [нашия фреймворк](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), който разработихме в предишната секция, трябваше ръчно да програмираме всички функции за производни в метода `backward`, който извършва обратното разпространение. Идеалният фреймворк би ни предоставил възможност да изчисляваме градиенти на *всеки израз*, който можем да дефинираме.

Друго важно нещо е възможността за извършване на изчисления на GPU или други специализирани изчислителни устройства, като [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Обучението на дълбоки невронни мрежи изисква *много* изчисления, и възможността за паралелизиране на тези изчисления на GPU е от голямо значение.

> ✅ Терминът 'паралелизиране' означава разпределяне на изчисленията върху множество устройства.

В момента двата най-популярни фреймворка за невронни мрежи са: [TensorFlow](http://TensorFlow.org) и [PyTorch](https://pytorch.org/). И двата предоставят ниско ниво API за работа с тензори както на CPU, така и на GPU. Освен ниско ниво API, има и високо ниво API, наречени съответно [Keras](https://keras.io/) и [PyTorch Lightning](https://pytorchlightning.ai/).

Ниско ниво API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
---------------|-------------------------------------|--------------------------------
Високо ниво API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Ниско ниво API** в двата фреймворка ви позволява да изграждате така наречените **изчислителни графи**. Този граф определя как да се изчисли изходът (обикновено функцията на загуба) с дадени входни параметри и може да бъде изпратен за изчисление на GPU, ако е наличен. Има функции за диференциране на този изчислителен граф и изчисляване на градиенти, които след това могат да се използват за оптимизиране на параметрите на модела.

**Високо ниво API** разглежда невронните мрежи като **последователност от слоеве** и прави изграждането на повечето невронни мрежи много по-лесно. Обучението на модела обикновено изисква подготовка на данните и след това извикване на функцията `fit`, за да се извърши обучението.

Високото ниво API позволява бързо изграждане на типични невронни мрежи, без да се налага да се занимавате с много детайли. В същото време ниското ниво API предлага много повече контрол върху процеса на обучение и затова се използва често в изследвания, когато се работи с нови архитектури на невронни мрежи.

Също така е важно да разберете, че можете да използвате и двата API заедно. Например, можете да разработите собствена архитектура на слой с ниско ниво API и след това да я използвате в по-голяма мрежа, изградена и обучена с високо ниво API. Или можете да дефинирате мрежа с високо ниво API като последователност от слоеве и след това да използвате собствен цикъл за обучение с ниско ниво API за оптимизация. И двата API използват едни и същи основни концепции и са проектирани да работят добре заедно.

## Обучение

В този курс предлагаме повечето съдържание както за PyTorch, така и за TensorFlow. Можете да изберете предпочитания от вас фреймворк и да преминете само през съответните тетрадки. Ако не сте сигурни кой фреймворк да изберете, прочетете някои дискусии в интернет относно **PyTorch vs. TensorFlow**. Можете също така да разгледате и двата фреймворка, за да получите по-добро разбиране.

Където е възможно, ще използваме високо ниво API за простота. Въпреки това, смятаме, че е важно да разберете как работят невронните мрежи от основата, затова в началото започваме с работа с ниско ниво API и тензори. Ако обаче искате да започнете бързо и не искате да отделяте много време за изучаване на тези детайли, можете да пропуснете тези части и да преминете директно към тетрадките с високо ниво API.

## ✍️ Упражнения: Фреймворкове

Продължете обучението си в следните тетрадки:

Ниско ниво API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
---------------|-------------------------------------|--------------------------------
Високо ниво API| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

След като овладеете фреймворковете, нека преговорим концепцията за overfitting.

# Overfitting

Overfitting е изключително важна концепция в машинното обучение и е много важно да я разберем правилно!

Разгледайте следния проблем с апроксимация на 5 точки (представени с `x` на графиките по-долу):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.bg.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.bg.jpg)
-------------------------|--------------------------
**Линеен модел, 2 параметъра** | **Нелинеен модел, 7 параметъра**
Грешка при обучение = 5.3 | Грешка при обучение = 0
Грешка при валидация = 5.1 | Грешка при валидация = 20

* Вляво виждаме добра апроксимация с права линия. Тъй като броят на параметрите е адекватен, моделът улавя правилно разпределението на точките.
* Вдясно моделът е твърде мощен. Тъй като имаме само 5 точки, а моделът има 7 параметъра, той може да се настрои така, че да премине през всички точки, като прави грешката при обучение 0. Това обаче пречи на модела да разбере правилния модел в данните, което води до висока грешка при валидация.

Много е важно да се намери правилният баланс между сложността на модела (броя на параметрите) и броя на обучителните примери.

## Защо възниква overfitting

  * Недостатъчно обучителни данни
  * Твърде мощен модел
  * Твърде много шум в данните

## Как да открием overfitting

Както се вижда от графиката по-горе, overfitting може да се открие чрез много ниска грешка при обучение и висока грешка при валидация. Обикновено по време на обучението ще видим как и грешката при обучение, и грешката при валидация започват да намаляват, но в даден момент грешката при валидация може да спре да намалява и да започне да се увеличава. Това ще бъде знак за overfitting и индикатор, че вероятно трябва да спрем обучението на този етап (или поне да направим моментна снимка на модела).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.bg.png)

## Как да предотвратим overfitting

Ако забележите, че възниква overfitting, можете да направите едно от следните:

 * Увеличете количеството обучителни данни
 * Намалете сложността на модела
 * Използвайте някаква [техника за регуляризация](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), като например [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), която ще разгледаме по-късно.

## Overfitting и компромисът между пристрастие и вариация

Overfitting всъщност е случай на по-общ проблем в статистиката, наречен [компромис между пристрастие и вариация](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Ако разгледаме възможните източници на грешка в нашия модел, можем да видим два типа грешки:

* **Грешки от пристрастие** възникват, когато алгоритъмът ни не може правилно да улови връзката между обучителните данни. Това може да се дължи на факта, че моделът ни не е достатъчно мощен (**underfitting**).
* **Грешки от вариация**, които възникват, когато моделът апроксимира шума в данните вместо смислените връзки (**overfitting**).

По време на обучението грешките от пристрастие намаляват (тъй като моделът ни се учи да апроксимира данните), а грешките от вариация се увеличават. Важно е да спрем обучението - или ръчно (когато открием overfitting), или автоматично (чрез въвеждане на регуляризация) - за да предотвратим overfitting.

## Заключение

В този урок научихте за разликите между различните API за двата най-популярни AI фреймворка, TensorFlow и PyTorch. Освен това научихте за една много важна тема - overfitting.

## 🚀 Предизвикателство

В придружаващите тетрадки ще намерите 'задачи' в края; преминете през тетрадките и изпълнете задачите.

## [Тест след лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Преглед и самостоятелно обучение

Направете проучване по следните теми:

- TensorFlow
- PyTorch
- Overfitting

Задайте си следните въпроси:

- Каква е разликата между TensorFlow и PyTorch?
- Каква е разликата между overfitting и underfitting?

## [Задание](lab/README.md)

В тази лабораторна работа трябва да решите два класификационни проблема, използвайки еднослойни и многослойни напълно свързани мрежи с PyTorch или TensorFlow.

* [Инструкции](lab/README.md)
* [Тетрадка](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да било недоразумения или погрешни интерпретации, произтичащи от използването на този превод.