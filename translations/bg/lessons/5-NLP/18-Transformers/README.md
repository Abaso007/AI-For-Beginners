<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f335dfcb4a993920504c387973a36957",
  "translation_date": "2025-09-23T14:33:59+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "bg"
}
-->
# Механизми за внимание и трансформери

## [Тест преди лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Един от най-важните проблеми в областта на NLP е **машинният превод**, основна задача, която стои зад инструменти като Google Translate. В тази секция ще се фокусираме върху машинния превод или, по-общо казано, върху всяка задача за *последователност-към-последователност* (която също се нарича **трансдукция на изречения**).

С RNNs, последователност-към-последователност се реализира чрез две рекурентни мрежи, където една мрежа, **кодировачът**, компресира входната последователност в скрито състояние, а друга мрежа, **декодировачът**, разгръща това скрито състояние в преведен резултат. Има няколко проблема с този подход:

* Финалното състояние на кодировачната мрежа трудно запомня началото на изречението, което води до ниско качество на модела за дълги изречения.
* Всички думи в последователността имат еднакво влияние върху резултата. В действителност обаче, специфични думи във входната последователност често имат по-голямо влияние върху изходните последователности от други.

**Механизмите за внимание** предоставят начин за претегляне на контекстуалното влияние на всеки входен вектор върху всяка прогноза на изхода на RNN. Това се реализира чрез създаване на преки връзки между междинните състояния на входния RNN и изходния RNN. По този начин, когато генерираме изходния символ y<sub>t</sub>, ще вземем предвид всички скрити състояния на входа h<sub>i</sub>, с различни теглови коефициенти &alpha;<sub>t,i</sub>.

![Изображение, показващо модел кодировач/декодировач с добавен слой за внимание](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.bg.png)

> Моделът кодировач-декодировач с механизъм за добавено внимание в [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитиран от [този блог пост](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Матрицата за внимание {&alpha;<sub>i,j</sub>} представлява степента, до която определени входни думи играят роля в генерирането на дадена дума в изходната последователност. По-долу е пример за такава матрица:

![Изображение, показващо пример за подравняване, намерено от RNNsearch-50, взето от Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.bg.png)

> Фигура от [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Фиг.3)

Механизмите за внимание са отговорни за голяма част от текущото или почти текущото състояние на изкуството в NLP. Добавянето на внимание обаче значително увеличава броя на параметрите на модела, което води до проблеми с мащабирането при RNNs. Основно ограничение при мащабирането на RNNs е, че рекурентната природа на моделите прави трудно групирането и паралелизирането на обучението. В RNN всеки елемент от последователността трябва да бъде обработен в последователен ред, което означава, че не може лесно да бъде паралелизиран.

![Кодировач Декодировач с Внимание](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Фигура от [Блогът на Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Приемането на механизми за внимание, комбинирано с това ограничение, доведе до създаването на съвременните трансформерни модели, които познаваме и използваме днес, като BERT и Open-GPT3.

## Трансформерни модели

Една от основните идеи зад трансформерите е да се избегне последователната природа на RNNs и да се създаде модел, който е паралелизируем по време на обучението. Това се постига чрез реализиране на две идеи:

* позиционно кодиране
* използване на механизъм за самовнимание за улавяне на модели вместо RNNs (или CNNs) (затова статията, която представя трансформерите, се нарича *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Позиционно кодиране/вграждане

Идеята за позиционно кодиране е следната. 
1. При използване на RNNs, относителната позиция на токените се представя чрез броя на стъпките и следователно не е необходимо да бъде изрично представена. 
2. Въпреки това, когато преминем към внимание, трябва да знаем относителните позиции на токените в рамките на последователността. 
3. За да получим позиционно кодиране, допълваме нашата последователност от токени с последователност от позиции на токените в последователността (т.е. последователност от числа 0,1, ...).
4. След това смесваме позицията на токена с вектор за вграждане на токена. За да трансформираме позицията (цяло число) във вектор, можем да използваме различни подходи:

* Обучаемо вграждане, подобно на вграждането на токени. Това е подходът, който разглеждаме тук. Прилагаме слоеве за вграждане върху токените и техните позиции, което води до вектори за вграждане със същите размери, които след това събираме.
* Функция за фиксирано позиционно кодиране, както е предложено в оригиналната статия.

<img src="images/pos-embedding.png" width="50%"/>

> Изображение от автора

Резултатът, който получаваме с позиционното вграждане, включва както оригиналния токен, така и неговата позиция в рамките на последователността.

### Многоглаво самовнимание

Следващата стъпка е да уловим някои модели в рамките на нашата последователност. За да направят това, трансформерите използват механизъм за **самовнимание**, който по същество е внимание, приложено към една и съща последователност като вход и изход. Прилагането на самовнимание ни позволява да вземем предвид **контекста** в рамките на изречението и да видим кои думи са взаимосвързани. Например, това ни позволява да видим кои думи се отнасят до кореференции, като *it*, и също така да вземем контекста предвид:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.bg.png)

> Изображение от [Блогът на Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

В трансформерите използваме **многоглаво внимание**, за да дадем на мрежата възможността да улавя различни типове зависимости, напр. дългосрочни срещу краткосрочни отношения между думи, кореференции срещу нещо друго и т.н.

[Тетрадка TensorFlow](TransformersTF.ipynb) съдържа повече подробности за реализацията на слоевете на трансформера.

### Внимание между кодировач и декодировач

В трансформерите вниманието се използва на две места:

* За улавяне на модели в рамките на входния текст чрез самовнимание
* За извършване на превод на последователности - това е слоят за внимание между кодировача и декодировача.

Вниманието между кодировач и декодировач е много подобно на механизма за внимание, използван в RNNs, както беше описано в началото на тази секция. Тази анимирана диаграма обяснява ролята на вниманието между кодировач и декодировач.

![Анимиран GIF, показващ как се извършват оценките в трансформерните модели.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Тъй като всяка входна позиция се картографира независимо към всяка изходна позиция, трансформерите могат да се паралелизират по-добре от RNNs, което позволява много по-големи и изразителни езикови модели. Всяка глава за внимание може да се използва за изучаване на различни отношения между думи, което подобрява задачите за обработка на естествен език.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) е много голяма многослойна трансформерна мрежа с 12 слоя за *BERT-base* и 24 за *BERT-large*. Моделът първо се предварително обучава върху голям корпус от текстови данни (WikiPedia + книги) чрез неуправляемо обучение (предсказване на маскирани думи в изречение). По време на предварителното обучение моделът усвоява значителни нива на разбиране на езика, които след това могат да бъдат използвани с други набори от данни чрез фина настройка. Този процес се нарича **трансферно обучение**.

![изображение от http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.bg.png)

> Изображение [източник](http://jalammar.github.io/illustrated-bert/)

## ✍️ Упражнения: Трансформери

Продължете обучението си в следните тетрадки:

* [Трансформери в PyTorch](TransformersPyTorch.ipynb)
* [Трансформери в TensorFlow](TransformersTF.ipynb)

## Заключение

В този урок научихте за трансформерите и механизмите за внимание, всички основни инструменти в NLP. Съществуват много вариации на трансформерните архитектури, включително BERT, DistilBERT, BigBird, OpenGPT3 и други, които могат да бъдат фино настроени. Пакетът [HuggingFace](https://github.com/huggingface/) предоставя репозитория за обучение на много от тези архитектури както с PyTorch, така и с TensorFlow.

## 🚀 Предизвикателство

## [Тест след лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Преглед и самостоятелно обучение

* [Блог пост](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), обясняващ класическата статия [Attention is all you need](https://arxiv.org/abs/1706.03762) за трансформерите.
* [Серия от блог постове](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) за трансформерите, обясняващи архитектурата в детайли.

## [Задание](assignment.md)

---

