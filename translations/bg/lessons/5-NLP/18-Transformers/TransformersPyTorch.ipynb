{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Механизми на вниманието и трансформери\n",
    "\n",
    "Един от основните недостатъци на рекурентните мрежи е, че всички думи в последователността имат еднакво влияние върху резултата. Това води до неефективност при стандартните LSTM encoder-decoder модели за задачи от тип последователност към последователност, като разпознаване на именувани обекти и машинен превод. В действителност, определени думи в входната последователност често имат по-голямо влияние върху изходните последователности от други.\n",
    "\n",
    "Да разгледаме модел от тип последователност към последователност, като машинен превод. Той се реализира чрез две рекурентни мрежи, където едната мрежа (**encoder**) компресира входната последователност в скрито състояние, а другата (**decoder**) разгъва това скрито състояние в преведен резултат. Проблемът с този подход е, че крайната състояние на мрежата трудно запомня началото на изречението, което води до ниско качество на модела при дълги изречения.\n",
    "\n",
    "**Механизмите на вниманието** предоставят начин за претегляне на контекстуалното влияние на всеки входен вектор върху всяка изходна прогноза на RNN. Това се реализира чрез създаване на преки връзки между междинните състояния на входната RNN и изходната RNN. По този начин, при генериране на изходен символ $y_t$, ще вземем предвид всички входни скрити състояния $h_i$, с различни теглови коефициенти $\\alpha_{t,i}$.\n",
    "\n",
    "![Изображение, показващо модел encoder/decoder с добавен слой за внимание](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.bg.png)\n",
    "*Моделът encoder-decoder с механизъм за добавено внимание в [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитиран от [този блог пост](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Матрицата на вниманието $\\{\\alpha_{i,j}\\}$ представлява степента, до която определени входни думи участват в генерирането на дадена дума в изходната последователност. По-долу е даден пример за такава матрица:\n",
    "\n",
    "![Изображение, показващо пример за подравняване, намерено от RNNsearch-50, взето от Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.bg.png)\n",
    "\n",
    "*Фигура, взета от [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Фиг.3)*\n",
    "\n",
    "Механизмите на вниманието са отговорни за голяма част от настоящите или почти настоящите върхови постижения в обработката на естествен език. Добавянето на внимание обаче значително увеличава броя на параметрите на модела, което води до проблеми със скалирането при RNN. Основно ограничение при скалирането на RNN е, че рекурентната природа на моделите затруднява групирането и паралелизирането на обучението. В RNN всеки елемент от последователността трябва да бъде обработен в последователен ред, което означава, че не може лесно да бъде паралелизиран.\n",
    "\n",
    "Приемането на механизми на вниманието в комбинация с това ограничение доведе до създаването на съвременните трансформерни модели, които познаваме и използваме днес, като BERT и OpenGPT3.\n",
    "\n",
    "## Трансформерни модели\n",
    "\n",
    "Вместо да предават контекста на всяка предишна прогноза в следващата стъпка на оценка, **трансформерните модели** използват **позиционни кодировки** и внимание, за да уловят контекста на даден вход в предоставен прозорец от текст. Изображението по-долу показва как позиционните кодировки с внимание могат да уловят контекста в даден прозорец.\n",
    "\n",
    "![Анимиран GIF, показващ как се извършват оценките в трансформерните модели.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Тъй като всяка входна позиция се съпоставя независимо с всяка изходна позиция, трансформерите могат да се паралелизират по-добре от RNN, което позволява създаването на много по-големи и изразителни езикови модели. Всяка глава на вниманието може да се използва за изучаване на различни взаимоотношения между думите, което подобрява задачите за обработка на естествен език.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) е много голяма многослойна трансформерна мрежа с 12 слоя за *BERT-base* и 24 за *BERT-large*. Моделът първо се предварително обучава върху голям корпус от текстови данни (Wikipedia + книги) с помощта на неконтролирано обучение (предсказване на маскирани думи в изречение). По време на предварителното обучение моделът усвоява значително ниво на езиково разбиране, което след това може да бъде използвано с други набори от данни чрез фина настройка. Този процес се нарича **трансферно обучение**.\n",
    "\n",
    "![изображение от http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.bg.png)\n",
    "\n",
    "Съществуват много вариации на трансформерните архитектури, включително BERT, DistilBERT, BigBird, OpenGPT3 и други, които могат да бъдат фино настроени. Пакетът [HuggingFace](https://github.com/huggingface/) предоставя хранилище за обучение на много от тези архитектури с PyTorch.\n",
    "\n",
    "## Използване на BERT за текстова класификация\n",
    "\n",
    "Нека видим как можем да използваме предварително обучен модел BERT за решаване на нашата традиционна задача: класификация на последователности. Ще класифицираме оригиналния ни набор от данни AG News.\n",
    "\n",
    "Първо, нека заредим библиотеката HuggingFace и нашия набор от данни:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тъй като ще използваме предварително обучен BERT модел, ще трябва да използваме специфичен токенизатор. Първо, ще заредим токенизатор, свързан с предварително обучен BERT модел.\n",
    "\n",
    "Библиотеката HuggingFace съдържа хранилище с предварително обучени модели, които можете да използвате, като просто посочите техните имена като аргументи към функциите `from_pretrained`. Всички необходими бинарни файлове за модела ще бъдат автоматично изтеглени.\n",
    "\n",
    "Въпреки това, понякога може да се наложи да заредите свои собствени модели. В този случай можете да посочите директорията, която съдържа всички съответни файлове, включително параметрите за токенизатора, файла `config.json` с параметрите на модела, бинарните тегла и др.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обектът `tokenizer` съдържа функцията `encode`, която може да се използва директно за кодиране на текст:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогава нека създадем итератори, които ще използваме по време на обучението за достъп до данните. Тъй като BERT използва своя собствена функция за кодиране, ще трябва да дефинираме функция за запълване, подобна на `padify`, която сме дефинирали преди:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашия случай ще използваме предварително обучен BERT модел, наречен `bert-base-uncased`. Нека заредим модела, използвайки пакета `BertForSequenceClassfication`. Това гарантира, че нашият модел вече има необходимата архитектура за класификация, включително финалния класификатор. Ще видите предупредително съобщение, което посочва, че теглата на финалния класификатор не са инициализирани и моделът ще изисква предварително обучение - това е напълно нормално, защото точно това ще направим!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега сме готови да започнем обучението! Тъй като BERT вече е предварително обучен, искаме да започнем с доста малка скорост на обучение, за да не разрушим началните тегла.\n",
    "\n",
    "Цялата тежка работа се извършва от модела `BertForSequenceClassification`. Когато извикаме модела върху тренировъчните данни, той връща както загубата, така и изхода на мрежата за входния минибатч. Използваме загубата за оптимизация на параметрите (`loss.backward()` извършва обратното преминаване), а `out` за изчисляване на точността на обучението, като сравняваме получените етикети `labs` (изчислени с помощта на `argmax`) с очакваните `labels`.\n",
    "\n",
    "За да контролираме процеса, натрупваме загубата и точността през няколко итерации и ги отпечатваме на всеки `report_freq` цикли на обучение.\n",
    "\n",
    "Това обучение вероятно ще отнеме доста време, затова ограничаваме броя на итерациите.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можете да видите (особено ако увеличите броя на итерациите и изчакате достатъчно дълго), че класификацията с BERT ни дава доста добра точност! Това е така, защото BERT вече разбира доста добре структурата на езика и ние само трябва да дообучим крайния класификатор. Въпреки това, тъй като BERT е голям модел, целият процес на обучение отнема много време и изисква сериозна изчислителна мощност! (GPU, и за предпочитане повече от един).\n",
    "\n",
    "> **Note:** В нашия пример използвахме един от най-малките предварително обучени модели на BERT. Съществуват по-големи модели, които вероятно ще дадат по-добри резултати.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка на производителността на модела\n",
    "\n",
    "Сега можем да оценим производителността на нашия модел върху тестовия набор от данни. Цикълът за оценка е доста подобен на цикъла за обучение, но не трябва да забравяме да превключим модела в режим на оценка, като извикаме `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основни моменти\n",
    "\n",
    "В този модул видяхме колко лесно е да вземем предварително обучен езиков модел от библиотеката **transformers** и да го адаптираме към нашата задача за класификация на текст. По същия начин, моделите BERT могат да се използват за извличане на обекти, отговаряне на въпроси и други задачи, свързани с обработка на естествен език.\n",
    "\n",
    "Моделите на трансформъри представляват най-съвременните технологии в NLP и в повечето случаи трябва да бъдат първото решение, с което започвате експерименти, когато внедрявате персонализирани NLP решения. Въпреки това, разбирането на основните принципи на рекурентните невронни мрежи, обсъдени в този модул, е изключително важно, ако искате да изграждате усъвършенствани невронни модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от отговорност**:  \nТози документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или погрешни интерпретации, произтичащи от използването на този превод.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-30T00:45:25+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "bg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}