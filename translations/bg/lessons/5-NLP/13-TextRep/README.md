<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:50:54+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "bg"
}
-->
# Представяне на текст като тензори

## [Тест преди лекцията](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Класификация на текст

В първата част на този раздел ще се фокусираме върху задачата за **класификация на текст**. Ще използваме набора от данни [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), който съдържа новинарски статии като следната:

* Категория: Наука/Технологии  
* Заглавие: Ky. Company Wins Grant to Study Peptides (AP)  
* Текст: AP - Компания, основана от изследовател по химия в Университета на Луисвил, спечели грант за разработване на...

Нашата цел ще бъде да класифицираме новината в една от категориите въз основа на текста.

## Представяне на текст

Ако искаме да решаваме задачи по обработка на естествен език (NLP) с невронни мрежи, трябва да намерим начин да представим текста като тензори. Компютрите вече представят текстовите символи като числа, които съответстват на шрифтове на екрана ви, използвайки кодировки като ASCII или UTF-8.

<img alt="Изображение, показващо диаграма за преобразуване на символ в ASCII и двоично представяне" src="images/ascii-character-map.png" width="50%"/>

> [Източник на изображението](https://www.seobility.net/en/wiki/ASCII)

Като хора, ние разбираме какво **представлява** всяка буква и как всички символи се съчетават, за да образуват думите в едно изречение. Въпреки това, компютрите сами по себе си нямат такова разбиране, и невронната мрежа трябва да научи значението по време на обучението.

Затова можем да използваме различни подходи за представяне на текст:

* **Представяне на ниво символи**, при което представяме текста, като третираме всеки символ като число. Ако имаме *C* различни символи в нашия текстов корпус, думата *Hello* ще бъде представена като тензор с размер 5x*C*. Всеки символ ще съответства на колона в тензора при one-hot кодиране.  
* **Представяне на ниво думи**, при което създаваме **речник** на всички думи в текста и след това представяме думите чрез one-hot кодиране. Този подход е донякъде по-добър, защото всяка буква сама по себе си няма голямо значение, и така чрез използване на по-високо ниво семантични концепции - думи - опростяваме задачата за невронната мрежа. Въпреки това, поради големия размер на речника, трябва да се справяме с високодименсионални разредени тензори.

Независимо от представянето, първо трябва да преобразуваме текста в последователност от **токени**, като един токен може да бъде символ, дума или дори част от дума. След това преобразуваме токена в число, обикновено използвайки **речник**, и това число може да бъде подадено в невронна мрежа чрез one-hot кодиране.

## N-грамове

В естествения език точният смисъл на думите може да бъде определен само в контекст. Например, значенията на *невронна мрежа* и *рибарска мрежа* са напълно различни. Един от начините да вземем това предвид е да изградим модела си върху двойки думи, като разглеждаме двойките думи като отделни токени в речника. По този начин изречението *Обичам да ходя на риболов* ще бъде представено чрез следната последователност от токени: *Обичам да*, *да ходя*, *ходя на*, *на риболов*. Проблемът с този подход е, че размерът на речника нараства значително, а комбинации като *на риболов* и *на пазар* се представят чрез различни токени, които не споделят никаква семантична прилика, въпреки че съдържат един и същ глагол.

В някои случаи можем да разгледаме използването на триграми - комбинации от три думи - също. Затова този подход често се нарича **n-грамове**. Също така има смисъл да използваме n-грамове с представяне на ниво символи, при което n-грамовете ще съответстват приблизително на различни срички.

## Чанта с думи и TF/IDF

Когато решаваме задачи като класификация на текст, трябва да можем да представим текста чрез един вектор с фиксиран размер, който ще използваме като вход за крайния плътен класификатор. Един от най-простите начини да направим това е да комбинираме всички индивидуални представяния на думи, например чрез тяхното събиране. Ако съберем one-hot кодирането на всяка дума, ще получим вектор на честотите, показващ колко пъти всяка дума се появява в текста. Такова представяне на текста се нарича **чанта с думи** (BoW).

<img src="images/bow.png" width="90%"/>

> Изображение от автора

BoW по същество представя кои думи се появяват в текста и в какви количества, което наистина може да бъде добър индикатор за съдържанието на текста. Например, новинарска статия за политика вероятно ще съдържа думи като *президент* и *държава*, докато научна публикация би имала нещо като *колайдер*, *откритие* и т.н. Така честотите на думите в много случаи могат да бъдат добър индикатор за съдържанието на текста.

Проблемът с BoW е, че определени често срещани думи, като *и*, *е* и т.н., се появяват в повечето текстове и имат най-високи честоти, което прикрива думите, които наистина са важни. Можем да намалим значимостта на тези думи, като вземем предвид честотата, с която думите се появяват в цялата колекция от документи. Това е основната идея зад подхода TF/IDF, който е разгледан по-подробно в тетрадките, приложени към този урок.

Въпреки това, нито един от тези подходи не може напълно да вземе предвид **семантиката** на текста. Нуждаем се от по-мощни модели на невронни мрежи, за да направим това, които ще обсъдим по-късно в този раздел.

## ✍️ Упражнения: Представяне на текст

Продължете обучението си в следните тетрадки:

* [Представяне на текст с PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Представяне на текст с TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## Заключение

Досега изучихме техники, които могат да добавят тегло на честотата към различни думи. Те обаче не могат да представят значението или реда. Както известният лингвист Дж. Р. Фърт каза през 1935 г., "Пълното значение на една дума винаги е контекстуално, и никакво изследване на значението извън контекста не може да бъде взето на сериозно." По-късно в курса ще научим как да улавяме контекстуална информация от текста чрез моделиране на езика.

## 🚀 Предизвикателство

Опитайте други упражнения, използвайки чанта с думи и различни модели на данни. Може да се вдъхновите от това [състезание в Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [Тест след лекцията](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Преглед и самостоятелно обучение

Практикувайте уменията си с техники за текстови вграждания и чанта с думи на [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Задание: Тетрадки](assignment.md)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.