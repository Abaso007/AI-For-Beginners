<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-25T22:07:45+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "bg"
}
-->
# Предварително обучени големи езикови модели

Във всички предишни задачи обучавахме невронна мрежа да изпълнява определена задача, използвайки етикетиран набор от данни. С големите трансформерни модели, като BERT, използваме моделиране на езика в самонаблюдаван режим, за да изградим езиков модел, който след това се специализира за конкретна задача чрез допълнително обучение в специфична област. Въпреки това е доказано, че големите езикови модели могат да решават много задачи без НИКАКВО обучение в специфична област. Семейството от модели, способни на това, се нарича **GPT**: Генеративен предварително обучен трансформер.

## [Тест преди лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## Генериране на текст и перплексност

Идеята за невронна мрежа, която може да изпълнява общи задачи без допълнително обучение, е представена в статията [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Основната идея е, че много други задачи могат да бъдат моделирани чрез **генериране на текст**, защото разбирането на текста по същество означава способност за неговото създаване. Тъй като моделът е обучен върху огромно количество текст, който обхваща човешкото знание, той става компетентен в широк спектър от теми.

> Разбирането и способността за създаване на текст включва и познания за света около нас. Хората също учат до голяма степен чрез четене, и мрежата GPT е подобна в това отношение.

Мрежите за генериране на текст работят чрез предсказване на вероятността за следващата дума $$P(w_N)$$. Въпреки това, безусловната вероятност за следващата дума е равна на честотата на тази дума в текстовия корпус. GPT може да ни даде **условна вероятност** за следващата дума, като се вземат предвид предишните: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Можете да научите повече за вероятностите в нашата [Учебна програма за начинаещи в науката за данни](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Качеството на модела за генериране на език може да бъде определено чрез **перплексност**. Това е вътрешна метрика, която ни позволява да измерим качеството на модела без специфичен за задачата набор от данни. Тя се основава на понятието за *вероятност на изречение* - моделът присвоява висока вероятност на изречение, което вероятно е реално (т.е. моделът не е **объркан** от него), и ниска вероятност на изречения, които имат по-малко смисъл (например *Може ли това да направи какво?*). Когато предоставим на модела изречения от реален текстов корпус, очакваме те да имат висока вероятност и ниска **перплексност**. Математически, тя се определя като нормализирана обратна вероятност на тестовия набор:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Можете да експериментирате с генериране на текст, използвайки [GPT-базиран текстов редактор от Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. В този редактор започвате да пишете текст, и натискането на **[TAB]** ще ви предложи няколко опции за завършване. Ако те са твърде кратки или не сте доволни от тях - натиснете [TAB] отново, и ще получите повече опции, включително по-дълги текстове.

## GPT е семейство

GPT не е един модел, а по-скоро колекция от модели, разработени и обучени от [OpenAI](https://openai.com).

Под моделите GPT имаме:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|Езиков модел с до 1.5 милиарда параметри. | Езиков модел с до 175 милиарда параметри | 100T параметри и приема както изображения, така и текстови входове, а изходът е текст. |

Моделите GPT-3 и GPT-4 са достъпни [като когнитивна услуга от Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste), както и чрез [OpenAI API](https://openai.com/api/).

## Инженеринг на подсказки

Тъй като GPT е обучен върху огромни обеми данни за разбиране на език и код, той предоставя изходи в отговор на входове (подсказки). Подсказките са входове или заявки към GPT, чрез които се предоставят инструкции на моделите за задачите, които трябва да бъдат изпълнени. За да получите желан резултат, е необходимо най-ефективната подсказка, която включва избор на правилните думи, формати, фрази или дори символи. Този подход се нарича [Инженеринг на подсказки](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Тази документация](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) предоставя повече информация за инженеринг на подсказки.

## ✍️ Примерен ноутбук: [Игра с OpenAI-GPT](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

Продължете обучението си с следните ноутбуци:

* [Генериране на текст с OpenAI-GPT и Hugging Face Transformers](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## Заключение

Новите общи предварително обучени езикови модели не само моделират структурата на езика, но и съдържат огромно количество естествен език. Следователно, те могат ефективно да се използват за решаване на някои задачи в обработката на естествен език в условия на нулеви или малки примери.

## [Тест след лекцията](https://ff-quizzes.netlify.app/en/ai/quiz/40)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.