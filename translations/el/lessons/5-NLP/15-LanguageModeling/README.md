<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T08:58:26+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "el"
}
-->
# Μοντελοποίηση Γλώσσας

Οι σημασιολογικές ενσωματώσεις, όπως το Word2Vec και το GloVe, αποτελούν στην πραγματικότητα το πρώτο βήμα προς τη **μοντελοποίηση γλώσσας** - τη δημιουργία μοντέλων που με κάποιο τρόπο *κατανοούν* (ή *αναπαριστούν*) τη φύση της γλώσσας.

## [Προ-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Η βασική ιδέα πίσω από τη μοντελοποίηση γλώσσας είναι η εκπαίδευση σε μη επισημασμένα σύνολα δεδομένων με μη επιβλεπόμενο τρόπο. Αυτό είναι σημαντικό επειδή έχουμε τεράστιες ποσότητες μη επισημασμένου κειμένου διαθέσιμες, ενώ η ποσότητα του επισημασμένου κειμένου είναι πάντα περιορισμένη από την προσπάθεια που μπορούμε να αφιερώσουμε για την επισήμανση. Τις περισσότερες φορές, μπορούμε να δημιουργήσουμε μοντέλα γλώσσας που μπορούν να **προβλέψουν λέξεις που λείπουν** στο κείμενο, επειδή είναι εύκολο να αποκρύψουμε μια τυχαία λέξη στο κείμενο και να τη χρησιμοποιήσουμε ως δείγμα εκπαίδευσης.

## Εκπαίδευση Ενσωματώσεων

Στα προηγούμενα παραδείγματα μας, χρησιμοποιήσαμε προεκπαιδευμένες σημασιολογικές ενσωματώσεις, αλλά είναι ενδιαφέρον να δούμε πώς μπορούν να εκπαιδευτούν αυτές οι ενσωματώσεις. Υπάρχουν αρκετές ιδέες που μπορούν να χρησιμοποιηθούν:

* **Μοντελοποίηση γλώσσας με N-Gram**, όπου προβλέπουμε ένα σύμβολο κοιτάζοντας τα N προηγούμενα σύμβολα (N-gram).
* **Continuous Bag-of-Words** (CBoW), όπου προβλέπουμε το μεσαίο σύμβολο $W_0$ σε μια ακολουθία συμβόλων $W_{-N}$, ..., $W_N$.
* **Skip-gram**, όπου προβλέπουμε ένα σύνολο γειτονικών συμβόλων {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} από το μεσαίο σύμβολο $W_0$.

![εικόνα από άρθρο για τη μετατροπή λέξεων σε διανύσματα](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.el.png)

> Εικόνα από [αυτό το άρθρο](https://arxiv.org/pdf/1301.3781.pdf)

## ✍️ Παράδειγμα Σημειωματάρια: Εκπαίδευση μοντέλου CBoW

Συνεχίστε τη μάθησή σας στα παρακάτω σημειωματάρια:

* [Εκπαίδευση CBoW Word2Vec με TensorFlow](CBoW-TF.ipynb)
* [Εκπαίδευση CBoW Word2Vec με PyTorch](CBoW-PyTorch.ipynb)

## Συμπέρασμα

Στο προηγούμενο μάθημα είδαμε ότι οι ενσωματώσεις λέξεων λειτουργούν σαν μαγεία! Τώρα γνωρίζουμε ότι η εκπαίδευση ενσωματώσεων λέξεων δεν είναι μια πολύπλοκη διαδικασία, και θα πρέπει να μπορούμε να εκπαιδεύσουμε τις δικές μας ενσωματώσεις λέξεων για κείμενο συγκεκριμένου τομέα, αν χρειαστεί.

## [Μετά-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Ανασκόπηση & Αυτοδιδασκαλία

* [Επίσημο tutorial PyTorch για Μοντελοποίηση Γλώσσας](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Επίσημο tutorial TensorFlow για εκπαίδευση μοντέλου Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Η χρήση του πλαισίου **gensim** για την εκπαίδευση των πιο συχνά χρησιμοποιούμενων ενσωματώσεων με λίγες γραμμές κώδικα περιγράφεται [σε αυτή την τεκμηρίωση](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## 🚀 [Εργασία: Εκπαίδευση Μοντέλου Skip-Gram](lab/README.md)

Στο εργαστήριο, σας προκαλούμε να τροποποιήσετε τον κώδικα από αυτό το μάθημα για να εκπαιδεύσετε ένα μοντέλο skip-gram αντί για CBoW. [Διαβάστε τις λεπτομέρειες](lab/README.md)

---

