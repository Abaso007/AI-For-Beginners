<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-29T09:18:32+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "el"
}
-->
# Μοντελοποίηση Γλώσσας

Οι σημασιολογικές ενσωματώσεις, όπως το Word2Vec και το GloVe, αποτελούν στην πραγματικότητα ένα πρώτο βήμα προς τη **μοντελοποίηση γλώσσας** - τη δημιουργία μοντέλων που με κάποιο τρόπο *κατανοούν* (ή *αναπαριστούν*) τη φύση της γλώσσας.

## [Προ-διάλεξης κουίζ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

Η βασική ιδέα πίσω από τη μοντελοποίηση γλώσσας είναι η εκπαίδευση σε μη επισημασμένα σύνολα δεδομένων με μη εποπτευόμενο τρόπο. Αυτό είναι σημαντικό επειδή έχουμε τεράστιες ποσότητες μη επισημασμένου κειμένου διαθέσιμες, ενώ η ποσότητα του επισημασμένου κειμένου θα είναι πάντα περιορισμένη από την προσπάθεια που μπορούμε να αφιερώσουμε για την επισήμανση. Τις περισσότερες φορές, μπορούμε να δημιουργήσουμε μοντέλα γλώσσας που μπορούν να **προβλέψουν λέξεις που λείπουν** από το κείμενο, επειδή είναι εύκολο να αποκρύψουμε μια τυχαία λέξη από το κείμενο και να τη χρησιμοποιήσουμε ως δείγμα εκπαίδευσης.

## Εκπαίδευση Ενσωματώσεων

Στα προηγούμενα παραδείγματα, χρησιμοποιήσαμε προεκπαιδευμένες σημασιολογικές ενσωματώσεις, αλλά είναι ενδιαφέρον να δούμε πώς μπορούν να εκπαιδευτούν αυτές οι ενσωματώσεις. Υπάρχουν αρκετές ιδέες που μπορούν να χρησιμοποιηθούν:

* **Γλωσσική μοντελοποίηση N-Gram**, όπου προβλέπουμε ένα σύμβολο κοιτάζοντας τα N προηγούμενα σύμβολα (N-gram).
* **Continuous Bag-of-Words** (CBoW), όπου προβλέπουμε το μεσαίο σύμβολο $W_0$ σε μια ακολουθία συμβόλων $W_{-N}$, ..., $W_N$.
* **Skip-gram**, όπου προβλέπουμε ένα σύνολο γειτονικών συμβόλων {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} από το μεσαίο σύμβολο $W_0$.

![εικόνα από άρθρο για τη μετατροπή λέξεων σε διανύσματα](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.el.png)

> Εικόνα από [αυτό το άρθρο](https://arxiv.org/pdf/1301.3781.pdf)

## ✍️ Παράδειγμα Σημειωματάρια: Εκπαίδευση Μοντέλου CBoW

Συνεχίστε τη μάθησή σας στα παρακάτω σημειωματάρια:

* [Εκπαίδευση CBoW Word2Vec με TensorFlow](CBoW-TF.ipynb)
* [Εκπαίδευση CBoW Word2Vec με PyTorch](CBoW-PyTorch.ipynb)

## Συμπέρασμα

Στο προηγούμενο μάθημα είδαμε ότι οι ενσωματώσεις λέξεων λειτουργούν σαν μαγεία! Τώρα γνωρίζουμε ότι η εκπαίδευση ενσωματώσεων λέξεων δεν είναι μια πολύπλοκη διαδικασία, και θα πρέπει να μπορούμε να εκπαιδεύσουμε τις δικές μας ενσωματώσεις λέξεων για κείμενα συγκεκριμένου τομέα, αν χρειαστεί.

## [Μετα-διάλεξης κουίζ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Ανασκόπηση & Αυτομελέτη

* [Επίσημο σεμινάριο PyTorch για Μοντελοποίηση Γλώσσας](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Επίσημο σεμινάριο TensorFlow για εκπαίδευση μοντέλου Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Η χρήση του πλαισίου **gensim** για την εκπαίδευση των πιο συχνά χρησιμοποιούμενων ενσωματώσεων με λίγες γραμμές κώδικα περιγράφεται [σε αυτή την τεκμηρίωση](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## 🚀 [Εργασία: Εκπαίδευση Μοντέλου Skip-Gram](lab/README.md)

Στο εργαστήριο, σας προκαλούμε να τροποποιήσετε τον κώδικα από αυτό το μάθημα για να εκπαιδεύσετε ένα μοντέλο skip-gram αντί για CBoW. [Διαβάστε τις λεπτομέρειες](lab/README.md)

---

**Αποποίηση Ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε κάθε προσπάθεια για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.