<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-29T09:27:16+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "el"
}
-->
# Αναπαράσταση Κειμένου ως Τανυστές

## [Προ-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Ταξινόμηση Κειμένου

Στο πρώτο μέρος αυτής της ενότητας, θα επικεντρωθούμε στην εργασία της **ταξινόμησης κειμένου**. Θα χρησιμοποιήσουμε το [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) Dataset, το οποίο περιέχει άρθρα ειδήσεων όπως το παρακάτω:

* Κατηγορία: Επιστήμη/Τεχνολογία
* Τίτλος: Ky. Company Wins Grant to Study Peptides (AP)
* Σώμα: AP - Μια εταιρεία που ιδρύθηκε από έναν ερευνητή χημείας στο Πανεπιστήμιο του Louisville κέρδισε μια επιχορήγηση για να αναπτύξει...

Ο στόχος μας θα είναι να ταξινομήσουμε το άρθρο ειδήσεων σε μία από τις κατηγορίες με βάση το κείμενο.

## Αναπαράσταση Κειμένου

Αν θέλουμε να λύσουμε εργασίες Επεξεργασίας Φυσικής Γλώσσας (NLP) με νευρωνικά δίκτυα, χρειαζόμαστε έναν τρόπο να αναπαραστήσουμε το κείμενο ως τανυστές. Οι υπολογιστές ήδη αναπαριστούν τους χαρακτήρες κειμένου ως αριθμούς που αντιστοιχούν σε γραμματοσειρές στην οθόνη σας, χρησιμοποιώντας κωδικοποιήσεις όπως ASCII ή UTF-8.

<img alt="Εικόνα που δείχνει διάγραμμα αντιστοίχισης ενός χαρακτήρα σε αναπαράσταση ASCII και δυαδική" src="images/ascii-character-map.png" width="50%"/>

> [Πηγή εικόνας](https://www.seobility.net/en/wiki/ASCII)

Ως άνθρωποι, κατανοούμε τι **αντιπροσωπεύει** κάθε γράμμα και πώς όλοι οι χαρακτήρες συνδυάζονται για να σχηματίσουν τις λέξεις μιας πρότασης. Ωστόσο, οι υπολογιστές από μόνοι τους δεν έχουν τέτοια κατανόηση, και το νευρωνικό δίκτυο πρέπει να μάθει τη σημασία κατά τη διάρκεια της εκπαίδευσης.

Επομένως, μπορούμε να χρησιμοποιήσουμε διαφορετικές προσεγγίσεις για την αναπαράσταση του κειμένου:

* **Αναπαράσταση σε επίπεδο χαρακτήρα**, όπου αναπαριστούμε το κείμενο αντιμετωπίζοντας κάθε χαρακτήρα ως αριθμό. Δεδομένου ότι έχουμε *C* διαφορετικούς χαρακτήρες στο σώμα κειμένου μας, η λέξη *Hello* θα αναπαριστάται από έναν τανυστή 5x*C*. Κάθε γράμμα θα αντιστοιχεί σε μια στήλη τανυστή σε one-hot κωδικοποίηση.
* **Αναπαράσταση σε επίπεδο λέξης**, όπου δημιουργούμε ένα **λεξιλόγιο** όλων των λέξεων στο κείμενό μας και στη συνέχεια αναπαριστούμε τις λέξεις χρησιμοποιώντας one-hot κωδικοποίηση. Αυτή η προσέγγιση είναι κάπως καλύτερη, επειδή κάθε γράμμα από μόνο του δεν έχει μεγάλη σημασία, και έτσι χρησιμοποιώντας υψηλότερες εννοιολογικές μονάδες - λέξεις - απλοποιούμε την εργασία για το νευρωνικό δίκτυο. Ωστόσο, λόγω του μεγάλου μεγέθους του λεξικού, πρέπει να αντιμετωπίσουμε υψηλής διάστασης αραιούς τανυστές.

Ανεξάρτητα από την αναπαράσταση, πρώτα πρέπει να μετατρέψουμε το κείμενο σε μια ακολουθία από **tokens**, όπου κάθε token είναι είτε ένας χαρακτήρας, μια λέξη, ή μερικές φορές ακόμα και μέρος μιας λέξης. Στη συνέχεια, μετατρέπουμε το token σε αριθμό, συνήθως χρησιμοποιώντας ένα **λεξιλόγιο**, και αυτός ο αριθμός μπορεί να εισαχθεί σε ένα νευρωνικό δίκτυο χρησιμοποιώντας one-hot κωδικοποίηση.

## N-Grams

Στη φυσική γλώσσα, η ακριβής σημασία των λέξεων μπορεί να καθοριστεί μόνο στο πλαίσιο. Για παράδειγμα, οι σημασίες των *νευρωνικό δίκτυο* και *δίκτυο ψαρέματος* είναι εντελώς διαφορετικές. Ένας από τους τρόπους να το λάβουμε υπόψη είναι να χτίσουμε το μοντέλο μας σε ζεύγη λέξεων, θεωρώντας τα ζεύγη λέξεων ως ξεχωριστά tokens λεξιλογίου. Με αυτόν τον τρόπο, η πρόταση *Μου αρέσει να πηγαίνω για ψάρεμα* θα αναπαριστάται από την ακόλουθη ακολουθία tokens: *Μου αρέσει*, *αρέσει να*, *να πηγαίνω*, *πηγαίνω για ψάρεμα*. Το πρόβλημα με αυτή την προσέγγιση είναι ότι το μέγεθος του λεξικού αυξάνεται σημαντικά, και συνδυασμοί όπως *πηγαίνω για ψάρεμα* και *πηγαίνω για ψώνια* αναπαριστώνται από διαφορετικά tokens, τα οποία δεν μοιράζονται καμία εννοιολογική ομοιότητα παρά το ίδιο ρήμα.

Σε ορισμένες περιπτώσεις, μπορούμε να εξετάσουμε τη χρήση tri-grams -- συνδυασμών τριών λέξεων -- επίσης. Έτσι, η προσέγγιση αυτή συχνά ονομάζεται **n-grams**. Επίσης, έχει νόημα να χρησιμοποιούμε n-grams με αναπαράσταση σε επίπεδο χαρακτήρα, όπου τα n-grams θα αντιστοιχούν περίπου σε διαφορετικές συλλαβές.

## Bag-of-Words και TF/IDF

Όταν λύνουμε εργασίες όπως η ταξινόμηση κειμένου, πρέπει να μπορούμε να αναπαραστήσουμε το κείμενο με έναν σταθερού μεγέθους διανύσματος, το οποίο θα χρησιμοποιήσουμε ως είσοδο στον τελικό πυκνό ταξινομητή. Ένας από τους απλούστερους τρόπους να το κάνουμε αυτό είναι να συνδυάσουμε όλες τις ατομικές αναπαραστάσεις λέξεων, π.χ. προσθέτοντάς τες. Αν προσθέσουμε τις one-hot κωδικοποιήσεις κάθε λέξης, θα καταλήξουμε με ένα διάνυσμα συχνοτήτων, που δείχνει πόσες φορές εμφανίζεται κάθε λέξη μέσα στο κείμενο. Αυτή η αναπαράσταση του κειμένου ονομάζεται **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Εικόνα από τον συγγραφέα

Ένα BoW ουσιαστικά αναπαριστά ποιες λέξεις εμφανίζονται στο κείμενο και σε ποιες ποσότητες, κάτι που μπορεί πράγματι να είναι μια καλή ένδειξη για το περιεχόμενο του κειμένου. Για παράδειγμα, ένα άρθρο ειδήσεων για την πολιτική είναι πιθανό να περιέχει λέξεις όπως *πρόεδρος* και *χώρα*, ενώ μια επιστημονική δημοσίευση θα έχει λέξεις όπως *επιταχυντής*, *ανακαλύφθηκε*, κ.λπ. Έτσι, οι συχνότητες λέξεων μπορούν σε πολλές περιπτώσεις να είναι ένας καλός δείκτης του περιεχομένου του κειμένου.

Το πρόβλημα με το BoW είναι ότι ορισμένες κοινές λέξεις, όπως *και*, *είναι*, κ.λπ., εμφανίζονται στα περισσότερα κείμενα και έχουν τις υψηλότερες συχνότητες, καλύπτοντας τις λέξεις που είναι πραγματικά σημαντικές. Μπορούμε να μειώσουμε τη σημασία αυτών των λέξεων λαμβάνοντας υπόψη τη συχνότητα με την οποία εμφανίζονται σε ολόκληρη τη συλλογή εγγράφων. Αυτή είναι η βασική ιδέα πίσω από την προσέγγιση TF/IDF, η οποία καλύπτεται με περισσότερες λεπτομέρειες στα σημειωματάρια που συνοδεύουν αυτό το μάθημα.

Ωστόσο, καμία από αυτές τις προσεγγίσεις δεν μπορεί να λάβει πλήρως υπόψη τη **σημασιολογία** του κειμένου. Χρειαζόμαστε πιο ισχυρά μοντέλα νευρωνικών δικτύων για να το κάνουμε αυτό, τα οποία θα συζητήσουμε αργότερα σε αυτή την ενότητα.

## ✍️ Ασκήσεις: Αναπαράσταση Κειμένου

Συνεχίστε τη μάθησή σας στα παρακάτω σημειωματάρια:

* [Αναπαράσταση Κειμένου με PyTorch](TextRepresentationPyTorch.ipynb)
* [Αναπαράσταση Κειμένου με TensorFlow](TextRepresentationTF.ipynb)

## Συμπέρασμα

Μέχρι στιγμής, έχουμε μελετήσει τεχνικές που μπορούν να προσθέσουν βάρος συχνότητας σε διαφορετικές λέξεις. Ωστόσο, αυτές δεν μπορούν να αναπαραστήσουν τη σημασία ή τη σειρά. Όπως είπε ο διάσημος γλωσσολόγος J. R. Firth το 1935, "Η πλήρης σημασία μιας λέξης είναι πάντα συμφραζόμενη, και καμία μελέτη της σημασίας εκτός συμφραζομένων δεν μπορεί να ληφθεί σοβαρά." Θα μάθουμε αργότερα στο μάθημα πώς να αποτυπώνουμε συμφραζόμενες πληροφορίες από το κείμενο χρησιμοποιώντας μοντελοποίηση γλώσσας.

## 🚀 Πρόκληση

Δοκιμάστε κάποιες άλλες ασκήσεις χρησιμοποιώντας bag-of-words και διαφορετικά μοντέλα δεδομένων. Ίσως εμπνευστείτε από αυτόν τον [διαγωνισμό στο Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Μετα-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Ανασκόπηση & Αυτομελέτη

Εξασκηθείτε στις δεξιότητές σας με τεχνικές ενσωμάτωσης κειμένου και bag-of-words στο [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Εργασία: Σημειωματάρια](assignment.md)

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε κάθε προσπάθεια για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.