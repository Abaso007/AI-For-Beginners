<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-29T09:25:58+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "el"
}
-->
# Ενσωματώσεις

## [Προ-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Όταν εκπαιδεύαμε ταξινομητές βασισμένους σε BoW ή TF/IDF, δουλεύαμε με διανύσματα υψηλής διάστασης τύπου bag-of-words με μήκος `vocab_size`, και μετατρέπαμε ρητά από διανύσματα χαμηλής διάστασης σε αραιές αναπαραστάσεις one-hot. Ωστόσο, αυτή η αναπαράσταση one-hot δεν είναι αποδοτική από άποψη μνήμης. Επιπλέον, κάθε λέξη αντιμετωπίζεται ανεξάρτητα από τις άλλες, δηλαδή τα διανύσματα one-hot δεν εκφράζουν καμία σημασιολογική ομοιότητα μεταξύ των λέξεων.

Η ιδέα της **ενσωμάτωσης** (embedding) είναι να αναπαραστήσουμε τις λέξεις με διανύσματα χαμηλότερης διάστασης, τα οποία με κάποιο τρόπο αντικατοπτρίζουν τη σημασιολογική έννοια μιας λέξης. Αργότερα θα συζητήσουμε πώς να δημιουργήσουμε ουσιαστικές ενσωματώσεις λέξεων, αλλά προς το παρόν ας σκεφτούμε τις ενσωματώσεις ως έναν τρόπο μείωσης της διάστασης ενός διανύσματος λέξης.

Έτσι, το επίπεδο ενσωμάτωσης θα λαμβάνει μια λέξη ως είσοδο και θα παράγει ένα διανυσματικό αποτέλεσμα με καθορισμένο μέγεθος `embedding_size`. Με μια έννοια, είναι πολύ παρόμοιο με ένα επίπεδο `Linear`, αλλά αντί να λαμβάνει ένα διάνυσμα one-hot, θα μπορεί να λαμβάνει έναν αριθμό λέξης ως είσοδο, επιτρέποντάς μας να αποφύγουμε τη δημιουργία μεγάλων διανυσμάτων one-hot.

Χρησιμοποιώντας ένα επίπεδο ενσωμάτωσης ως πρώτο επίπεδο στο δίκτυο ταξινόμησής μας, μπορούμε να μεταβούμε από ένα μοντέλο bag-of-words σε ένα μοντέλο **embedding bag**, όπου πρώτα μετατρέπουμε κάθε λέξη στο κείμενό μας στην αντίστοιχη ενσωμάτωσή της και στη συνέχεια υπολογίζουμε κάποια συνάρτηση συσσωμάτωσης πάνω σε όλες αυτές τις ενσωματώσεις, όπως `sum`, `average` ή `max`.

![Εικόνα που δείχνει έναν ταξινομητή ενσωμάτωσης για πέντε λέξεις ακολουθίας.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.el.png)

> Εικόνα από τον συγγραφέα

## ✍️ Ασκήσεις: Ενσωματώσεις

Συνεχίστε τη μάθησή σας στα παρακάτω notebooks:
* [Ενσωματώσεις με PyTorch](EmbeddingsPyTorch.ipynb)
* [Ενσωματώσεις με TensorFlow](EmbeddingsTF.ipynb)

## Σημασιολογικές Ενσωματώσεις: Word2Vec

Ενώ το επίπεδο ενσωμάτωσης έμαθε να αντιστοιχεί λέξεις σε διανυσματική αναπαράσταση, αυτή η αναπαράσταση δεν είχε απαραίτητα μεγάλη σημασιολογική αξία. Θα ήταν χρήσιμο να μάθουμε μια διανυσματική αναπαράσταση τέτοια ώστε παρόμοιες λέξεις ή συνώνυμα να αντιστοιχούν σε διανύσματα που είναι κοντά μεταξύ τους με βάση κάποια διανυσματική απόσταση (π.χ. Ευκλείδεια απόσταση).

Για να το πετύχουμε αυτό, πρέπει να προεκπαιδεύσουμε το μοντέλο ενσωμάτωσης σε μια μεγάλη συλλογή κειμένου με συγκεκριμένο τρόπο. Ένας τρόπος για να εκπαιδεύσουμε σημασιολογικές ενσωματώσεις είναι το [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Βασίζεται σε δύο κύριες αρχιτεκτονικές που χρησιμοποιούνται για την παραγωγή κατανεμημένης αναπαράστασης λέξεων:

 - **Συνεχές bag-of-words** (CBoW) — σε αυτή την αρχιτεκτονική, εκπαιδεύουμε το μοντέλο να προβλέπει μια λέξη από το περιβάλλον της. Δεδομένου του ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, ο στόχος του μοντέλου είναι να προβλέψει το $W_0$ από το $(W_{-2},W_{-1},W_1,W_2)$.
 - **Συνεχές skip-gram** — είναι το αντίθετο του CBoW. Το μοντέλο χρησιμοποιεί το παράθυρο των λέξεων περιβάλλοντος για να προβλέψει την τρέχουσα λέξη.

Το CBoW είναι ταχύτερο, ενώ το skip-gram είναι πιο αργό, αλλά αποδίδει καλύτερα στις σπάνιες λέξεις.

![Εικόνα που δείχνει τους αλγορίθμους CBoW και Skip-Gram για τη μετατροπή λέξεων σε διανύσματα.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.el.png)

> Εικόνα από [αυτό το άρθρο](https://arxiv.org/pdf/1301.3781.pdf)

Οι προεκπαιδευμένες ενσωματώσεις Word2Vec (όπως και άλλα παρόμοια μοντέλα, όπως το GloVe) μπορούν επίσης να χρησιμοποιηθούν αντί για επίπεδο ενσωμάτωσης σε νευρωνικά δίκτυα. Ωστόσο, πρέπει να διαχειριστούμε τα λεξιλόγια, επειδή το λεξιλόγιο που χρησιμοποιήθηκε για την προεκπαίδευση του Word2Vec/GloVe πιθανότατα διαφέρει από το λεξιλόγιο στο κείμενό μας. Ρίξτε μια ματιά στα παραπάνω Notebooks για να δείτε πώς μπορεί να λυθεί αυτό το πρόβλημα.

## Ενσωματώσεις με Πλαίσιο

Ένας βασικός περιορισμός των παραδοσιακών προεκπαιδευμένων αναπαραστάσεων ενσωμάτωσης όπως το Word2Vec είναι το πρόβλημα της αποσαφήνισης της σημασίας των λέξεων. Ενώ οι προεκπαιδευμένες ενσωματώσεις μπορούν να αποτυπώσουν κάποια σημασία των λέξεων στο πλαίσιο, κάθε πιθανή σημασία μιας λέξης κωδικοποιείται στην ίδια ενσωμάτωση. Αυτό μπορεί να προκαλέσει προβλήματα σε μεταγενέστερα μοντέλα, καθώς πολλές λέξεις, όπως η λέξη "play", έχουν διαφορετικές σημασίες ανάλογα με το πλαίσιο στο οποίο χρησιμοποιούνται.

Για παράδειγμα, η λέξη "play" στις παρακάτω προτάσεις έχει αρκετά διαφορετική σημασία:

- Πήγα σε μια **παράσταση** στο θέατρο.
- Ο Γιάννης θέλει να **παίξει** με τους φίλους του.

Οι παραπάνω προεκπαιδευμένες ενσωματώσεις αναπαριστούν και τις δύο αυτές σημασίες της λέξης "play" στην ίδια ενσωμάτωση. Για να ξεπεράσουμε αυτόν τον περιορισμό, πρέπει να δημιουργήσουμε ενσωματώσεις βασισμένες στο **γλωσσικό μοντέλο**, το οποίο εκπαιδεύεται σε ένα μεγάλο σώμα κειμένου και "γνωρίζει" πώς οι λέξεις μπορούν να συνδυαστούν σε διαφορετικά πλαίσια. Η συζήτηση για τις ενσωματώσεις με πλαίσιο είναι εκτός του πεδίου αυτού του μαθήματος, αλλά θα επανέλθουμε σε αυτές όταν μιλήσουμε για τα γλωσσικά μοντέλα αργότερα στο μάθημα.

## Συμπέρασμα

Σε αυτό το μάθημα, ανακαλύψατε πώς να δημιουργείτε και να χρησιμοποιείτε επίπεδα ενσωμάτωσης στο TensorFlow και το Pytorch για να αποτυπώνετε καλύτερα τις σημασιολογικές έννοιες των λέξεων.

## 🚀 Πρόκληση

Το Word2Vec έχει χρησιμοποιηθεί για μερικές ενδιαφέρουσες εφαρμογές, όπως η δημιουργία στίχων τραγουδιών και ποίησης. Ρίξτε μια ματιά σε [αυτό το άρθρο](https://www.politetype.com/blog/word2vec-color-poems) που εξηγεί πώς ο συγγραφέας χρησιμοποίησε το Word2Vec για να δημιουργήσει ποίηση. Δείτε επίσης [αυτό το βίντεο του Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) για μια διαφορετική εξήγηση αυτής της τεχνικής. Στη συνέχεια, προσπαθήστε να εφαρμόσετε αυτές τις τεχνικές στο δικό σας σώμα κειμένου, ίσως από δεδομένα που θα βρείτε στο Kaggle.

## [Μετά-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Ανασκόπηση & Αυτομελέτη

Διαβάστε αυτό το άρθρο για το Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Εργασία: Notebooks](assignment.md)

---

**Αποποίηση Ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.