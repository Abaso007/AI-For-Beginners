<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-29T09:21:26+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "el"
}
-->
# Προεκπαιδευμένα Μεγάλα Γλωσσικά Μοντέλα

Σε όλες τις προηγούμενες εργασίες μας, εκπαιδεύαμε ένα νευρωνικό δίκτυο για να εκτελέσει μια συγκεκριμένη εργασία χρησιμοποιώντας ένα επισημασμένο σύνολο δεδομένων. Με τα μεγάλα μοντέλα μετασχηματιστών, όπως το BERT, χρησιμοποιούμε τη μοντελοποίηση γλώσσας με αυτοεπιβλεπόμενο τρόπο για να δημιουργήσουμε ένα γλωσσικό μοντέλο, το οποίο στη συνέχεια εξειδικεύεται για συγκεκριμένες εργασίες μέσω περαιτέρω εκπαίδευσης σε δεδομένα συγκεκριμένου τομέα. Ωστόσο, έχει αποδειχθεί ότι τα μεγάλα γλωσσικά μοντέλα μπορούν επίσης να λύσουν πολλές εργασίες χωρίς ΚΑΜΙΑ εκπαίδευση σε δεδομένα συγκεκριμένου τομέα. Μια οικογένεια μοντέλων που μπορεί να το κάνει αυτό ονομάζεται **GPT**: Γενετικός Προεκπαιδευμένος Μετασχηματιστής.

## [Κουίζ πριν τη διάλεξη](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## Δημιουργία Κειμένου και Περιπλοκότητα

Η ιδέα ενός νευρωνικού δικτύου που μπορεί να εκτελεί γενικές εργασίες χωρίς εκπαίδευση για συγκεκριμένες εργασίες παρουσιάζεται στο άρθρο [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Η βασική ιδέα είναι ότι πολλές άλλες εργασίες μπορούν να μοντελοποιηθούν μέσω της **δημιουργίας κειμένου**, επειδή η κατανόηση του κειμένου ουσιαστικά σημαίνει την ικανότητα παραγωγής του. Επειδή το μοντέλο εκπαιδεύεται σε τεράστιο όγκο κειμένου που περιλαμβάνει ανθρώπινη γνώση, αποκτά επίσης γνώσεις για μια μεγάλη ποικιλία θεμάτων.

> Η κατανόηση και η ικανότητα παραγωγής κειμένου συνεπάγεται επίσης τη γνώση του κόσμου γύρω μας. Οι άνθρωποι μαθαίνουν επίσης σε μεγάλο βαθμό διαβάζοντας, και το δίκτυο GPT είναι παρόμοιο από αυτή την άποψη.

Τα δίκτυα δημιουργίας κειμένου λειτουργούν προβλέποντας την πιθανότητα της επόμενης λέξης $$P(w_N)$$. Ωστόσο, η μη εξαρτημένη πιθανότητα της επόμενης λέξης ισούται με τη συχνότητα αυτής της λέξης στο σύνολο κειμένου. Το GPT μπορεί να μας δώσει την **εξαρτημένη πιθανότητα** της επόμενης λέξης, δεδομένων των προηγούμενων: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Μπορείτε να διαβάσετε περισσότερα για τις πιθανότητες στο [Πρόγραμμα Σπουδών Επιστήμης Δεδομένων για Αρχάριους](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Η ποιότητα ενός μοντέλου δημιουργίας γλώσσας μπορεί να οριστεί χρησιμοποιώντας την **περιπλοκότητα**. Είναι μια εσωτερική μέτρηση που μας επιτρέπει να αξιολογήσουμε την ποιότητα του μοντέλου χωρίς να χρειάζεται σύνολο δεδομένων για συγκεκριμένη εργασία. Βασίζεται στην έννοια της *πιθανότητας μιας πρότασης* - το μοντέλο αποδίδει υψηλή πιθανότητα σε μια πρόταση που είναι πιθανό να είναι πραγματική (δηλαδή το μοντέλο δεν είναι **περίπλοκο** από αυτήν) και χαμηλή πιθανότητα σε προτάσεις που έχουν λιγότερο νόημα (π.χ. *Μπορεί να κάνει τι;*). Όταν δίνουμε στο μοντέλο μας προτάσεις από ένα πραγματικό σύνολο κειμένου, θα περιμέναμε να έχουν υψηλή πιθανότητα και χαμηλή **περιπλοκότητα**. Μαθηματικά, ορίζεται ως κανονικοποιημένη αντίστροφη πιθανότητα του συνόλου δοκιμής:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Μπορείτε να πειραματιστείτε με τη δημιουργία κειμένου χρησιμοποιώντας τον [επεξεργαστή κειμένου με GPT από το Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. Σε αυτόν τον επεξεργαστή, ξεκινάτε να γράφετε το κείμενό σας και πατώντας **[TAB]** θα σας προσφερθούν διάφορες επιλογές ολοκλήρωσης. Αν είναι πολύ σύντομες ή δεν σας ικανοποιούν, πατήστε ξανά [TAB] για περισσότερες επιλογές, συμπεριλαμβανομένων μεγαλύτερων κομματιών κειμένου.

## Το GPT είναι μια Οικογένεια

Το GPT δεν είναι ένα μόνο μοντέλο, αλλά μια συλλογή μοντέλων που έχουν αναπτυχθεί και εκπαιδευτεί από την [OpenAI](https://openai.com).

Στην οικογένεια των μοντέλων GPT περιλαμβάνονται:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| Γλωσσικό μοντέλο με έως 1,5 δισεκατομμύρια παραμέτρους. | Γλωσσικό μοντέλο με έως 175 δισεκατομμύρια παραμέτρους. | 100 τρισεκατομμύρια παράμετροι που δέχεται τόσο εικόνες όσο και κείμενο ως εισόδους και παράγει κείμενο. |

Τα μοντέλα GPT-3 και GPT-4 είναι διαθέσιμα [ως υπηρεσία γνωστικής υπολογιστικής από το Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) και μέσω του [OpenAI API](https://openai.com/api/).

## Μηχανική Ερωτημάτων (Prompt Engineering)

Επειδή το GPT έχει εκπαιδευτεί σε τεράστιους όγκους δεδομένων για να κατανοεί τη γλώσσα και τον κώδικα, παρέχει εξόδους σε απάντηση σε εισόδους (ερωτήματα). Τα ερωτήματα είναι οι είσοδοι ή οι ερωτήσεις προς το GPT, όπου παρέχονται οδηγίες στα μοντέλα για τις εργασίες που πρέπει να ολοκληρώσουν. Για να επιτύχετε το επιθυμητό αποτέλεσμα, χρειάζεστε το πιο αποτελεσματικό ερώτημα, το οποίο περιλαμβάνει την επιλογή των κατάλληλων λέξεων, μορφών, φράσεων ή ακόμα και συμβόλων. Αυτή η προσέγγιση ονομάζεται [Μηχανική Ερωτημάτων](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Αυτή η τεκμηρίωση](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) σας παρέχει περισσότερες πληροφορίες για τη μηχανική ερωτημάτων.

## ✍️ Παράδειγμα Σημειωματάριου: [Πειραματισμός με το OpenAI-GPT](GPT-PyTorch.ipynb)

Συνεχίστε τη μάθησή σας στα παρακάτω σημειωματάρια:

* [Δημιουργία κειμένου με το OpenAI-GPT και τους Hugging Face Transformers](GPT-PyTorch.ipynb)

## Συμπέρασμα

Τα νέα γενικά προεκπαιδευμένα γλωσσικά μοντέλα δεν μοντελοποιούν μόνο τη δομή της γλώσσας, αλλά περιέχουν και τεράστιο όγκο φυσικής γλώσσας. Έτσι, μπορούν να χρησιμοποιηθούν αποτελεσματικά για την επίλυση ορισμένων εργασιών Επεξεργασίας Φυσικής Γλώσσας (NLP) σε συνθήκες μηδενικής ή ελάχιστης εκπαίδευσης.

## [Κουίζ μετά τη διάλεξη](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.