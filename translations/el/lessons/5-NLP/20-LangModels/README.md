<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T08:59:18+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "el"
}
-->
# Προεκπαιδευμένα Μεγάλα Γλωσσικά Μοντέλα

Σε όλες τις προηγούμενες εργασίες μας, εκπαιδεύαμε ένα νευρωνικό δίκτυο για να εκτελέσει μια συγκεκριμένη εργασία χρησιμοποιώντας ένα σύνολο δεδομένων με ετικέτες. Με τα μεγάλα μοντέλα μετασχηματιστών, όπως το BERT, χρησιμοποιούμε τη μοντελοποίηση γλώσσας με αυτοεπιβλεπόμενο τρόπο για να δημιουργήσουμε ένα γλωσσικό μοντέλο, το οποίο στη συνέχεια εξειδικεύεται για συγκεκριμένες εργασίες μέσω περαιτέρω εκπαίδευσης σε δεδομένα συγκεκριμένου τομέα. Ωστόσο, έχει αποδειχθεί ότι τα μεγάλα γλωσσικά μοντέλα μπορούν επίσης να λύσουν πολλές εργασίες χωρίς ΚΑΜΙΑ εκπαίδευση συγκεκριμένου τομέα. Μια οικογένεια μοντέλων που μπορεί να το κάνει αυτό ονομάζεται **GPT**: Generative Pre-Trained Transformer.

## [Κουίζ πριν τη διάλεξη](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## Δημιουργία Κειμένου και Περιπλοκότητα

Η ιδέα ενός νευρωνικού δικτύου που μπορεί να εκτελεί γενικές εργασίες χωρίς εκπαίδευση για συγκεκριμένες εργασίες παρουσιάζεται στο άρθρο [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Η βασική ιδέα είναι ότι πολλές άλλες εργασίες μπορούν να μοντελοποιηθούν μέσω της **δημιουργίας κειμένου**, επειδή η κατανόηση του κειμένου ουσιαστικά σημαίνει την ικανότητα παραγωγής του. Επειδή το μοντέλο εκπαιδεύεται σε τεράστιο όγκο κειμένου που περιλαμβάνει ανθρώπινη γνώση, αποκτά επίσης γνώσεις για μια μεγάλη ποικιλία θεμάτων.

> Η κατανόηση και η ικανότητα παραγωγής κειμένου συνεπάγεται επίσης τη γνώση του κόσμου γύρω μας. Οι άνθρωποι μαθαίνουν σε μεγάλο βαθμό διαβάζοντας, και το δίκτυο GPT είναι παρόμοιο από αυτή την άποψη.

Τα δίκτυα δημιουργίας κειμένου λειτουργούν προβλέποντας την πιθανότητα της επόμενης λέξης $$P(w_N)$$. Ωστόσο, η μη εξαρτώμενη πιθανότητα της επόμενης λέξης ισούται με τη συχνότητα αυτής της λέξης στο σύνολο κειμένου. Το GPT μπορεί να μας δώσει την **εξαρτώμενη πιθανότητα** της επόμενης λέξης, δεδομένων των προηγούμενων: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Μπορείτε να διαβάσετε περισσότερα για τις πιθανότητες στο [Πρόγραμμα Σπουδών Επιστήμης Δεδομένων για Αρχάριους](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Η ποιότητα ενός μοντέλου δημιουργίας γλώσσας μπορεί να οριστεί χρησιμοποιώντας την **περιπλοκότητα**. Είναι μια εσωτερική μέτρηση που μας επιτρέπει να αξιολογήσουμε την ποιότητα του μοντέλου χωρίς κάποιο σύνολο δεδομένων συγκεκριμένης εργασίας. Βασίζεται στην έννοια της *πιθανότητας μιας πρότασης* - το μοντέλο αποδίδει υψηλή πιθανότητα σε μια πρόταση που είναι πιθανό να είναι πραγματική (δηλαδή το μοντέλο δεν είναι **περίπλοκο** από αυτήν) και χαμηλή πιθανότητα σε προτάσεις που έχουν λιγότερο νόημα (π.χ. *Μπορεί να κάνει τι;*). Όταν δίνουμε στο μοντέλο μας προτάσεις από πραγματικό σύνολο κειμένου, αναμένουμε να έχουν υψηλή πιθανότητα και χαμηλή **περιπλοκότητα**. Μαθηματικά, ορίζεται ως κανονικοποιημένη αντίστροφη πιθανότητα του συνόλου δοκιμής:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Μπορείτε να πειραματιστείτε με τη δημιουργία κειμένου χρησιμοποιώντας τον [επεξεργαστή κειμένου με GPT από το Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. Σε αυτόν τον επεξεργαστή, ξεκινάτε να γράφετε το κείμενό σας και πατώντας **[TAB]** θα σας προσφερθούν διάφορες επιλογές ολοκλήρωσης. Αν είναι πολύ σύντομες ή δεν σας ικανοποιούν, πατήστε ξανά [TAB] και θα έχετε περισσότερες επιλογές, συμπεριλαμβανομένων μεγαλύτερων κομματιών κειμένου.

## Το GPT είναι μια Οικογένεια

Το GPT δεν είναι ένα μόνο μοντέλο, αλλά μια συλλογή μοντέλων που αναπτύχθηκαν και εκπαιδεύτηκαν από την [OpenAI](https://openai.com).

Στην οικογένεια των μοντέλων GPT περιλαμβάνονται:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|Γλωσσικό μοντέλο με έως 1,5 δισεκατομμύρια παραμέτρους. | Γλωσσικό μοντέλο με έως 175 δισεκατομμύρια παραμέτρους. | 100 τρισεκατομμύρια παράμετροι που δέχεται τόσο εικόνες όσο και κείμενο ως εισόδους και παράγει κείμενο. |

Τα μοντέλα GPT-3 και GPT-4 είναι διαθέσιμα [ως υπηρεσία γνωστικής υπολογιστικής από το Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) και μέσω του [OpenAI API](https://openai.com/api/).

## Μηχανική Ερωτημάτων (Prompt Engineering)

Επειδή το GPT έχει εκπαιδευτεί σε τεράστιους όγκους δεδομένων για να κατανοεί τη γλώσσα και τον κώδικα, παρέχει εξόδους σε απάντηση σε εισόδους (ερωτήματα). Τα ερωτήματα είναι οι είσοδοι ή οι ερωτήσεις προς το GPT, όπου παρέχονται οδηγίες στα μοντέλα για τις εργασίες που πρέπει να ολοκληρώσουν. Για να επιτύχετε το επιθυμητό αποτέλεσμα, χρειάζεστε το πιο αποτελεσματικό ερώτημα, το οποίο περιλαμβάνει την επιλογή των κατάλληλων λέξεων, μορφών, φράσεων ή ακόμα και συμβόλων. Αυτή η προσέγγιση ονομάζεται [Μηχανική Ερωτημάτων](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Αυτή η τεκμηρίωση](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) σας παρέχει περισσότερες πληροφορίες για τη μηχανική ερωτημάτων.

## ✍️ Παράδειγμα Σημειωματάριου: [Πειραματισμός με το OpenAI-GPT](GPT-PyTorch.ipynb)

Συνεχίστε τη μάθησή σας στα παρακάτω σημειωματάρια:

* [Δημιουργία κειμένου με το OpenAI-GPT και τους Hugging Face Transformers](GPT-PyTorch.ipynb)

## Συμπέρασμα

Τα νέα γενικά προεκπαιδευμένα γλωσσικά μοντέλα δεν μοντελοποιούν μόνο τη δομή της γλώσσας, αλλά περιέχουν και τεράστιο όγκο φυσικής γλώσσας. Έτσι, μπορούν να χρησιμοποιηθούν αποτελεσματικά για την επίλυση ορισμένων εργασιών επεξεργασίας φυσικής γλώσσας σε συνθήκες μηδενικής ή ελάχιστης εκπαίδευσης.

## [Κουίζ μετά τη διάλεξη](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

