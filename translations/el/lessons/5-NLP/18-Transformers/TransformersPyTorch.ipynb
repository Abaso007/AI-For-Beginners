{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Μηχανισμοί προσοχής και μετασχηματιστές\n",
    "\n",
    "Ένα σημαντικό μειονέκτημα των επαναλαμβανόμενων δικτύων είναι ότι όλες οι λέξεις σε μια ακολουθία έχουν την ίδια επίδραση στο αποτέλεσμα. Αυτό προκαλεί υποβέλτιστη απόδοση με τα τυπικά μοντέλα LSTM encoder-decoder για εργασίες ακολουθίας προς ακολουθία, όπως η Αναγνώριση Οντοτήτων και η Μηχανική Μετάφραση. Στην πραγματικότητα, συγκεκριμένες λέξεις στην είσοδο συχνά έχουν μεγαλύτερη επίδραση στα αποτελέσματα από άλλες.\n",
    "\n",
    "Ας εξετάσουμε ένα μοντέλο ακολουθίας προς ακολουθία, όπως η μηχανική μετάφραση. Αυτό υλοποιείται από δύο επαναλαμβανόμενα δίκτυα, όπου το ένα δίκτυο (**encoder**) συμπυκνώνει την είσοδο σε μια κρυφή κατάσταση, και το άλλο (**decoder**) ξεδιπλώνει αυτή την κρυφή κατάσταση στο μεταφρασμένο αποτέλεσμα. Το πρόβλημα με αυτή την προσέγγιση είναι ότι η τελική κατάσταση του δικτύου δυσκολεύεται να θυμηθεί την αρχή της πρότασης, προκαλώντας έτσι χαμηλή ποιότητα του μοντέλου σε μεγάλες προτάσεις.\n",
    "\n",
    "**Οι Μηχανισμοί Προσοχής** παρέχουν έναν τρόπο να δίνεται βάρος στην επίδραση κάθε εισόδου στην πρόβλεψη κάθε εξόδου του RNN. Αυτό υλοποιείται δημιουργώντας συντομεύσεις μεταξύ των ενδιάμεσων καταστάσεων του εισόδου RNN και του εξόδου RNN. Με αυτόν τον τρόπο, κατά τη δημιουργία του συμβόλου εξόδου $y_t$, λαμβάνουμε υπόψη όλες τις κρυφές καταστάσεις εισόδου $h_i$, με διαφορετικούς συντελεστές βάρους $\\alpha_{t,i}$.\n",
    "\n",
    "![Εικόνα που δείχνει ένα μοντέλο encoder/decoder με ένα πρόσθετο στρώμα προσοχής](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.el.png)\n",
    "*Το μοντέλο encoder-decoder με μηχανισμό πρόσθετης προσοχής στο [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), από [αυτή την ανάρτηση](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Ο πίνακας προσοχής $\\{\\alpha_{i,j}\\}$ αντιπροσωπεύει τον βαθμό στον οποίο συγκεκριμένες λέξεις εισόδου επηρεάζουν τη δημιουργία μιας δεδομένης λέξης στην ακολουθία εξόδου. Παρακάτω είναι ένα παράδειγμα ενός τέτοιου πίνακα:\n",
    "\n",
    "![Εικόνα που δείχνει ένα δείγμα ευθυγράμμισης που βρέθηκε από το RNNsearch-50, από το Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.el.png)\n",
    "\n",
    "*Εικόνα από [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Οι μηχανισμοί προσοχής είναι υπεύθυνοι για μεγάλο μέρος της τρέχουσας ή σχεδόν τρέχουσας κατάστασης της τέχνης στην Επεξεργασία Φυσικής Γλώσσας. Ωστόσο, η προσθήκη προσοχής αυξάνει σημαντικά τον αριθμό των παραμέτρων του μοντέλου, γεγονός που οδήγησε σε προβλήματα κλιμάκωσης με τα RNNs. Ένας βασικός περιορισμός της κλιμάκωσης των RNNs είναι ότι η επαναλαμβανόμενη φύση των μοντέλων καθιστά δύσκολη την ομαδοποίηση και την παράλληλη εκπαίδευση. Σε ένα RNN κάθε στοιχείο μιας ακολουθίας πρέπει να επεξεργαστεί με τη σειρά, πράγμα που σημαίνει ότι δεν μπορεί να παραλληλιστεί εύκολα.\n",
    "\n",
    "Η υιοθέτηση μηχανισμών προσοχής σε συνδυασμό με αυτόν τον περιορισμό οδήγησε στη δημιουργία των πλέον κορυφαίων μοντέλων μετασχηματιστών που γνωρίζουμε και χρησιμοποιούμε σήμερα, από το BERT έως το OpenGPT3.\n",
    "\n",
    "## Μοντέλα μετασχηματιστών\n",
    "\n",
    "Αντί να προωθούν το πλαίσιο κάθε προηγούμενης πρόβλεψης στο επόμενο βήμα αξιολόγησης, τα **μοντέλα μετασχηματιστών** χρησιμοποιούν **κωδικοποιήσεις θέσης** και προσοχή για να συλλάβουν το πλαίσιο μιας δεδομένης εισόδου μέσα σε ένα παρεχόμενο παράθυρο κειμένου. Η παρακάτω εικόνα δείχνει πώς οι κωδικοποιήσεις θέσης με προσοχή μπορούν να συλλάβουν το πλαίσιο μέσα σε ένα δεδομένο παράθυρο.\n",
    "\n",
    "![Κινούμενο GIF που δείχνει πώς πραγματοποιούνται οι αξιολογήσεις στα μοντέλα μετασχηματιστών.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Επειδή κάθε θέση εισόδου αντιστοιχίζεται ανεξάρτητα σε κάθε θέση εξόδου, οι μετασχηματιστές μπορούν να παραλληλίζονται καλύτερα από τα RNNs, γεγονός που επιτρέπει πολύ μεγαλύτερα και πιο εκφραστικά γλωσσικά μοντέλα. Κάθε κεφαλή προσοχής μπορεί να χρησιμοποιηθεί για να μάθει διαφορετικές σχέσεις μεταξύ λέξεων, βελτιώνοντας τις εργασίες Επεξεργασίας Φυσικής Γλώσσας.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) είναι ένα πολύ μεγάλο πολυεπίπεδο δίκτυο μετασχηματιστών με 12 επίπεδα για το *BERT-base* και 24 για το *BERT-large*. Το μοντέλο εκπαιδεύεται αρχικά σε μεγάλο σώμα δεδομένων κειμένου (WikiPedia + βιβλία) χρησιμοποιώντας μη επιβλεπόμενη εκπαίδευση (πρόβλεψη λέξεων που έχουν καλυφθεί σε μια πρόταση). Κατά τη διάρκεια της αρχικής εκπαίδευσης, το μοντέλο απορροφά σημαντικό επίπεδο κατανόησης της γλώσσας, το οποίο μπορεί στη συνέχεια να αξιοποιηθεί με άλλα σύνολα δεδομένων μέσω λεπτομερούς προσαρμογής. Αυτή η διαδικασία ονομάζεται **μεταφορά μάθησης**.\n",
    "\n",
    "![Εικόνα από http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.el.png)\n",
    "\n",
    "Υπάρχουν πολλές παραλλαγές αρχιτεκτονικών μετασχηματιστών, όπως BERT, DistilBERT, BigBird, OpenGPT3 και άλλες, που μπορούν να προσαρμοστούν. Το [πακέτο HuggingFace](https://github.com/huggingface/) παρέχει αποθετήριο για την εκπαίδευση πολλών από αυτές τις αρχιτεκτονικές με PyTorch.\n",
    "\n",
    "## Χρήση του BERT για ταξινόμηση κειμένου\n",
    "\n",
    "Ας δούμε πώς μπορούμε να χρησιμοποιήσουμε το προεκπαιδευμένο μοντέλο BERT για να λύσουμε την παραδοσιακή μας εργασία: ταξινόμηση ακολουθιών. Θα ταξινομήσουμε το αρχικό μας σύνολο δεδομένων AG News. \n",
    "\n",
    "Πρώτα, ας φορτώσουμε τη βιβλιοθήκη HuggingFace και το σύνολο δεδομένων μας:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επειδή θα χρησιμοποιήσουμε ένα προεκπαιδευμένο μοντέλο BERT, θα χρειαστεί να χρησιμοποιήσουμε έναν συγκεκριμένο tokenizer. Πρώτα, θα φορτώσουμε έναν tokenizer που συνδέεται με το προεκπαιδευμένο μοντέλο BERT.\n",
    "\n",
    "Η βιβλιοθήκη HuggingFace περιέχει ένα αποθετήριο προεκπαιδευμένων μοντέλων, τα οποία μπορείτε να χρησιμοποιήσετε απλώς καθορίζοντας τα ονόματά τους ως επιχειρήματα στις συναρτήσεις `from_pretrained`. Όλα τα απαραίτητα δυαδικά αρχεία για το μοντέλο θα κατεβούν αυτόματα.\n",
    "\n",
    "Ωστόσο, σε ορισμένες περιπτώσεις θα χρειαστεί να φορτώσετε τα δικά σας μοντέλα, οπότε μπορείτε να καθορίσετε τον κατάλογο που περιέχει όλα τα σχετικά αρχεία, συμπεριλαμβανομένων των παραμέτρων για τον tokenizer, το αρχείο `config.json` με τις παραμέτρους του μοντέλου, τα δυαδικά βάρη, κ.λπ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το αντικείμενο `tokenizer` περιέχει τη συνάρτηση `encode` που μπορεί να χρησιμοποιηθεί άμεσα για την κωδικοποίηση κειμένου:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια, ας δημιουργήσουμε επαναλήπτες που θα χρησιμοποιήσουμε κατά τη διάρκεια της εκπαίδευσης για να έχουμε πρόσβαση στα δεδομένα. Επειδή το BERT χρησιμοποιεί τη δική του συνάρτηση κωδικοποίησης, θα χρειαστεί να ορίσουμε μια συνάρτηση συμπλήρωσης παρόμοια με τη `padify` που έχουμε ορίσει προηγουμένως:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην περίπτωσή μας, θα χρησιμοποιήσουμε ένα προεκπαιδευμένο μοντέλο BERT που ονομάζεται `bert-base-uncased`. Ας φορτώσουμε το μοντέλο χρησιμοποιώντας το πακέτο `BertForSequenceClassification`. Αυτό διασφαλίζει ότι το μοντέλο μας έχει ήδη την απαιτούμενη αρχιτεκτονική για ταξινόμηση, συμπεριλαμβανομένου του τελικού ταξινομητή. Θα δείτε ένα προειδοποιητικό μήνυμα που αναφέρει ότι τα βάρη του τελικού ταξινομητή δεν έχουν αρχικοποιηθεί και ότι το μοντέλο θα χρειαστεί προεκπαίδευση - αυτό είναι απολύτως εντάξει, γιατί ακριβώς αυτό πρόκειται να κάνουμε!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα είμαστε έτοιμοι να ξεκινήσουμε την εκπαίδευση! Επειδή το BERT είναι ήδη προ-εκπαιδευμένο, θέλουμε να ξεκινήσουμε με σχετικά μικρό ρυθμό εκμάθησης, ώστε να μην καταστρέψουμε τα αρχικά βάρη.\n",
    "\n",
    "Όλη η δύσκολη δουλειά γίνεται από το μοντέλο `BertForSequenceClassification`. Όταν καλούμε το μοντέλο στα δεδομένα εκπαίδευσης, επιστρέφει τόσο την απώλεια (loss) όσο και την έξοδο του δικτύου για το εισαγόμενο minibatch. Χρησιμοποιούμε την απώλεια για τη βελτιστοποίηση των παραμέτρων (το `loss.backward()` εκτελεί το backward pass) και το `out` για τον υπολογισμό της ακρίβειας εκπαίδευσης, συγκρίνοντας τις ετικέτες που προκύπτουν `labs` (υπολογίζονται με χρήση του `argmax`) με τις αναμενόμενες `labels`.\n",
    "\n",
    "Για να ελέγξουμε τη διαδικασία, συσσωρεύουμε την απώλεια και την ακρίβεια σε αρκετές επαναλήψεις και τις εκτυπώνουμε κάθε `report_freq` κύκλους εκπαίδευσης.\n",
    "\n",
    "Αυτή η εκπαίδευση πιθανότατα θα διαρκέσει αρκετό χρόνο, οπότε περιορίζουμε τον αριθμό των επαναλήψεων.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορείτε να δείτε (ειδικά αν αυξήσετε τον αριθμό των επαναλήψεων και περιμένετε αρκετά) ότι η ταξινόμηση με BERT μας δίνει αρκετά καλή ακρίβεια! Αυτό συμβαίνει επειδή το BERT ήδη κατανοεί αρκετά καλά τη δομή της γλώσσας, και το μόνο που χρειάζεται είναι να προσαρμόσουμε τον τελικό ταξινομητή. Ωστόσο, επειδή το BERT είναι ένα μεγάλο μοντέλο, όλη η διαδικασία εκπαίδευσης διαρκεί πολύ χρόνο και απαιτεί σημαντική υπολογιστική ισχύ! (GPU, και κατά προτίμηση περισσότερες από μία).\n",
    "\n",
    "> **Note:** Στο παράδειγμά μας, χρησιμοποιούμε ένα από τα μικρότερα προεκπαιδευμένα μοντέλα BERT. Υπάρχουν μεγαλύτερα μοντέλα που πιθανότατα θα αποδώσουν καλύτερα αποτελέσματα.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Αξιολόγηση της απόδοσης του μοντέλου\n",
    "\n",
    "Τώρα μπορούμε να αξιολογήσουμε την απόδοση του μοντέλου μας στο σύνολο δεδομένων δοκιμής. Ο βρόχος αξιολόγησης είναι αρκετά παρόμοιος με τον βρόχο εκπαίδευσης, αλλά δεν πρέπει να ξεχνάμε να αλλάξουμε το μοντέλο σε λειτουργία αξιολόγησης καλώντας `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συμπέρασμα\n",
    "\n",
    "Σε αυτήν την ενότητα, είδαμε πόσο εύκολο είναι να πάρουμε ένα προεκπαιδευμένο γλωσσικό μοντέλο από τη βιβλιοθήκη **transformers** και να το προσαρμόσουμε στην εργασία ταξινόμησης κειμένου μας. Παρομοίως, τα μοντέλα BERT μπορούν να χρησιμοποιηθούν για εξαγωγή οντοτήτων, απάντηση ερωτήσεων και άλλες εργασίες επεξεργασίας φυσικής γλώσσας.\n",
    "\n",
    "Τα μοντέλα Transformer αντιπροσωπεύουν την πιο σύγχρονη τεχνολογία στην επεξεργασία φυσικής γλώσσας, και στις περισσότερες περιπτώσεις θα πρέπει να είναι η πρώτη λύση με την οποία ξεκινάτε να πειραματίζεστε όταν υλοποιείτε προσαρμοσμένες λύσεις NLP. Ωστόσο, η κατανόηση των βασικών αρχών των επαναλαμβανόμενων νευρωνικών δικτύων που συζητήθηκαν σε αυτήν την ενότητα είναι εξαιρετικά σημαντική αν θέλετε να δημιουργήσετε προηγμένα νευρωνικά μοντέλα.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση Ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε κάθε προσπάθεια για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-29T10:44:06+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}