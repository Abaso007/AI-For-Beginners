<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ae074cd940fc2f4dc24fc07b66ccbd99",
  "translation_date": "2025-08-29T08:48:47+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md",
  "language_code": "el"
}
-->
# Τεχνικές Εκπαίδευσης Βαθιάς Μάθησης

Καθώς τα νευρωνικά δίκτυα γίνονται πιο βαθιά, η διαδικασία της εκπαίδευσής τους γίνεται όλο και πιο απαιτητική. Ένα σημαντικό πρόβλημα είναι τα λεγόμενα [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) ή [exploding gradients](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.). [Αυτό το άρθρο](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) παρέχει μια καλή εισαγωγή σε αυτά τα προβλήματα.

Για να κάνουμε την εκπαίδευση βαθιών δικτύων πιο αποτελεσματική, υπάρχουν μερικές τεχνικές που μπορούμε να χρησιμοποιήσουμε.

## Διατήρηση τιμών σε λογικά όρια

Για να κάνουμε τους αριθμητικούς υπολογισμούς πιο σταθερούς, θέλουμε να διασφαλίσουμε ότι όλες οι τιμές μέσα στο νευρωνικό μας δίκτυο βρίσκονται σε λογική κλίμακα, συνήθως [-1..1] ή [0..1]. Δεν είναι μια πολύ αυστηρή απαίτηση, αλλά η φύση των υπολογισμών κινητής υποδιαστολής είναι τέτοια που τιμές διαφορετικών μεγεθών δεν μπορούν να επεξεργαστούν με ακρίβεια μαζί. Για παράδειγμα, αν προσθέσουμε 10<sup>-10</sup> και 10<sup>10</sup>, πιθανότατα θα πάρουμε 10<sup>10</sup>, επειδή η μικρότερη τιμή θα "μετατραπεί" στην ίδια τάξη μεγέθους με τη μεγαλύτερη, και έτσι η μαντίσα θα χαθεί.

Οι περισσότερες συναρτήσεις ενεργοποίησης έχουν μη γραμμικότητες γύρω από το [-1..1], και έτσι έχει νόημα να κλιμακώσουμε όλα τα δεδομένα εισόδου στο διάστημα [-1..1] ή [0..1].

## Αρχικοποίηση Βαρών

Ιδανικά, θέλουμε οι τιμές να βρίσκονται στο ίδιο εύρος μετά τη διέλευση από τα επίπεδα του δικτύου. Επομένως, είναι σημαντικό να αρχικοποιήσουμε τα βάρη με τέτοιο τρόπο ώστε να διατηρείται η κατανομή των τιμών.

Η κανονική κατανομή **N(0,1)** δεν είναι καλή ιδέα, επειδή αν έχουμε *n* εισόδους, η τυπική απόκλιση της εξόδου θα είναι *n*, και οι τιμές πιθανότατα θα βγουν εκτός του διαστήματος [0..1].

Οι ακόλουθες αρχικοποιήσεις χρησιμοποιούνται συχνά:

 * Ομοιόμορφη κατανομή -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/√n_in)** εγγυάται ότι για εισόδους με μέση τιμή μηδέν και τυπική απόκλιση 1, η ίδια μέση τιμή/τυπική απόκλιση θα παραμείνει
 * **N(0,√2/(n_in+n_out))** -- η λεγόμενη **Xavier initialization** (`glorot`), βοηθά στη διατήρηση των σημάτων εντός εύρους τόσο κατά την προώθηση όσο και κατά την οπισθοδιάδοση

## Κανονικοποίηση Παρτίδας

Ακόμα και με σωστή αρχικοποίηση βαρών, τα βάρη μπορούν να γίνουν αυθαίρετα μεγάλα ή μικρά κατά την εκπαίδευση, και θα βγάλουν τα σήματα εκτός σωστού εύρους. Μπορούμε να επαναφέρουμε τα σήματα χρησιμοποιώντας μία από τις τεχνικές **κανονικοποίησης**. Ενώ υπάρχουν αρκετές (Weight normalization, Layer Normalization), η πιο συχνά χρησιμοποιούμενη είναι η Κανονικοποίηση Παρτίδας.

Η ιδέα της **κανονικοποίησης παρτίδας** είναι να λαμβάνουμε υπόψη όλες τις τιμές μέσα στη μικροπαρτίδα και να εκτελούμε κανονικοποίηση (δηλαδή να αφαιρούμε τη μέση τιμή και να διαιρούμε με την τυπική απόκλιση) βάσει αυτών των τιμών. Εφαρμόζεται ως επίπεδο δικτύου που κάνει αυτή την κανονικοποίηση μετά την εφαρμογή των βαρών, αλλά πριν τη συνάρτηση ενεργοποίησης. Ως αποτέλεσμα, είναι πιθανό να δούμε υψηλότερη τελική ακρίβεια και ταχύτερη εκπαίδευση.

Εδώ είναι το [αρχικό άρθρο](https://arxiv.org/pdf/1502.03167.pdf) για την κανονικοποίηση παρτίδας, η [εξήγηση στη Wikipedia](https://en.wikipedia.org/wiki/Batch_normalization), και [ένα καλό εισαγωγικό άρθρο](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (και ένα [στα ρωσικά](https://habrahabr.ru/post/309302/)).

## Dropout

Το **Dropout** είναι μια ενδιαφέρουσα τεχνική που αφαιρεί ένα συγκεκριμένο ποσοστό τυχαίων νευρώνων κατά την εκπαίδευση. Εφαρμόζεται επίσης ως επίπεδο με μία παράμετρο (ποσοστό νευρώνων που αφαιρούνται, συνήθως 10%-50%), και κατά την εκπαίδευση μηδενίζει τυχαία στοιχεία του διανύσματος εισόδου, πριν το περάσει στο επόμενο επίπεδο.

Παρόλο που αυτό μπορεί να ακούγεται σαν μια περίεργη ιδέα, μπορείτε να δείτε την επίδραση του dropout στην εκπαίδευση ενός ταξινομητή ψηφίων MNIST στο σημειωματάριο [`Dropout.ipynb`](Dropout.ipynb). Επιταχύνει την εκπαίδευση και μας επιτρέπει να επιτύχουμε υψηλότερη ακρίβεια σε λιγότερες εποχές εκπαίδευσης.

Αυτή η επίδραση μπορεί να εξηγηθεί με διάφορους τρόπους:

 * Μπορεί να θεωρηθεί ως ένας τυχαίος παράγοντας σοκ για το μοντέλο, που το βγάζει από το τοπικό ελάχιστο
 * Μπορεί να θεωρηθεί ως *έμμεσος μέσος όρος μοντέλου*, επειδή μπορούμε να πούμε ότι κατά το dropout εκπαιδεύουμε ελαφρώς διαφορετικό μοντέλο

> *Μερικοί λένε ότι όταν ένας μεθυσμένος προσπαθεί να μάθει κάτι, θα το θυμάται καλύτερα το επόμενο πρωί, σε σύγκριση με έναν νηφάλιο, επειδή ένας εγκέφαλος με κάποιους δυσλειτουργικούς νευρώνες προσπαθεί να προσαρμοστεί καλύτερα για να κατανοήσει το νόημα. Δεν έχουμε δοκιμάσει ποτέ αν αυτό είναι αλήθεια ή όχι.*

## Πρόληψη υπερπροσαρμογής

Ένα από τα πολύ σημαντικά σημεία της βαθιάς μάθησης είναι να μπορούμε να αποτρέψουμε την [υπερπροσαρμογή](../../3-NeuralNetworks/05-Frameworks/Overfitting.md). Παρόλο που μπορεί να είναι δελεαστικό να χρησιμοποιήσουμε ένα πολύ ισχυρό μοντέλο νευρωνικού δικτύου, πρέπει πάντα να ισορροπούμε τον αριθμό των παραμέτρων του μοντέλου με τον αριθμό των δειγμάτων εκπαίδευσης.

> Βεβαιωθείτε ότι κατανοείτε την έννοια της [υπερπροσαρμογής](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) που έχουμε εισαγάγει νωρίτερα!

Υπάρχουν αρκετοί τρόποι για να αποτρέψουμε την υπερπροσαρμογή:

 * Πρώιμη διακοπή -- παρακολουθούμε συνεχώς το σφάλμα στο σύνολο επικύρωσης και σταματάμε την εκπαίδευση όταν το σφάλμα επικύρωσης αρχίζει να αυξάνεται.
 * Ρητή Αποσύνθεση Βαρών / Κανονικοποίηση -- προσθέτουμε μια επιπλέον ποινή στη συνάρτηση απώλειας για υψηλές απόλυτες τιμές βαρών, που αποτρέπει το μοντέλο από το να δίνει πολύ ασταθή αποτελέσματα
 * Μέσος όρος Μοντέλου -- εκπαιδεύουμε αρκετά μοντέλα και στη συνέχεια κάνουμε μέσο όρο του αποτελέσματος. Αυτό βοηθά στη μείωση της διακύμανσης.
 * Dropout (Έμμεσος Μέσος Όρος Μοντέλου)

## Βελτιστοποιητές / Αλγόριθμοι Εκπαίδευσης

Ένα άλλο σημαντικό σημείο της εκπαίδευσης είναι η επιλογή ενός καλού αλγορίθμου εκπαίδευσης. Παρόλο που η κλασική **gradient descent** είναι μια λογική επιλογή, μπορεί μερικές φορές να είναι πολύ αργή ή να οδηγήσει σε άλλα προβλήματα.

Στη βαθιά μάθηση, χρησιμοποιούμε **Stochastic Gradient Descent** (SGD), που είναι μια gradient descent εφαρμοσμένη σε μικροπαρτίδες, επιλεγμένες τυχαία από το σύνολο εκπαίδευσης. Τα βάρη προσαρμόζονται χρησιμοποιώντας τον ακόλουθο τύπο:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### Ορμή

Στο **momentum SGD**, διατηρούμε ένα μέρος της κλίσης από προηγούμενα βήματα. Είναι παρόμοιο με το όταν κινούμαστε κάπου με αδράνεια, και δεχόμαστε ένα χτύπημα σε διαφορετική κατεύθυνση, η τροχιά μας δεν αλλάζει αμέσως, αλλά διατηρεί ένα μέρος της αρχικής κίνησης. Εδώ εισάγουμε έναν άλλο διάνυσμα v για να εκπροσωπήσουμε την *ταχύτητα*:

* v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
* w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

Εδώ η παράμετρος γ υποδεικνύει το βαθμό στον οποίο λαμβάνουμε υπόψη την αδράνεια: γ=0 αντιστοιχεί στην κλασική SGD; γ=1 είναι μια καθαρή εξίσωση κίνησης.

### Adam, Adagrad, κλπ.

Επειδή σε κάθε επίπεδο πολλαπλασιάζουμε τα σήματα με κάποια μήτρα W<sub>i</sub>, ανάλογα με το ||W<sub>i</sub>||, η κλίση μπορεί είτε να μειωθεί και να είναι κοντά στο 0, είτε να αυξηθεί απεριόριστα. Αυτό είναι η ουσία του προβλήματος Exploding/Vanishing Gradients.

Μία από τις λύσεις σε αυτό το πρόβλημα είναι να χρησιμοποιούμε μόνο την κατεύθυνση της κλίσης στην εξίσωση, και να αγνοούμε την απόλυτη τιμή, δηλαδή

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), όπου ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

Αυτός ο αλγόριθμος ονομάζεται **Adagrad**. Άλλοι αλγόριθμοι που χρησιμοποιούν την ίδια ιδέα: **RMSProp**, **Adam**

> **Adam** θεωρείται ένας πολύ αποτελεσματικός αλγόριθμος για πολλές εφαρμογές, οπότε αν δεν είστε σίγουροι ποιον να χρησιμοποιήσετε - χρησιμοποιήστε τον Adam.

### Κοπή Κλίσης

Η κοπή κλίσης είναι μια επέκταση της παραπάνω ιδέας. Όταν το ||∇ℒ|| ≤ θ, θεωρούμε την αρχική κλίση στη βελτιστοποίηση βαρών, και όταν ||∇ℒ|| > θ - διαιρούμε την κλίση με την κανονικοποίησή της. Εδώ το θ είναι μια παράμετρος, στις περισσότερες περιπτώσεις μπορούμε να πάρουμε θ=1 ή θ=10.

### Μείωση Ρυθμού Μάθησης

Η επιτυχία της εκπαίδευσης συχνά εξαρτάται από την παράμετρο ρυθμού μάθησης η. Είναι λογικό να υποθέσουμε ότι μεγαλύτερες τιμές του η οδηγούν σε ταχύτερη εκπαίδευση, κάτι που συνήθως θέλουμε στην αρχή της εκπαίδευσης, και στη συνέχεια μικρότερες τιμές του η μας επιτρέπουν να βελτιστοποιήσουμε το δίκτυο. Επομένως, στις περισσότερες περιπτώσεις θέλουμε να μειώσουμε το η κατά τη διάρκεια της εκπαίδευσης.

Αυτό μπορεί να γίνει πολλαπλασιάζοντας το η με κάποιο αριθμό (π.χ. 0.98) μετά από κάθε εποχή εκπαίδευσης, ή χρησιμοποιώντας πιο περίπλοκο **πρόγραμμα ρυθμού μάθησης**.

## Διαφορετικές Αρχιτεκτονικές Δικτύων

Η επιλογή της σωστής αρχιτεκτονικής δικτύου για το πρόβλημά σας μπορεί να είναι δύσκολη. Κανονικά, θα επιλέγαμε μια αρχιτεκτονική που έχει αποδειχθεί ότι λειτουργεί για την συγκεκριμένη εργασία μας (ή παρόμοια). Εδώ είναι μια [καλή επισκόπηση](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) των αρχιτεκτονικών νευρωνικών δικτύων για την όραση υπολογιστών.

> Είναι σημαντικό να επιλέξετε μια αρχιτεκτονική που θα είναι αρκετά ισχυρή για τον αριθμό των δειγμάτων εκπαίδευσης που έχουμε. Η επιλογή υπερβολικά ισχυρού μοντέλου μπορεί να οδηγήσει σε [υπερπροσαρμογή](../../3-NeuralNetworks/05-Frameworks/Overfitting.md).

Ένας άλλος καλός τρόπος θα ήταν να χρησιμοποιήσετε μια αρχιτεκτονική που θα προσαρμόζεται αυτόματα στην απαιτούμενη πολυπλοκότητα. Σε κάποιο βαθμό, η αρχιτεκτονική **ResNet** και **Inception** είναι αυτοπροσαρμοζόμενες. [Περισσότερα για τις αρχιτεκτονικές όρασης υπολογιστών](../07-ConvNets/CNN_Architectures.md).

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.