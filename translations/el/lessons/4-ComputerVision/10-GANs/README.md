<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-29T08:57:33+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "el"
}
-->
# Γενετικά Ανταγωνιστικά Δίκτυα

Στην προηγούμενη ενότητα, μάθαμε για τα **γενετικά μοντέλα**: μοντέλα που μπορούν να δημιουργήσουν νέες εικόνες παρόμοιες με αυτές του συνόλου εκπαίδευσης. Το VAE ήταν ένα καλό παράδειγμα γενετικού μοντέλου.

## [Προ-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Ωστόσο, αν προσπαθήσουμε να δημιουργήσουμε κάτι πραγματικά ουσιαστικό, όπως έναν πίνακα σε λογική ανάλυση, με το VAE, θα δούμε ότι η εκπαίδευση δεν συγκλίνει καλά. Για αυτή την περίπτωση χρήσης, πρέπει να μάθουμε για μια άλλη αρχιτεκτονική που στοχεύει ειδικά στα γενετικά μοντέλα - τα **Γενετικά Ανταγωνιστικά Δίκτυα**, ή GANs.

Η βασική ιδέα ενός GAN είναι να έχουμε δύο νευρωνικά δίκτυα που εκπαιδεύονται το ένα ενάντια στο άλλο:

<img src="images/gan_architecture.png" width="70%"/>

> Εικόνα από τον [Dmitry Soshnikov](http://soshnikov.com)

> ✅ Λίγο λεξιλόγιο:
> * **Generator** είναι ένα δίκτυο που λαμβάνει κάποιο τυχαίο διάνυσμα και παράγει την εικόνα ως αποτέλεσμα.
> * **Discriminator** είναι ένα δίκτυο που λαμβάνει μια εικόνα και πρέπει να αποφασίσει αν είναι πραγματική εικόνα (από το σύνολο εκπαίδευσης) ή αν δημιουργήθηκε από τον generator. Ουσιαστικά είναι ένας ταξινομητής εικόνων.

### Discriminator

Η αρχιτεκτονική του discriminator δεν διαφέρει από ένα συνηθισμένο δίκτυο ταξινόμησης εικόνων. Στην πιο απλή περίπτωση, μπορεί να είναι ένας πλήρως συνδεδεμένος ταξινομητής, αλλά πιθανότατα θα είναι ένα [συνελικτικό δίκτυο](../07-ConvNets/README.md).

> ✅ Ένα GAN που βασίζεται σε συνελικτικά δίκτυα ονομάζεται [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

Ένας συνελικτικός discriminator αποτελείται από τα εξής επίπεδα: αρκετές συνελίξεις+συμπυκνώσεις (με μειούμενο χωρικό μέγεθος) και ένα ή περισσότερα πλήρως συνδεδεμένα επίπεδα για την εξαγωγή του "διανύσματος χαρακτηριστικών", και έναν τελικό δυαδικό ταξινομητή.

> ✅ Η "συμπύκνωση" σε αυτό το πλαίσιο είναι μια τεχνική που μειώνει το μέγεθος της εικόνας. "Τα επίπεδα συμπύκνωσης μειώνουν τις διαστάσεις των δεδομένων συνδυάζοντας τις εξόδους ομάδων νευρώνων σε ένα επίπεδο σε έναν μόνο νευρώνα στο επόμενο επίπεδο." - [πηγή](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

Ο Generator είναι λίγο πιο περίπλοκος. Μπορείτε να τον θεωρήσετε ως έναν αντιστραμμένο discriminator. Ξεκινώντας από ένα λανθάνον διάνυσμα (αντί για ένα διάνυσμα χαρακτηριστικών), έχει ένα πλήρως συνδεδεμένο επίπεδο για να το μετατρέψει στο απαιτούμενο μέγεθος/σχήμα, ακολουθούμενο από αποσυνελίξεις+μεγέθυνση. Αυτό είναι παρόμοιο με το *decoder* μέρος ενός [αυτοκωδικοποιητή](../09-Autoencoders/README.md).

> ✅ Επειδή το επίπεδο συνελίξεων υλοποιείται ως γραμμικό φίλτρο που διατρέχει την εικόνα, η αποσυνελίξη είναι ουσιαστικά παρόμοια με τη συνελίξη και μπορεί να υλοποιηθεί χρησιμοποιώντας την ίδια λογική επιπέδου.

<img src="images/gan_arch_detail.png" width="70%"/>

> Εικόνα από τον [Dmitry Soshnikov](http://soshnikov.com)

### Εκπαίδευση του GAN

Τα GAN ονομάζονται **ανταγωνιστικά** επειδή υπάρχει συνεχής ανταγωνισμός μεταξύ του generator και του discriminator. Κατά τη διάρκεια αυτού του ανταγωνισμού, τόσο ο generator όσο και ο discriminator βελτιώνονται, με αποτέλεσμα το δίκτυο να μαθαίνει να παράγει όλο και καλύτερες εικόνες.

Η εκπαίδευση γίνεται σε δύο στάδια:

* **Εκπαίδευση του discriminator**. Αυτή η εργασία είναι αρκετά απλή: δημιουργούμε μια παρτίδα εικόνων από τον generator, τις επισημαίνουμε με 0 (που αντιστοιχεί σε ψεύτικες εικόνες) και παίρνουμε μια παρτίδα εικόνων από το σύνολο εισόδου (με ετικέτα 1, πραγματικές εικόνες). Υπολογίζουμε κάποια *απώλεια του discriminator* και εκτελούμε backprop.
* **Εκπαίδευση του generator**. Αυτό είναι λίγο πιο περίπλοκο, επειδή δεν γνωρίζουμε την αναμενόμενη έξοδο για τον generator άμεσα. Παίρνουμε ολόκληρο το δίκτυο GAN που αποτελείται από έναν generator ακολουθούμενο από έναν discriminator, το τροφοδοτούμε με τυχαία διανύσματα και περιμένουμε το αποτέλεσμα να είναι 1 (που αντιστοιχεί σε πραγματικές εικόνες). Στη συνέχεια, παγώνουμε τις παραμέτρους του discriminator (δεν θέλουμε να εκπαιδευτεί σε αυτό το βήμα) και εκτελούμε backprop.

Κατά τη διάρκεια αυτής της διαδικασίας, οι απώλειες του generator και του discriminator δεν μειώνονται σημαντικά. Στην ιδανική περίπτωση, θα πρέπει να ταλαντώνονται, υποδεικνύοντας ότι και τα δύο δίκτυα βελτιώνουν την απόδοσή τους.

## ✍️ Ασκήσεις: GANs

* [GAN Notebook σε TensorFlow/Keras](GANTF.ipynb)
* [GAN Notebook σε PyTorch](GANPyTorch.ipynb)

### Προβλήματα με την εκπαίδευση των GAN

Τα GAN είναι γνωστά για τη δυσκολία τους στην εκπαίδευση. Εδώ είναι μερικά προβλήματα:

* **Κατάρρευση τρόπου λειτουργίας**. Με αυτόν τον όρο εννοούμε ότι ο generator μαθαίνει να παράγει μία επιτυχημένη εικόνα που ξεγελά τον discriminator, αλλά όχι μια ποικιλία διαφορετικών εικόνων.
* **Ευαισθησία στις υπερπαραμέτρους**. Συχνά μπορεί να δείτε ότι ένα GAN δεν συγκλίνει καθόλου και ξαφνικά μια μείωση του ρυθμού εκμάθησης οδηγεί σε σύγκλιση.
* Διατήρηση της **ισορροπίας** μεταξύ του generator και του discriminator. Σε πολλές περιπτώσεις, η απώλεια του discriminator μπορεί να πέσει στο μηδέν σχετικά γρήγορα, με αποτέλεσμα ο generator να μην μπορεί να εκπαιδευτεί περαιτέρω. Για να το ξεπεράσουμε αυτό, μπορούμε να δοκιμάσουμε να ορίσουμε διαφορετικούς ρυθμούς εκμάθησης για τον generator και τον discriminator ή να παραλείψουμε την εκπαίδευση του discriminator αν η απώλεια είναι ήδη πολύ χαμηλή.
* Εκπαίδευση για **υψηλή ανάλυση**. Αντικατοπτρίζοντας το ίδιο πρόβλημα με τους αυτοκωδικοποιητές, αυτό το πρόβλημα προκαλείται επειδή η ανακατασκευή πολλών επιπέδων συνελικτικού δικτύου οδηγεί σε τεχνουργήματα. Αυτό το πρόβλημα συνήθως λύνεται με τη λεγόμενη **προοδευτική ανάπτυξη**, όπου πρώτα εκπαιδεύονται λίγα επίπεδα σε εικόνες χαμηλής ανάλυσης και στη συνέχεια "ξεκλειδώνονται" ή προστίθενται επίπεδα. Μια άλλη λύση θα ήταν η προσθήκη επιπλέον συνδέσεων μεταξύ των επιπέδων και η εκπαίδευση πολλών αναλύσεων ταυτόχρονα - δείτε αυτή την εργασία [Multi-Scale Gradient GANs](https://arxiv.org/abs/1903.06048) για λεπτομέρειες.

## Μεταφορά Στυλ

Τα GAN είναι ένας εξαιρετικός τρόπος για τη δημιουργία καλλιτεχνικών εικόνων. Μια άλλη ενδιαφέρουσα τεχνική είναι η λεγόμενη **μεταφορά στυλ**, η οποία παίρνει μια **εικόνα περιεχομένου** και την ανασχεδιάζει σε διαφορετικό στυλ, εφαρμόζοντας φίλτρα από μια **εικόνα στυλ**.

Ο τρόπος που λειτουργεί είναι ο εξής:
* Ξεκινάμε με μια τυχαία εικόνα θορύβου (ή με μια εικόνα περιεχομένου, αλλά για λόγους κατανόησης είναι πιο εύκολο να ξεκινήσουμε από τυχαίο θόρυβο).
* Στόχος μας είναι να δημιουργήσουμε μια εικόνα που να είναι κοντά τόσο στην εικόνα περιεχομένου όσο και στην εικόνα στυλ. Αυτό θα καθοριστεί από δύο συναρτήσεις απώλειας:
   - Η **απώλεια περιεχομένου** υπολογίζεται με βάση τα χαρακτηριστικά που εξάγονται από το CNN σε ορισμένα επίπεδα από την τρέχουσα εικόνα και την εικόνα περιεχομένου.
   - Η **απώλεια στυλ** υπολογίζεται μεταξύ της τρέχουσας εικόνας και της εικόνας στυλ με έναν έξυπνο τρόπο χρησιμοποιώντας πίνακες Gram (περισσότερες λεπτομέρειες στο [παράδειγμα notebook](StyleTransfer.ipynb)).
* Για να κάνουμε την εικόνα πιο ομαλή και να αφαιρέσουμε τον θόρυβο, εισάγουμε επίσης την **απώλεια παραλλαγής**, η οποία υπολογίζει τη μέση απόσταση μεταξύ γειτονικών pixel.
* Ο κύριος βρόχος βελτιστοποίησης προσαρμόζει την τρέχουσα εικόνα χρησιμοποιώντας gradient descent (ή κάποιον άλλο αλγόριθμο βελτιστοποίησης) για να ελαχιστοποιήσει τη συνολική απώλεια, η οποία είναι ένας σταθμισμένος συνδυασμός όλων των απωλειών.

## ✍️ Παράδειγμα: [Μεταφορά Στυλ](StyleTransfer.ipynb)

## [Μετά-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Συμπέρασμα

Σε αυτό το μάθημα, μάθατε για τα GANs και πώς να τα εκπαιδεύετε. Μάθατε επίσης για τις ειδικές προκλήσεις που μπορεί να αντιμετωπίσει αυτός ο τύπος Νευρωνικού Δικτύου και κάποιες στρατηγικές για να τις ξεπεράσετε.

## 🚀 Πρόκληση

Εκτελέστε το [notebook Μεταφοράς Στυλ](StyleTransfer.ipynb) χρησιμοποιώντας τις δικές σας εικόνες.

## Ανασκόπηση & Αυτομελέτη

Για αναφορά, διαβάστε περισσότερα για τα GANs στις παρακάτω πηγές:

* Marco Pasini, [10 Μαθήματα που Έμαθα Εκπαιδεύοντας GANs για Ένα Χρόνο](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), μια *de facto* αρχιτεκτονική GAN που αξίζει να εξετάσετε.
* [Δημιουργία Γενετικής Τέχνης χρησιμοποιώντας GANs στο Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Εργασία

Επισκεφθείτε ξανά ένα από τα δύο notebooks που σχετίζονται με αυτό το μάθημα και επανεκπαιδεύστε το GAN στις δικές σας εικόνες. Τι μπορείτε να δημιουργήσετε;

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.