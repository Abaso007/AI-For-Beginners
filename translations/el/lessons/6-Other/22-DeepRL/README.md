<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-29T08:40:26+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "el"
}
-->
# Βαθιά Ενισχυτική Μάθηση

Η ενισχυτική μάθηση (RL) θεωρείται ένα από τα βασικά παραδείγματα μηχανικής μάθησης, μαζί με την επιβλεπόμενη και τη μη επιβλεπόμενη μάθηση. Ενώ στην επιβλεπόμενη μάθηση βασιζόμαστε σε ένα σύνολο δεδομένων με γνωστά αποτελέσματα, η RL βασίζεται στη **μάθηση μέσω πράξης**. Για παράδειγμα, όταν βλέπουμε για πρώτη φορά ένα παιχνίδι στον υπολογιστή, αρχίζουμε να παίζουμε, ακόμα και αν δεν γνωρίζουμε τους κανόνες, και σύντομα βελτιώνουμε τις δεξιότητές μας απλώς μέσω της διαδικασίας του παιχνιδιού και της προσαρμογής της συμπεριφοράς μας.

## [Κουίζ πριν τη διάλεξη](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Για να εκτελέσουμε RL, χρειαζόμαστε:

* Ένα **περιβάλλον** ή **προσομοιωτή** που θέτει τους κανόνες του παιχνιδιού. Θα πρέπει να μπορούμε να εκτελούμε πειράματα στον προσομοιωτή και να παρατηρούμε τα αποτελέσματα.
* Μια **συνάρτηση ανταμοιβής**, η οποία δείχνει πόσο επιτυχημένο ήταν το πείραμά μας. Στην περίπτωση της εκμάθησης ενός παιχνιδιού στον υπολογιστή, η ανταμοιβή θα ήταν η τελική μας βαθμολογία.

Με βάση τη συνάρτηση ανταμοιβής, θα πρέπει να μπορούμε να προσαρμόσουμε τη συμπεριφορά μας και να βελτιώσουμε τις δεξιότητές μας, ώστε την επόμενη φορά να παίζουμε καλύτερα. Η κύρια διαφορά μεταξύ άλλων τύπων μηχανικής μάθησης και RL είναι ότι στη RL συνήθως δεν γνωρίζουμε αν κερδίζουμε ή χάνουμε μέχρι να τελειώσει το παιχνίδι. Έτσι, δεν μπορούμε να πούμε αν μια συγκεκριμένη κίνηση από μόνη της είναι καλή ή όχι - λαμβάνουμε ανταμοιβή μόνο στο τέλος του παιχνιδιού.

Κατά τη διάρκεια της RL, συνήθως εκτελούμε πολλά πειράματα. Σε κάθε πείραμα, πρέπει να ισορροπούμε μεταξύ της εφαρμογής της βέλτιστης στρατηγικής που έχουμε μάθει μέχρι στιγμής (**εκμετάλλευση**) και της εξερεύνησης νέων πιθανών καταστάσεων (**εξερεύνηση**).

## OpenAI Gym

Ένα εξαιρετικό εργαλείο για τη RL είναι το [OpenAI Gym](https://gym.openai.com/) - ένα **περιβάλλον προσομοίωσης**, το οποίο μπορεί να προσομοιώσει πολλά διαφορετικά περιβάλλοντα, από παιχνίδια Atari μέχρι τη φυσική πίσω από την ισορροπία ενός κονταριού. Είναι ένα από τα πιο δημοφιλή περιβάλλοντα προσομοίωσης για την εκπαίδευση αλγορίθμων ενισχυτικής μάθησης και συντηρείται από την [OpenAI](https://openai.com/).

> **Note**: Μπορείτε να δείτε όλα τα διαθέσιμα περιβάλλοντα από το OpenAI Gym [εδώ](https://gym.openai.com/envs/#classic_control).

## Ισορροπία CartPole

Πιθανότατα έχετε δει σύγχρονες συσκευές ισορροπίας όπως το *Segway* ή τα *Gyroscooters*. Αυτές μπορούν να ισορροπούν αυτόματα προσαρμόζοντας τους τροχούς τους σε απόκριση ενός σήματος από έναν επιταχυνσιόμετρο ή γυροσκόπιο. Σε αυτή την ενότητα, θα μάθουμε πώς να λύσουμε ένα παρόμοιο πρόβλημα - την ισορροπία ενός κονταριού. Είναι παρόμοιο με την κατάσταση όπου ένας ακροβάτης πρέπει να ισορροπήσει ένα κοντάρι στο χέρι του - αλλά αυτή η ισορροπία συμβαίνει μόνο σε μία διάσταση.

Μια απλοποιημένη έκδοση της ισορροπίας είναι γνωστή ως πρόβλημα **CartPole**. Στον κόσμο του CartPole, έχουμε έναν οριζόντιο ολισθητήρα που μπορεί να κινηθεί αριστερά ή δεξιά, και ο στόχος είναι να ισορροπήσουμε ένα κάθετο κοντάρι πάνω στον ολισθητήρα καθώς αυτός κινείται.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Για να δημιουργήσουμε και να χρησιμοποιήσουμε αυτό το περιβάλλον, χρειαζόμαστε μερικές γραμμές κώδικα Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Κάθε περιβάλλον μπορεί να προσεγγιστεί με τον ίδιο ακριβώς τρόπο:
* Το `env.reset` ξεκινά ένα νέο πείραμα
* Το `env.step` εκτελεί ένα βήμα προσομοίωσης. Λαμβάνει μια **ενέργεια** από τον **χώρο ενεργειών** και επιστρέφει μια **παρατήρηση** (από τον χώρο παρατηρήσεων), καθώς και μια ανταμοιβή και μια σημαία τερματισμού.

Στο παραπάνω παράδειγμα εκτελούμε μια τυχαία ενέργεια σε κάθε βήμα, γι' αυτό η διάρκεια του πειράματος είναι πολύ μικρή:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Ο στόχος ενός αλγορίθμου RL είναι να εκπαιδεύσει ένα μοντέλο - την αποκαλούμενη **πολιτική** π - η οποία θα επιστρέφει την ενέργεια σε απόκριση μιας δεδομένης κατάστασης. Μπορούμε επίσης να θεωρήσουμε την πολιτική ως πιθανή, π.χ. για οποιαδήποτε κατάσταση *s* και ενέργεια *a* θα επιστρέφει την πιθανότητα π(*a*|*s*) ότι πρέπει να πάρουμε την *a* στην κατάσταση *s*.

## Αλγόριθμος Πολιτικών Κλίσεων

Ο πιο προφανής τρόπος να μοντελοποιήσουμε μια πολιτική είναι δημιουργώντας ένα νευρωνικό δίκτυο που θα λαμβάνει καταστάσεις ως είσοδο και θα επιστρέφει τις αντίστοιχες ενέργειες (ή μάλλον τις πιθανότητες όλων των ενεργειών). Με μια έννοια, θα ήταν παρόμοιο με μια κανονική εργασία ταξινόμησης, με μια σημαντική διαφορά - δεν γνωρίζουμε εκ των προτέρων ποιες ενέργειες πρέπει να πάρουμε σε κάθε βήμα.

Η ιδέα εδώ είναι να εκτιμήσουμε αυτές τις πιθανότητες. Δημιουργούμε ένα διάνυσμα **σωρευτικών ανταμοιβών** που δείχνει τη συνολική μας ανταμοιβή σε κάθε βήμα του πειράματος. Επίσης, εφαρμόζουμε **έκπτωση ανταμοιβής** πολλαπλασιάζοντας τις πρώιμες ανταμοιβές με κάποιο συντελεστή γ=0.99, ώστε να μειώσουμε τη σημασία των πρώιμων ανταμοιβών. Στη συνέχεια, ενισχύουμε εκείνα τα βήματα κατά μήκος της διαδρομής του πειράματος που αποφέρουν μεγαλύτερες ανταμοιβές.

> Μάθετε περισσότερα για τον αλγόριθμο Πολιτικών Κλίσεων και δείτε τον σε δράση στο [παράδειγμα notebook](CartPole-RL-TF.ipynb).

## Αλγόριθμος Actor-Critic

Μια βελτιωμένη έκδοση της προσέγγισης Πολιτικών Κλίσεων ονομάζεται **Actor-Critic**. Η βασική ιδέα πίσω από αυτό είναι ότι το νευρωνικό δίκτυο θα εκπαιδεύεται να επιστρέφει δύο πράγματα:

* Την πολιτική, η οποία καθορίζει ποια ενέργεια να πάρουμε. Αυτό το μέρος ονομάζεται **actor**
* Την εκτίμηση της συνολικής ανταμοιβής που μπορούμε να περιμένουμε να λάβουμε σε αυτή την κατάσταση - αυτό το μέρος ονομάζεται **critic**.

Με μια έννοια, αυτή η αρχιτεκτονική μοιάζει με ένα [GAN](../../4-ComputerVision/10-GANs/README.md), όπου έχουμε δύο δίκτυα που εκπαιδεύονται το ένα εναντίον του άλλου. Στο μοντέλο actor-critic, ο actor προτείνει την ενέργεια που πρέπει να πάρουμε, και ο critic προσπαθεί να είναι κριτικός και να εκτιμήσει το αποτέλεσμα. Ωστόσο, ο στόχος μας είναι να εκπαιδεύσουμε αυτά τα δίκτυα σε αρμονία.

Επειδή γνωρίζουμε τόσο τις πραγματικές σωρευτικές ανταμοιβές όσο και τα αποτελέσματα που επιστρέφει ο critic κατά τη διάρκεια του πειράματος, είναι σχετικά εύκολο να δημιουργήσουμε μια συνάρτηση απώλειας που θα ελαχιστοποιεί τη διαφορά μεταξύ τους. Αυτό θα μας δώσει την **απώλεια του critic**. Μπορούμε να υπολογίσουμε την **απώλεια του actor** χρησιμοποιώντας την ίδια προσέγγιση όπως στον αλγόριθμο πολιτικών κλίσεων.

Μετά την εκτέλεση ενός από αυτούς τους αλγορίθμους, μπορούμε να περιμένουμε το CartPole μας να συμπεριφέρεται ως εξής:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Ασκήσεις: Πολιτικές Κλίσεις και Actor-Critic RL

Συνεχίστε τη μάθησή σας στα παρακάτω notebooks:

* [RL στο TensorFlow](CartPole-RL-TF.ipynb)
* [RL στο PyTorch](CartPole-RL-PyTorch.ipynb)

## Άλλες Εργασίες RL

Η Ενισχυτική Μάθηση σήμερα είναι ένας ταχέως αναπτυσσόμενος τομέας έρευνας. Μερικά ενδιαφέροντα παραδείγματα ενισχυτικής μάθησης είναι:

* Η εκμάθηση ενός υπολογιστή να παίζει **παιχνίδια Atari**. Το δύσκολο μέρος σε αυτό το πρόβλημα είναι ότι δεν έχουμε απλή κατάσταση που να αναπαρίσταται ως διάνυσμα, αλλά μάλλον ένα στιγμιότυπο οθόνης - και πρέπει να χρησιμοποιήσουμε CNN για να μετατρέψουμε αυτή την εικόνα σε διάνυσμα χαρακτηριστικών ή να εξάγουμε πληροφορίες ανταμοιβής. Τα παιχνίδια Atari είναι διαθέσιμα στο Gym.
* Η εκμάθηση ενός υπολογιστή να παίζει επιτραπέζια παιχνίδια, όπως το Σκάκι και το Go. Πρόσφατα, προγράμματα αιχμής όπως το **Alpha Zero** εκπαιδεύτηκαν από το μηδέν με δύο πράκτορες να παίζουν ο ένας εναντίον του άλλου και να βελτιώνονται σε κάθε βήμα.
* Στη βιομηχανία, η RL χρησιμοποιείται για τη δημιουργία συστημάτων ελέγχου από προσομοίωση. Μια υπηρεσία που ονομάζεται [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) έχει σχεδιαστεί ειδικά για αυτό.

## Συμπέρασμα

Μάθαμε τώρα πώς να εκπαιδεύουμε πράκτορες ώστε να επιτυγχάνουν καλά αποτελέσματα απλώς παρέχοντάς τους μια συνάρτηση ανταμοιβής που ορίζει την επιθυμητή κατάσταση του παιχνιδιού και δίνοντάς τους την ευκαιρία να εξερευνήσουν έξυπνα τον χώρο αναζήτησης. Δοκιμάσαμε με επιτυχία δύο αλγορίθμους και πετύχαμε ένα καλό αποτέλεσμα σε σχετικά σύντομο χρονικό διάστημα. Ωστόσο, αυτή είναι μόνο η αρχή του ταξιδιού σας στη RL, και θα πρέπει σίγουρα να εξετάσετε το ενδεχόμενο να παρακολουθήσετε ένα ξεχωριστό μάθημα αν θέλετε να εμβαθύνετε.

## 🚀 Πρόκληση

Εξερευνήστε τις εφαρμογές που αναφέρονται στην ενότητα "Άλλες Εργασίες RL" και προσπαθήστε να υλοποιήσετε μία!

## [Κουίζ μετά τη διάλεξη](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Ανασκόπηση & Αυτομελέτη

Μάθετε περισσότερα για την κλασική ενισχυτική μάθηση στο [Πρόγραμμα Σπουδών Μηχανικής Μάθησης για Αρχάριους](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Παρακολουθήστε [αυτό το εξαιρετικό βίντεο](https://www.youtube.com/watch?v=qv6UVOQ0F44) που μιλάει για το πώς ένας υπολογιστής μπορεί να μάθει να παίζει Super Mario.

## Εργασία: [Εκπαίδευση ενός Mountain Car](lab/README.md)

Ο στόχος σας κατά τη διάρκεια αυτής της εργασίας θα είναι να εκπαιδεύσετε ένα διαφορετικό περιβάλλον Gym - το [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

**Αποποίηση Ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.