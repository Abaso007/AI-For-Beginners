<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T08:49:12+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "el"
}
-->
# Βαθιά Ενισχυτική Μάθηση

Η ενισχυτική μάθηση (Reinforcement Learning - RL) θεωρείται ένα από τα βασικά παραδείγματα μηχανικής μάθησης, δίπλα στη supervised learning και την unsupervised learning. Ενώ στη supervised learning βασιζόμαστε σε ένα σύνολο δεδομένων με γνωστά αποτελέσματα, η RL βασίζεται στη **μάθηση μέσω πράξης**. Για παράδειγμα, όταν βλέπουμε για πρώτη φορά ένα παιχνίδι στον υπολογιστή, ξεκινάμε να παίζουμε, ακόμα και αν δεν γνωρίζουμε τους κανόνες, και σύντομα βελτιώνουμε τις ικανότητές μας απλώς μέσω της διαδικασίας του παιχνιδιού και της προσαρμογής της συμπεριφοράς μας.

## [Προ-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Για να εκτελέσουμε RL, χρειαζόμαστε:

* Ένα **περιβάλλον** ή **προσομοιωτή** που θέτει τους κανόνες του παιχνιδιού. Πρέπει να μπορούμε να εκτελούμε πειράματα στον προσομοιωτή και να παρατηρούμε τα αποτελέσματα.
* Μια **συνάρτηση ανταμοιβής**, που δείχνει πόσο επιτυχημένο ήταν το πείραμά μας. Στην περίπτωση της εκμάθησης ενός παιχνιδιού στον υπολογιστή, η ανταμοιβή θα ήταν το τελικό μας σκορ.

Με βάση τη συνάρτηση ανταμοιβής, πρέπει να μπορούμε να προσαρμόσουμε τη συμπεριφορά μας και να βελτιώσουμε τις ικανότητές μας, ώστε την επόμενη φορά να παίζουμε καλύτερα. Η κύρια διαφορά μεταξύ άλλων τύπων μηχανικής μάθησης και RL είναι ότι στη RL συνήθως δεν γνωρίζουμε αν κερδίζουμε ή χάνουμε μέχρι να τελειώσει το παιχνίδι. Έτσι, δεν μπορούμε να πούμε αν μια συγκεκριμένη κίνηση από μόνη της είναι καλή ή όχι - λαμβάνουμε ανταμοιβή μόνο στο τέλος του παιχνιδιού.

Κατά τη διάρκεια της RL, συνήθως εκτελούμε πολλά πειράματα. Σε κάθε πείραμα, πρέπει να ισορροπήσουμε μεταξύ της εφαρμογής της βέλτιστης στρατηγικής που έχουμε μάθει μέχρι στιγμής (**εκμετάλλευση**) και της εξερεύνησης νέων πιθανών καταστάσεων (**εξερεύνηση**).

## OpenAI Gym

Ένα εξαιρετικό εργαλείο για τη RL είναι το [OpenAI Gym](https://gym.openai.com/) - ένα **περιβάλλον προσομοίωσης**, το οποίο μπορεί να προσομοιώσει πολλά διαφορετικά περιβάλλοντα, από παιχνίδια Atari μέχρι τη φυσική πίσω από την ισορροπία ενός κονταριού. Είναι ένα από τα πιο δημοφιλή περιβάλλοντα προσομοίωσης για την εκπαίδευση αλγορίθμων ενισχυτικής μάθησης και συντηρείται από την [OpenAI](https://openai.com/).

> **Note**: Μπορείτε να δείτε όλα τα διαθέσιμα περιβάλλοντα του OpenAI Gym [εδώ](https://gym.openai.com/envs/#classic_control).

## Ισορροπία CartPole

Πιθανότατα έχετε δει σύγχρονες συσκευές ισορροπίας όπως το *Segway* ή τα *Gyroscooters*. Αυτές μπορούν να ισορροπούν αυτόματα προσαρμόζοντας τους τροχούς τους σε απόκριση ενός σήματος από έναν επιταχυνσιόμετρο ή γυροσκόπιο. Σε αυτή την ενότητα, θα μάθουμε πώς να λύσουμε ένα παρόμοιο πρόβλημα - την ισορροπία ενός κονταριού. Είναι παρόμοιο με την κατάσταση όπου ένας ακροβάτης πρέπει να ισορροπήσει ένα κοντάρι στο χέρι του - αλλά αυτή η ισορροπία συμβαίνει μόνο σε μία διάσταση.

Μια απλοποιημένη έκδοση της ισορροπίας είναι γνωστή ως πρόβλημα **CartPole**. Στον κόσμο του CartPole, έχουμε έναν οριζόντιο ολισθητήρα που μπορεί να κινηθεί αριστερά ή δεξιά, και ο στόχος είναι να ισορροπήσουμε ένα κάθετο κοντάρι πάνω στον ολισθητήρα καθώς αυτός κινείται.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Για να δημιουργήσουμε και να χρησιμοποιήσουμε αυτό το περιβάλλον, χρειαζόμαστε μερικές γραμμές κώδικα Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Κάθε περιβάλλον μπορεί να προσπελαστεί με τον ίδιο τρόπο:
* Το `env.reset` ξεκινά ένα νέο πείραμα
* Το `env.step` εκτελεί ένα βήμα προσομοίωσης. Λαμβάνει μια **ενέργεια** από τον **χώρο ενεργειών** και επιστρέφει μια **παρατήρηση** (από τον χώρο παρατηρήσεων), καθώς και μια ανταμοιβή και μια σημαία τερματισμού.

Στο παραπάνω παράδειγμα, εκτελούμε μια τυχαία ενέργεια σε κάθε βήμα, γι' αυτό και η διάρκεια του πειράματος είναι πολύ μικρή:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Ο στόχος ενός αλγορίθμου RL είναι να εκπαιδεύσει ένα μοντέλο - την αποκαλούμενη **πολιτική** &pi; - που θα επιστρέφει την ενέργεια σε απόκριση μιας δεδομένης κατάστασης. Μπορούμε επίσης να θεωρήσουμε την πολιτική ως πιθανή, π.χ. για οποιαδήποτε κατάσταση *s* και ενέργεια *a*, θα επιστρέφει την πιθανότητα &pi;(*a*|*s*) ότι πρέπει να πάρουμε την ενέργεια *a* στην κατάσταση *s*.

## Αλγόριθμος Policy Gradients

Ο πιο προφανής τρόπος να μοντελοποιήσουμε μια πολιτική είναι δημιουργώντας ένα νευρωνικό δίκτυο που θα λαμβάνει καταστάσεις ως είσοδο και θα επιστρέφει τις αντίστοιχες ενέργειες (ή μάλλον τις πιθανότητες όλων των ενεργειών). Με μια έννοια, θα ήταν παρόμοιο με μια κανονική εργασία ταξινόμησης, με μια σημαντική διαφορά - δεν γνωρίζουμε εκ των προτέρων ποιες ενέργειες πρέπει να πάρουμε σε κάθε βήμα.

Η ιδέα εδώ είναι να εκτιμήσουμε αυτές τις πιθανότητες. Δημιουργούμε ένα διάνυσμα **σωρευτικών ανταμοιβών**, το οποίο δείχνει τη συνολική μας ανταμοιβή σε κάθε βήμα του πειράματος. Επίσης, εφαρμόζουμε **έκπτωση ανταμοιβών** πολλαπλασιάζοντας τις προηγούμενες ανταμοιβές με κάποιο συντελεστή &gamma;=0.99, ώστε να μειώσουμε τη σημασία των προηγούμενων ανταμοιβών. Στη συνέχεια, ενισχύουμε εκείνα τα βήματα κατά μήκος της διαδρομής του πειράματος που αποφέρουν μεγαλύτερες ανταμοιβές.

> Μάθετε περισσότερα για τον αλγόριθμο Policy Gradient και δείτε τον σε δράση στο [παράδειγμα notebook](CartPole-RL-TF.ipynb).

## Αλγόριθμος Actor-Critic

Μια βελτιωμένη έκδοση της προσέγγισης Policy Gradients ονομάζεται **Actor-Critic**. Η βασική ιδέα πίσω από αυτό είναι ότι το νευρωνικό δίκτυο θα εκπαιδεύεται να επιστρέφει δύο πράγματα:

* Την πολιτική, που καθορίζει ποια ενέργεια να πάρουμε. Αυτό το μέρος ονομάζεται **actor**.
* Την εκτίμηση της συνολικής ανταμοιβής που μπορούμε να περιμένουμε να λάβουμε σε αυτή την κατάσταση - αυτό το μέρος ονομάζεται **critic**.

Με μια έννοια, αυτή η αρχιτεκτονική μοιάζει με ένα [GAN](../../4-ComputerVision/10-GANs/README.md), όπου έχουμε δύο δίκτυα που εκπαιδεύονται το ένα ενάντια στο άλλο. Στο μοντέλο actor-critic, ο actor προτείνει την ενέργεια που πρέπει να πάρουμε, και ο critic προσπαθεί να είναι κριτικός και να εκτιμήσει το αποτέλεσμα. Ωστόσο, ο στόχος μας είναι να εκπαιδεύσουμε αυτά τα δίκτυα σε αρμονία.

Επειδή γνωρίζουμε τόσο τις πραγματικές σωρευτικές ανταμοιβές όσο και τα αποτελέσματα που επιστρέφει ο critic κατά τη διάρκεια του πειράματος, είναι σχετικά εύκολο να δημιουργήσουμε μια συνάρτηση απώλειας που θα ελαχιστοποιεί τη διαφορά μεταξύ τους. Αυτό θα μας δώσει την **απώλεια του critic**. Μπορούμε να υπολογίσουμε την **απώλεια του actor** χρησιμοποιώντας την ίδια προσέγγιση όπως στον αλγόριθμο policy gradient.

Μετά την εκτέλεση ενός από αυτούς τους αλγορίθμους, μπορούμε να περιμένουμε το CartPole μας να συμπεριφέρεται έτσι:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Ασκήσεις: Policy Gradients και Actor-Critic RL

Συνεχίστε τη μάθησή σας στα παρακάτω notebooks:

* [RL στο TensorFlow](CartPole-RL-TF.ipynb)
* [RL στο PyTorch](CartPole-RL-PyTorch.ipynb)

## Άλλες Εργασίες RL

Η Ενισχυτική Μάθηση είναι σήμερα ένας ταχέως αναπτυσσόμενος τομέας έρευνας. Μερικά ενδιαφέροντα παραδείγματα ενισχυτικής μάθησης είναι:

* Η εκμάθηση ενός υπολογιστή να παίζει **παιχνίδια Atari**. Το δύσκολο μέρος σε αυτό το πρόβλημα είναι ότι δεν έχουμε απλή κατάσταση που να αναπαρίσταται ως διάνυσμα, αλλά μάλλον ένα στιγμιότυπο οθόνης - και πρέπει να χρησιμοποιήσουμε CNN για να μετατρέψουμε αυτή την εικόνα σε ένα διάνυσμα χαρακτηριστικών ή να εξάγουμε πληροφορίες ανταμοιβής. Τα παιχνίδια Atari είναι διαθέσιμα στο Gym.
* Η εκμάθηση ενός υπολογιστή να παίζει επιτραπέζια παιχνίδια, όπως το Σκάκι και το Go. Πρόσφατα, προγράμματα αιχμής όπως το **Alpha Zero** εκπαιδεύτηκαν από το μηδέν με δύο πράκτορες να παίζουν ο ένας εναντίον του άλλου και να βελτιώνονται σε κάθε βήμα.
* Στη βιομηχανία, η RL χρησιμοποιείται για τη δημιουργία συστημάτων ελέγχου από προσομοίωση. Μια υπηρεσία που ονομάζεται [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) έχει σχεδιαστεί ειδικά για αυτό.

## Συμπέρασμα

Μάθαμε τώρα πώς να εκπαιδεύουμε πράκτορες ώστε να επιτυγχάνουν καλά αποτελέσματα απλώς παρέχοντάς τους μια συνάρτηση ανταμοιβής που ορίζει την επιθυμητή κατάσταση του παιχνιδιού και δίνοντάς τους την ευκαιρία να εξερευνήσουν έξυπνα τον χώρο αναζήτησης. Δοκιμάσαμε με επιτυχία δύο αλγορίθμους και πετύχαμε ένα καλό αποτέλεσμα σε σχετικά σύντομο χρονικό διάστημα. Ωστόσο, αυτό είναι μόνο η αρχή του ταξιδιού σας στη RL, και θα πρέπει σίγουρα να εξετάσετε το ενδεχόμενο να παρακολουθήσετε ένα ξεχωριστό μάθημα αν θέλετε να εμβαθύνετε.

## 🚀 Πρόκληση

Εξερευνήστε τις εφαρμογές που αναφέρονται στην ενότητα "Άλλες Εργασίες RL" και προσπαθήστε να υλοποιήσετε μία!

## [Μετά-διάλεξης κουίζ](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Ανασκόπηση & Αυτομελέτη

Μάθετε περισσότερα για την κλασική ενισχυτική μάθηση στο [Πρόγραμμα Σπουδών Μηχανικής Μάθησης για Αρχάριους](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Παρακολουθήστε [αυτό το εξαιρετικό βίντεο](https://www.youtube.com/watch?v=qv6UVOQ0F44) που μιλάει για το πώς ένας υπολογιστής μπορεί να μάθει να παίζει Super Mario.

## Εργασία: [Εκπαιδεύστε ένα Mountain Car](lab/README.md)

Ο στόχος σας σε αυτή την εργασία θα είναι να εκπαιδεύσετε ένα διαφορετικό περιβάλλον Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

