{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Εκπαίδευση Ενισχυτικής Μάθησης (RL) για Ισορροπία Cartpole\n",
    "\n",
    "Αυτό το σημειωματάριο αποτελεί μέρος του [Προγράμματος Σπουδών AI για Αρχάριους](http://aka.ms/ai-beginners). Έχει εμπνευστεί από το [επίσημο tutorial του PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) και από αυτήν την [υλοποίηση Cartpole με PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Σε αυτό το παράδειγμα, θα χρησιμοποιήσουμε Ενισχυτική Μάθηση (RL) για να εκπαιδεύσουμε ένα μοντέλο να ισορροπεί έναν πόλο πάνω σε ένα καρότσι που μπορεί να κινείται αριστερά και δεξιά σε οριζόντια κλίμακα. Θα χρησιμοποιήσουμε το περιβάλλον [OpenAI Gym](https://www.gymlibrary.ml/) για να προσομοιώσουμε τον πόλο.\n",
    "\n",
    "> **Σημείωση**: Μπορείτε να εκτελέσετε τον κώδικα αυτού του μαθήματος τοπικά (π.χ. από το Visual Studio Code), οπότε η προσομοίωση θα ανοίξει σε νέο παράθυρο. Όταν εκτελείτε τον κώδικα online, ίσως χρειαστεί να κάνετε κάποιες προσαρμογές στον κώδικα, όπως περιγράφεται [εδώ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Θα ξεκινήσουμε διασφαλίζοντας ότι το Gym είναι εγκατεστημένο:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα ας δημιουργήσουμε το περιβάλλον CartPole και ας δούμε πώς να το χειριστούμε. Ένα περιβάλλον έχει τις εξής ιδιότητες:\n",
    "\n",
    "* **Χώρος ενεργειών** είναι το σύνολο των πιθανών ενεργειών που μπορούμε να εκτελέσουμε σε κάθε βήμα της προσομοίωσης  \n",
    "* **Χώρος παρατηρήσεων** είναι ο χώρος των παρατηρήσεων που μπορούμε να κάνουμε  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ας δούμε πώς λειτουργεί η προσομοίωση. Η παρακάτω επανάληψη εκτελεί την προσομοίωση, μέχρι η `env.step` να μην επιστρέφει τη σημαία τερματισμού `done`. Θα επιλέγουμε τυχαίες ενέργειες χρησιμοποιώντας το `env.action_space.sample()`, που σημαίνει ότι το πείραμα πιθανότατα θα αποτύχει πολύ γρήγορα (το περιβάλλον CartPole τερματίζεται όταν η ταχύτητα του CartPole, η θέση του ή η γωνία του βρίσκονται εκτός ορισμένων ορίων).\n",
    "\n",
    "> Η προσομοίωση θα ανοίξει σε νέο παράθυρο. Μπορείτε να εκτελέσετε τον κώδικα πολλές φορές και να δείτε πώς συμπεριφέρεται.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορείτε να παρατηρήσετε ότι οι παρατηρήσεις περιέχουν 4 αριθμούς. Αυτοί είναι:\n",
    "- Θέση του καροτσιού\n",
    "- Ταχύτητα του καροτσιού\n",
    "- Γωνία του πόλου\n",
    "- Ρυθμός περιστροφής του πόλου\n",
    "\n",
    "`rew` είναι η ανταμοιβή που λαμβάνουμε σε κάθε βήμα. Μπορείτε να δείτε ότι στο περιβάλλον CartPole λαμβάνετε 1 πόντο για κάθε βήμα προσομοίωσης, και ο στόχος είναι να μεγιστοποιήσετε τη συνολική ανταμοιβή, δηλαδή τον χρόνο που το CartPole μπορεί να ισορροπήσει χωρίς να πέσει.\n",
    "\n",
    "Κατά τη διάρκεια της ενισχυτικής μάθησης, ο στόχος μας είναι να εκπαιδεύσουμε μια **πολιτική** $\\pi$, η οποία για κάθε κατάσταση $s$ θα μας λέει ποια ενέργεια $a$ να πάρουμε, δηλαδή ουσιαστικά $a = \\pi(s)$.\n",
    "\n",
    "Αν θέλετε μια πιθανή λύση, μπορείτε να σκεφτείτε την πολιτική ως επιστροφή ενός συνόλου πιθανοτήτων για κάθε ενέργεια, δηλαδή $\\pi(a|s)$ θα σημαίνει την πιθανότητα να πάρουμε την ενέργεια $a$ στην κατάσταση $s$.\n",
    "\n",
    "## Μέθοδος Πολιτικής Κλίσης\n",
    "\n",
    "Στον πιο απλό αλγόριθμο RL, που ονομάζεται **Πολιτική Κλίσης**, θα εκπαιδεύσουμε ένα νευρωνικό δίκτυο να προβλέπει την επόμενη ενέργεια.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα εκπαιδεύσουμε το δίκτυο εκτελώντας πολλά πειράματα και ενημερώνοντας το δίκτυό μας μετά από κάθε εκτέλεση. Ας ορίσουμε μια συνάρτηση που θα εκτελεί το πείραμα και θα επιστρέφει τα αποτελέσματα (η λεγόμενη **ίχνος**) - όλες τις καταστάσεις, τις ενέργειες (και τις προτεινόμενες πιθανότητές τους) και τις ανταμοιβές:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορείτε να εκτελέσετε ένα επεισόδιο με μη εκπαιδευμένο δίκτυο και να παρατηρήσετε ότι η συνολική ανταμοιβή (γνωστή και ως διάρκεια του επεισοδίου) είναι πολύ χαμηλή:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ένα από τα δύσκολα σημεία του αλγορίθμου πολιτικής κλίσης είναι η χρήση **εκπτωτικών ανταμοιβών**. Η ιδέα είναι ότι υπολογίζουμε το διάνυσμα των συνολικών ανταμοιβών σε κάθε βήμα του παιχνιδιού, και κατά τη διάρκεια αυτής της διαδικασίας εκπτύσσουμε τις πρώιμες ανταμοιβές χρησιμοποιώντας κάποιον συντελεστή $gamma$. Επίσης, κανονικοποιούμε το προκύπτον διάνυσμα, επειδή θα το χρησιμοποιήσουμε ως βάρος για να επηρεάσουμε την εκπαίδευσή μας:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα ας ξεκινήσουμε την πραγματική εκπαίδευση! Θα τρέξουμε 300 επεισόδια, και σε κάθε επεισόδιο θα κάνουμε τα εξής:\n",
    "\n",
    "1. Εκτελούμε το πείραμα και συλλέγουμε την ιχνηλάτηση.\n",
    "1. Υπολογίζουμε τη διαφορά (`gradients`) μεταξύ των ενεργειών που πραγματοποιήθηκαν και των προβλεπόμενων πιθανοτήτων. Όσο μικρότερη είναι η διαφορά, τόσο πιο σίγουροι είμαστε ότι έχουμε πάρει τη σωστή ενέργεια.\n",
    "1. Υπολογίζουμε τις προεξοφλημένες ανταμοιβές και πολλαπλασιάζουμε τα gradients με τις προεξοφλημένες ανταμοιβές - αυτό θα διασφαλίσει ότι τα βήματα με υψηλότερες ανταμοιβές θα έχουν μεγαλύτερη επίδραση στο τελικό αποτέλεσμα από αυτά με χαμηλότερες ανταμοιβές.\n",
    "1. Οι αναμενόμενες ενέργειες-στόχοι για το νευρωνικό μας δίκτυο θα προέρχονται εν μέρει από τις προβλεπόμενες πιθανότητες κατά τη διάρκεια της εκτέλεσης και εν μέρει από τα υπολογισμένα gradients. Θα χρησιμοποιήσουμε την παράμετρο `alpha` για να καθορίσουμε σε ποιο βαθμό λαμβάνονται υπόψη τα gradients και οι ανταμοιβές - αυτό ονομάζεται *ρυθμός μάθησης* του αλγορίθμου ενίσχυσης.\n",
    "1. Τέλος, εκπαιδεύουμε το δίκτυό μας στις καταστάσεις και τις αναμενόμενες ενέργειες, και επαναλαμβάνουμε τη διαδικασία.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα ας εκτελέσουμε το επεισόδιο με απόδοση για να δούμε το αποτέλεσμα:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ελπίζουμε να βλέπετε ότι η ράβδος μπορεί πλέον να ισορροπεί αρκετά καλά!\n",
    "\n",
    "## Μοντέλο Actor-Critic\n",
    "\n",
    "Το μοντέλο Actor-Critic αποτελεί περαιτέρω εξέλιξη των policy gradients, στο οποίο δημιουργούμε ένα νευρωνικό δίκτυο για να μάθει τόσο την πολιτική όσο και τις εκτιμώμενες ανταμοιβές. Το δίκτυο θα έχει δύο εξόδους (ή μπορείτε να το δείτε ως δύο ξεχωριστά δίκτυα):\n",
    "* **Actor** θα προτείνει την ενέργεια που πρέπει να ληφθεί, δίνοντάς μας την κατανομή πιθανοτήτων κατάστασης, όπως στο μοντέλο policy gradient.\n",
    "* **Critic** θα εκτιμήσει ποια θα ήταν η ανταμοιβή από αυτές τις ενέργειες. Επιστρέφει τις συνολικές εκτιμώμενες ανταμοιβές στο μέλλον για τη δεδομένη κατάσταση.\n",
    "\n",
    "Ας ορίσουμε ένα τέτοιο μοντέλο:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα χρειαστεί να τροποποιήσουμε ελαφρώς τις συναρτήσεις `discounted_rewards` και `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα θα εκτελέσουμε τον κύριο βρόχο εκπαίδευσης. Θα χρησιμοποιήσουμε τη διαδικασία χειροκίνητης εκπαίδευσης του δικτύου υπολογίζοντας κατάλληλες συναρτήσεις απώλειας και ενημερώνοντας τις παραμέτρους του δικτύου:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συμπέρασμα\n",
    "\n",
    "Είδαμε δύο αλγόριθμους Ενισχυτικής Μάθησης (RL) σε αυτή την επίδειξη: τον απλό αλγόριθμο policy gradient και τον πιο εξελιγμένο actor-critic. Μπορείτε να παρατηρήσετε ότι αυτοί οι αλγόριθμοι λειτουργούν με αφηρημένες έννοιες όπως η κατάσταση, η δράση και η ανταμοιβή - γι' αυτό μπορούν να εφαρμοστούν σε πολύ διαφορετικά περιβάλλοντα.\n",
    "\n",
    "Η Ενισχυτική Μάθηση μας επιτρέπει να μάθουμε την καλύτερη στρατηγική για την επίλυση ενός προβλήματος απλώς παρατηρώντας την τελική ανταμοιβή. Το γεγονός ότι δεν χρειαζόμαστε σύνολα δεδομένων με ετικέτες μας δίνει τη δυνατότητα να επαναλαμβάνουμε προσομοιώσεις πολλές φορές για να βελτιστοποιήσουμε τα μοντέλα μας. Παρ' όλα αυτά, υπάρχουν ακόμα πολλές προκλήσεις στην Ενισχυτική Μάθηση, τις οποίες μπορείτε να ανακαλύψετε αν αποφασίσετε να εστιάσετε περισσότερο σε αυτόν τον συναρπαστικό τομέα της Τεχνητής Νοημοσύνης.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T09:37:13+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}