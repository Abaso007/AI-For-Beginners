<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-29T09:07:48+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "el"
}
-->
# Εισαγωγή στα Νευρωνικά Δίκτυα. Πολυεπίπεδος Αντιληπτήρας

Στην προηγούμενη ενότητα, μάθατε για το απλούστερο μοντέλο νευρωνικού δικτύου - τον μονοεπίπεδο αντιληπτήρα, ένα γραμμικό μοντέλο ταξινόμησης δύο κατηγοριών.

Σε αυτή την ενότητα θα επεκτείνουμε αυτό το μοντέλο σε ένα πιο ευέλικτο πλαίσιο, που μας επιτρέπει να:

* πραγματοποιούμε **ταξινόμηση πολλαπλών κατηγοριών** εκτός από δύο κατηγοριών
* επιλύουμε **προβλήματα παλινδρόμησης** εκτός από ταξινόμηση
* διαχωρίζουμε κατηγορίες που δεν είναι γραμμικά διαχωρίσιμες

Θα αναπτύξουμε επίσης το δικό μας αρθρωτό πλαίσιο σε Python, που θα μας επιτρέπει να κατασκευάζουμε διαφορετικές αρχιτεκτονικές νευρωνικών δικτύων.

## [Προ-διάλεξης κουίζ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Τυποποίηση της Μηχανικής Μάθησης

Ας ξεκινήσουμε με την τυποποίηση του προβλήματος της Μηχανικής Μάθησης. Υποθέτουμε ότι έχουμε ένα σύνολο δεδομένων εκπαίδευσης **X** με ετικέτες **Y**, και πρέπει να κατασκευάσουμε ένα μοντέλο *f* που θα κάνει τις πιο ακριβείς προβλέψεις. Η ποιότητα των προβλέψεων μετριέται από τη **συνάρτηση απώλειας** ℒ. Οι ακόλουθες συναρτήσεις απώλειας χρησιμοποιούνται συχνά:

* Για πρόβλημα παλινδρόμησης, όταν πρέπει να προβλέψουμε έναν αριθμό, μπορούμε να χρησιμοποιήσουμε **απόλυτο σφάλμα** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ή **τετραγωνικό σφάλμα** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Για ταξινόμηση, χρησιμοποιούμε **0-1 απώλεια** (που είναι ουσιαστικά το ίδιο με την **ακρίβεια** του μοντέλου), ή **λογιστική απώλεια**.

Για τον μονοεπίπεδο αντιληπτήρα, η συνάρτηση *f* οριζόταν ως γραμμική συνάρτηση *f(x)=wx+b* (όπου *w* είναι ο πίνακας βαρών, *x* είναι το διάνυσμα χαρακτηριστικών εισόδου, και *b* είναι το διάνυσμα μεροληψίας). Για διαφορετικές αρχιτεκτονικές νευρωνικών δικτύων, αυτή η συνάρτηση μπορεί να πάρει πιο σύνθετη μορφή.

> Στην περίπτωση της ταξινόμησης, είναι συχνά επιθυμητό να λαμβάνουμε πιθανότητες των αντίστοιχων κατηγοριών ως έξοδο του δικτύου. Για να μετατρέψουμε αυθαίρετους αριθμούς σε πιθανότητες (π.χ. να κανονικοποιήσουμε την έξοδο), χρησιμοποιούμε συχνά τη συνάρτηση **softmax** σ, και η συνάρτηση *f* γίνεται *f(x)=σ(wx+b)*

Στον ορισμό της *f* παραπάνω, *w* και *b* ονομάζονται **παράμετροι** θ=⟨*w,b*⟩. Δεδομένου του συνόλου δεδομένων ⟨**X**,**Y**⟩, μπορούμε να υπολογίσουμε ένα συνολικό σφάλμα για ολόκληρο το σύνολο δεδομένων ως συνάρτηση των παραμέτρων θ.

> ✅ **Ο στόχος της εκπαίδευσης του νευρωνικού δικτύου είναι να ελαχιστοποιηθεί το σφάλμα μεταβάλλοντας τις παραμέτρους θ**

## Βελτιστοποίηση με Καθοδική Κλίση

Υπάρχει μια γνωστή μέθοδος βελτιστοποίησης συναρτήσεων που ονομάζεται **καθοδική κλίση**. Η ιδέα είναι ότι μπορούμε να υπολογίσουμε την παράγωγο (στην πολυδιάστατη περίπτωση ονομάζεται **κλίση**) της συνάρτησης απώλειας ως προς τις παραμέτρους, και να μεταβάλλουμε τις παραμέτρους με τέτοιο τρόπο ώστε το σφάλμα να μειώνεται. Αυτό μπορεί να τυποποιηθεί ως εξής:

* Αρχικοποιούμε τις παραμέτρους με κάποιες τυχαίες τιμές w<sup>(0)</sup>, b<sup>(0)</sup>
* Επαναλαμβάνουμε το ακόλουθο βήμα πολλές φορές:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

Κατά την εκπαίδευση, τα βήματα βελτιστοποίησης υποτίθεται ότι υπολογίζονται λαμβάνοντας υπόψη ολόκληρο το σύνολο δεδομένων (θυμηθείτε ότι η απώλεια υπολογίζεται ως άθροισμα όλων των δειγμάτων εκπαίδευσης). Ωστόσο, στην πραγματική ζωή λαμβάνουμε μικρά τμήματα του συνόλου δεδομένων που ονομάζονται **μικροπαρτίδες**, και υπολογίζουμε τις κλίσεις βάσει ενός υποσυνόλου δεδομένων. Επειδή το υποσύνολο λαμβάνεται τυχαία κάθε φορά, αυτή η μέθοδος ονομάζεται **στοχαστική καθοδική κλίση** (SGD).

## Πολυεπίπεδοι Αντιληπτήρες και Ανάδρομη Διάδοση

Το μονοεπίπεδο δίκτυο, όπως είδαμε παραπάνω, είναι ικανό να ταξινομεί γραμμικά διαχωρίσιμες κατηγορίες. Για να κατασκευάσουμε ένα πιο πλούσιο μοντέλο, μπορούμε να συνδυάσουμε αρκετά επίπεδα του δικτύου. Μαθηματικά, αυτό θα σήμαινε ότι η συνάρτηση *f* θα είχε πιο σύνθετη μορφή και θα υπολογιζόταν σε αρκετά βήματα:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

Εδώ, α είναι μια **μη γραμμική συνάρτηση ενεργοποίησης**, σ είναι η συνάρτηση softmax, και οι παράμετροι θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Ο αλγόριθμος καθοδικής κλίσης θα παραμείνει ο ίδιος, αλλά θα είναι πιο δύσκολο να υπολογιστούν οι κλίσεις. Δεδομένου του κανόνα αλυσίδας για τη διαφοροποίηση, μπορούμε να υπολογίσουμε τις παραγώγους ως:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ Ο κανόνας αλυσίδας χρησιμοποιείται για τον υπολογισμό των παραγώγων της συνάρτησης απώλειας ως προς τις παραμέτρους.

Σημειώστε ότι το αριστερότερο μέρος όλων αυτών των εκφράσεων είναι το ίδιο, και έτσι μπορούμε να υπολογίσουμε αποτελεσματικά τις παραγώγους ξεκινώντας από τη συνάρτηση απώλειας και πηγαίνοντας "προς τα πίσω" μέσω του γραφήματος υπολογισμού. Έτσι, η μέθοδος εκπαίδευσης ενός πολυεπίπεδου αντιληπτήρα ονομάζεται **ανάδρομη διάδοση**, ή 'backprop'.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: αναφορά εικόνας

> ✅ Θα καλύψουμε την ανάδρομη διάδοση με πολύ περισσότερες λεπτομέρειες στο παράδειγμα του σημειωματάριου μας.  

## Συμπέρασμα

Σε αυτό το μάθημα, κατασκευάσαμε τη δική μας βιβλιοθήκη νευρωνικών δικτύων και τη χρησιμοποιήσαμε για μια απλή δισδιάστατη ταξινόμηση.

## 🚀 Πρόκληση

Στο συνοδευτικό σημειωματάριο, θα υλοποιήσετε το δικό σας πλαίσιο για την κατασκευή και εκπαίδευση πολυεπίπεδων αντιληπτήρων. Θα μπορέσετε να δείτε λεπτομερώς πώς λειτουργούν τα σύγχρονα νευρωνικά δίκτυα.

Μεταβείτε στο σημειωματάριο [OwnFramework](OwnFramework.ipynb) και δουλέψτε πάνω σε αυτό.

## [Μετα-διάλεξης κουίζ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Ανασκόπηση & Αυτομελέτη

Η ανάδρομη διάδοση είναι ένας κοινός αλγόριθμος που χρησιμοποιείται στην Τεχνητή Νοημοσύνη και τη Μηχανική Μάθηση, αξίζει να μελετηθεί [πιο λεπτομερώς](https://wikipedia.org/wiki/Backpropagation)

## [Εργασία](lab/README.md)

Σε αυτό το εργαστήριο, σας ζητείται να χρησιμοποιήσετε το πλαίσιο που κατασκευάσατε σε αυτό το μάθημα για να επιλύσετε την ταξινόμηση χειρόγραφων ψηφίων MNIST.

* [Οδηγίες](lab/README.md)
* [Σημειωματάριο](lab/MyFW_MNIST.ipynb)

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.