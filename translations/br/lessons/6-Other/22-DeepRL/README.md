<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-26T10:15:56+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "br"
}
-->
# Aprendizado por Refor√ßo Profundo

O aprendizado por refor√ßo (RL) √© considerado um dos paradigmas b√°sicos de aprendizado de m√°quina, ao lado do aprendizado supervisionado e n√£o supervisionado. Enquanto no aprendizado supervisionado dependemos de um conjunto de dados com resultados conhecidos, o RL √© baseado no **aprender fazendo**. Por exemplo, quando vemos um jogo de computador pela primeira vez, come√ßamos a jogar, mesmo sem conhecer as regras, e logo conseguimos melhorar nossas habilidades apenas pelo processo de jogar e ajustar nosso comportamento.

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Para realizar RL, precisamos de:

* Um **ambiente** ou **simulador** que define as regras do jogo. Devemos ser capazes de executar experimentos no simulador e observar os resultados.
* Uma **fun√ß√£o de recompensa**, que indica qu√£o bem-sucedido foi nosso experimento. No caso de aprender a jogar um jogo de computador, a recompensa seria nossa pontua√ß√£o final.

Com base na fun√ß√£o de recompensa, devemos ser capazes de ajustar nosso comportamento e melhorar nossas habilidades, para que na pr√≥xima vez joguemos melhor. A principal diferen√ßa entre outros tipos de aprendizado de m√°quina e RL √© que no RL geralmente n√£o sabemos se ganhamos ou perdemos at√© terminarmos o jogo. Assim, n√£o podemos dizer se um movimento espec√≠fico √© bom ou n√£o - s√≥ recebemos uma recompensa no final do jogo.

Durante o RL, normalmente realizamos muitos experimentos. Em cada experimento, precisamos equilibrar entre seguir a estrat√©gia ideal que aprendemos at√© agora (**explora√ß√£o**) e explorar novos estados poss√≠veis (**explora√ß√£o**).

## OpenAI Gym

Uma ferramenta excelente para RL √© o [OpenAI Gym](https://gym.openai.com/) - um **ambiente de simula√ß√£o**, que pode simular muitos ambientes diferentes, desde jogos do Atari at√© a f√≠sica por tr√°s do equil√≠brio de um p√™ndulo. √â um dos ambientes de simula√ß√£o mais populares para treinar algoritmos de aprendizado por refor√ßo e √© mantido pela [OpenAI](https://openai.com/).

> **Note**: Voc√™ pode ver todos os ambientes dispon√≠veis no OpenAI Gym [aqui](https://gym.openai.com/envs/#classic_control).

## Equil√≠brio do CartPole

Provavelmente todos j√° viram dispositivos modernos de equil√≠brio, como o *Segway* ou *Gyroscooters*. Eles conseguem se equilibrar automaticamente ajustando suas rodas em resposta a um sinal de um aceler√¥metro ou girosc√≥pio. Nesta se√ß√£o, aprenderemos como resolver um problema semelhante - equilibrar um p√™ndulo. √â similar a uma situa√ß√£o em que um artista de circo precisa equilibrar um p√™ndulo na m√£o - mas esse equil√≠brio ocorre apenas em 1D.

Uma vers√£o simplificada do equil√≠brio √© conhecida como problema **CartPole**. No mundo do CartPole, temos um slider horizontal que pode se mover para a esquerda ou para a direita, e o objetivo √© equilibrar um p√™ndulo vertical no topo do slider enquanto ele se move.

<img alt="um cartpole" src="images/cartpole.png" width="200"/>

Para criar e usar este ambiente, precisamos de algumas linhas de c√≥digo Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Cada ambiente pode ser acessado exatamente da mesma maneira:
* `env.reset` inicia um novo experimento
* `env.step` realiza um passo de simula√ß√£o. Ele recebe uma **a√ß√£o** do **espa√ßo de a√ß√µes** e retorna uma **observa√ß√£o** (do espa√ßo de observa√ß√£o), bem como uma recompensa e um sinal de t√©rmino.

No exemplo acima, realizamos uma a√ß√£o aleat√≥ria em cada passo, o que faz com que a vida do experimento seja muito curta:

![cartpole sem equil√≠brio](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

O objetivo de um algoritmo de RL √© treinar um modelo - a chamada **pol√≠tica** œÄ - que retornar√° a a√ß√£o em resposta a um estado dado. Tamb√©m podemos considerar a pol√≠tica como probabil√≠stica, por exemplo, para qualquer estado *s* e a√ß√£o *a*, ela retornar√° a probabilidade œÄ(*a*|*s*) de que devemos tomar *a* no estado *s*.

## Algoritmo de Gradientes de Pol√≠tica

A maneira mais √≥bvia de modelar uma pol√≠tica √© criando uma rede neural que receba estados como entrada e retorne a√ß√µes correspondentes (ou melhor, as probabilidades de todas as a√ß√µes). Em certo sentido, seria semelhante a uma tarefa de classifica√ß√£o normal, com uma grande diferen√ßa - n√£o sabemos de antem√£o quais a√ß√µes devemos tomar em cada passo.

A ideia aqui √© estimar essas probabilidades. Constru√≠mos um vetor de **recompensas acumuladas**, que mostra nossa recompensa total em cada passo do experimento. Tamb√©m aplicamos **desconto de recompensa**, multiplicando recompensas anteriores por algum coeficiente Œ≥=0.99, para diminuir o papel das recompensas anteriores. Em seguida, refor√ßamos os passos ao longo do caminho do experimento que geram maiores recompensas.

> Saiba mais sobre o algoritmo de Gradientes de Pol√≠tica e veja-o em a√ß√£o no [notebook de exemplo](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Algoritmo Ator-Cr√≠tico

Uma vers√£o aprimorada da abordagem de Gradientes de Pol√≠tica √© chamada de **Ator-Cr√≠tico**. A ideia principal por tr√°s disso √© que a rede neural seria treinada para retornar duas coisas:

* A pol√≠tica, que determina qual a√ß√£o tomar. Esta parte √© chamada de **ator**
* A estimativa da recompensa total que podemos esperar obter neste estado - esta parte √© chamada de **cr√≠tico**.

Em certo sentido, essa arquitetura se assemelha a um [GAN](../../4-ComputerVision/10-GANs/README.md), onde temos duas redes que s√£o treinadas uma contra a outra. No modelo ator-cr√≠tico, o ator prop√µe a a√ß√£o que precisamos tomar, e o cr√≠tico tenta ser cr√≠tico e estimar o resultado. No entanto, nosso objetivo √© treinar essas redes em conjunto.

Como sabemos tanto as recompensas acumuladas reais quanto os resultados retornados pelo cr√≠tico durante o experimento, √© relativamente f√°cil construir uma fun√ß√£o de perda que minimize a diferen√ßa entre eles. Isso nos daria a **perda do cr√≠tico**. Podemos calcular a **perda do ator** usando a mesma abordagem do algoritmo de gradientes de pol√≠tica.

Depois de executar um desses algoritmos, podemos esperar que nosso CartPole se comporte assim:

![um cartpole equilibrado](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Exerc√≠cios: Gradientes de Pol√≠tica e RL Ator-Cr√≠tico

Continue seu aprendizado nos seguintes notebooks:

* [RL em TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL em PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Outras Tarefas de RL

O aprendizado por refor√ßo atualmente √© um campo de pesquisa em r√°pido crescimento. Alguns exemplos interessantes de aprendizado por refor√ßo s√£o:

* Ensinar um computador a jogar **jogos do Atari**. A parte desafiadora desse problema √© que n√£o temos um estado simples representado como um vetor, mas sim uma captura de tela - e precisamos usar uma CNN para converter essa imagem de tela em um vetor de caracter√≠sticas ou extrair informa√ß√µes de recompensa. Os jogos do Atari est√£o dispon√≠veis no Gym.
* Ensinar um computador a jogar jogos de tabuleiro, como Xadrez e Go. Recentemente, programas de √∫ltima gera√ß√£o como **Alpha Zero** foram treinados do zero por dois agentes jogando um contra o outro e melhorando a cada passo.
* Na ind√∫stria, o RL √© usado para criar sistemas de controle a partir de simula√ß√µes. Um servi√ßo chamado [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) √© especificamente projetado para isso.

## Conclus√£o

Agora aprendemos como treinar agentes para alcan√ßar bons resultados apenas fornecendo-lhes uma fun√ß√£o de recompensa que define o estado desejado do jogo e dando-lhes a oportunidade de explorar inteligentemente o espa√ßo de busca. Experimentamos com sucesso dois algoritmos e alcan√ßamos um bom resultado em um per√≠odo relativamente curto de tempo. No entanto, este √© apenas o come√ßo de sua jornada no RL, e voc√™ definitivamente deve considerar fazer um curso separado se quiser se aprofundar.

## üöÄ Desafio

Explore as aplica√ß√µes listadas na se√ß√£o 'Outras Tarefas de RL' e tente implementar uma delas!

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Revis√£o e Autoestudo

Saiba mais sobre aprendizado por refor√ßo cl√°ssico em nosso [Curr√≠culo de Aprendizado de M√°quina para Iniciantes](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Assista a [este √≥timo v√≠deo](https://www.youtube.com/watch?v=qv6UVOQ0F44) que fala sobre como um computador pode aprender a jogar Super Mario.

## Tarefa: [Treine um Mountain Car](lab/README.md)

Seu objetivo nesta tarefa ser√° treinar um ambiente diferente do Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.