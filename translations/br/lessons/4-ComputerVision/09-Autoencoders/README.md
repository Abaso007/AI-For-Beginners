<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1b8d9e1b3a6f1daa864b1ff3dfc3076d",
  "translation_date": "2025-09-23T08:21:00+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "br"
}
-->
# Autoencoders

Ao treinar CNNs, um dos problemas √© que precisamos de muitos dados rotulados. No caso de classifica√ß√£o de imagens, precisamos separar as imagens em diferentes classes, o que exige esfor√ßo manual.

## [Quiz pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/17)

No entanto, podemos querer usar dados brutos (n√£o rotulados) para treinar extratores de caracter√≠sticas de CNN, o que √© chamado de **aprendizado auto-supervisionado**. Em vez de r√≥tulos, usaremos as imagens de treinamento como entrada e sa√≠da da rede. A ideia principal do **autoencoder** √© que teremos uma **rede codificadora** que converte a imagem de entrada em algum **espa√ßo latente** (normalmente √© apenas um vetor de tamanho menor), e ent√£o uma **rede decodificadora**, cujo objetivo ser√° reconstruir a imagem original.

> ‚úÖ Um [autoencoder](https://wikipedia.org/wiki/Autoencoder) √© "um tipo de rede neural artificial usada para aprender codifica√ß√µes eficientes de dados n√£o rotulados."

Como estamos treinando um autoencoder para capturar o m√°ximo de informa√ß√µes da imagem original poss√≠vel para uma reconstru√ß√£o precisa, a rede tenta encontrar a melhor **representa√ß√£o** das imagens de entrada para capturar seu significado.

![Diagrama de AutoEncoder](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.br.jpg)

> Imagem do [blog do Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Cen√°rios para usar Autoencoders

Embora reconstruir imagens originais n√£o pare√ßa √∫til por si s√≥, h√° alguns cen√°rios em que os autoencoders s√£o especialmente √∫teis:

* **Redu√ß√£o da dimens√£o de imagens para visualiza√ß√£o** ou **treinamento de representa√ß√µes de imagens**. Normalmente, autoencoders oferecem melhores resultados do que PCA, porque levam em conta a natureza espacial das imagens e caracter√≠sticas hier√°rquicas.
* **Remo√ß√£o de ru√≠do**, ou seja, eliminar ru√≠dos da imagem. Como o ru√≠do carrega muitas informa√ß√µes in√∫teis, o autoencoder n√£o consegue encaixar tudo em um espa√ßo latente relativamente pequeno e, assim, captura apenas a parte importante da imagem. Ao treinar removedores de ru√≠do, come√ßamos com imagens originais e usamos imagens com ru√≠do artificialmente adicionado como entrada para o autoencoder.
* **Super-resolu√ß√£o**, aumentando a resolu√ß√£o da imagem. Come√ßamos com imagens de alta resolu√ß√£o e usamos a imagem com resolu√ß√£o mais baixa como entrada do autoencoder.
* **Modelos generativos**. Depois de treinar o autoencoder, a parte decodificadora pode ser usada para criar novos objetos a partir de vetores latentes aleat√≥rios.

## Autoencoders Variacionais (VAE)

Autoencoders tradicionais reduzem a dimens√£o dos dados de entrada de alguma forma, identificando as caracter√≠sticas importantes das imagens de entrada. No entanto, os vetores latentes frequentemente n√£o fazem muito sentido. Em outras palavras, usando o conjunto de dados MNIST como exemplo, identificar quais d√≠gitos correspondem a diferentes vetores latentes n√£o √© uma tarefa f√°cil, porque vetores latentes pr√≥ximos n√£o necessariamente correspondem aos mesmos d√≠gitos.

Por outro lado, para treinar modelos *generativos*, √© melhor ter algum entendimento do espa√ßo latente. Essa ideia nos leva ao **autoencoder variacional** (VAE).

O VAE √© um autoencoder que aprende a prever a *distribui√ß√£o estat√≠stica* dos par√¢metros latentes, chamada de **distribui√ß√£o latente**. Por exemplo, podemos querer que os vetores latentes sejam distribu√≠dos normalmente com alguma m√©dia z<sub>mean</sub> e desvio padr√£o z<sub>sigma</sub> (tanto a m√©dia quanto o desvio padr√£o s√£o vetores de alguma dimensionalidade d). O codificador no VAE aprende a prever esses par√¢metros, e ent√£o o decodificador usa um vetor aleat√≥rio dessa distribui√ß√£o para reconstruir o objeto.

Resumindo:

 * A partir do vetor de entrada, prevemos `z_mean` e `z_log_sigma` (em vez de prever o desvio padr√£o diretamente, prevemos seu logaritmo)
 * Amostramos um vetor `sample` da distribui√ß√£o N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * O decodificador tenta decodificar a imagem original usando `sample` como vetor de entrada

 <img src="images/vae.png" width="50%">

> Imagem deste [post no blog](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) por Isaak Dykeman

Autoencoders variacionais usam uma fun√ß√£o de perda complexa que consiste em duas partes:

* **Perda de reconstru√ß√£o** √© a fun√ß√£o de perda que mostra qu√£o pr√≥xima a imagem reconstru√≠da est√° do alvo (pode ser o Erro Quadr√°tico M√©dio, ou MSE). √â a mesma fun√ß√£o de perda usada em autoencoders normais.
* **Perda KL**, que garante que as distribui√ß√µes das vari√°veis latentes permane√ßam pr√≥ximas da distribui√ß√£o normal. Baseia-se na no√ß√£o de [diverg√™ncia de Kullback-Leibler](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - uma m√©trica para estimar qu√£o semelhantes duas distribui√ß√µes estat√≠sticas s√£o.

Uma vantagem importante dos VAEs √© que eles permitem gerar novas imagens relativamente f√°cil, porque sabemos de qual distribui√ß√£o amostrar os vetores latentes. Por exemplo, se treinarmos um VAE com vetor latente 2D no MNIST, podemos ent√£o variar os componentes do vetor latente para obter diferentes d√≠gitos:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Imagem por [Dmitry Soshnikov](http://soshnikov.com)

Observe como as imagens se misturam umas √†s outras, √† medida que come√ßamos a obter vetores latentes de diferentes partes do espa√ßo de par√¢metros latentes. Tamb√©m podemos visualizar esse espa√ßo em 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Imagem por [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Exerc√≠cios: Autoencoders

Saiba mais sobre autoencoders nos notebooks correspondentes:

* [Autoencoders em TensorFlow](AutoencodersTF.ipynb)
* [Autoencoders em PyTorch](AutoEncodersPyTorch.ipynb)

## Propriedades dos Autoencoders

* **Espec√≠ficos para os dados** - eles funcionam bem apenas com o tipo de imagens em que foram treinados. Por exemplo, se treinarmos uma rede de super-resolu√ß√£o em flores, ela n√£o funcionar√° bem em retratos. Isso ocorre porque a rede pode produzir imagens de alta resolu√ß√£o ao usar detalhes finos das caracter√≠sticas aprendidas no conjunto de dados de treinamento.
* **Com perdas** - a imagem reconstru√≠da n√£o √© id√™ntica √† imagem original. A natureza da perda √© definida pela *fun√ß√£o de perda* usada durante o treinamento.
* Funciona com **dados n√£o rotulados**

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Conclus√£o

Nesta li√ß√£o, voc√™ aprendeu sobre os v√°rios tipos de autoencoders dispon√≠veis para o cientista de IA. Aprendeu como constru√≠-los e como us√°-los para reconstruir imagens. Tamb√©m aprendeu sobre o VAE e como us√°-lo para gerar novas imagens.

## üöÄ Desafio

Nesta li√ß√£o, voc√™ aprendeu sobre o uso de autoencoders para imagens. Mas eles tamb√©m podem ser usados para m√∫sica! Confira o projeto [MusicVAE](https://magenta.tensorflow.org/music-vae) do Magenta, que usa autoencoders para aprender a reconstruir m√∫sica. Fa√ßa alguns [experimentos](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) com esta biblioteca para ver o que voc√™ pode criar.

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Revis√£o e Autoestudo

Para refer√™ncia, leia mais sobre autoencoders nestes recursos:

* [Construindo Autoencoders em Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Post no blog NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Autoencoders Variacionais Explicados](https://kvfrans.com/variational-autoencoders-explained/)
* [Autoencoders Variacionais Condicionais](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Tarefa

No final deste [notebook usando TensorFlow](AutoencodersTF.ipynb), voc√™ encontrar√° uma 'tarefa' - use-a como sua tarefa.

---

