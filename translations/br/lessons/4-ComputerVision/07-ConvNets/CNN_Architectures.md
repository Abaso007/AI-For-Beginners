<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f7b97b375358cb51a1e098df306bf73",
  "translation_date": "2025-08-26T09:33:07+00:00",
  "source_file": "lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md",
  "language_code": "br"
}
-->
# Arquiteturas Conhecidas de CNN

### VGG-16

VGG-16 √© uma rede que alcan√ßou 92,7% de precis√£o na classifica√ß√£o top-5 do ImageNet em 2014. Ela possui a seguinte estrutura de camadas:

![Camadas do ImageNet](../../../../../translated_images/vgg-16-arch1.d901a5583b3a51baeaab3e768567d921e5d54befa46e1e642616c5458c934028.br.jpg)

Como voc√™ pode ver, a VGG segue uma arquitetura piramidal tradicional, que √© uma sequ√™ncia de camadas de convolu√ß√£o e pooling.

![Pir√¢mide do ImageNet](../../../../../translated_images/vgg-16-arch.64ff2137f50dd49fdaa786e3f3a975b3f22615efd13efb19c5d22f12e01451a1.br.jpg)

> Imagem de [Researchgate](https://www.researchgate.net/figure/Vgg16-model-structure-To-get-the-VGG-NIN-model-we-replace-the-2-nd-4-th-6-th-7-th_fig2_335194493)

### ResNet

ResNet √© uma fam√≠lia de modelos proposta pela Microsoft Research em 2015. A ideia principal da ResNet √© usar **blocos residuais**:

<img src="images/resnet-block.png" width="300"/>

> Imagem deste [artigo](https://arxiv.org/pdf/1512.03385.pdf)

A raz√£o para usar a passagem de identidade √© fazer com que a camada preveja **a diferen√ßa** entre o resultado de uma camada anterior e a sa√≠da do bloco residual - da√≠ o nome *residual*. Esses blocos s√£o muito mais f√°ceis de treinar, e √© poss√≠vel construir redes com centenas desses blocos (as variantes mais comuns s√£o ResNet-52, ResNet-101 e ResNet-152).

Voc√™ tamb√©m pode pensar nessa rede como sendo capaz de ajustar sua complexidade ao conjunto de dados. Inicialmente, quando voc√™ come√ßa a treinar a rede, os valores dos pesos s√£o pequenos, e a maior parte do sinal passa pelas camadas de identidade. √Ä medida que o treinamento avan√ßa e os pesos se tornam maiores, a import√¢ncia dos par√¢metros da rede cresce, e a rede se ajusta para acomodar o poder expressivo necess√°rio para classificar corretamente as imagens de treinamento.

### Google Inception

A arquitetura Google Inception leva essa ideia um passo adiante e constr√≥i cada camada da rede como uma combina√ß√£o de v√°rios caminhos diferentes:

<img src="images/inception.png" width="400"/>

> Imagem de [Researchgate](https://www.researchgate.net/figure/Inception-module-with-dimension-reductions-left-and-schema-for-Inception-ResNet-v1_fig2_355547454)

Aqui, precisamos enfatizar o papel das convolu√ß√µes 1x1, porque, √† primeira vista, elas n√£o fazem sentido. Por que precisar√≠amos passar pela imagem com um filtro 1x1? No entanto, √© importante lembrar que os filtros de convolu√ß√£o tamb√©m trabalham com v√°rios canais de profundidade (originalmente - cores RGB, em camadas subsequentes - canais para diferentes filtros), e a convolu√ß√£o 1x1 √© usada para misturar esses canais de entrada usando diferentes pesos trein√°veis. Ela tamb√©m pode ser vista como uma redu√ß√£o de dimensionalidade (pooling) na dimens√£o dos canais.

Aqui est√° [um bom artigo sobre convolu√ß√£o 1x1](https://medium.com/analytics-vidhya/talented-mr-1x1-comprehensive-look-at-1x1-convolution-in-deep-learning-f6b355825578) e [o artigo original](https://arxiv.org/pdf/1312.4400.pdf).

### MobileNet

MobileNet √© uma fam√≠lia de modelos com tamanho reduzido, adequada para dispositivos m√≥veis. Use-os se voc√™ tiver poucos recursos e puder sacrificar um pouco de precis√£o. A ideia principal por tr√°s deles √© a chamada **convolu√ß√£o separ√°vel por profundidade**, que permite representar filtros de convolu√ß√£o como uma composi√ß√£o de convolu√ß√µes espaciais e convolu√ß√µes 1x1 sobre os canais de profundidade. Isso reduz significativamente o n√∫mero de par√¢metros, tornando a rede menor em tamanho e tamb√©m mais f√°cil de treinar com menos dados.

Aqui est√° [um bom artigo sobre MobileNet](https://medium.com/analytics-vidhya/image-classification-with-mobilenet-cc6fbb2cd470).

## Conclus√£o

Nesta unidade, voc√™ aprendeu o conceito principal por tr√°s das redes neurais de vis√£o computacional - redes convolucionais. Arquiteturas reais que alimentam classifica√ß√£o de imagens, detec√ß√£o de objetos e at√© redes de gera√ß√£o de imagens s√£o todas baseadas em CNNs, apenas com mais camadas e alguns truques adicionais de treinamento.

## üöÄ Desafio

Nos notebooks que acompanham esta unidade, h√° anota√ß√µes no final sobre como obter maior precis√£o. Fa√ßa alguns experimentos para ver se voc√™ consegue alcan√ßar uma precis√£o maior.

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/207)

## Revis√£o e Autoestudo

Embora as CNNs sejam mais frequentemente usadas para tarefas de Vis√£o Computacional, elas s√£o geralmente boas para extrair padr√µes de tamanho fixo. Por exemplo, se estivermos lidando com sons, tamb√©m podemos querer usar CNNs para procurar padr√µes espec√≠ficos no sinal de √°udio - nesse caso, os filtros seriam unidimensionais (e essa CNN seria chamada de 1D-CNN). Al√©m disso, √†s vezes uma 3D-CNN √© usada para extrair caracter√≠sticas em um espa√ßo multidimensional, como certos eventos ocorrendo em v√≠deos - a CNN pode capturar certos padr√µes de mudan√ßa de caracter√≠sticas ao longo do tempo. Fa√ßa uma revis√£o e autoestudo sobre outras tarefas que podem ser realizadas com CNNs.

## [Tarefa](lab/README.md)

Neste laborat√≥rio, sua tarefa √© classificar diferentes ra√ßas de gatos e c√£es. Essas imagens s√£o mais complexas do que o conjunto de dados MNIST, possuem dimens√µes maiores e h√° mais de 10 classes.

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional feita por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.