<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b708c9b85b833864c73c6281f1e6b96e",
  "translation_date": "2025-09-23T08:26:13+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "br"
}
-->
# Embeddings

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Ao treinar classificadores baseados em BoW ou TF/IDF, trabalhamos com vetores de bag-of-words de alta dimensionalidade com comprimento `vocab_size`, e est√°vamos convertendo explicitamente de vetores de representa√ß√£o posicional de baixa dimensionalidade para uma representa√ß√£o esparsa de uma √∫nica posi√ß√£o ativa (one-hot). No entanto, essa representa√ß√£o one-hot n√£o √© eficiente em termos de mem√≥ria. Al√©m disso, cada palavra √© tratada de forma independente, ou seja, vetores codificados em one-hot n√£o expressam nenhuma similaridade sem√¢ntica entre palavras.

A ideia de **embedding** √© representar palavras por vetores densos de menor dimensionalidade, que de alguma forma refletem o significado sem√¢ntico de uma palavra. Mais adiante, discutiremos como construir embeddings de palavras significativos, mas, por enquanto, vamos apenas pensar em embeddings como uma forma de reduzir a dimensionalidade de um vetor de palavras.

Assim, a camada de embedding receberia uma palavra como entrada e produziria um vetor de sa√≠da com o tamanho especificado `embedding_size`. De certa forma, √© muito semelhante a uma camada `Linear`, mas, em vez de receber um vetor codificado em one-hot, ela ser√° capaz de receber um n√∫mero de palavra como entrada, permitindo evitar a cria√ß√£o de grandes vetores codificados em one-hot.

Ao usar uma camada de embedding como a primeira camada em nossa rede de classifica√ß√£o, podemos mudar de um modelo bag-of-words para um modelo **embedding bag**, onde primeiro convertemos cada palavra em nosso texto no embedding correspondente e, em seguida, calculamos alguma fun√ß√£o agregada sobre todos esses embeddings, como `sum`, `average` ou `max`.

![Imagem mostrando um classificador de embedding para cinco palavras de sequ√™ncia.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.br.png)

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Embeddings

Continue seu aprendizado nos seguintes notebooks:
* [Embeddings com PyTorch](EmbeddingsPyTorch.ipynb)
* [Embeddings com TensorFlow](EmbeddingsTF.ipynb)

## Embeddings Sem√¢nticos: Word2Vec

Embora a camada de embedding tenha aprendido a mapear palavras para representa√ß√µes vetoriais, essa representa√ß√£o n√£o necessariamente possui muito significado sem√¢ntico. Seria interessante aprender uma representa√ß√£o vetorial tal que palavras semelhantes ou sin√¥nimos correspondam a vetores pr√≥ximos entre si em termos de alguma dist√¢ncia vetorial (por exemplo, dist√¢ncia Euclidiana).

Para isso, precisamos pr√©-treinar nosso modelo de embedding em uma grande cole√ß√£o de textos de uma maneira espec√≠fica. Uma forma de treinar embeddings sem√¢nticos √© chamada [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ela se baseia em duas arquiteturas principais que s√£o usadas para produzir uma representa√ß√£o distribu√≠da de palavras:

 - **Continuous bag-of-words** (CBoW) ‚Äî nesta arquitetura, treinamos o modelo para prever uma palavra a partir do contexto ao redor. Dado o ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, o objetivo do modelo √© prever $W_0$ a partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** √© o oposto do CBoW. O modelo usa uma janela de palavras de contexto ao redor para prever a palavra atual.

CBoW √© mais r√°pido, enquanto skip-gram √© mais lento, mas faz um trabalho melhor ao representar palavras menos frequentes.

![Imagem mostrando os algoritmos CBoW e Skip-Gram para converter palavras em vetores.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.br.png)

> Imagem retirada [deste artigo](https://arxiv.org/pdf/1301.3781.pdf)

Embeddings pr√©-treinados do Word2Vec (assim como outros modelos similares, como GloVe) tamb√©m podem ser usados no lugar da camada de embedding em redes neurais. No entanto, precisamos lidar com vocabul√°rios, porque o vocabul√°rio usado para pr√©-treinar o Word2Vec/GloVe provavelmente ser√° diferente do vocabul√°rio em nosso corpus de texto. Confira os notebooks acima para ver como esse problema pode ser resolvido.

## Embeddings Contextuais

Uma limita√ß√£o importante das representa√ß√µes tradicionais de embeddings pr√©-treinados, como Word2Vec, √© o problema de desambigua√ß√£o de sentidos das palavras. Embora embeddings pr√©-treinados possam capturar parte do significado das palavras no contexto, todos os poss√≠veis significados de uma palavra s√£o codificados no mesmo embedding. Isso pode causar problemas em modelos posteriores, j√° que muitas palavras, como a palavra 'play', t√™m significados diferentes dependendo do contexto em que s√£o usadas.

Por exemplo, a palavra 'play' nas duas frases abaixo tem significados bem diferentes:

- Eu fui a uma **pe√ßa** no teatro.
- John quer **brincar** com seus amigos.

Os embeddings pr√©-treinados acima representam ambos os significados da palavra 'play' no mesmo embedding. Para superar essa limita√ß√£o, precisamos construir embeddings baseados no **modelo de linguagem**, que √© treinado em um grande corpus de texto e *sabe* como as palavras podem ser combinadas em diferentes contextos. Discutir embeddings contextuais est√° fora do escopo deste tutorial, mas voltaremos a eles ao falar sobre modelos de linguagem mais adiante no curso.

## Conclus√£o

Nesta li√ß√£o, voc√™ descobriu como construir e usar camadas de embedding no TensorFlow e PyTorch para refletir melhor os significados sem√¢nticos das palavras.

## üöÄ Desafio

Word2Vec tem sido usado em algumas aplica√ß√µes interessantes, incluindo a gera√ß√£o de letras de m√∫sicas e poesias. Confira [este artigo](https://www.politetype.com/blog/word2vec-color-poems), que explica como o autor usou Word2Vec para gerar poesia. Assista tamb√©m [este v√≠deo de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) para descobrir uma explica√ß√£o diferente dessa t√©cnica. Depois, tente aplicar essas t√©cnicas ao seu pr√≥prio corpus de texto, talvez obtido no Kaggle.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Revis√£o e Autoestudo

Leia este artigo sobre Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Assignment: Notebooks](assignment.md)

---

