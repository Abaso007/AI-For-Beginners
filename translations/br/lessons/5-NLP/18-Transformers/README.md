<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-26T08:38:53+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "br"
}
-->
# Mecanismos de Aten√ß√£o e Transformers

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Um dos problemas mais importantes no dom√≠nio de NLP √© a **tradu√ß√£o autom√°tica**, uma tarefa essencial que sustenta ferramentas como o Google Tradutor. Nesta se√ß√£o, vamos focar na tradu√ß√£o autom√°tica ou, de forma mais geral, em qualquer tarefa de *sequ√™ncia para sequ√™ncia* (tamb√©m chamada de **transdu√ß√£o de senten√ßas**).

Com RNNs, a tarefa de sequ√™ncia para sequ√™ncia √© implementada por duas redes recorrentes, onde uma rede, o **codificador**, comprime uma sequ√™ncia de entrada em um estado oculto, enquanto outra rede, o **decodificador**, expande esse estado oculto em um resultado traduzido. Existem alguns problemas com essa abordagem:

* O estado final da rede codificadora tem dificuldade em lembrar o in√≠cio de uma senten√ßa, causando baixa qualidade do modelo para senten√ßas longas.
* Todas as palavras em uma sequ√™ncia t√™m o mesmo impacto no resultado. Na realidade, no entanto, palavras espec√≠ficas na sequ√™ncia de entrada frequentemente t√™m mais impacto nos resultados sequenciais do que outras.

**Mecanismos de Aten√ß√£o** fornecem um meio de ponderar o impacto contextual de cada vetor de entrada em cada previs√£o de sa√≠da da RNN. Isso √© implementado criando atalhos entre os estados intermedi√°rios da RNN de entrada e a RNN de sa√≠da. Dessa forma, ao gerar o s√≠mbolo de sa√≠da y<sub>t</sub>, levamos em conta todos os estados ocultos de entrada h<sub>i</sub>, com diferentes coeficientes de peso Œ±<sub>t,i</sub>.

![Imagem mostrando um modelo codificador/decodificador com uma camada de aten√ß√£o aditiva](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.br.png)

> O modelo codificador-decodificador com mecanismo de aten√ß√£o aditiva em [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citado deste [post de blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

A matriz de aten√ß√£o {Œ±<sub>i,j</sub>} representaria o grau em que certas palavras de entrada influenciam a gera√ß√£o de uma determinada palavra na sequ√™ncia de sa√≠da. Abaixo est√° um exemplo de tal matriz:

![Imagem mostrando um alinhamento de exemplo encontrado pelo RNNsearch-50, retirada de Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.br.png)

> Figura de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Os mecanismos de aten√ß√£o s√£o respons√°veis por grande parte do estado da arte atual ou pr√≥ximo ao atual em NLP. No entanto, adicionar aten√ß√£o aumenta significativamente o n√∫mero de par√¢metros do modelo, o que levou a problemas de escalabilidade com RNNs. Uma restri√ß√£o chave na escalabilidade das RNNs √© que a natureza recorrente dos modelos torna desafiador agrupar e paralelizar o treinamento. Em uma RNN, cada elemento de uma sequ√™ncia precisa ser processado em ordem sequencial, o que significa que n√£o pode ser facilmente paralelizado.

![Codificador Decodificador com Aten√ß√£o](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figura do [Blog do Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

A ado√ß√£o de mecanismos de aten√ß√£o combinada com essa restri√ß√£o levou √† cria√ß√£o dos modelos Transformers, agora estado da arte, que conhecemos e usamos hoje, como BERT e Open-GPT3.

## Modelos Transformers

Uma das principais ideias por tr√°s dos transformers √© evitar a natureza sequencial das RNNs e criar um modelo que seja paraleliz√°vel durante o treinamento. Isso √© alcan√ßado implementando duas ideias:

* codifica√ß√£o posicional
* uso de mecanismo de autoaten√ß√£o para capturar padr√µes em vez de RNNs (ou CNNs) (√© por isso que o artigo que introduz transformers √© chamado *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Codifica√ß√£o/Embelezamento Posicional

A ideia da codifica√ß√£o posicional √© a seguinte:  
1. Ao usar RNNs, a posi√ß√£o relativa dos tokens √© representada pelo n√∫mero de passos e, portanto, n√£o precisa ser explicitamente representada.  
2. No entanto, ao mudar para aten√ß√£o, precisamos saber as posi√ß√µes relativas dos tokens dentro de uma sequ√™ncia.  
3. Para obter codifica√ß√£o posicional, aumentamos nossa sequ√™ncia de tokens com uma sequ√™ncia de posi√ß√µes dos tokens na sequ√™ncia (ou seja, uma sequ√™ncia de n√∫meros 0,1, ...).  
4. Em seguida, misturamos a posi√ß√£o do token com um vetor de embelezamento do token. Para transformar a posi√ß√£o (inteiro) em um vetor, podemos usar diferentes abordagens:

* Embelezamento trein√°vel, semelhante ao embelezamento de tokens. Esta √© a abordagem que consideramos aqui. Aplicamos camadas de embelezamento tanto nos tokens quanto em suas posi√ß√µes, resultando em vetores de embelezamento de mesmas dimens√µes, que ent√£o somamos.
* Fun√ß√£o de codifica√ß√£o de posi√ß√£o fixa, conforme proposto no artigo original.

<img src="images/pos-embedding.png" width="50%"/>

> Imagem do autor

O resultado que obtemos com o embelezamento posicional incorpora tanto o token original quanto sua posi√ß√£o dentro de uma sequ√™ncia.

### Autoaten√ß√£o Multi-Head

Em seguida, precisamos capturar alguns padr√µes dentro de nossa sequ√™ncia. Para isso, os transformers usam um mecanismo de **autoaten√ß√£o**, que √© essencialmente aten√ß√£o aplicada √† mesma sequ√™ncia como entrada e sa√≠da. Aplicar autoaten√ß√£o nos permite levar em conta o **contexto** dentro da senten√ßa e ver quais palavras est√£o inter-relacionadas. Por exemplo, permite ver quais palavras s√£o referidas por correfer√™ncias, como *it*, e tamb√©m levar o contexto em considera√ß√£o:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.br.png)

> Imagem do [Blog do Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Nos transformers, usamos **Aten√ß√£o Multi-Head** para dar √† rede o poder de capturar v√°rios tipos diferentes de depend√™ncias, como rela√ß√µes de palavras de longo prazo vs. curto prazo, correfer√™ncia vs. algo mais, etc.

[Notebook TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) cont√©m mais detalhes sobre a implementa√ß√£o das camadas de transformers.

### Aten√ß√£o Codificador-Decodificador

Nos transformers, a aten√ß√£o √© usada em dois lugares:

* Para capturar padr√µes dentro do texto de entrada usando autoaten√ß√£o.
* Para realizar tradu√ß√£o de sequ√™ncia - √© a camada de aten√ß√£o entre codificador e decodificador.

A aten√ß√£o codificador-decodificador √© muito semelhante ao mecanismo de aten√ß√£o usado em RNNs, conforme descrito no in√≠cio desta se√ß√£o. Este diagrama animado explica o papel da aten√ß√£o codificador-decodificador.

![GIF animado mostrando como as avalia√ß√µes s√£o realizadas em modelos transformers.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Como cada posi√ß√£o de entrada √© mapeada independentemente para cada posi√ß√£o de sa√≠da, os transformers podem paralelizar melhor do que as RNNs, o que permite modelos de linguagem muito maiores e mais expressivos. Cada cabe√ßa de aten√ß√£o pode ser usada para aprender diferentes rela√ß√µes entre palavras que melhoram tarefas de Processamento de Linguagem Natural.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) √© uma rede transformer muito grande com v√°rias camadas: 12 camadas para *BERT-base* e 24 para *BERT-large*. O modelo √© primeiro pr√©-treinado em um grande corpus de dados de texto (WikiPedia + livros) usando treinamento n√£o supervisionado (prevendo palavras mascaradas em uma senten√ßa). Durante o pr√©-treinamento, o modelo absorve n√≠veis significativos de compreens√£o de linguagem que podem ser aproveitados com outros conjuntos de dados usando ajuste fino. Esse processo √© chamado de **aprendizado por transfer√™ncia**.

![imagem de http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.br.png)

> Imagem [fonte](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Exerc√≠cios: Transformers

Continue seu aprendizado nos seguintes notebooks:

* [Transformers em PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformers em TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Conclus√£o

Nesta li√ß√£o, voc√™ aprendeu sobre Transformers e Mecanismos de Aten√ß√£o, ferramentas essenciais na caixa de ferramentas de NLP. Existem muitas varia√ß√µes de arquiteturas de Transformers, incluindo BERT, DistilBERT, BigBird, OpenGPT3 e mais, que podem ser ajustadas. O pacote [HuggingFace](https://github.com/huggingface/) fornece um reposit√≥rio para treinar muitas dessas arquiteturas com PyTorch e TensorFlow.

## üöÄ Desafio

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Revis√£o e Autoestudo

* [Post de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), explicando o cl√°ssico artigo [Attention is all you need](https://arxiv.org/abs/1706.03762) sobre transformers.
* [Uma s√©rie de posts de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sobre transformers, explicando a arquitetura em detalhes.

## [Tarefa](assignment.md)

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional feita por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.