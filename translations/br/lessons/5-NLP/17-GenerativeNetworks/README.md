<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-26T08:22:15+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "br"
}
-->
# Redes Generativas

## [Question√°rio pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Redes Neurais Recorrentes (RNNs) e suas variantes com c√©lulas controladas, como Long Short Term Memory Cells (LSTMs) e Gated Recurrent Units (GRUs), fornecem um mecanismo para modelagem de linguagem, pois conseguem aprender a ordem das palavras e prever a pr√≥xima palavra em uma sequ√™ncia. Isso nos permite usar RNNs para **tarefas generativas**, como gera√ß√£o de texto comum, tradu√ß√£o autom√°tica e at√© legendagem de imagens.

> ‚úÖ Pense em todas as vezes que voc√™ se beneficiou de tarefas generativas, como a conclus√£o de texto enquanto voc√™ digita. Pesquise sobre seus aplicativos favoritos para ver se eles utilizam RNNs.

Na arquitetura de RNN que discutimos na unidade anterior, cada unidade RNN produzia o pr√≥ximo estado oculto como sa√≠da. No entanto, tamb√©m podemos adicionar outra sa√≠da a cada unidade recorrente, o que nos permitiria gerar uma **sequ√™ncia** (que tem o mesmo comprimento da sequ√™ncia original). Al√©m disso, podemos usar unidades RNN que n√£o aceitam uma entrada em cada etapa, mas apenas um vetor de estado inicial, e ent√£o produzem uma sequ√™ncia de sa√≠das.

Isso permite diferentes arquiteturas neurais, como mostrado na imagem abaixo:

![Imagem mostrando padr√µes comuns de redes neurais recorrentes.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.br.jpg)

> Imagem do post no blog [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) por [Andrej Karpaty](http://karpathy.github.io/)

* **One-to-one** √© uma rede neural tradicional com uma entrada e uma sa√≠da.
* **One-to-many** √© uma arquitetura generativa que aceita um valor de entrada e gera uma sequ√™ncia de valores de sa√≠da. Por exemplo, se quisermos treinar uma rede de **legenda de imagem** que produza uma descri√ß√£o textual de uma imagem, podemos usar a imagem como entrada, pass√°-la por uma CNN para obter seu estado oculto e, em seguida, usar uma cadeia recorrente para gerar a legenda palavra por palavra.
* **Many-to-one** corresponde √†s arquiteturas RNN que descrevemos na unidade anterior, como classifica√ß√£o de texto.
* **Many-to-many**, ou **sequ√™ncia-para-sequ√™ncia**, corresponde a tarefas como **tradu√ß√£o autom√°tica**, onde temos uma primeira RNN que coleta todas as informa√ß√µes da sequ√™ncia de entrada no estado oculto, e outra cadeia RNN que desenrola esse estado na sequ√™ncia de sa√≠da.

Nesta unidade, focaremos em modelos generativos simples que nos ajudam a gerar texto. Para simplificar, usaremos tokeniza√ß√£o em n√≠vel de caracteres.

Treinaremos esta RNN para gerar texto passo a passo. Em cada etapa, pegaremos uma sequ√™ncia de caracteres de comprimento `nchars` e pediremos √† rede para gerar o pr√≥ximo caractere de sa√≠da para cada caractere de entrada:

![Imagem mostrando um exemplo de gera√ß√£o de RNN da palavra 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.br.png)

Ao gerar texto (durante a infer√™ncia), come√ßamos com um **prompt**, que √© passado pelas c√©lulas RNN para gerar seu estado intermedi√°rio, e ent√£o a gera√ß√£o come√ßa a partir desse estado. Geramos um caractere por vez e passamos o estado e o caractere gerado para outra c√©lula RNN para gerar o pr√≥ximo, at√© que tenhamos gerado caracteres suficientes.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Redes Generativas

Continue seu aprendizado nos notebooks a seguir:

* [Redes Generativas com PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Redes Generativas com TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Gera√ß√£o de texto suave e temperatura

A sa√≠da de cada c√©lula RNN √© uma distribui√ß√£o de probabilidade de caracteres. Se sempre escolhermos o caractere com a maior probabilidade como o pr√≥ximo caractere no texto gerado, o texto frequentemente pode se tornar "c√≠clico", repetindo as mesmas sequ√™ncias de caracteres repetidamente, como neste exemplo:

```
today of the second the company and a second the company ...
```

No entanto, se analisarmos a distribui√ß√£o de probabilidade para o pr√≥ximo caractere, pode ser que a diferen√ßa entre algumas das maiores probabilidades n√£o seja t√£o grande, por exemplo, um caractere pode ter probabilidade 0,2, outro 0,19, etc. Por exemplo, ao procurar o pr√≥ximo caractere na sequ√™ncia '*play*', o pr√≥ximo caractere pode ser tanto um espa√ßo quanto **e** (como na palavra *player*).

Isso nos leva √† conclus√£o de que nem sempre √© "justo" selecionar o caractere com maior probabilidade, pois escolher o segundo maior ainda pode levar a um texto significativo. √â mais s√°bio **amostrar** caracteres da distribui√ß√£o de probabilidade fornecida pela sa√≠da da rede. Tamb√©m podemos usar um par√¢metro, **temperatura**, que ajustar√° a distribui√ß√£o de probabilidade, caso queiramos adicionar mais aleatoriedade ou torn√°-la mais inclinada, se quisermos nos ater mais aos caracteres de maior probabilidade.

Explore como essa gera√ß√£o de texto suave √© implementada nos notebooks vinculados acima.

## Conclus√£o

Embora a gera√ß√£o de texto possa ser √∫til por si s√≥, os maiores benef√≠cios v√™m da capacidade de gerar texto usando RNNs a partir de algum vetor de caracter√≠sticas inicial. Por exemplo, a gera√ß√£o de texto √© usada como parte da tradu√ß√£o autom√°tica (sequ√™ncia-para-sequ√™ncia, neste caso o vetor de estado do *encoder* √© usado para gerar ou *decodificar* a mensagem traduzida) ou para gerar descri√ß√µes textuais de uma imagem (nesse caso, o vetor de caracter√≠sticas viria de um extrator CNN).

## üöÄ Desafio

Fa√ßa algumas li√ß√µes no Microsoft Learn sobre este t√≥pico:

* Gera√ß√£o de Texto com [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Revis√£o e Autoestudo

Aqui est√£o alguns artigos para expandir seu conhecimento:

* Diferentes abordagens para gera√ß√£o de texto com Cadeia de Markov, LSTM e GPT-2: [post no blog](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exemplo de gera√ß√£o de texto na [documenta√ß√£o do Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Tarefa](lab/README.md)

Vimos como gerar texto caractere por caractere. No laborat√≥rio, voc√™ explorar√° a gera√ß√£o de texto em n√≠vel de palavras.

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.