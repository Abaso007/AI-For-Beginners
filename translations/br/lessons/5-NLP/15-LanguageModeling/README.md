<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T08:24:46+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "br"
}
-->
# Modelagem de Linguagem

Embeddings sem√¢nticos, como Word2Vec e GloVe, s√£o, na verdade, um primeiro passo em dire√ß√£o √† **modelagem de linguagem** - criando modelos que de alguma forma *entendem* (ou *representam*) a natureza da linguagem.

## [Pr√©-quiz da aula](https://ff-quizzes.netlify.app/en/ai/quiz/29)

A ideia principal por tr√°s da modelagem de linguagem √© trein√°-los em conjuntos de dados n√£o rotulados de forma n√£o supervisionada. Isso √© importante porque temos uma enorme quantidade de texto n√£o rotulado dispon√≠vel, enquanto a quantidade de texto rotulado sempre ser√° limitada pelo esfor√ßo que podemos dedicar √† rotulagem. Na maioria das vezes, podemos construir modelos de linguagem que conseguem **prever palavras ausentes** no texto, porque √© f√°cil mascarar uma palavra aleat√≥ria no texto e us√°-la como um exemplo de treinamento.

## Treinando Embeddings

Nos nossos exemplos anteriores, usamos embeddings sem√¢nticos pr√©-treinados, mas √© interessante ver como esses embeddings podem ser treinados. Existem v√°rias ideias poss√≠veis que podem ser utilizadas:

* **Modelagem de linguagem N-Gram**, onde prevemos um token olhando para os N tokens anteriores (N-grama).
* **Continuous Bag-of-Words** (CBoW), onde prevemos o token do meio $W_0$ em uma sequ√™ncia de tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, onde prevemos um conjunto de tokens vizinhos {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} a partir do token do meio $W_0$.

![imagem do artigo sobre convers√£o de palavras em vetores](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.br.png)

> Imagem retirada [deste artigo](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Exemplos de Notebooks: Treinando o modelo CBoW

Continue seu aprendizado nos seguintes notebooks:

* [Treinando CBoW Word2Vec com TensorFlow](CBoW-TF.ipynb)
* [Treinando CBoW Word2Vec com PyTorch](CBoW-PyTorch.ipynb)

## Conclus√£o

Na li√ß√£o anterior, vimos que embeddings de palavras funcionam como m√°gica! Agora sabemos que treinar embeddings de palavras n√£o √© uma tarefa muito complexa, e devemos ser capazes de treinar nossos pr√≥prios embeddings para textos espec√≠ficos de dom√≠nio, se necess√°rio.

## [P√≥s-quiz da aula](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Revis√£o e Autoestudo

* [Tutorial oficial do PyTorch sobre Modelagem de Linguagem](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutorial oficial do TensorFlow sobre treinamento do modelo Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* O uso do framework **gensim** para treinar os embeddings mais comumente usados em poucas linhas de c√≥digo √© descrito [nesta documenta√ß√£o](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Tarefa: Treinar Modelo Skip-Gram](lab/README.md)

No laborat√≥rio, desafiamos voc√™ a modificar o c√≥digo desta li√ß√£o para treinar um modelo skip-gram em vez de CBoW. [Leia os detalhes](lab/README.md)

---

