<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-26T08:27:55+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "br"
}
-->
# Representando Texto como Tensores

## [Pr√©-quiz da aula](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Classifica√ß√£o de Texto

Ao longo da primeira parte desta se√ß√£o, vamos focar na tarefa de **classifica√ß√£o de texto**. Usaremos o Dataset [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), que cont√©m artigos de not√≠cias como o seguinte:

* Categoria: Ci√™ncia/Tecnologia  
* T√≠tulo: Empresa de Ky. Ganha Subs√≠dio para Estudar Pept√≠deos (AP)  
* Corpo: AP - Uma empresa fundada por um pesquisador de qu√≠mica da Universidade de Louisville ganhou um subs√≠dio para desenvolver...

Nosso objetivo ser√° classificar a not√≠cia em uma das categorias com base no texto.

## Representando texto

Se quisermos resolver tarefas de Processamento de Linguagem Natural (PLN) com redes neurais, precisamos de uma maneira de representar texto como tensores. Os computadores j√° representam caracteres textuais como n√∫meros que mapeiam para fontes na sua tela usando codifica√ß√µes como ASCII ou UTF-8.

<img alt="Imagem mostrando um diagrama que mapeia um caractere para uma representa√ß√£o ASCII e bin√°ria" src="images/ascii-character-map.png" width="50%"/>

> [Fonte da imagem](https://www.seobility.net/en/wiki/ASCII)

Como humanos, entendemos o que cada letra **representa** e como todos os caracteres se juntam para formar as palavras de uma frase. No entanto, os computadores, por si s√≥, n√£o t√™m esse entendimento, e a rede neural precisa aprender o significado durante o treinamento.

Portanto, podemos usar diferentes abordagens para representar texto:

* **Representa√ß√£o no n√≠vel de caracteres**, onde representamos o texto tratando cada caractere como um n√∫mero. Dado que temos *C* caracteres diferentes em nosso corpus de texto, a palavra *Hello* seria representada por um tensor de 5x*C*. Cada letra corresponderia a uma coluna do tensor em codifica√ß√£o one-hot.  
* **Representa√ß√£o no n√≠vel de palavras**, na qual criamos um **vocabul√°rio** de todas as palavras em nosso texto e, em seguida, representamos as palavras usando codifica√ß√£o one-hot. Essa abordagem √© um pouco melhor, porque cada letra, por si s√≥, n√£o tem muito significado. Assim, ao usar conceitos sem√¢nticos de n√≠vel mais alto - palavras - simplificamos a tarefa para a rede neural. No entanto, dado o tamanho grande do dicion√°rio, precisamos lidar com tensores esparsos de alta dimens√£o.

Independentemente da representa√ß√£o, primeiro precisamos converter o texto em uma sequ√™ncia de **tokens**, sendo um token um caractere, uma palavra ou, √†s vezes, at√© mesmo parte de uma palavra. Depois, convertemos o token em um n√∫mero, geralmente usando um **vocabul√°rio**, e esse n√∫mero pode ser alimentado em uma rede neural usando codifica√ß√£o one-hot.

## N-Gramas

Na linguagem natural, o significado preciso das palavras s√≥ pode ser determinado no contexto. Por exemplo, os significados de *rede neural* e *rede de pesca* s√£o completamente diferentes. Uma das maneiras de levar isso em conta √© construir nosso modelo com pares de palavras, considerando pares de palavras como tokens separados no vocabul√°rio. Dessa forma, a frase *Eu gosto de pescar* ser√° representada pela seguinte sequ√™ncia de tokens: *Eu gosto*, *gosto de*, *de pescar*. O problema com essa abordagem √© que o tamanho do dicion√°rio cresce significativamente, e combina√ß√µes como *de pescar* e *de comprar* s√£o representadas por tokens diferentes, que n√£o compartilham nenhuma semelhan√ßa sem√¢ntica, apesar do mesmo verbo.

Em alguns casos, podemos considerar o uso de tri-gramas -- combina√ß√µes de tr√™s palavras -- tamb√©m. Assim, essa abordagem √© frequentemente chamada de **n-gramas**. Al√©m disso, faz sentido usar n-gramas com representa√ß√£o no n√≠vel de caracteres, caso em que os n-gramas corresponder√£o aproximadamente a diferentes s√≠labas.

## Bag-of-Words e TF/IDF

Ao resolver tarefas como classifica√ß√£o de texto, precisamos ser capazes de representar o texto por um vetor de tamanho fixo, que usaremos como entrada para o classificador denso final. Uma das maneiras mais simples de fazer isso √© combinar todas as representa√ß√µes individuais das palavras, por exemplo, somando-as. Se somarmos as codifica√ß√µes one-hot de cada palavra, acabaremos com um vetor de frequ√™ncias, mostrando quantas vezes cada palavra aparece no texto. Essa representa√ß√£o de texto √© chamada de **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Imagem do autor

Um BoW essencialmente representa quais palavras aparecem no texto e em quais quantidades, o que pode ser uma boa indica√ß√£o sobre o que o texto trata. Por exemplo, um artigo de not√≠cias sobre pol√≠tica provavelmente cont√©m palavras como *presidente* e *pa√≠s*, enquanto uma publica√ß√£o cient√≠fica teria algo como *colisor*, *descoberto*, etc. Assim, as frequ√™ncias das palavras podem, em muitos casos, ser um bom indicador do conte√∫do do texto.

O problema com o BoW √© que certas palavras comuns, como *e*, *√©*, etc., aparecem na maioria dos textos e t√™m as maiores frequ√™ncias, mascarando as palavras que s√£o realmente importantes. Podemos reduzir a import√¢ncia dessas palavras levando em conta a frequ√™ncia com que elas ocorrem em toda a cole√ß√£o de documentos. Essa √© a ideia principal por tr√°s da abordagem TF/IDF, que √© abordada em mais detalhes nos notebooks anexados a esta li√ß√£o.

No entanto, nenhuma dessas abordagens pode levar totalmente em conta a **sem√¢ntica** do texto. Precisamos de modelos de redes neurais mais poderosos para fazer isso, o que discutiremos mais adiante nesta se√ß√£o.

## ‚úçÔ∏è Exerc√≠cios: Representa√ß√£o de Texto

Continue seu aprendizado nos seguintes notebooks:

* [Representa√ß√£o de Texto com PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Representa√ß√£o de Texto com TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Conclus√£o

At√© agora, estudamos t√©cnicas que podem adicionar peso de frequ√™ncia a diferentes palavras. No entanto, elas n√£o conseguem representar o significado ou a ordem. Como o famoso linguista J. R. Firth disse em 1935: "O significado completo de uma palavra √© sempre contextual, e nenhum estudo de significado fora do contexto pode ser levado a s√©rio." Aprenderemos mais adiante no curso como capturar informa√ß√µes contextuais do texto usando modelagem de linguagem.

## üöÄ Desafio

Experimente outros exerc√≠cios usando bag-of-words e diferentes modelos de dados. Voc√™ pode se inspirar nesta [competi√ß√£o no Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [P√≥s-quiz da aula](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Revis√£o e Autoestudo

Pratique suas habilidades com t√©cnicas de embeddings de texto e bag-of-words no [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste).

## [Tarefa: Notebooks](assignment.md)

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional feita por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.