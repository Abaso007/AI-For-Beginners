<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-28T15:41:19+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "no"
}
-->
# Rammeverk for nevrale nettverk

Som vi allerede har l√¶rt, for √• kunne trene nevrale nettverk effektivt m√• vi gj√∏re to ting:

* Operere p√• tensorer, f.eks. multiplisere, legge til og beregne funksjoner som sigmoid eller softmax
* Beregne gradienter av alle uttrykk for √• utf√∏re gradientnedstigningsoptimalisering

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Mens `numpy`-biblioteket kan h√•ndtere den f√∏rste delen, trenger vi en mekanisme for √• beregne gradienter. I [v√•rt rammeverk](../04-OwnFramework/OwnFramework.ipynb) som vi utviklet i forrige seksjon, m√•tte vi manuelt programmere alle deriverte funksjoner inne i `backward`-metoden, som utf√∏rer tilbakepropagering. Ideelt sett b√∏r et rammeverk gi oss muligheten til √• beregne gradienter av *ethvert uttrykk* vi kan definere.

En annen viktig ting er √• kunne utf√∏re beregninger p√• GPU, eller andre spesialiserte beregningsenheter, som [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Trening av dype nevrale nettverk krever *mange* beregninger, og det er sv√¶rt viktig √• kunne parallellisere disse beregningene p√• GPU-er.

> ‚úÖ Begrepet 'parallellisere' betyr √• fordele beregningene over flere enheter.

For √∏yeblikket er de to mest popul√¶re rammeverkene for nevrale nettverk: [TensorFlow](http://TensorFlow.org) og [PyTorch](https://pytorch.org/). Begge tilbyr et lavniv√•-API for √• operere med tensorer p√• b√•de CPU og GPU. I tillegg til lavniv√•-API finnes det ogs√• h√∏yniv√•-API, kalt [Keras](https://keras.io/) og [PyTorch Lightning](https://pytorchlightning.ai/) henholdsvis.

Lavniv√•-API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------|-------------------------------------|--------------------------------
H√∏yniv√•-API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Lavniv√•-API-er** i begge rammeverk lar deg bygge s√•kalte **beregningsgrafer**. Denne grafen definerer hvordan man beregner utdata (vanligvis tapsfunksjonen) med gitte inngangsparametere, og kan sendes til beregning p√• GPU, hvis tilgjengelig. Det finnes funksjoner for √• differensiere denne beregningsgrafen og beregne gradienter, som deretter kan brukes til √• optimalisere modellparametere.

**H√∏yniv√•-API-er** betrakter nevrale nettverk som en **sekvens av lag**, og gj√∏r det mye enklere √• konstruere de fleste nevrale nettverk. √Ö trene modellen krever vanligvis √• forberede dataene og deretter kalle en `fit`-funksjon for √• utf√∏re jobben.

H√∏yniv√•-API lar deg konstruere typiske nevrale nettverk veldig raskt uten √• bekymre deg for mange detaljer. Samtidig gir lavniv√•-API mye mer kontroll over treningsprosessen, og brukes derfor mye i forskning, n√•r man arbeider med nye arkitekturer for nevrale nettverk.

Det er ogs√• viktig √• forst√• at du kan bruke begge API-er sammen, f.eks. kan du utvikle din egen arkitektur for nettverkslag ved hjelp av lavniv√•-API, og deretter bruke den inne i et st√∏rre nettverk konstruert og trent med h√∏yniv√•-API. Eller du kan definere et nettverk ved hjelp av h√∏yniv√•-API som en sekvens av lag, og deretter bruke din egen lavniv√• treningssl√∏yfe for √• utf√∏re optimalisering. Begge API-er bruker de samme grunnleggende underliggende konseptene, og de er designet for √• fungere godt sammen.

## L√¶ring

I dette kurset tilbyr vi det meste av innholdet b√•de for PyTorch og TensorFlow. Du kan velge ditt foretrukne rammeverk og kun g√• gjennom de tilsvarende notatb√∏kene. Hvis du ikke er sikker p√• hvilket rammeverk du skal velge, les noen diskusjoner p√• nettet om **PyTorch vs. TensorFlow**. Du kan ogs√• ta en titt p√• begge rammeverkene for √• f√• bedre forst√•else.

Der det er mulig, vil vi bruke h√∏yniv√•-API-er for enkelhetens skyld. Imidlertid mener vi det er viktig √• forst√• hvordan nevrale nettverk fungerer fra grunnen av, derfor starter vi med √• arbeide med lavniv√•-API og tensorer. Men hvis du √∏nsker √• komme i gang raskt og ikke vil bruke mye tid p√• √• l√¶re disse detaljene, kan du hoppe over dem og g√• direkte til notatb√∏kene for h√∏yniv√•-API.

## ‚úçÔ∏è √òvelser: Rammeverk

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:

Lavniv√•-API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------|-------------------------------------|--------------------------------
H√∏yniv√•-API | [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Etter √• ha mestret rammeverkene, la oss oppsummere begrepet overtilpasning.

# Overtilpasning

Overtilpasning er et ekstremt viktig konsept innen maskinl√¶ring, og det er veldig viktig √• forst√• det riktig!

Tenk p√• f√∏lgende problem med √• tiln√¶rme 5 punkter (representert med `x` p√• grafene nedenfor):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.no.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.no.jpg)
-------------------------|--------------------------
**Line√¶r modell, 2 parametere** | **Ikke-line√¶r modell, 7 parametere**
Treningsfeil = 5.3 | Treningsfeil = 0
Valideringsfeil = 5.1 | Valideringsfeil = 20

* Til venstre ser vi en god rett linje-tiln√¶rming. Fordi antall parametere er passende, f√•r modellen tak i poengfordelingen riktig.
* Til h√∏yre er modellen for kraftig. Fordi vi bare har 5 punkter og modellen har 7 parametere, kan den justere seg slik at den passer gjennom alle punktene, noe som gj√∏r treningsfeilen til 0. Dette hindrer imidlertid modellen i √• forst√• det riktige m√∏nsteret bak dataene, og dermed er valideringsfeilen veldig h√∏y.

Det er veldig viktig √• finne en riktig balanse mellom modellens kompleksitet (antall parametere) og antall treningspr√∏ver.

## Hvorfor oppst√•r overtilpasning

  * Ikke nok treningsdata
  * For kraftig modell
  * For mye st√∏y i inngangsdata

## Hvordan oppdage overtilpasning

Som du kan se fra grafen ovenfor, kan overtilpasning oppdages ved en veldig lav treningsfeil og en h√∏y valideringsfeil. Normalt under trening vil vi se b√•de trenings- og valideringsfeil begynne √• synke, og deretter p√• et tidspunkt kan valideringsfeilen slutte √• synke og begynne √• stige. Dette vil v√¶re et tegn p√• overtilpasning, og en indikator p√• at vi sannsynligvis b√∏r stoppe treningen p√• dette punktet (eller i det minste ta et √∏yeblikksbilde av modellen).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.no.png)

## Hvordan forhindre overtilpasning

Hvis du ser at overtilpasning oppst√•r, kan du gj√∏re f√∏lgende:

 * √òke mengden treningsdata
 * Redusere modellens kompleksitet
 * Bruke en [regulariseringsteknikk](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), som [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), som vi vil se n√¶rmere p√• senere.

## Overtilpasning og Bias-Variance Tradeoff

Overtilpasning er faktisk et tilfelle av et mer generelt problem innen statistikk kalt [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Hvis vi vurderer de mulige kildene til feil i modellen v√•r, kan vi se to typer feil:

* **Bias-feil** skyldes at algoritmen v√•r ikke klarer √• fange forholdet mellom treningsdataene riktig. Det kan skyldes at modellen v√•r ikke er kraftig nok (**undertilpasning**).
* **Variansfeil**, som skyldes at modellen tiln√¶rmer seg st√∏y i inngangsdataene i stedet for meningsfulle forhold (**overtilpasning**).

Under trening synker bias-feilen (ettersom modellen v√•r l√¶rer √• tiln√¶rme dataene), og variansfeilen √∏ker. Det er viktig √• stoppe treningen - enten manuelt (n√•r vi oppdager overtilpasning) eller automatisk (ved √• innf√∏re regularisering) - for √• forhindre overtilpasning.

## Konklusjon

I denne leksjonen l√¶rte du om forskjellene mellom de ulike API-ene for de to mest popul√¶re AI-rammeverkene, TensorFlow og PyTorch. I tillegg l√¶rte du om et veldig viktig tema, overtilpasning.

## üöÄ Utfordring

I de medf√∏lgende notatb√∏kene finner du 'oppgaver' nederst; arbeid gjennom notatb√∏kene og fullf√∏r oppgavene.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Gjennomgang og selvstudium

Gj√∏r litt research p√• f√∏lgende temaer:

- TensorFlow
- PyTorch
- Overtilpasning

Still deg selv f√∏lgende sp√∏rsm√•l:

- Hva er forskjellen mellom TensorFlow og PyTorch?
- Hva er forskjellen mellom overtilpasning og undertilpasning?

## [Oppgave](lab/README.md)

I denne laben blir du bedt om √• l√∏se to klassifiseringsproblemer ved hjelp av enkelt- og flerlags fullt tilkoblede nettverk ved bruk av PyTorch eller TensorFlow.

* [Instruksjoner](lab/README.md)
* [Notatbok](lab/LabFrameworks.ipynb)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.