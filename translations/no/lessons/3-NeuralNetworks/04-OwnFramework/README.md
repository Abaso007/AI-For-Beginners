<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-28T15:37:32+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "no"
}
-->
# Introduksjon til nevrale nettverk. Multi-lags perceptron

I forrige seksjon lÃ¦rte du om den enkleste modellen for nevrale nettverk - enlags perceptron, en lineÃ¦r to-klasse klassifikasjonsmodell.

I denne seksjonen vil vi utvide denne modellen til et mer fleksibelt rammeverk som lar oss:

* utfÃ¸re **multi-klasse klassifikasjon** i tillegg til to-klasse
* lÃ¸se **regresjonsproblemer** i tillegg til klassifikasjon
* skille klasser som ikke er lineÃ¦rt separerbare

Vi vil ogsÃ¥ utvikle vÃ¥rt eget modulÃ¦re rammeverk i Python som lar oss konstruere ulike arkitekturer for nevrale nettverk.

## [Quiz fÃ¸r forelesning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalisering av maskinlÃ¦ring

La oss starte med Ã¥ formalisere problemet innen maskinlÃ¦ring. Anta at vi har et treningsdatasett **X** med etiketter **Y**, og vi trenger Ã¥ bygge en modell *f* som gir de mest nÃ¸yaktige prediksjonene. Kvaliteten pÃ¥ prediksjonene mÃ¥les ved hjelp av en **tapfunksjon** â„’. FÃ¸lgende tapfunksjoner brukes ofte:

* For regresjonsproblemer, nÃ¥r vi trenger Ã¥ forutsi et tall, kan vi bruke **absolutt feil** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadratisk feil** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* For klassifikasjon bruker vi **0-1 tap** (som i hovedsak er det samme som **nÃ¸yaktighet** til modellen), eller **logistisk tap**.

For enlags perceptron ble funksjonen *f* definert som en lineÃ¦r funksjon *f(x)=wx+b* (her er *w* vektormatrise, *x* er vektoren av inngangsfunksjoner, og *b* er bias-vektor). For ulike arkitekturer av nevrale nettverk kan denne funksjonen ta en mer kompleks form.

> I tilfelle klassifikasjon er det ofte Ã¸nskelig Ã¥ fÃ¥ sannsynligheter for de tilsvarende klassene som nettverksutgang. For Ã¥ konvertere vilkÃ¥rlige tall til sannsynligheter (f.eks. for Ã¥ normalisere utgangen), bruker vi ofte **softmax**-funksjonen Ïƒ, og funksjonen *f* blir *f(x)=Ïƒ(wx+b)*

I definisjonen av *f* ovenfor kalles *w* og *b* **parametere** Î¸=âŸ¨*w,b*âŸ©. Gitt datasettet âŸ¨**X**,**Y**âŸ©, kan vi beregne en samlet feil for hele datasettet som en funksjon av parametrene Î¸.

> âœ… **MÃ¥let med trening av nevrale nettverk er Ã¥ minimere feilen ved Ã¥ variere parametrene Î¸**

## Gradientnedstigningsoptimalisering

Det finnes en velkjent metode for funksjonsoptimalisering kalt **gradientnedstigning**. Ideen er at vi kan beregne en deriverte (i flerdimensjonale tilfeller kalt **gradient**) av tapfunksjonen med hensyn til parametrene, og variere parametrene slik at feilen reduseres. Dette kan formaliseres som fÃ¸lger:

* Initialiser parametrene med noen tilfeldige verdier w<sup>(0)</sup>, b<sup>(0)</sup>
* Gjenta fÃ¸lgende steg mange ganger:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Under treningen skal optimaliseringsstegene beregnes med tanke pÃ¥ hele datasettet (husk at tapet beregnes som en sum gjennom alle treningsprÃ¸ver). I praksis tar vi imidlertid smÃ¥ deler av datasettet kalt **minibatcher**, og beregner gradienter basert pÃ¥ et delsett av data. Fordi delsettet velges tilfeldig hver gang, kalles en slik metode **stokastisk gradientnedstigning** (SGD).

## Multi-lags perceptron og backpropagation

Ettlags nettverk, som vi har sett ovenfor, er i stand til Ã¥ klassifisere lineÃ¦rt separerbare klasser. For Ã¥ bygge en rikere modell kan vi kombinere flere lag i nettverket. Matematisk betyr dette at funksjonen *f* vil ha en mer kompleks form og beregnes i flere steg:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Her er Î± en **ikke-lineÃ¦r aktiveringsfunksjon**, Ïƒ er en softmax-funksjon, og parametrene Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientnedstigningsalgoritmen forblir den samme, men det blir mer utfordrende Ã¥ beregne gradientene. Ved hjelp av kjederegelen for derivasjon kan vi beregne derivatene som:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Kjederegelen for derivasjon brukes til Ã¥ beregne derivater av tapfunksjonen med hensyn til parametrene.

Merk at den venstre delen av alle disse uttrykkene er den samme, og derfor kan vi effektivt beregne derivatene ved Ã¥ starte fra tapfunksjonen og gÃ¥ "bakover" gjennom beregningsgrafen. Dermed kalles metoden for Ã¥ trene et multi-lags perceptron **backpropagation**, eller 'backprop'.

<img alt="beregningsgraf" src="images/ComputeGraphGrad.png"/>

> TODO: bildehenvisning

> âœ… Vi vil dekke backpropagation i mye stÃ¸rre detalj i vÃ¥rt notatbokeksempel.  

## Konklusjon

I denne leksjonen har vi bygget vÃ¥rt eget bibliotek for nevrale nettverk, og vi har brukt det til en enkel todimensjonal klassifikasjonsoppgave.

## ğŸš€ Utfordring

I den medfÃ¸lgende notatboken vil du implementere ditt eget rammeverk for Ã¥ bygge og trene multi-lags perceptron. Du vil kunne se i detalj hvordan moderne nevrale nettverk fungerer.

GÃ¥ videre til [OwnFramework](OwnFramework.ipynb)-notatboken og arbeid deg gjennom den.

## [Quiz etter forelesning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Gjennomgang og selvstudium

Backpropagation er en vanlig algoritme brukt i AI og maskinlÃ¦ring, og det er verdt Ã¥ studere [mer i detalj](https://wikipedia.org/wiki/Backpropagation)

## [Oppgave](lab/README.md)

I denne laben blir du bedt om Ã¥ bruke rammeverket du konstruerte i denne leksjonen til Ã¥ lÃ¸se MNIST-hÃ¥ndskrevet sifferklassifikasjon.

* [Instruksjoner](lab/README.md)
* [Notatbok](lab/MyFW_MNIST.ipynb)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber nÃ¸yaktighet, vÃ¦r oppmerksom pÃ¥ at automatiske oversettelser kan inneholde feil eller unÃ¸yaktigheter. Det originale dokumentet pÃ¥ sitt opprinnelige sprÃ¥k bÃ¸r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforstÃ¥elser eller feiltolkninger som oppstÃ¥r ved bruk av denne oversettelsen.