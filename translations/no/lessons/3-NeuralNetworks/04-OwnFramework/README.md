<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "789d6c3fb6fc7948a470b33078a5983a",
  "translation_date": "2025-09-23T09:44:29+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "no"
}
-->
# Introduksjon til nevrale nettverk. Multi-lags perceptron

I forrige seksjon l√¶rte du om den enkleste modellen for nevrale nettverk ‚Äì en √©n-lags perceptron, en line√¶r to-klasse klassifiseringsmodell.

I denne seksjonen vil vi utvide denne modellen til et mer fleksibelt rammeverk som lar oss:

* utf√∏re **multi-klasse klassifisering** i tillegg til to-klasse
* l√∏se **regresjonsproblemer** i tillegg til klassifisering
* skille klasser som ikke er line√¶rt separerbare

Vi vil ogs√• utvikle v√•rt eget modul√¶re rammeverk i Python som lar oss konstruere ulike arkitekturer for nevrale nettverk.

## [Pre-forelesningsquiz](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalisering av maskinl√¶ring

La oss starte med √• formalisere problemet med maskinl√¶ring. Anta at vi har et treningsdatasett **X** med etiketter **Y**, og vi m√• bygge en modell *f* som gir de mest n√∏yaktige prediksjonene. Kvaliteten p√• prediksjonene m√•les med **tapfunksjonen** &lagran;. F√∏lgende tapfunksjoner brukes ofte:

* For regresjonsproblemer, n√•r vi trenger √• forutsi et tall, kan vi bruke **absolutt feil** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadratisk feil** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* For klassifisering bruker vi **0-1 tap** (som i hovedsak er det samme som **modellens n√∏yaktighet**), eller **logistisk tap**.

For √©n-lags perceptron ble funksjonen *f* definert som en line√¶r funksjon *f(x)=wx+b* (her er *w* vektmatrisen, *x* er vektoren av input-funksjoner, og *b* er bias-vektoren). For ulike arkitekturer for nevrale nettverk kan denne funksjonen ha en mer kompleks form.

> N√•r det gjelder klassifisering, er det ofte √∏nskelig √• f√• sannsynligheter for de tilsvarende klassene som nettverksutgang. For √• konvertere vilk√•rlige tall til sannsynligheter (f.eks. for √• normalisere utgangen), bruker vi ofte **softmax**-funksjonen &sigma;, og funksjonen *f* blir *f(x)=&sigma;(wx+b)*

I definisjonen av *f* ovenfor kalles *w* og *b* **parametere** &theta;=‚ü®*w,b*‚ü©. Gitt datasettet ‚ü®**X**,**Y**‚ü©, kan vi beregne en samlet feil for hele datasettet som en funksjon av parametere &theta;.

> ‚úÖ **M√•let med trening av nevrale nettverk er √• minimere feilen ved √• variere parametere &theta;**

## Gradientnedstigningsoptimalisering

Det finnes en velkjent metode for funksjonsoptimalisering kalt **gradientnedstigning**. Ideen er at vi kan beregne en derivert (i flerdimensjonale tilfeller kalt **gradient**) av tapfunksjonen med hensyn til parametere, og variere parametere slik at feilen reduseres. Dette kan formaliseres som f√∏lger:

* Initialiser parametere med noen tilfeldige verdier w<sup>(0)</sup>, b<sup>(0)</sup>
* Gjenta f√∏lgende steg mange ganger:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Under trening skal optimaliseringsstegene beregnes med hensyn til hele datasettet (husk at tap beregnes som en sum gjennom alle treningspr√∏ver). Men i praksis tar vi sm√• deler av datasettet kalt **minibatcher**, og beregner gradienter basert p√• en delmengde av data. Fordi delmengden tas tilfeldig hver gang, kalles en slik metode **stokastisk gradientnedstigning** (SGD).

## Multi-lags perceptron og backpropagation

√ân-lags nettverk, som vi har sett ovenfor, er i stand til √• klassifisere line√¶rt separerbare klasser. For √• bygge en rikere modell kan vi kombinere flere lag i nettverket. Matematisk vil det bety at funksjonen *f* vil ha en mer kompleks form og beregnes i flere steg:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Her er &alpha; en **ikke-line√¶r aktiveringsfunksjon**, &sigma; er en softmax-funksjon, og parametere &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientnedstigningsalgoritmen vil forbli den samme, men det vil v√¶re mer utfordrende √• beregne gradienter. Gitt kjederegelen for derivasjon, kan vi beregne derivertene som:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ‚úÖ Kjederegelen for derivasjon brukes til √• beregne derivertene av tapfunksjonen med hensyn til parametere.

Merk at den venstre delen av alle disse uttrykkene er den samme, og dermed kan vi effektivt beregne derivertene ved √• starte fra tapfunksjonen og g√• "bakover" gjennom beregningsgrafen. Dermed kalles metoden for trening av en multi-lags perceptron **backpropagation**, eller 'backprop'.

<img alt="beregningsgraf" src="images/ComputeGraphGrad.png"/>

> TODO: bildehenvisning

> ‚úÖ Vi vil dekke backpropagation i mye mer detalj i v√•rt notatbokeksempel.  

## Konklusjon

I denne leksjonen har vi bygget v√•rt eget bibliotek for nevrale nettverk, og vi har brukt det til en enkel todimensjonal klassifiseringsoppgave.

## üöÄ Utfordring

I den medf√∏lgende notatboken vil du implementere ditt eget rammeverk for √• bygge og trene multi-lags perceptron. Du vil kunne se i detalj hvordan moderne nevrale nettverk fungerer.

G√• videre til [OwnFramework](OwnFramework.ipynb)-notatboken og arbeid deg gjennom den.

## [Post-forelesningsquiz](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Gjennomgang og selvstudium

Backpropagation er en vanlig algoritme brukt i AI og ML, verdt √• studere [i mer detalj](https://wikipedia.org/wiki/Backpropagation)

## [Oppgave](lab/README.md)

I denne labben blir du bedt om √• bruke rammeverket du konstruerte i denne leksjonen til √• l√∏se MNIST-h√•ndskrevne sifferklassifisering.

* [Instruksjoner](lab/README.md)
* [Notatbok](lab/MyFW_MNIST.ipynb)

---

