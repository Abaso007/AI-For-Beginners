<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T09:38:25+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "no"
}
-->
# Dyp Forsterkningsl√¶ring

Forsterkningsl√¶ring (RL) regnes som en av de grunnleggende paradigmer innen maskinl√¶ring, ved siden av veiledet l√¶ring og uveiledet l√¶ring. Mens vi i veiledet l√¶ring baserer oss p√• datasett med kjente utfall, er RL basert p√• **l√¶ring gjennom handling**. For eksempel, n√•r vi ser et dataspill for f√∏rste gang, begynner vi √• spille, selv uten √• kjenne reglene, og snart blir vi bedre bare ved √• spille og justere oppf√∏rselen v√•r.

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/43)

For √• utf√∏re RL trenger vi:

* Et **milj√∏** eller en **simulator** som setter reglene for spillet. Vi m√• kunne kj√∏re eksperimenter i simulatoren og observere resultatene.
* En **bel√∏nningsfunksjon**, som indikerer hvor vellykket eksperimentet v√•rt var. N√•r vi l√¶rer √• spille et dataspill, vil bel√∏nningen v√¶re v√•r endelige poengsum.

Basert p√• bel√∏nningsfunksjonen b√∏r vi kunne justere oppf√∏rselen v√•r og forbedre ferdighetene v√•re, slik at vi spiller bedre neste gang. Den st√∏rste forskjellen mellom RL og andre typer maskinl√¶ring er at vi i RL vanligvis ikke vet om vi vinner eller taper f√∏r spillet er ferdig. Dermed kan vi ikke si om en enkelt handling er god eller d√•rlig ‚Äì vi mottar bel√∏nningen f√∏rst ved slutten av spillet.

Under RL utf√∏rer vi typisk mange eksperimenter. I hvert eksperiment m√• vi balansere mellom √• f√∏lge den optimale strategien vi har l√¶rt s√• langt (**utnyttelse**) og utforske nye mulige tilstander (**utforskning**).

## OpenAI Gym

Et fantastisk verkt√∏y for RL er [OpenAI Gym](https://gym.openai.com/) ‚Äì et **simuleringsmilj√∏** som kan simulere mange forskjellige milj√∏er, fra Atari-spill til fysikken bak stangbalansering. Det er et av de mest popul√¶re simuleringsmilj√∏ene for trening av forsterkningsl√¶ringsalgoritmer, og vedlikeholdes av [OpenAI](https://openai.com/).

> **Note**: Du kan se alle milj√∏ene som er tilgjengelige fra OpenAI Gym [her](https://gym.openai.com/envs/#classic_control).

## CartPole-balansering

Dere har sikkert sett moderne balanseringsenheter som *Segway* eller *Gyroscooters*. De kan automatisk balansere ved √• justere hjulene sine basert p√• signaler fra et akselerometer eller gyroskop. I denne delen skal vi l√¶re √• l√∏se et lignende problem ‚Äì √• balansere en stang. Det ligner p√• en situasjon der en sirkusartist m√• balansere en stang p√• h√•nden ‚Äì men denne stangbalanseringen skjer kun i √©n dimensjon.

En forenklet versjon av balansering er kjent som **CartPole**-problemet. I CartPole-verdenen har vi en horisontal skyver som kan bevege seg til venstre eller h√∏yre, og m√•let er √• balansere en vertikal stang p√• toppen av skyveren mens den beveger seg.

<img alt="en cartpole" src="images/cartpole.png" width="200"/>

For √• opprette og bruke dette milj√∏et trenger vi noen f√• linjer med Python-kode:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Hvert milj√∏ kan n√•s p√• n√∏yaktig samme m√•te:
* `env.reset` starter et nytt eksperiment
* `env.step` utf√∏rer et simuleringssteg. Det mottar en **handling** fra **handlingsrommet**, og returnerer en **observasjon** (fra observasjonsrommet), samt en bel√∏nning og en avslutningsflagg.

I eksemplet ovenfor utf√∏rer vi en tilfeldig handling ved hvert steg, noe som gj√∏r at eksperimentets levetid er veldig kort:

![ikke-balanserende cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

M√•let med en RL-algoritme er √• trene en modell ‚Äì den s√•kalte **policy** &pi; ‚Äì som vil returnere handlingen som svar p√• en gitt tilstand. Vi kan ogs√• betrakte policy som probabilistisk, dvs. for enhver tilstand *s* og handling *a* vil den returnere sannsynligheten &pi;(*a*|*s*) for at vi b√∏r ta *a* i tilstand *s*.

## Policy Gradients-algoritme

Den mest √•penbare m√•ten √• modellere en policy p√• er ved √• opprette et nevralt nettverk som tar tilstander som input og returnerer tilsvarende handlinger (eller snarere sannsynlighetene for alle handlinger). P√• en m√•te vil det v√¶re lik en vanlig klassifiseringsoppgave, med en stor forskjell ‚Äì vi vet ikke p√• forh√•nd hvilke handlinger vi b√∏r ta ved hvert steg.

Ideen her er √• estimere disse sannsynlighetene. Vi bygger en vektor av **kumulative bel√∏nninger** som viser v√•r totale bel√∏nning ved hvert steg av eksperimentet. Vi bruker ogs√• **bel√∏nningsdiskontering** ved √• multiplisere tidligere bel√∏nninger med en koeffisient &gamma;=0.99, for √• redusere betydningen av tidligere bel√∏nninger. Deretter forsterker vi de stegene langs eksperimentbanen som gir st√∏rre bel√∏nninger.

> L√¶r mer om Policy Gradient-algoritmen og se den i aksjon i [eksempelfilen](CartPole-RL-TF.ipynb).

## Actor-Critic-algoritme

En forbedret versjon av Policy Gradients-tiln√¶rmingen kalles **Actor-Critic**. Hovedideen bak denne er at det nevrale nettverket skal trenes til √• returnere to ting:

* Policy, som bestemmer hvilken handling som skal tas. Denne delen kalles **actor**.
* Estimeringen av den totale bel√∏nningen vi kan forvente √• f√• i denne tilstanden ‚Äì denne delen kalles **critic**.

P√• en m√•te ligner denne arkitekturen en [GAN](../../4-ComputerVision/10-GANs/README.md), der vi har to nettverk som trenes mot hverandre. I Actor-Critic-modellen foresl√•r actor handlingen vi m√• ta, og critic pr√∏ver √• v√¶re kritisk og estimere resultatet. M√•let v√•rt er imidlertid √• trene disse nettverkene i harmoni.

Fordi vi kjenner b√•de de reelle kumulative bel√∏nningene og resultatene returnert av critic under eksperimentet, er det relativt enkelt √• bygge en tapsfunksjon som minimerer forskjellen mellom dem. Dette gir oss **critic loss**. Vi kan beregne **actor loss** ved √• bruke samme tiln√¶rming som i Policy Gradient-algoritmen.

Etter √• ha kj√∏rt en av disse algoritmene, kan vi forvente at v√•r CartPole oppf√∏rer seg slik:

![en balanserende cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è √òvelser: Policy Gradients og Actor-Critic RL

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:

* [RL i TensorFlow](CartPole-RL-TF.ipynb)
* [RL i PyTorch](CartPole-RL-PyTorch.ipynb)

## Andre RL-oppgaver

Forsterkningsl√¶ring er i dag et raskt voksende forskningsfelt. Noen interessante eksempler p√• forsterkningsl√¶ring er:

* √Ö l√¶re en datamaskin √• spille **Atari-spill**. Den utfordrende delen av dette problemet er at vi ikke har en enkel tilstand representert som en vektor, men snarere et skjermbilde ‚Äì og vi m√• bruke CNN for √• konvertere dette skjermbildet til en funksjonsvektor eller for √• trekke ut bel√∏nningsinformasjon. Atari-spill er tilgjengelige i Gym.
* √Ö l√¶re en datamaskin √• spille brettspill, som sjakk og Go. Nylig ble toppmoderne programmer som **Alpha Zero** trent fra bunnen av ved at to agenter spilte mot hverandre og forbedret seg ved hvert steg.
* I industrien brukes RL til √• lage kontrollsystemer fra simulering. En tjeneste kalt [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) er spesielt designet for dette.

## Konklusjon

Vi har n√• l√¶rt hvordan vi kan trene agenter til √• oppn√• gode resultater bare ved √• gi dem en bel√∏nningsfunksjon som definerer √∏nsket tilstand for spillet, og ved √• gi dem muligheten til intelligent √• utforske s√∏keomr√•det. Vi har pr√∏vd to algoritmer med suksess og oppn√•dd gode resultater p√• relativt kort tid. Dette er imidlertid bare begynnelsen p√• din reise inn i RL, og du b√∏r definitivt vurdere √• ta et eget kurs hvis du vil fordype deg.

## üöÄ Utfordring

Utforsk applikasjonene som er oppf√∏rt i delen 'Andre RL-oppgaver' og pr√∏v √• implementere en!

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Gjennomgang og selvstudium

L√¶r mer om klassisk forsterkningsl√¶ring i v√•rt [Maskinl√¶ring for nybegynnere-kurs](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Se [denne flotte videoen](https://www.youtube.com/watch?v=qv6UVOQ0F44) som forklarer hvordan en datamaskin kan l√¶re √• spille Super Mario.

## Oppgave: [Tren en Mountain Car](lab/README.md)

M√•let ditt under denne oppgaven vil v√¶re √• trene et annet Gym-milj√∏ ‚Äì [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

