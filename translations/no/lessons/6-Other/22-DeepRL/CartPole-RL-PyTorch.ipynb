{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening av RL for å balansere Cartpole\n",
    "\n",
    "Denne notatboken er en del av [AI for Beginners Curriculum](http://aka.ms/ai-beginners). Den er inspirert av [offisiell PyTorch-veiledning](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) og [denne Cartpole PyTorch-implementasjonen](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "I dette eksempelet skal vi bruke RL til å trene en modell til å balansere en stang på en vogn som kan bevege seg til venstre og høyre på en horisontal skala. Vi vil bruke [OpenAI Gym](https://www.gymlibrary.ml/) miljøet for å simulere stangen.\n",
    "\n",
    "> **Note**: Du kan kjøre koden fra denne leksjonen lokalt (f.eks. fra Visual Studio Code), i så fall vil simuleringen åpnes i et nytt vindu. Når du kjører koden online, kan det være nødvendig å gjøre noen justeringer i koden, som beskrevet [her](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Vi starter med å forsikre oss om at Gym er installert:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss opprette CartPole-miljøet og se hvordan vi kan operere i det. Et miljø har følgende egenskaper:\n",
    "\n",
    "* **Handlingsrom** er settet av mulige handlinger vi kan utføre på hvert steg i simuleringen\n",
    "* **Observasjonsrom** er rommet av observasjoner vi kan gjøre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss se hvordan simuleringen fungerer. Følgende løkke kjører simuleringen til `env.step` ikke returnerer avslutningsflagget `done`. Vi vil tilfeldig velge handlinger ved hjelp av `env.action_space.sample()`, noe som betyr at eksperimentet sannsynligvis vil mislykkes veldig raskt (CartPole-miljøet avsluttes når hastigheten til CartPole, dens posisjon eller vinkel er utenfor visse grenser).\n",
    "\n",
    "> Simuleringen vil åpne i et nytt vindu. Du kan kjøre koden flere ganger og se hvordan den oppfører seg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan legge merke til at observasjonene inneholder 4 tall. Disse er:  \n",
    "- Posisjon til vognen  \n",
    "- Hastighet til vognen  \n",
    "- Vinkel på stangen  \n",
    "- Rotasjonshastighet til stangen  \n",
    "\n",
    "`rew` er belønningen vi mottar for hvert steg. Du kan se at i CartPole-miljøet får du 1 poeng for hvert simuleringssteg, og målet er å maksimere total belønning, altså tiden CartPole klarer å balansere uten å falle.  \n",
    "\n",
    "Under forsterkende læring er målet vårt å trene en **policy** $\\pi$, som for hver tilstand $s$ vil fortelle oss hvilken handling $a$ vi skal utføre, altså $a = \\pi(s)$.  \n",
    "\n",
    "Hvis du ønsker en probabilistisk løsning, kan du tenke på policy som å returnere et sett med sannsynligheter for hver handling, altså $\\pi(a|s)$ som betyr sannsynligheten for at vi bør utføre handling $a$ i tilstand $s$.  \n",
    "\n",
    "## Policy Gradient-metoden  \n",
    "\n",
    "I den enkleste RL-algoritmen, kalt **Policy Gradient**, vil vi trene et nevralt nettverk til å forutsi neste handling.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi skal trene nettverket ved å kjøre mange eksperimenter og oppdatere nettverket vårt etter hver kjøring. La oss definere en funksjon som vil kjøre eksperimentet og returnere resultatene (såkalt **spor**) - alle tilstander, handlinger (og deres anbefalte sannsynligheter) og belønninger:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan kjøre én episode med et utrent nettverk og observere at total belønning (AKA lengden på episoden) er veldig lav:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En av de vanskelige aspektene ved policy gradient-algoritmen er å bruke **diskonterte belønninger**. Ideen er at vi beregner vektoren av totale belønninger på hvert steg i spillet, og under denne prosessen diskonterer vi de tidlige belønningene ved hjelp av en koeffisient $gamma$. Vi normaliserer også den resulterende vektoren, fordi vi vil bruke den som vekt for å påvirke treningen vår:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå skal vi begynne treningen! Vi skal kjøre 300 episoder, og i hver episode vil vi gjøre følgende:\n",
    "\n",
    "1. Kjør eksperimentet og samle sporet.\n",
    "2. Beregn forskjellen (`gradients`) mellom handlingene som ble utført og de predikerte sannsynlighetene. Jo mindre forskjellen er, desto mer sikre er vi på at vi har tatt riktig handling.\n",
    "3. Beregn diskonterte belønninger og multipliser gradientene med de diskonterte belønningene - dette vil sørge for at steg med høyere belønninger har større innvirkning på sluttresultatet enn de med lavere belønninger.\n",
    "4. Forventede målhandlinger for vårt nevrale nettverk vil delvis bli hentet fra de predikerte sannsynlighetene under kjøringen, og delvis fra de beregnede gradientene. Vi bruker parameteren `alpha` for å bestemme i hvilken grad gradienter og belønninger tas i betraktning - dette kalles *læringsraten* til forsterkningsalgoritmen.\n",
    "5. Til slutt trener vi nettverket vårt på tilstander og forventede handlinger, og gjentar prosessen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå la oss kjøre episoden med rendering for å se resultatet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forhåpentligvis kan du nå se at stangen balanserer ganske bra!\n",
    "\n",
    "## Actor-Critic Modell\n",
    "\n",
    "Actor-Critic-modellen er en videreutvikling av policy gradients, der vi bygger et nevralt nettverk for å lære både policy og estimerte belønninger. Nettverket vil ha to utganger (eller du kan se det som to separate nettverk):\n",
    "* **Actor** vil anbefale handlingen som skal tas ved å gi oss sannsynlighetsfordelingen for tilstanden, som i policy gradient-modellen.\n",
    "* **Critic** vil estimere hva belønningen vil være fra disse handlingene. Den returnerer totale estimerte belønninger i fremtiden for den gitte tilstanden.\n",
    "\n",
    "La oss definere en slik modell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi må gjøre noen små endringer i funksjonene `discounted_rewards` og `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå skal vi kjøre hovedtreningsløkken. Vi vil bruke en manuell nettverkstreningsprosess ved å beregne riktige tapsfunksjoner og oppdatere nettverksparametere:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppsummering\n",
    "\n",
    "Vi har sett to RL-algoritmer i denne demonstrasjonen: enkel policy gradient og den mer sofistikerte actor-critic. Du kan se at disse algoritmene opererer med abstrakte begreper som tilstand, handling og belønning – derfor kan de brukes i svært forskjellige miljøer.\n",
    "\n",
    "Forsterkende læring lar oss finne den beste strategien for å løse et problem kun ved å se på den endelige belønningen. Det faktum at vi ikke trenger merkede datasett, gjør at vi kan gjenta simuleringer mange ganger for å optimalisere modellene våre. Likevel er det fortsatt mange utfordringer innen RL, som du kan lære mer om hvis du velger å fordype deg i dette spennende området innen AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T16:14:32+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}