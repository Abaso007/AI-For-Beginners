<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "178c0b5ee5395733eb18aec51e71a0a9",
  "translation_date": "2025-09-23T09:39:16+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/README.md",
  "language_code": "no"
}
-->
# Forh√•ndstrente nettverk og overf√∏ringsl√¶ring

√Ö trene CNN-er kan ta mye tid, og det kreves store mengder data for denne oppgaven. Mye av tiden brukes imidlertid p√• √• l√¶re de beste lavniv√•filtrene som et nettverk kan bruke for √• trekke ut m√∏nstre fra bilder. Et naturlig sp√∏rsm√•l oppst√•r: Kan vi bruke et nevralt nettverk som er trent p√• ett datasett og tilpasse det til √• klassifisere andre bilder uten √• m√•tte gjennomf√∏re en full treningsprosess?

## [Forh√•ndsquiz](https://ff-quizzes.netlify.app/en/ai/quiz/15)

Denne tiln√¶rmingen kalles **overf√∏ringsl√¶ring**, fordi vi overf√∏rer noe kunnskap fra √©n nevralt nettverksmodell til en annen. I overf√∏ringsl√¶ring starter vi vanligvis med en forh√•ndstrent modell, som er trent p√• et stort bildedatasett, som for eksempel **ImageNet**. Disse modellene kan allerede gj√∏re en god jobb med √• trekke ut ulike funksjoner fra generiske bilder, og i mange tilfeller kan det √• bygge en klassifiserer p√• toppen av disse funksjonene gi gode resultater.

> ‚úÖ Overf√∏ringsl√¶ring er et begrep som ogs√• finnes i andre akademiske felt, som utdanning. Det refererer til prosessen med √• ta kunnskap fra ett omr√•de og anvende det p√• et annet.

## Forh√•ndstrente modeller som funksjonsekstraktorer

De konvolusjonsnettverkene vi har snakket om i forrige seksjon, inneholder flere lag, hvor hvert lag skal trekke ut noen funksjoner fra bildet, fra lavniv√• pikselkombinasjoner (som horisontale/vertikale linjer eller str√∏k) til h√∏yere niv√• kombinasjoner av funksjoner, som tilsvarer ting som et √∏ye p√• en flamme. Hvis vi trener et CNN p√• et tilstrekkelig stort datasett med generiske og varierte bilder, b√∏r nettverket l√¶re √• trekke ut disse vanlige funksjonene.

B√•de Keras og PyTorch inneholder funksjoner for enkelt √• laste inn forh√•ndstrente nevrale nettverksvekter for noen vanlige arkitekturer, hvorav de fleste er trent p√• ImageNet-bilder. De mest brukte er beskrevet p√• siden [CNN-arkitekturer](../07-ConvNets/CNN_Architectures.md) fra forrige leksjon. Spesielt kan du vurdere √• bruke en av f√∏lgende:

* **VGG-16/VGG-19**, som er relativt enkle modeller som fortsatt gir god n√∏yaktighet. Ofte er det et godt valg √• bruke VGG som et f√∏rste fors√∏k for √• se hvordan overf√∏ringsl√¶ring fungerer.
* **ResNet** er en familie av modeller foresl√•tt av Microsoft Research i 2015. De har flere lag og krever derfor mer ressurser.
* **MobileNet** er en familie av modeller med redusert st√∏rrelse, egnet for mobile enheter. Bruk dem hvis du har begrensede ressurser og kan ofre litt n√∏yaktighet.

Her er eksempler p√• funksjoner som er trukket ut fra et bilde av en katt av VGG-16-nettverket:

![Funksjoner trukket ut av VGG-16](../../../../../translated_images/features.6291f9c7ba3a0b951af88fc9864632b9115365410765680680d30c927dd67354.no.png)

## Datasett: Katter vs. Hunder

I dette eksempelet vil vi bruke et datasett med [Katter og Hunder](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), som er sv√¶rt n√¶rt en reell bildeklassifiseringsscenario.

## ‚úçÔ∏è √òvelse: Overf√∏ringsl√¶ring

La oss se overf√∏ringsl√¶ring i praksis i de tilh√∏rende notatb√∏kene:

* [Overf√∏ringsl√¶ring - PyTorch](TransferLearningPyTorch.ipynb)
* [Overf√∏ringsl√¶ring - TensorFlow](TransferLearningTF.ipynb)

## Visualisering av en "ideell katt"

Et forh√•ndstrent nevralt nettverk inneholder ulike m√∏nstre i sin *hjerne*, inkludert forestillinger om en **ideell katt** (samt ideell hund, ideell sebra, osv.). Det ville v√¶rt interessant √• p√• en eller annen m√•te **visualisere dette bildet**. Men det er ikke enkelt, fordi m√∏nstrene er spredt over nettverksvektene og ogs√• organisert i en hierarkisk struktur.

En tiln√¶rming vi kan bruke, er √• starte med et tilfeldig bilde og deretter bruke **gradient descent-optimalisering** for √• justere bildet slik at nettverket begynner √• tro at det er en katt.

![Bildeoptimaliseringssl√∏yfe](../../../../../translated_images/ideal-cat-loop.999fbb8ff306e044f997032f4eef9152b453e6a990e449bbfb107de2493cc37e.no.png)

Men hvis vi gj√∏r dette, vil vi f√• noe som ligner veldig p√• tilfeldig st√∏y. Dette er fordi *det finnes mange m√•ter √• f√• nettverket til √• tro at inngangsbilde er en katt*, inkludert noen som ikke gir mening visuelt. Selv om disse bildene inneholder mange m√∏nstre som er typiske for en katt, er det ingenting som begrenser dem til √• v√¶re visuelt distinkte.

For √• forbedre resultatet kan vi legge til et annet ledd i tapsfunksjonen, som kalles **variasjonstap**. Dette er en metrikk som viser hvor like nabopikslene i bildet er. Ved √• minimere variasjonstap blir bildet jevnere og st√∏y fjernes, noe som avsl√∏rer mer visuelt tiltalende m√∏nstre. Her er et eksempel p√• slike "ideelle" bilder, som klassifiseres som katt og som sebra med h√∏y sannsynlighet:

![Ideell Katt](../../../../../translated_images/ideal-cat.203dd4597643d6b0bd73038b87f9c0464322725e3a06ab145d25d4a861c70592.no.png) | ![Ideell Sebra](../../../../../translated_images/ideal-zebra.7f70e8b54ee15a7a314000bb5df38a6cfe086ea04d60df4d3ef313d046b98a2b.no.png)
-----|-----
*Ideell Katt* | *Ideell Sebra*

En lignende tiln√¶rming kan brukes til √• utf√∏re s√•kalte **adversarielle angrep** p√• et nevralt nettverk. Anta at vi √∏nsker √• lure et nevralt nettverk og f√• en hund til √• se ut som en katt. Hvis vi tar et bilde av en hund, som nettverket gjenkjenner som en hund, kan vi justere det litt ved hjelp av gradient descent-optimalisering, til nettverket begynner √• klassifisere det som en katt:

![Bilde av en Hund](../../../../../translated_images/original-dog.8f68a67d2fe0911f33041c0f7fce8aa4ea919f9d3917ec4b468298522aeb6356.no.png) | ![Bilde av en hund klassifisert som en katt](../../../../../translated_images/adversarial-dog.d9fc7773b0142b89752539bfbf884118de845b3851c5162146ea0b8809fc820f.no.png)
-----|-----
*Originalbilde av en hund* | *Bilde av en hund klassifisert som en katt*

Se koden for √• reprodusere resultatene ovenfor i f√∏lgende notatbok:

* [Ideell og Adversariell Katt - TensorFlow](AdversarialCat_TF.ipynb)

## Konklusjon

Ved √• bruke overf√∏ringsl√¶ring kan du raskt sette sammen en klassifiserer for en tilpasset objektklassifiseringsoppgave og oppn√• h√∏y n√∏yaktighet. Du kan se at mer komplekse oppgaver som vi l√∏ser n√•, krever h√∏yere beregningskraft og ikke enkelt kan l√∏ses p√• en CPU. I neste enhet vil vi pr√∏ve √• bruke en mer lettvektsimplementasjon for √• trene den samme modellen med lavere beregningsressurser, noe som resulterer i bare litt lavere n√∏yaktighet.

## üöÄ Utfordring

I de medf√∏lgende notatb√∏kene er det notater nederst om hvordan overf√∏ringskunnskap fungerer best med noe lignende treningsdata (for eksempel en ny type dyr). Gj√∏r noen eksperimenter med helt nye typer bilder for √• se hvor godt eller d√•rlig dine overf√∏ringskunnskapsmodeller presterer.

## [Etterforelesningsquiz](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Gjennomgang og Selvstudium

Les gjennom [TrainingTricks.md](TrainingTricks.md) for √• utdype kunnskapen din om andre m√•ter √• trene modellene dine p√•.

## [Oppgave](lab/README.md)

I dette laboratoriet vil vi bruke det virkelige [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) kj√¶ledyrdatasettet med 35 raser av katter og hunder, og vi vil bygge en overf√∏ringsl√¶ringsklassifiserer.

---

