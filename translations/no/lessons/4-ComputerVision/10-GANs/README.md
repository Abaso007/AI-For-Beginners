<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0ff65b4da07b23697235de2beb2a3c25",
  "translation_date": "2025-09-23T09:41:22+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "no"
}
-->
# Generative Adversarial Networks

I forrige seksjon l√¶rte vi om **generative modeller**: modeller som kan generere nye bilder som ligner p√• de i treningsdatasettet. VAE var et godt eksempel p√• en generativ modell.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Men hvis vi pr√∏ver √• generere noe virkelig meningsfullt, som et maleri med rimelig oppl√∏sning, med VAE, vil vi se at treningen ikke konvergerer godt. For dette brukstilfellet b√∏r vi l√¶re om en annen arkitektur som er spesifikt rettet mot generative modeller - **Generative Adversarial Networks**, eller GANs.

Hovedideen med en GAN er √• ha to nevrale nettverk som trenes mot hverandre:

<img src="images/gan_architecture.png" width="70%"/>

> Bilde av [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Litt vokabular:
> * **Generator** er et nettverk som tar en tilfeldig vektor og produserer et bilde som resultat.
> * **Discriminator** er et nettverk som tar et bilde og skal avgj√∏re om det er et ekte bilde (fra treningsdatasettet) eller om det ble generert av en generator. Det er i hovedsak en bildekategoriserer.

### Discriminator

Arkitekturen til en discriminator skiller seg ikke fra et vanlig bildekategoriseringsnettverk. I det enkleste tilfellet kan det v√¶re en fullt tilkoblet kategoriserer, men mest sannsynlig vil det v√¶re et [konvolusjonsnettverk](../07-ConvNets/README.md).

> ‚úÖ En GAN basert p√• konvolusjonsnettverk kalles en [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

En CNN-discriminator best√•r av f√∏lgende lag: flere konvolusjoner+pooling (med minkende romlig st√∏rrelse) og ett eller flere fullt tilkoblede lag for √• f√• en "funksjonsvektor", og til slutt en bin√¶r kategoriserer.

> ‚úÖ 'Pooling' i denne sammenhengen er en teknikk som reduserer st√∏rrelsen p√• bildet. "Pooling-lag reduserer dimensjonene til dataene ved √• kombinere utgangene fra nevronklynger i ett lag til et enkelt nevron i neste lag." - [kilde](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

En generator er litt mer komplisert. Du kan se p√• den som en omvendt discriminator. Fra en latent vektor (i stedet for en funksjonsvektor) har den et fullt tilkoblet lag for √• konvertere det til √∏nsket st√∏rrelse/form, etterfulgt av dekonvolusjoner+oppskalering. Dette ligner p√• *dekoder*-delen av [autoencoder](../09-Autoencoders/README.md).

> ‚úÖ Fordi konvolusjonslaget implementeres som et line√¶rt filter som beveger seg over bildet, er dekonvolusjon i hovedsak lik konvolusjon og kan implementeres med samme laglogikk.

<img src="images/gan_arch_detail.png" width="70%"/>

> Bilde av [Dmitry Soshnikov](http://soshnikov.com)

### Trening av GAN

GANs kalles **adversarial** fordi det er en konstant konkurranse mellom generatoren og discriminator. Under denne konkurransen forbedres b√•de generatoren og discriminator, og nettverket l√¶rer dermed √• produsere bedre og bedre bilder.

Treningen skjer i to trinn:

* **Trening av discriminator**. Denne oppgaven er ganske rett frem: vi genererer en batch med bilder fra generatoren, merker dem som 0, som st√•r for falske bilder, og tar en batch med bilder fra inngangsdatasettet (med merkelapp 1, ekte bilder). Vi f√•r en *discriminator loss* og utf√∏rer backprop.
* **Trening av generator**. Dette er litt mer komplisert, fordi vi ikke vet det forventede resultatet for generatoren direkte. Vi tar hele GAN-nettverket som best√•r av en generator etterfulgt av en discriminator, mater det med noen tilfeldige vektorer, og forventer at resultatet skal v√¶re 1 (tilsvarende ekte bilder). Vi fryser deretter parameterne til discriminator (vi vil ikke trene den i dette trinnet) og utf√∏rer backprop.

Under denne prosessen g√•r verken generator- eller discriminator-tapene betydelig ned. I en ideell situasjon b√∏r de svinge, noe som tilsvarer at begge nettverk forbedrer ytelsen.

## ‚úçÔ∏è √òvelser: GANs

* [GAN Notebook i TensorFlow/Keras](GANTF.ipynb)
* [GAN Notebook i PyTorch](GANPyTorch.ipynb)

### Problemer med GAN-trening

GANs er kjent for √• v√¶re spesielt vanskelige √• trene. Her er noen problemer:

* **Mode Collapse**. Dette betyr at generatoren l√¶rer √• produsere ett vellykket bilde som lurer discriminator, og ikke en variasjon av forskjellige bilder.
* **F√∏lsomhet for hyperparametere**. Ofte kan du se at en GAN ikke konvergerer i det hele tatt, og s√• plutselig f√∏rer en reduksjon i l√¶ringsraten til konvergens.
* √Ö opprettholde en **balanse** mellom generatoren og discriminator. I mange tilfeller kan discriminator-tapet falle til null relativt raskt, noe som resulterer i at generatoren ikke kan trenes videre. For √• overvinne dette kan vi pr√∏ve √• sette forskjellige l√¶ringsrater for generatoren og discriminator, eller hoppe over trening av discriminator hvis tapet allerede er for lavt.
* Trening for **h√∏y oppl√∏sning**. Dette problemet, som ogs√• oppst√•r med autoencodere, utl√∏ses fordi rekonstruksjon av for mange lag i et konvolusjonsnettverk f√∏rer til artefakter. Dette problemet l√∏ses vanligvis med s√•kalt **progressiv vekst**, der f√∏rst noen f√• lag trenes p√• lavoppl√∏selige bilder, og deretter "l√•ses opp" eller legges til flere lag. En annen l√∏sning er √• legge til ekstra forbindelser mellom lagene og trene flere oppl√∏sninger samtidig - se denne [Multi-Scale Gradient GANs-artikkelen](https://arxiv.org/abs/1903.06048) for detaljer.

## Stiloverf√∏ring

GANs er en flott m√•te √• generere kunstneriske bilder p√•. En annen interessant teknikk er s√•kalt **stiloverf√∏ring**, som tar ett **innholdsbilde** og tegner det p√• nytt i en annen stil ved √• bruke filtre fra et **stilbilde**.

Slik fungerer det:
* Vi starter med et tilfeldig st√∏ybilde (eller med et innholdsbilde, men for forst√•elsens skyld er det enklere √• starte med tilfeldig st√∏y).
* M√•let v√•rt er √• lage et bilde som er n√¶rt b√•de innholdsbilde og stilbilde. Dette bestemmes av to tapfunksjoner:
   - **Innholdstap** beregnes basert p√• funksjonene som CNN trekker ut fra noen lag fra det n√•v√¶rende bildet og innholdsbilde.
   - **Stiltap** beregnes mellom det n√•v√¶rende bildet og stilbildet p√• en smart m√•te ved hjelp av Gram-matriser (mer detaljer i [eksempelfilen](StyleTransfer.ipynb)).
* For √• gj√∏re bildet jevnere og fjerne st√∏y, introduserer vi ogs√• **Variasjonstap**, som beregner gjennomsnittlig avstand mellom nabopiksler.
* Hovedoptimaliseringsl√∏kken justerer det n√•v√¶rende bildet ved hjelp av gradient descent (eller en annen optimaliseringsalgoritme) for √• minimere det totale tapet, som er en vektet sum av alle tre tapene.

## ‚úçÔ∏è Eksempel: [Stiloverf√∏ring](StyleTransfer.ipynb)

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Konklusjon

I denne leksjonen l√¶rte du om GANs og hvordan du trener dem. Du l√¶rte ogs√• om de spesielle utfordringene denne typen nevrale nettverk kan m√∏te, og noen strategier for √• overvinne dem.

## üöÄ Utfordring

Kj√∏r gjennom [stiloverf√∏ringsnotatboken](StyleTransfer.ipynb) med dine egne bilder.

## Gjennomgang og selvstudium

For referanse, les mer om GANs i disse ressursene:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), en *de facto* GAN-arkitektur √• vurdere
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Oppgave

G√• tilbake til en av de to notatb√∏kene knyttet til denne leksjonen og tren GAN p√• dine egne bilder. Hva kan du skape?

---

