<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-28T15:27:17+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "no"
}
-->
# Generative Adversarial Networks

I forrige seksjon l√¶rte vi om **generative modeller**: modeller som kan generere nye bilder som ligner p√• de i treningsdatasettet. VAE var et godt eksempel p√• en generativ modell.

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/110)

Men hvis vi pr√∏ver √• generere noe virkelig meningsfullt, som et maleri med rimelig oppl√∏sning, med VAE, vil vi se at treningen ikke konvergerer godt. For dette brukstilfellet b√∏r vi l√¶re om en annen arkitektur som er spesifikt rettet mot generative modeller - **Generative Adversarial Networks**, eller GANs.

Hovedideen med en GAN er √• ha to nevrale nettverk som trenes mot hverandre:

<img src="images/gan_architecture.png" width="70%"/>

> Bilde av [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Litt vokabular:
> * **Generator** er et nettverk som tar en tilfeldig vektor og produserer et bilde som resultat.
> * **Discriminator** er et nettverk som tar et bilde og skal avgj√∏re om det er et ekte bilde (fra treningsdatasettet) eller om det ble generert av en generator. Det er i hovedsak en bildekategoriserer.

### Discriminator

Arkitekturen til discriminator skiller seg ikke fra et vanlig bildekategoriseringsnettverk. I det enkleste tilfellet kan det v√¶re en fullt tilkoblet klassifiserer, men mest sannsynlig vil det v√¶re et [konvolusjonsnettverk](../07-ConvNets/README.md).

> ‚úÖ En GAN basert p√• konvolusjonsnettverk kalles en [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

En CNN-discriminator best√•r av f√∏lgende lag: flere konvolusjoner+poolings (med minkende romlig st√∏rrelse) og √©n eller flere fullt tilkoblede lag for √• f√• en "funksjonsvektor", og til slutt en bin√¶r klassifiserer.

> ‚úÖ 'Pooling' i denne sammenhengen er en teknikk som reduserer st√∏rrelsen p√• bildet. "Pooling-lag reduserer dimensjonene av data ved √• kombinere utgangene fra nevronklynger p√• ett lag til et enkelt nevron p√• neste lag." - [kilde](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

En generator er litt mer komplisert. Du kan betrakte den som en omvendt discriminator. Fra en latent vektor (i stedet for en funksjonsvektor) har den et fullt tilkoblet lag for √• konvertere det til √∏nsket st√∏rrelse/form, etterfulgt av dekonvolusjoner+oppskalering. Dette ligner p√• *dekoder*-delen av [autoencoder](../09-Autoencoders/README.md).

> ‚úÖ Fordi konvolusjonslaget implementeres som et line√¶rt filter som beveger seg over bildet, er dekonvolusjon i hovedsak lik konvolusjon og kan implementeres med samme laglogikk.

<img src="images/gan_arch_detail.png" width="70%"/>

> Bilde av [Dmitry Soshnikov](http://soshnikov.com)

### Trening av GAN

GANs kalles **adversarial** fordi det er en konstant konkurranse mellom generatoren og discriminatoren. Under denne konkurransen forbedres b√•de generatoren og discriminatoren, og nettverket l√¶rer √• produsere bedre og bedre bilder.

Treningen skjer i to stadier:

* **Trening av discriminator**. Denne oppgaven er ganske enkel: vi genererer en batch med bilder fra generatoren, merker dem med 0, som st√•r for falske bilder, og tar en batch med bilder fra input-datasettet (med merkelapp 1, ekte bilder). Vi f√•r en *discriminator loss* og utf√∏rer backprop.
* **Trening av generator**. Dette er litt mer komplisert, fordi vi ikke direkte vet det forventede resultatet for generatoren. Vi tar hele GAN-nettverket som best√•r av en generator etterfulgt av en discriminator, mater det med noen tilfeldige vektorer, og forventer at resultatet skal v√¶re 1 (tilsvarende ekte bilder). Vi fryser deretter parameterne til discriminatoren (vi vil ikke at den skal trenes i dette steget) og utf√∏rer backprop.

Under denne prosessen g√•r verken generator- eller discriminator-tapene betydelig ned. I ideelle situasjoner b√∏r de oscillere, noe som tilsvarer at begge nettverk forbedrer ytelsen.

## ‚úçÔ∏è √òvelser: GANs

* [GAN Notebook i TensorFlow/Keras](GANTF.ipynb)
* [GAN Notebook i PyTorch](GANPyTorch.ipynb)

### Problemer med GAN-trening

GANs er kjent for √• v√¶re spesielt vanskelige √• trene. Her er noen problemer:

* **Mode Collapse**. Med dette begrepet mener vi at generatoren l√¶rer √• produsere ett vellykket bilde som lurer discriminatoren, og ikke en variasjon av forskjellige bilder.
* **Sensitivitet for hyperparametere**. Ofte kan du se at en GAN ikke konvergerer i det hele tatt, og s√• plutselig f√∏rer en reduksjon i l√¶ringsraten til konvergens.
* √Ö opprettholde en **balanse** mellom generatoren og discriminatoren. I mange tilfeller kan discriminator-tapet falle til null relativt raskt, noe som resulterer i at generatoren ikke kan trenes videre. For √• overvinne dette kan vi pr√∏ve √• sette forskjellige l√¶ringsrater for generatoren og discriminatoren, eller hoppe over trening av discriminator hvis tapet allerede er for lavt.
* Trening for **h√∏y oppl√∏sning**. Dette reflekterer det samme problemet som med autoencodere, og problemet oppst√•r fordi rekonstruksjon av for mange lag i konvolusjonsnettverket f√∏rer til artefakter. Dette problemet l√∏ses vanligvis med s√•kalt **progressiv vekst**, der f√∏rst noen f√• lag trenes p√• lavoppl√∏ste bilder, og deretter "l√•ses opp" eller legges til lag. En annen l√∏sning er √• legge til ekstra forbindelser mellom lagene og trene flere oppl√∏sninger samtidig - se denne [Multi-Scale Gradient GANs-artikkelen](https://arxiv.org/abs/1903.06048) for detaljer.

## Style Transfer

GANs er en flott m√•te √• generere kunstneriske bilder p√•. En annen interessant teknikk er s√•kalt **style transfer**, som tar ett **innholdsbilde** og tegner det p√• nytt i en annen stil, ved √• bruke filtre fra **stilbilde**.

Slik fungerer det:
* Vi starter med et tilfeldig st√∏ybilde (eller med et innholdsbilde, men for forst√•elsens skyld er det lettere √• starte med tilfeldig st√∏y).
* M√•let v√•rt vil v√¶re √• lage et bilde som er n√¶rt b√•de innholdsbilde og stilbilde. Dette bestemmes av to tapfunksjoner:
   - **Content loss** beregnes basert p√• funksjonene som er hentet ut av CNN p√• noen lag fra n√•v√¶rende bilde og innholdsbilde.
   - **Style loss** beregnes mellom n√•v√¶rende bilde og stilbilde p√• en smart m√•te ved hjelp av Gram-matriser (mer detaljer i [eksempelfilen](StyleTransfer.ipynb)).
* For √• gj√∏re bildet jevnere og fjerne st√∏y, introduserer vi ogs√• **Variation loss**, som beregner gjennomsnittlig avstand mellom nabopiksler.
* Den viktigste optimaliseringssl√∏yfen justerer n√•v√¶rende bilde ved hjelp av gradient descent (eller en annen optimaliseringsalgoritme) for √• minimere det totale tapet, som er en vektet sum av alle tre tapene.

## ‚úçÔ∏è Eksempel: [Style Transfer](StyleTransfer.ipynb)

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/210)

## Konklusjon

I denne leksjonen l√¶rte du om GANs og hvordan du trener dem. Du l√¶rte ogs√• om de spesielle utfordringene denne typen nevrale nettverk kan m√∏te, og noen strategier for √• komme forbi dem.

## üöÄ Utfordring

Kj√∏r gjennom [Style Transfer-notebooken](StyleTransfer.ipynb) med dine egne bilder.

## Gjennomgang og selvstudium

For referanse, les mer om GANs i disse ressursene:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), en *de facto* GAN-arkitektur √• vurdere
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Oppgave

G√• tilbake til en av de to notebookene knyttet til denne leksjonen og tren GAN p√• dine egne bilder. Hva kan du skape?

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.