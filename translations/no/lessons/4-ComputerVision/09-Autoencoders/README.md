<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-28T15:24:32+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "no"
}
-->
# Autoenkodere

N√•r vi trener CNN-er, er en av utfordringene at vi trenger mye merket data. N√•r det gjelder bildeklassifisering, m√• vi dele bilder inn i forskjellige klasser, noe som krever manuelt arbeid.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Vi kan imidlertid √∏nske √• bruke r√• (umerket) data for √• trene CNN-funksjonsekstraktorer, noe som kalles **selv-supervisert l√¶ring**. I stedet for etiketter bruker vi treningsbilder b√•de som nettverksinput og output. Hovedideen med **autoenkoder** er at vi har et **enkodernettverk** som konverterer inputbildet til et **latent rom** (vanligvis bare en vektor av mindre st√∏rrelse), og deretter et **dekodernettverk**, som har som m√•l √• rekonstruere det originale bildet.

> ‚úÖ En [autoenkoder](https://wikipedia.org/wiki/Autoencoder) er "en type kunstig nevralt nettverk som brukes til √• l√¶re effektive kodinger av umerket data."

Siden vi trener en autoenkoder til √• fange s√• mye informasjon som mulig fra det originale bildet for n√∏yaktig rekonstruksjon, pr√∏ver nettverket √• finne den beste **innkapslingen** av inputbilder for √• fange meningen.

![AutoEncoder Diagram](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.no.jpg)

> Bilde fra [Keras-bloggen](https://blog.keras.io/building-autoencoders-in-keras.html)

## Scenarier for bruk av autoenkodere

Selv om det √• rekonstruere originale bilder ikke virker nyttig i seg selv, finnes det noen scenarier der autoenkodere er spesielt nyttige:

* **Redusere dimensjonen p√• bilder for visualisering** eller **trene bildeinnkapslinger**. Vanligvis gir autoenkodere bedre resultater enn PCA, fordi de tar hensyn til bildenes romlige natur og hierarkiske funksjoner.
* **Fjerne st√∏y**, alts√• fjerne st√∏y fra bildet. Fordi st√∏y inneholder mye unyttig informasjon, kan ikke autoenkoderen f√• plass til alt i det relativt lille latente rommet, og dermed fanger den bare den viktige delen av bildet. N√•r vi trener st√∏yfjernere, starter vi med originale bilder og bruker bilder med kunstig lagt til st√∏y som input for autoenkoderen.
* **Superoppl√∏sning**, √∏ke bildekvaliteten. Vi starter med bilder med h√∏y oppl√∏sning og bruker bildet med lavere oppl√∏sning som input for autoenkoderen.
* **Generative modeller**. N√•r vi har trent autoenkoderen, kan dekoderdelen brukes til √• lage nye objekter basert p√• tilfeldige latente vektorer.

## Variasjonelle autoenkodere (VAE)

Tradisjonelle autoenkodere reduserer dimensjonen p√• inputdata p√• en eller annen m√•te, og finner de viktige funksjonene i inputbildene. Latente vektorer gir imidlertid ofte ikke mye mening. Med andre ord, hvis vi tar MNIST-datasettet som et eksempel, er det ikke lett √• finne ut hvilke sifre som tilsvarer forskjellige latente vektorer, fordi n√¶rliggende latente vektorer ikke n√∏dvendigvis tilsvarer de samme sifrene.

P√• den annen side, for √• trene *generative* modeller er det bedre √• ha en viss forst√•else av det latente rommet. Denne ideen leder oss til **variabel autoenkoder** (VAE).

VAE er en autoenkoder som l√¶rer √• forutsi *statistisk fordeling* av de latente parameterne, den s√•kalte **latente fordelingen**. For eksempel kan vi √∏nske at latente vektorer skal v√¶re normalt fordelt med en viss gjennomsnitt z<sub>mean</sub> og standardavvik z<sub>sigma</sub> (b√•de gjennomsnitt og standardavvik er vektorer med en viss dimensjonalitet d). Enkoder i VAE l√¶rer √• forutsi disse parameterne, og deretter tar dekoderen en tilfeldig vektor fra denne fordelingen for √• rekonstruere objektet.

For √• oppsummere:

 * Fra inputvektoren forutsier vi `z_mean` og `z_log_sigma` (i stedet for √• forutsi standardavviket direkte, forutsier vi logaritmen av det)
 * Vi tar en pr√∏vevektor `sample` fra fordelingen N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Dekoderen pr√∏ver √• dekode det originale bildet ved √• bruke `sample` som inputvektor

 <img src="images/vae.png" width="50%">

> Bilde fra [denne bloggposten](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) av Isaak Dykeman

Variasjonelle autoenkodere bruker en kompleks tapsfunksjon som best√•r av to deler:

* **Rekonstruksjonstap** er tapsfunksjonen som viser hvor n√¶rt et rekonstruert bilde er m√•let (det kan v√¶re Mean Squared Error, eller MSE). Det er den samme tapsfunksjonen som i vanlige autoenkodere.
* **KL-tap**, som sikrer at den latente variabelens fordeling holder seg n√¶r normalfordelingen. Det er basert p√• begrepet [Kullback-Leibler-divergens](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - en metrikk for √• estimere hvor like to statistiske fordelinger er.

En viktig fordel med VAE-er er at de lar oss generere nye bilder relativt enkelt, fordi vi vet hvilken fordeling vi skal ta latente vektorer fra. For eksempel, hvis vi trener VAE med en 2D latent vektor p√• MNIST, kan vi deretter variere komponentene i den latente vektoren for √• f√• forskjellige sifre:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Bilde av [Dmitry Soshnikov](http://soshnikov.com)

Legg merke til hvordan bildene glir over i hverandre, ettersom vi begynner √• ta latente vektorer fra forskjellige deler av det latente parameterrommet. Vi kan ogs√• visualisere dette rommet i 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Bilde av [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è √òvelser: Autoenkodere

L√¶r mer om autoenkodere i disse tilh√∏rende notatb√∏kene:

* [Autoenkodere i TensorFlow](AutoencodersTF.ipynb)
* [Autoenkodere i PyTorch](AutoEncodersPyTorch.ipynb)

## Egenskaper ved autoenkodere

* **Dataspesifikke** - de fungerer bare godt med den typen bilder de er trent p√•. For eksempel, hvis vi trener et superoppl√∏sningsnettverk p√• blomster, vil det ikke fungere godt p√• portretter. Dette er fordi nettverket kan produsere bilder med h√∏yere oppl√∏sning ved √• ta fine detaljer fra funksjoner l√¶rt fra treningsdatasettet.
* **Tapsbaserte** - det rekonstruerte bildet er ikke det samme som det originale bildet. Tapets natur er definert av *tapsfunksjonen* som brukes under trening.
* Fungerer p√• **umerket data**

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Konklusjon

I denne leksjonen l√¶rte du om de forskjellige typene autoenkodere som er tilgjengelige for AI-forskeren. Du l√¶rte hvordan du bygger dem, og hvordan du bruker dem til √• rekonstruere bilder. Du l√¶rte ogs√• om VAE og hvordan du bruker det til √• generere nye bilder.

## üöÄ Utfordring

I denne leksjonen l√¶rte du om bruk av autoenkodere for bilder. Men de kan ogs√• brukes for musikk! Sjekk ut Magenta-prosjektets [MusicVAE](https://magenta.tensorflow.org/music-vae)-prosjekt, som bruker autoenkodere for √• l√¶re √• rekonstruere musikk. Gj√∏r noen [eksperimenter](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) med dette biblioteket for √• se hva du kan skape.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Gjennomgang & Selvstudium

For referanse, les mer om autoenkodere i disse ressursene:

* [Bygge autoenkodere i Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Bloggpost p√• NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Variasjonelle autoenkodere forklart](https://kvfrans.com/variational-autoencoders-explained/)
* [Betingede variasjonelle autoenkodere](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Oppgave

P√• slutten av [denne notatboken med TensorFlow](AutoencodersTF.ipynb), finner du en 'oppgave' - bruk denne som din oppgave.

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.