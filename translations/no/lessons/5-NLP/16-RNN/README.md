<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-28T15:59:55+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "no"
}
-->
# Rekurrente Nevrale Nettverk

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/31)

I tidligere seksjoner har vi brukt rike semantiske representasjoner av tekst og en enkel line√¶r klassifikator p√• toppen av embeddingene. Det denne arkitekturen gj√∏r, er √• fange opp den samlede meningen av ordene i en setning, men den tar ikke hensyn til **rekkef√∏lgen** av ordene, fordi aggregeringsoperasjonen p√• embeddingene fjerner denne informasjonen fra den opprinnelige teksten. Siden disse modellene ikke kan modellere ordrekkef√∏lge, kan de ikke l√∏se mer komplekse eller tvetydige oppgaver som tekstgenerering eller sp√∏rsm√•l-svar.

For √• fange opp meningen i en tekstsekvens, m√• vi bruke en annen nevralt nettverksarkitektur, som kalles et **rekurrent nevralt nettverk**, eller RNN. I et RNN sender vi setningen v√•r gjennom nettverket ett symbol om gangen, og nettverket produserer en **tilstand**, som vi deretter sender tilbake til nettverket sammen med neste symbol.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.no.png)

> Bilde av forfatteren

Gitt en inngangssekvens av tokens X<sub>0</sub>,...,X<sub>n</sub>, lager RNN en sekvens av nevrale nettverksblokker og trener denne sekvensen ende-til-ende ved hjelp av backpropagation. Hver nettverksblokk tar et par (X<sub>i</sub>,S<sub>i</sub>) som input og produserer S<sub>i+1</sub> som resultat. Den endelige tilstanden S<sub>n</sub> (eller output Y<sub>n</sub>) g√•r inn i en line√¶r klassifikator for √• produsere resultatet. Alle nettverksblokkene deler de samme vektene og trenes ende-til-ende med √©n backpropagation-passering.

Siden tilstandsvektorene S<sub>0</sub>,...,S<sub>n</sub> sendes gjennom nettverket, er det i stand til √• l√¶re de sekvensielle avhengighetene mellom ordene. For eksempel, n√•r ordet *ikke* dukker opp et sted i sekvensen, kan det l√¶re √• negere visse elementer i tilstandsvektoren, noe som resulterer i negasjon.

> ‚úÖ Siden vektene til alle RNN-blokkene i bildet ovenfor er delte, kan det samme bildet representeres som √©n blokk (til h√∏yre) med en rekurrent tilbakemeldingssl√∏yfe, som sender nettverkets output-tilstand tilbake til input.

## Anatomien til en RNN-celle

La oss se hvordan en enkel RNN-celle er organisert. Den aksepterer den forrige tilstanden S<sub>i-1</sub> og det n√•v√¶rende symbolet X<sub>i</sub> som input, og m√• produsere output-tilstanden S<sub>i</sub> (og noen ganger er vi ogs√• interessert i en annen output Y<sub>i</sub>, som i tilfellet med generative nettverk).

En enkel RNN-celle har to vektmatriser inni seg: √©n som transformerer et input-symbol (vi kaller den W), og en annen som transformerer en input-tilstand (H). I dette tilfellet beregnes output fra nettverket som œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b), der œÉ er aktiveringsfunksjonen og b er en ekstra bias.

<img alt="RNN Celle Anatomi" src="images/rnn-anatomy.png" width="50%"/>

> Bilde av forfatteren

I mange tilfeller sendes input-tokens gjennom embedding-laget f√∏r de g√•r inn i RNN for √• redusere dimensjonaliteten. I dette tilfellet, hvis dimensjonen til input-vektorene er *emb_size*, og tilstandsvektoren er *hid_size* - er st√∏rrelsen p√• W *emb_size*√ó*hid_size*, og st√∏rrelsen p√• H er *hid_size*√ó*hid_size*.

## Long Short Term Memory (LSTM)

Et av hovedproblemene med klassiske RNN-er er det s√•kalte **forsvinnende gradienter**-problemet. Fordi RNN-er trenes ende-til-ende i √©n backpropagation-passering, har de vanskeligheter med √• propagere feil til de f√∏rste lagene i nettverket, og dermed kan ikke nettverket l√¶re relasjoner mellom fjerne tokens. En av m√•tene √• unng√• dette problemet p√• er √• introdusere **eksplisitt tilstandsh√•ndtering** ved √• bruke s√•kalte **porter**. Det finnes to velkjente arkitekturer av denne typen: **Long Short Term Memory** (LSTM) og **Gated Relay Unit** (GRU).

![Bilde som viser et eksempel p√• en long short term memory-celle](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Bildekilde TBD

LSTM-nettverket er organisert p√• en m√•te som ligner p√• RNN, men det er to tilstander som sendes fra lag til lag: den faktiske tilstanden C og den skjulte vektoren H. Ved hver enhet blir den skjulte vektoren H<sub>i</sub> sl√•tt sammen med input X<sub>i</sub>, og de kontrollerer hva som skjer med tilstanden C via **porter**. Hver port er et nevralt nettverk med sigmoid-aktivering (output i omr√•det [0,1]), som kan betraktes som en bitmaske n√•r den multipliseres med tilstandsvektoren. Det finnes f√∏lgende porter (fra venstre til h√∏yre p√• bildet ovenfor):

* **Glemselsporten** tar en skjult vektor og bestemmer hvilke komponenter av vektoren C vi trenger √• glemme, og hvilke vi skal beholde.
* **Inputporten** tar noe informasjon fra input- og skjulte vektorer og setter det inn i tilstanden.
* **Outputporten** transformerer tilstanden via et line√¶rt lag med *tanh*-aktivering, og velger deretter noen av komponentene ved hjelp av en skjult vektor H<sub>i</sub> for √• produsere en ny tilstand C<sub>i+1</sub>.

Komponentene i tilstanden C kan betraktes som noen flagg som kan sl√•s av og p√•. For eksempel, n√•r vi m√∏ter et navn som *Alice* i sekvensen, kan vi anta at det refererer til en kvinnelig karakter, og heve flagget i tilstanden som indikerer at vi har et kvinnelig substantiv i setningen. N√•r vi senere m√∏ter frasen *og Tom*, vil vi heve flagget som indikerer at vi har et flertallssubstantiv. Dermed kan vi ved √• manipulere tilstanden holde oversikt over de grammatiske egenskapene til setningsdelene.

> ‚úÖ En utmerket ressurs for √• forst√• det indre av LSTM er denne flotte artikkelen [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) av Christopher Olah.

## Toveis og flerlags RNN-er

Vi har diskutert rekurrente nettverk som opererer i √©n retning, fra begynnelsen av en sekvens til slutten. Det virker naturlig, fordi det ligner m√•ten vi leser og lytter til tale. Men siden vi i mange praktiske tilfeller har tilfeldig tilgang til inngangssekvensen, kan det v√¶re fornuftig √• kj√∏re rekurrent beregning i begge retninger. Slike nettverk kalles **toveis** RNN-er. N√•r vi arbeider med et toveis nettverk, trenger vi to skjulte tilstandsvektorer, √©n for hver retning.

Et rekurrent nettverk, enten det er √©nveis eller toveis, fanger opp visse m√∏nstre i en sekvens og kan lagre dem i en tilstandsvektor eller sende dem til output. Som med konvolusjonsnettverk, kan vi bygge et annet rekurrent lag p√• toppen av det f√∏rste for √• fange opp h√∏yere niv√• m√∏nstre og bygge videre p√• lavniv√•m√∏nstre som det f√∏rste laget har hentet ut. Dette f√∏rer oss til begrepet **flerlags RNN**, som best√•r av to eller flere rekurrente nettverk, der output fra det forrige laget sendes til neste lag som input.

![Bilde som viser et flerlags long-short-term-memory-RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.no.jpg)

*Bilde fra [denne fantastiske artikkelen](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) av Fernando L√≥pez*

## ‚úçÔ∏è √òvelser: Embeddings

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:

* [RNN-er med PyTorch](RNNPyTorch.ipynb)
* [RNN-er med TensorFlow](RNNTF.ipynb)

## Konklusjon

I denne enheten har vi sett at RNN-er kan brukes til sekvensklassifisering, men de kan faktisk h√•ndtere mange flere oppgaver, som tekstgenerering, maskinoversettelse og mer. Vi vil se n√¶rmere p√• disse oppgavene i neste enhet.

## üöÄ Utfordring

Les gjennom noe litteratur om LSTM-er og vurder deres anvendelser:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Gjennomgang og selvstudium

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) av Christopher Olah.

## [Oppgave: Notatb√∏ker](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.