<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "51be6057374d01d70e07dd5ec88ebc0d",
  "translation_date": "2025-09-23T09:45:38+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "no"
}
-->
# Generative nettverk

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Rekurrente nevrale nettverk (RNNs) og deres gatede cellevarianter som Long Short Term Memory Cells (LSTMs) og Gated Recurrent Units (GRUs) gir en mekanisme for spr√•kmodellering ved at de kan l√¶re ordrekkef√∏lge og gi prediksjoner for neste ord i en sekvens. Dette gj√∏r det mulig √• bruke RNNs til **generative oppgaver**, som vanlig tekstgenerering, maskinoversettelse og til og med bildetekstgenerering.

> ‚úÖ Tenk p√• alle gangene du har dratt nytte av generative oppgaver, som tekstfullf√∏ring mens du skriver. Unders√∏k dine favorittapplikasjoner for √• se om de har brukt RNNs.

I RNN-arkitekturen vi diskuterte i forrige enhet, produserte hver RNN-enhet den neste skjulte tilstanden som et output. Men vi kan ogs√• legge til en annen output til hver rekurrente enhet, som gj√∏r det mulig √• generere en **sekvens** (som er like lang som den opprinnelige sekvensen). Videre kan vi bruke RNN-enheter som ikke tar imot input ved hvert steg, men bare tar en initial tilstandsvektor og deretter produserer en sekvens av outputs.

Dette √•pner for ulike nevrale arkitekturer som vist i bildet nedenfor:

![Bilde som viser vanlige m√∏nstre for rekurrente nevrale nettverk.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.no.jpg)

> Bilde fra blogginnlegget [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) av [Andrej Karpaty](http://karpathy.github.io/)

* **En-til-en** er et tradisjonelt nevralt nettverk med √©n input og √©n output
* **En-til-mange** er en generativ arkitektur som tar √©n input-verdi og genererer en sekvens av output-verdier. For eksempel, hvis vi √∏nsker √• trene et **bildetekstnettverk** som genererer en tekstbeskrivelse av et bilde, kan vi ta et bilde som input, sende det gjennom en CNN for √• f√• en skjult tilstand, og deretter la en rekurrent kjede generere tekst ord for ord.
* **Mange-til-en** tilsvarer RNN-arkitekturer vi beskrev i forrige enhet, som tekstklassifisering.
* **Mange-til-mange**, eller **sekvens-til-sekvens**, tilsvarer oppgaver som **maskinoversettelse**, der vi f√∏rst har en RNN som samler all informasjon fra input-sekvensen inn i den skjulte tilstanden, og en annen RNN-kjede som ruller ut denne tilstanden til output-sekvensen.

I denne enheten vil vi fokusere p√• enkle generative modeller som hjelper oss med √• generere tekst. For enkelhets skyld vil vi bruke tegnbasert tokenisering.

Vi vil trene denne RNN-en til √• generere tekst steg for steg. P√• hvert steg tar vi en sekvens av tegn med lengde `nchars` og ber nettverket generere neste output-tegn for hvert input-tegn:

![Bilde som viser et eksempel p√• RNN-generering av ordet 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.no.png)

N√•r vi genererer tekst (under inferens), starter vi med en **prompt**, som sendes gjennom RNN-celler for √• generere dens mellomliggende tilstand, og deretter starter genereringen fra denne tilstanden. Vi genererer ett tegn om gangen og sender tilstanden og det genererte tegnet til en annen RNN-celle for √• generere det neste, helt til vi har generert nok tegn.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Bilde av forfatteren

## ‚úçÔ∏è √òvelser: Generative nettverk

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:

* [Generative nettverk med PyTorch](GenerativePyTorch.ipynb)
* [Generative nettverk med TensorFlow](GenerativeTF.ipynb)

## Myk tekstgenerering og temperatur

Outputen fra hver RNN-celle er en sannsynlighetsfordeling av tegn. Hvis vi alltid velger tegnet med h√∏yest sannsynlighet som neste tegn i generert tekst, kan teksten ofte bli "syklisk" mellom de samme tegnsekvensene igjen og igjen, som i dette eksempelet:

```
today of the second the company and a second the company ...
```

Men hvis vi ser p√• sannsynlighetsfordelingen for neste tegn, kan det v√¶re at forskjellen mellom noen av de h√∏yeste sannsynlighetene ikke er stor, f.eks. ett tegn kan ha sannsynlighet 0.2, et annet - 0.19, osv. For eksempel, n√•r vi ser etter neste tegn i sekvensen '*play*', kan neste tegn like gjerne v√¶re et mellomrom eller **e** (som i ordet *player*).

Dette leder oss til konklusjonen at det ikke alltid er "rettferdig" √• velge tegnet med h√∏yest sannsynlighet, fordi det √• velge det nest h√∏yeste fortsatt kan f√∏re til meningsfull tekst. Det er mer fornuftig √• **samle** tegn fra sannsynlighetsfordelingen gitt av nettverksoutputen. Vi kan ogs√• bruke en parameter, **temperatur**, som vil jevne ut sannsynlighetsfordelingen hvis vi √∏nsker √• legge til mer tilfeldighet, eller gj√∏re den brattere hvis vi vil holde oss mer til tegnene med h√∏yest sannsynlighet.

Utforsk hvordan denne myke tekstgenereringen er implementert i notatb√∏kene som er lenket ovenfor.

## Konklusjon

Selv om tekstgenerering kan v√¶re nyttig i seg selv, kommer de st√∏rste fordelene fra evnen til √• generere tekst ved hjelp av RNNs fra en initial funksjonsvektor. For eksempel brukes tekstgenerering som en del av maskinoversettelse (sekvens-til-sekvens, i dette tilfellet brukes tilstandsvektoren fra *encoder* til √• generere eller *dekode* den oversatte meldingen), eller til √• generere tekstbeskrivelser av et bilde (i dette tilfellet vil funksjonsvektoren komme fra CNN-ekstraktoren).

## üöÄ Utfordring

Ta noen leksjoner p√• Microsoft Learn om dette emnet

* Tekstgenerering med [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Gjennomgang & Selvstudium

Her er noen artikler for √• utvide kunnskapen din

* Ulike tiln√¶rminger til tekstgenerering med Markov Chain, LSTM og GPT-2: [blogginnlegg](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Eksempel p√• tekstgenerering i [Keras-dokumentasjon](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Oppgave](lab/README.md)

Vi har sett hvordan man genererer tekst tegn-for-tegn. I laboratoriet vil du utforske tekstgenerering p√• ordniv√•.

---

