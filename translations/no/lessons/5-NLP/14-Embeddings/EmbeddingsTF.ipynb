{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innebygginger\n",
    "\n",
    "I vårt forrige eksempel jobbet vi med høy-dimensjonale bag-of-words vektorer med lengde `vocab_size`, og vi konverterte eksplisitt lav-dimensjonale posisjonsrepresentasjonsvektorer til sparsomme én-hot representasjoner. Denne én-hot representasjonen er ikke minneeffektiv. I tillegg behandles hvert ord uavhengig av hverandre, så én-hot kodede vektorer uttrykker ikke semantiske likheter mellom ord.\n",
    "\n",
    "I denne enheten skal vi fortsette å utforske **News AG**-datasettet. For å begynne, la oss laste inn dataene og hente noen definisjoner fra forrige enhet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hva er en embedding?\n",
    "\n",
    "Ideen med **embedding** er å representere ord ved hjelp av lavdimensjonale tette vektorer som reflekterer den semantiske betydningen av ordet. Vi skal senere diskutere hvordan man bygger meningsfulle ord-embeddings, men for nå kan vi bare tenke på embeddings som en måte å redusere dimensjonaliteten til en ordvektor. \n",
    "\n",
    "En embedding-lag tar altså et ord som input og produserer en utgangsvektor med spesifisert `embedding_size`. På en måte ligner det veldig på et `Dense`-lag, men i stedet for å ta en one-hot kodet vektor som input, kan det ta et ordnummer.\n",
    "\n",
    "Ved å bruke et embedding-lag som det første laget i nettverket vårt, kan vi gå fra bag-of-words til en **embedding bag**-modell, der vi først konverterer hvert ord i teksten vår til den tilsvarende embedding, og deretter beregner en aggregasjonsfunksjon over alle disse embeddingene, som for eksempel `sum`, `average` eller `max`.  \n",
    "\n",
    "![Bilde som viser en embedding-klassifiserer for fem sekvensord.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.no.png)\n",
    "\n",
    "Vårt klassifiserings-nevrale nettverk består av følgende lag:\n",
    "\n",
    "* `TextVectorization`-lag, som tar en streng som input og produserer en tensor av token-numre. Vi vil spesifisere en rimelig vokabularstørrelse `vocab_size` og ignorere mindre brukte ord. Input-formen vil være 1, og utgangsformen vil være $n$, siden vi får $n$ tokens som resultat, hver av dem inneholder numre fra 0 til `vocab_size`.\n",
    "* `Embedding`-lag, som tar $n$ numre og reduserer hvert nummer til en tett vektor med en gitt lengde (100 i vårt eksempel). Dermed vil input-tensoren med form $n$ bli transformert til en $n\\times 100$ tensor. \n",
    "* Aggregasjonslag, som tar gjennomsnittet av denne tensoren langs den første aksen, dvs. det vil beregne gjennomsnittet av alle $n$ input-tensorer som tilsvarer forskjellige ord. For å implementere dette laget vil vi bruke et `Lambda`-lag og sende inn funksjonen for å beregne gjennomsnittet. Utgangen vil ha formen 100, og det vil være den numeriske representasjonen av hele input-sekvensen.\n",
    "* Endelig `Dense` lineær klassifiserer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sammendraget, i kolonnen **output-form**, tilsvarer den første tensor-dimensjonen `None` minibatch-størrelsen, og den andre tilsvarer lengden på token-sekvensen. Alle token-sekvenser i minibatch har forskjellige lengder. Vi skal diskutere hvordan vi håndterer dette i neste seksjon.\n",
    "\n",
    "La oss nå trene nettverket:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Merk** at vi bygger vektorisering basert på et delsett av dataene. Dette gjøres for å akselerere prosessen, og det kan føre til en situasjon der ikke alle tokenene fra teksten vår er til stede i vokabularet. I så fall vil disse tokenene bli ignorert, noe som kan resultere i litt lavere nøyaktighet. Imidlertid gir et delsett av tekst ofte en god vokabularestimering i virkeligheten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Håndtering av variabel sekvenslengde\n",
    "\n",
    "La oss forstå hvordan trening skjer i minibatcher. I eksempelet ovenfor har inndata-tensoren dimensjon 1, og vi bruker minibatcher med lengde 128, slik at den faktiske størrelsen på tensoren er $128 \\times 1$. Imidlertid er antallet tokens i hver setning forskjellig. Hvis vi bruker `TextVectorization`-laget på en enkelt inndata, vil antallet tokens som returneres være forskjellig, avhengig av hvordan teksten er tokenisert:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Men når vi bruker vektoriseringen på flere sekvenser, må den produsere en tensor med rektangulær form, så den fyller ubrukte elementer med PAD-tokenet (som i vårt tilfelle er null):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her kan vi se innebyggingene:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantiske embeddinger: Word2Vec\n",
    "\n",
    "I vårt forrige eksempel lærte embedding-laget å kartlegge ord til vektorrepresentasjoner, men disse representasjonene hadde ingen semantisk betydning. Det hadde vært fint å lære en vektorrepresentasjon der lignende ord eller synonymer tilsvarer vektorer som er nær hverandre i henhold til en eller annen vektordistanse (for eksempel euklidisk distanse).\n",
    "\n",
    "For å oppnå dette må vi forhåndstrene vår embedding-modell på en stor samling tekst ved hjelp av en teknikk som [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Den er basert på to hovedarkitekturer som brukes for å produsere en distribuert representasjon av ord:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), der vi trener modellen til å forutsi et ord basert på den omkringliggende konteksten. Gitt ngrammet $(W_{-2},W_{-1},W_0,W_1,W_2)$, er målet for modellen å forutsi $W_0$ ut fra $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** er motsatt av CBoW. Modellen bruker det omkringliggende vinduet av kontekstord for å forutsi det nåværende ordet.\n",
    "\n",
    "CBoW er raskere, mens skip-gram er tregere, men det gjør en bedre jobb med å representere sjeldne ord.\n",
    "\n",
    "![Bilde som viser både CBoW- og Skip-Gram-algoritmer for å konvertere ord til vektorer.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.no.png)\n",
    "\n",
    "For å eksperimentere med Word2Vec-embedding forhåndstrent på Google News-datasettet, kan vi bruke **gensim**-biblioteket. Nedenfor finner vi ordene som ligner mest på 'neural'.\n",
    "\n",
    "> **Merk:** Når du først oppretter ordvektorer, kan nedlastingen ta litt tid!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan også hente vektorinnlegget fra ordet, som skal brukes i treningen av klassifiseringsmodellen. Innlegget har 300 komponenter, men her viser vi bare de første 20 komponentene av vektoren for tydelighet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det som er flott med semantiske innebygginger er at du kan manipulere vektorkodingen basert på semantikk. For eksempel kan vi be om å finne et ord hvis vektorrepresentasjon er så nær som mulig ordene *konge* og *kvinne*, og så langt som mulig fra ordet *mann*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Et eksempel ovenfor bruker litt intern GenSym-magi, men den underliggende logikken er faktisk ganske enkel. En interessant ting med innebygginger er at du kan utføre normale vektoroperasjoner på innebyggingsvektorer, og det vil reflektere operasjoner på ords **betydninger**. Eksempelet ovenfor kan uttrykkes i form av vektoroperasjoner: vi beregner vektoren som tilsvarer **KONGE-MANN+KVINNE** (operasjonene `+` og `-` utføres på vektorrepresentasjoner av tilsvarende ord), og deretter finner vi det nærmeste ordet i ordboken til den vektoren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Vi måtte legge til små koeffisienter til *man*- og *woman*-vektorene – prøv å fjerne dem for å se hva som skjer.\n",
    "\n",
    "For å finne den nærmeste vektoren bruker vi TensorFlow-mekanismen til å beregne en vektor av avstander mellom vår vektor og alle vektorene i vokabularet, og deretter finner vi indeksen til det minste ordet ved hjelp av `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mens Word2Vec virker som en flott måte å uttrykke ordsemantikk på, har det mange ulemper, inkludert følgende:\n",
    "\n",
    "* Både CBoW- og skip-gram-modellene er **prediktive embeddings**, og de tar kun hensyn til lokal kontekst. Word2Vec utnytter ikke global kontekst.\n",
    "* Word2Vec tar ikke hensyn til ordets **morfologi**, altså det faktum at betydningen av et ord kan avhenge av ulike deler av ordet, som roten.\n",
    "\n",
    "**FastText** forsøker å overvinne den andre begrensningen og bygger videre på Word2Vec ved å lære vektorrepresentasjoner for hvert ord og tegn-n-grammene som finnes i hvert ord. Verdiene av representasjonene blir deretter gjennomsnittlig til én vektor ved hver treningssteg. Selv om dette legger til mye ekstra beregning under pretrening, gjør det det mulig for ord-embeddings å kode sub-ord-informasjon.\n",
    "\n",
    "En annen metode, **GloVe**, bruker en annen tilnærming til ord-embeddings, basert på faktorisering av ord-kontekst-matrisen. Først bygger den en stor matrise som teller antall forekomster av ord i ulike kontekster, og deretter forsøker den å representere denne matrisen i lavere dimensjoner på en måte som minimerer rekonstruksjonstap.\n",
    "\n",
    "Gensim-biblioteket støtter disse ord-embeddingene, og du kan eksperimentere med dem ved å endre modellinnlastingskoden ovenfor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruke forhåndstrente embeddings i Keras\n",
    "\n",
    "Vi kan endre eksempelet ovenfor for å forhåndsfylle matrisen i vår embedding-lag med semantiske embeddings, som Word2Vec. Ordforrådene til den forhåndstrente embedding og tekstkorpuset vil sannsynligvis ikke samsvare, så vi må velge ett. Her utforsker vi de to mulige alternativene: å bruke tokenizer-ordforrådet og å bruke ordforrådet fra Word2Vec-embeddings.\n",
    "\n",
    "### Bruke tokenizer-ordforråd\n",
    "\n",
    "Når vi bruker tokenizer-ordforrådet, vil noen av ordene fra ordforrådet ha tilsvarende Word2Vec-embeddings, og noen vil mangle. Gitt at vår ordforrådsstørrelse er `vocab_size`, og lengden på Word2Vec embedding-vektoren er `embed_size`, vil embedding-laget bli representert av en vektmatrise med formen `vocab_size`$\\times$`embed_size`. Vi vil fylle denne matrisen ved å gå gjennom ordforrådet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ord som ikke finnes i Word2Vec-ordforrådet, kan vi enten la dem være nuller, eller generere en tilfeldig vektor.\n",
    "\n",
    "Nå kan vi definere et innebyggingslag med forhåndstrente vekter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Merk**: Legg merke til at vi setter `trainable=False` når vi oppretter `Embedding`, noe som betyr at vi ikke trener opp Embedding-laget på nytt. Dette kan føre til at nøyaktigheten blir litt lavere, men det gjør treningen raskere.\n",
    "\n",
    "### Bruke embedding-ordforråd\n",
    "\n",
    "Et problem med den tidligere tilnærmingen er at ordfordrådene som brukes i TextVectorization og Embedding er forskjellige. For å løse dette problemet kan vi bruke en av følgende løsninger:\n",
    "* Trene Word2Vec-modellen på nytt med vårt ordfordråd.\n",
    "* Laste inn datasettet vårt med ordfordrådet fra den forhåndstrente Word2Vec-modellen. Ordfordråd som brukes til å laste inn datasettet kan spesifiseres under innlasting.\n",
    "\n",
    "Den sistnevnte tilnærmingen virker enklere, så la oss implementere den. Først og fremst vil vi opprette et `TextVectorization`-lag med det spesifiserte ordfordrådet, hentet fra Word2Vec-embeddingene:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteket for gensim ord-innkapslinger inneholder en praktisk funksjon, `get_keras_embeddings`, som automatisk vil opprette det tilsvarende Keras-innkapslingslaget for deg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En av grunnene til at vi ikke ser høyere nøyaktighet er fordi noen ord fra datasettet vårt mangler i den forhåndstrente GloVe-ordforrådet, og derfor blir de i hovedsak ignorert. For å løse dette kan vi trene våre egne embeddings basert på datasettet vårt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontekstuelle embeddinger\n",
    "\n",
    "En viktig begrensning med tradisjonelle forhåndstrente embedding-representasjoner som Word2Vec er at, selv om de kan fange opp noe av betydningen av et ord, kan de ikke skille mellom ulike betydninger. Dette kan føre til problemer i modeller som bruker dem videre.\n",
    "\n",
    "For eksempel har ordet 'play' forskjellige betydninger i disse to setningene:\n",
    "- Jeg dro på et **skuespill** på teateret.\n",
    "- John vil **leke** med vennene sine.\n",
    "\n",
    "De forhåndstrente embeddingene vi har snakket om representerer begge betydningene av ordet 'play' i samme embedding. For å overkomme denne begrensningen, må vi bygge embeddinger basert på **språkmodellen**, som er trent på en stor tekstsamling og *vet* hvordan ord kan settes sammen i ulike kontekster. Å diskutere kontekstuelle embeddinger er utenfor rammen for denne opplæringen, men vi vil komme tilbake til dem når vi snakker om språkmodeller i neste enhet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T17:48:34+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}