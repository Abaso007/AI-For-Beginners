<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-28T16:03:09+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "no"
}
-->
# Representere tekst som tensorer

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Tekstklassifisering

I den f√∏rste delen av denne seksjonen vil vi fokusere p√• oppgaven **tekstklassifisering**. Vi skal bruke [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)-datasettet, som inneholder nyhetsartikler som f√∏lgende:

* Kategori: Vitenskap/Teknologi  
* Tittel: Ky. Selskap vinner stipend for √• studere peptider (AP)  
* Br√∏dtekst: AP - Et selskap grunnlagt av en kjemiforsker ved University of Louisville vant et stipend for √• utvikle...

M√•let v√•rt vil v√¶re √• klassifisere nyhetsartikkelen i en av kategoriene basert p√• teksten.

## Representere tekst

Hvis vi √∏nsker √• l√∏se oppgaver innen naturlig spr√•kprosessering (NLP) med nevrale nettverk, trenger vi en m√•te √• representere tekst som tensorer p√•. Datamaskiner representerer allerede teksttegn som tall som tilsvarer fonter p√• skjermen ved hjelp av kodinger som ASCII eller UTF-8.

<img alt="Bilde som viser et diagram som kartlegger et tegn til en ASCII- og bin√¶r representasjon" src="images/ascii-character-map.png" width="50%"/>

> [Bildekilde](https://www.seobility.net/en/wiki/ASCII)

Som mennesker forst√•r vi hva hver bokstav **representerer**, og hvordan alle tegnene sammen danner ordene i en setning. Datamaskiner har imidlertid ikke en slik forst√•else, og et nevralt nettverk m√• l√¶re betydningen under trening.

Derfor kan vi bruke ulike tiln√¶rminger for √• representere tekst:

* **Tegn-niv√• representasjon**, der vi representerer tekst ved √• behandle hvert tegn som et tall. Gitt at vi har *C* forskjellige tegn i tekstkorpuset v√•rt, vil ordet *Hello* bli representert som en 5x*C* tensor. Hver bokstav tilsvarer en tensor-kolonne i en one-hot-koding.  
* **Ord-niv√• representasjon**, der vi lager et **ordforr√•d** av alle ordene i teksten v√•r, og deretter representerer ordene ved hjelp av one-hot-koding. Denne tiln√¶rmingen er p√• noen m√•ter bedre, fordi hver bokstav i seg selv ikke har mye mening. Ved √• bruke h√∏yere semantiske konsepter - ord - forenkler vi oppgaven for det nevrale nettverket. Men gitt den store ordbokst√∏rrelsen, m√• vi h√•ndtere h√∏y-dimensjonale, sparsomme tensorer.

Uansett representasjon m√• vi f√∏rst konvertere teksten til en sekvens av **tokens**, der en token kan v√¶re et tegn, et ord, eller noen ganger til og med en del av et ord. Deretter konverterer vi tokenet til et tall, vanligvis ved hjelp av et **ordforr√•d**, og dette tallet kan mates inn i et nevralt nettverk ved hjelp av one-hot-koding.

## N-Grammer

I naturlig spr√•k kan den presise betydningen av ord bare bestemmes i kontekst. For eksempel har *nevralt nettverk* og *fiskegarn* helt forskjellige betydninger. En m√•te √• ta dette i betraktning p√• er √• bygge modellen v√•r p√• par av ord og betrakte ordpar som separate tokens i ordforr√•det. P√• denne m√•ten vil setningen *Jeg liker √• fiske* bli representert av f√∏lgende sekvens av tokens: *Jeg liker*, *liker √•*, *√• fiske*. Problemet med denne tiln√¶rmingen er at ordbokst√∏rrelsen √∏ker betydelig, og kombinasjoner som *√• fiske* og *√• handle* representeres av forskjellige tokens, som ikke deler noen semantisk likhet til tross for samme verb.

I noen tilfeller kan vi vurdere √• bruke tri-grammer ‚Äì kombinasjoner av tre ord ‚Äì ogs√•. Dermed kalles denne tiln√¶rmingen ofte **n-grammer**. Det gir ogs√• mening √• bruke n-grammer med tegn-niv√• representasjon, der n-grammer grovt sett tilsvarer forskjellige stavelser.

## Bag-of-Words og TF/IDF

N√•r vi l√∏ser oppgaver som tekstklassifisering, m√• vi kunne representere tekst som en fast-st√∏rrelse vektor, som vi vil bruke som input til den endelige tette klassifisatoren. En av de enkleste m√•tene √• gj√∏re dette p√• er √• kombinere alle individuelle ordrepresentasjoner, for eksempel ved √• legge dem sammen. Hvis vi legger sammen one-hot-kodingene av hvert ord, ender vi opp med en frekvensvektor som viser hvor mange ganger hvert ord vises i teksten. En slik representasjon av tekst kalles **bag-of-words** (BoW).

<img src="images/bow.png" width="90%"/>

> Bilde av forfatteren

En BoW representerer i hovedsak hvilke ord som vises i teksten og i hvilke mengder, noe som kan v√¶re en god indikasjon p√• hva teksten handler om. For eksempel er det sannsynlig at en nyhetsartikkel om politikk inneholder ord som *president* og *land*, mens en vitenskapelig publikasjon kan inneholde ord som *kollider*, *oppdaget*, osv. Dermed kan ordfrekvenser i mange tilfeller v√¶re en god indikator p√• tekstinnhold.

Problemet med BoW er at visse vanlige ord, som *og*, *er*, osv., vises i de fleste tekster og har de h√∏yeste frekvensene, noe som maskerer ordene som virkelig er viktige. Vi kan redusere viktigheten av disse ordene ved √• ta hensyn til frekvensen ordene forekommer i hele dokumentkolleksjonen. Dette er hovedideen bak TF/IDF-tiln√¶rmingen, som dekkes mer detaljert i notatb√∏kene som er vedlagt denne leksjonen.

Ingen av disse tiln√¶rmingene kan imidlertid fullt ut ta hensyn til tekstens **semantikk**. Vi trenger kraftigere modeller for nevrale nettverk for √• gj√∏re dette, noe vi vil diskutere senere i denne seksjonen.

## ‚úçÔ∏è √òvelser: Tekstrepresentasjon

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:

* [Tekstrepresentasjon med PyTorch](TextRepresentationPyTorch.ipynb)  
* [Tekstrepresentasjon med TensorFlow](TextRepresentationTF.ipynb)  

## Konklusjon

S√• langt har vi studert teknikker som kan legge frekvensvekt p√• forskjellige ord. De er imidlertid ikke i stand til √• representere mening eller rekkef√∏lge. Som den ber√∏mte lingvisten J. R. Firth sa i 1935: "Den fullstendige betydningen av et ord er alltid kontekstuelt, og ingen studie av betydning utenfor kontekst kan tas seri√∏st." Vi vil senere i kurset l√¶re hvordan vi kan fange opp kontekstuell informasjon fra tekst ved hjelp av spr√•kmodellering.

## üöÄ Utfordring

Pr√∏v noen andre √∏velser ved √• bruke bag-of-words og forskjellige datamodeller. Du kan finne inspirasjon i denne [konkurransen p√• Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Gjennomgang og selvstudium

√òv p√• ferdighetene dine med tekstembedding og bag-of-words-teknikker p√• [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste).

## [Oppgave: Notatb√∏ker](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.