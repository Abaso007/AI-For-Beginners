{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mga Generative na Network\n",
    "\n",
    "Ang Recurrent Neural Networks (RNNs) at ang mga gated cell variant nito tulad ng Long Short Term Memory Cells (LSTMs) at Gated Recurrent Units (GRUs) ay nagbibigay ng mekanismo para sa pagmomodelo ng wika, ibig sabihin, kaya nilang matutunan ang pagkakasunod-sunod ng mga salita at magbigay ng prediksyon para sa susunod na salita sa isang sequence. Dahil dito, magagamit natin ang RNNs para sa **mga generative na gawain**, tulad ng ordinaryong pagbuo ng teksto, pagsasalin ng makina, at maging sa paglalagay ng caption sa mga imahe.\n",
    "\n",
    "Sa arkitektura ng RNN na tinalakay natin sa nakaraang unit, ang bawat RNN unit ay gumagawa ng susunod na hidden state bilang output. Gayunpaman, maaari rin tayong magdagdag ng isa pang output sa bawat recurrent unit, na magpapahintulot sa atin na mag-output ng isang **sequence** (na may parehong haba sa orihinal na sequence). Bukod pa rito, maaari tayong gumamit ng mga RNN unit na hindi tumatanggap ng input sa bawat hakbang, at sa halip ay gumagamit lamang ng isang initial state vector, at pagkatapos ay gumagawa ng isang sequence ng mga output.\n",
    "\n",
    "Sa notebook na ito, magpo-focus tayo sa mga simpleng generative na modelo na tumutulong sa atin na bumuo ng teksto. Para sa pagiging simple, magtatayo tayo ng **character-level network**, na bumubuo ng teksto letra kada letra. Sa panahon ng training, kailangan nating kumuha ng isang text corpus, at hatiin ito sa mga sequence ng letra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset,test_dataset,classes,vocab = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagbuo ng bokabularyo ng karakter\n",
    "\n",
    "Para makabuo ng generative network sa antas ng karakter, kailangan nating hatiin ang teksto sa mga indibidwal na karakter sa halip na mga salita. Magagawa ito sa pamamagitan ng pagde-define ng ibang tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 82\n",
      "Encoding of 'a' is 1\n",
      "Character with code 13 is c\n"
     ]
    }
   ],
   "source": [
    "def char_tokenizer(words):\n",
    "    return list(words) #[word for word in words]\n",
    "\n",
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(char_tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size = {vocab_size}\")\n",
    "print(f\"Encoding of 'a' is {vocab.get_stoi()['a']}\")\n",
    "print(f\"Character with code 13 is {vocab.get_itos()[13]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tingnan natin ang halimbawa kung paano natin mai-encode ang teksto mula sa ating dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  2,  3,  4,  5,  6,  3,  7,  8,  1,  9, 10,  3, 11,  2,  1,\n",
       "        12,  3,  7,  1, 13, 14,  3, 15, 16,  5, 17,  3,  5, 18,  8,  3,  7,  2,\n",
       "         1, 13, 14,  3, 19, 20,  8, 21,  5,  8,  9, 10, 22,  3, 20,  8, 21,  5,\n",
       "         8,  9, 10,  3, 23,  3,  4, 18, 17,  9,  5, 23, 10,  8,  2,  2,  8,  9,\n",
       "        10, 24,  3,  0,  1,  2,  2,  3,  4,  5,  9,  8,  8,  5, 25, 10,  3, 26,\n",
       "        12, 27, 16, 26,  2, 27, 16, 28, 29, 30,  1, 16, 26,  3, 17, 31,  3, 21,\n",
       "         2,  5,  9,  1, 23, 13, 32, 16, 27, 13, 10, 24,  3,  1,  9,  8,  3, 10,\n",
       "         8,  8, 27, 16, 28,  3, 28,  9,  8,  8, 16,  3,  1, 28,  1, 27, 16,  6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enc(x):\n",
    "    return torch.LongTensor(encode(x,voc=vocab,tokenizer=char_tokenizer))\n",
    "\n",
    "enc(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagsasanay ng isang generative RNN\n",
    "\n",
    "Ang paraan ng pagsasanay natin sa RNN upang makabuo ng teksto ay ang sumusunod. Sa bawat hakbang, kukuha tayo ng isang sequence ng mga karakter na may haba na `nchars`, at hihilingin sa network na bumuo ng susunod na output na karakter para sa bawat input na karakter:\n",
    "\n",
    "![Larawan na nagpapakita ng halimbawa ng RNN na bumubuo ng salitang 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.tl.png)\n",
    "\n",
    "Depende sa aktwal na sitwasyon, maaaring gusto rin nating isama ang ilang espesyal na karakter, tulad ng *end-of-sequence* `<eos>`. Sa ating kaso, nais lamang nating sanayin ang network para sa walang katapusang pagbuo ng teksto, kaya't itatakda natin ang laki ng bawat sequence na maging katumbas ng `nchars` na mga token. Dahil dito, ang bawat halimbawa ng pagsasanay ay binubuo ng `nchars` na mga input at `nchars` na mga output (na siyang input sequence na inilipat ng isang simbolo sa kaliwa). Ang minibatch ay binubuo ng ilang ganitong mga sequence.\n",
    "\n",
    "Ang paraan ng pagbuo natin ng mga minibatch ay ang pagkuha ng bawat teksto ng balita na may haba na `l`, at pagbuo ng lahat ng posibleng input-output na kombinasyon mula rito (magkakaroon ng `l-nchars` na ganitong mga kombinasyon). Ang mga ito ay bubuo ng isang minibatch, at ang laki ng mga minibatch ay mag-iiba sa bawat hakbang ng pagsasanay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  ..., 28, 29, 30],\n",
       "         [ 1,  2,  2,  ..., 29, 30,  1],\n",
       "         [ 2,  2,  3,  ..., 30,  1, 16],\n",
       "         ...,\n",
       "         [20,  8, 21,  ...,  1, 28,  1],\n",
       "         [ 8, 21,  5,  ..., 28,  1, 27],\n",
       "         [21,  5,  8,  ...,  1, 27, 16]]),\n",
       " tensor([[ 1,  2,  2,  ..., 29, 30,  1],\n",
       "         [ 2,  2,  3,  ..., 30,  1, 16],\n",
       "         [ 2,  3,  4,  ...,  1, 16, 26],\n",
       "         ...,\n",
       "         [ 8, 21,  5,  ..., 28,  1, 27],\n",
       "         [21,  5,  8,  ...,  1, 27, 16],\n",
       "         [ 5,  8,  9,  ..., 27, 16,  6]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nchars = 100\n",
    "\n",
    "def get_batch(s,nchars=nchars):\n",
    "    ins = torch.zeros(len(s)-nchars,nchars,dtype=torch.long,device=device)\n",
    "    outs = torch.zeros(len(s)-nchars,nchars,dtype=torch.long,device=device)\n",
    "    for i in range(len(s)-nchars):\n",
    "        ins[i] = enc(s[i:i+nchars])\n",
    "        outs[i] = enc(s[i+1:i+nchars+1])\n",
    "    return ins,outs\n",
    "\n",
    "get_batch(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, magde-define tayo ng generator network. Maaari itong ibase sa anumang recurrent cell na tinalakay natin sa nakaraang unit (simple, LSTM, o GRU). Sa ating halimbawa, gagamit tayo ng LSTM.\n",
    "\n",
    "Dahil ang network ay tumatanggap ng mga karakter bilang input, at maliit lamang ang laki ng bokabularyo, hindi na natin kailangan ng embedding layer; ang one-hot-encoded na input ay maaaring direktang ipadala sa LSTM cell. Gayunpaman, dahil mga numero ng karakter ang ipinapasa bilang input, kailangan nating i-one-hot-encode ang mga ito bago ipadala sa LSTM. Ginagawa ito sa pamamagitan ng pagtawag sa `one_hot` function sa panahon ng `forward` pass. Ang output encoder ay magiging isang linear layer na magko-convert ng hidden state sa one-hot-encoded na output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(vocab_size,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, s=None):\n",
    "        x = torch.nn.functional.one_hot(x,vocab_size).to(torch.float32)\n",
    "        x,s = self.rnn(x,s)\n",
    "        return self.fc(x),s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa panahon ng pagsasanay, nais nating makapag-sample ng mga nilikhang teksto. Upang magawa ito, magde-define tayo ng `generate` na function na maglalabas ng output na string na may habang `size`, simula sa paunang string na `start`.\n",
    "\n",
    "Ganito ang paraan ng paggana nito. Una, ipapasa natin ang buong start string sa network, at kukunin ang output state na `s` at ang susunod na hinulaang karakter na `out`. Dahil ang `out` ay naka-one-hot encode, kukunin natin ang `argmax` upang makuha ang index ng karakter na `nc` sa bokabularyo, at gagamitin ang `itos` upang malaman ang aktwal na karakter at idagdag ito sa listahan ng mga karakter na `chars`. Ang prosesong ito ng pagbuo ng isang karakter ay inuulit nang `size` na beses upang makabuo ng kinakailangang bilang ng mga karakter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(net,size=100,start='today '):\n",
    "        chars = list(start)\n",
    "        out, s = net(enc(chars).view(1,-1).to(device))\n",
    "        for i in range(size):\n",
    "            nc = torch.argmax(out[0][-1])\n",
    "            chars.append(vocab.get_itos()[nc])\n",
    "            out, s = net(nc.view(1,-1),s)\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, simulan na natin ang pagsasanay! Ang training loop ay halos pareho sa lahat ng ating mga naunang halimbawa, ngunit sa halip na accuracy, magpi-print tayo ng mga halimbawa ng generated text tuwing 1000 epochs.\n",
    "\n",
    "Kailangang bigyan ng espesyal na pansin ang paraan ng pagkalkula ng loss. Kailangan nating kalkulahin ang loss gamit ang one-hot-encoded na output na `out`, at ang inaasahang text na `text_out`, na siyang listahan ng mga character indices. Sa kabutihang-palad, ang function na `cross_entropy` ay inaasahan ang unnormalized network output bilang unang argumento, at ang class number bilang pangalawa, na eksaktong tumutugma sa kung ano ang mayroon tayo. Gumagawa rin ito ng awtomatikong pag-average batay sa minibatch size.\n",
    "\n",
    "Nililimitahan din natin ang pagsasanay sa pamamagitan ng `samples_to_train` na mga sample, upang hindi masyadong magtagal ang proseso. Hinihikayat namin kayong mag-eksperimento at subukang magpatakbo ng mas mahabang pagsasanay, posibleng para sa ilang epochs (kung saan kakailanganin ninyong gumawa ng isa pang loop sa paligid ng code na ito).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss = 4.398899078369141\n",
      "today sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr sr s\n",
      "Current loss = 2.161320447921753\n",
      "today and to the tor to to the tor to to the tor to to the tor to to the tor to to the tor to to the tor t\n",
      "Current loss = 1.6722588539123535\n",
      "today and the court to the could to the could to the could to the could to the could to the could to the c\n",
      "Current loss = 2.423795223236084\n",
      "today and a second to the conternation of the conternation of the conternation of the conternation of the \n",
      "Current loss = 1.702607274055481\n",
      "today and the company to the company to the company to the company to the company to the company to the co\n",
      "Current loss = 1.692358136177063\n",
      "today and the company to the company to the company to the company to the company to the company to the co\n",
      "Current loss = 1.9722288846969604\n",
      "today and the control the control the control the control the control the control the control the control \n",
      "Current loss = 1.8705692291259766\n",
      "today and the second to the second to the second to the second to the second to the second to the second t\n",
      "Current loss = 1.7626899480819702\n",
      "today and a security and a security and a security and a security and a security and a security and a secu\n",
      "Current loss = 1.5574463605880737\n",
      "today and the company and the company and the company and the company and the company and the company and \n",
      "Current loss = 1.5620026588439941\n",
      "today and the be that the be the be that the be the be that the be the be that the be the be that the be t\n"
     ]
    }
   ],
   "source": [
    "net = LSTMGenerator(vocab_size,64).to(device)\n",
    "\n",
    "samples_to_train = 10000\n",
    "optimizer = torch.optim.Adam(net.parameters(),0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "net.train()\n",
    "for i,x in enumerate(train_dataset):\n",
    "    # x[0] is class label, x[1] is text\n",
    "    if len(x[1])-nchars<10:\n",
    "        continue\n",
    "    samples_to_train-=1\n",
    "    if not samples_to_train: break\n",
    "    text_in, text_out = get_batch(x[1])\n",
    "    optimizer.zero_grad()\n",
    "    out,s = net(text_in)\n",
    "    loss = torch.nn.functional.cross_entropy(out.view(-1,vocab_size),text_out.flatten()) #cross_entropy(out,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%1000==0:\n",
    "        print(f\"Current loss = {loss.item()}\")\n",
    "        print(generate(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ang halimbawang ito ay nakabubuo na ng medyo maayos na teksto, ngunit maaari pa itong mapahusay sa iba't ibang paraan:\n",
    "\n",
    "* **Mas maayos na minibatch generation**. Ang paraan ng paghahanda natin ng data para sa training ay sa pamamagitan ng pagbuo ng isang minibatch mula sa isang sample. Hindi ito ideal, dahil ang mga minibatch ay magkakaiba ang laki, at ang ilan sa mga ito ay hindi pa nga maaaring mabuo, dahil mas maliit ang teksto kaysa sa `nchars`. Bukod dito, ang maliliit na minibatch ay hindi sapat na nagagamit ang GPU. Mas mainam na kumuha ng isang malaking bahagi ng teksto mula sa lahat ng mga sample, pagkatapos ay bumuo ng lahat ng input-output na pares, i-shuffle ang mga ito, at bumuo ng mga minibatch na magkakapareho ang laki.\n",
    "\n",
    "* **Multilayer LSTM**. Makatuwiran na subukan ang 2 o 3 layer ng LSTM cells. Tulad ng nabanggit natin sa nakaraang unit, bawat layer ng LSTM ay kumukuha ng partikular na mga pattern mula sa teksto, at sa kaso ng character-level generator, maaari nating asahan na ang mas mababang LSTM level ay responsable sa pagkuha ng mga pantig, at ang mas mataas na mga level - para sa mga salita at kombinasyon ng mga salita. Madali itong maipatupad sa pamamagitan ng pagpapasa ng number-of-layers parameter sa LSTM constructor.\n",
    "\n",
    "* Maaari mo ring subukang mag-eksperimento gamit ang **GRU units** at tingnan kung alin ang mas mahusay ang performance, pati na rin ang **iba't ibang laki ng hidden layer**. Ang sobrang laki ng hidden layer ay maaaring magresulta sa overfitting (halimbawa, matututunan ng network ang eksaktong teksto), at ang mas maliit na laki ay maaaring hindi makabuo ng magandang resulta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malambot na pagbuo ng teksto at temperatura\n",
    "\n",
    "Sa nakaraang depinisyon ng `generate`, palagi nating pinipili ang karakter na may pinakamataas na posibilidad bilang susunod na karakter sa nabubuong teksto. Dahil dito, madalas na \"umuulit\" ang teksto sa parehong mga pagkakasunod-sunod ng karakter, tulad ng sa halimbawang ito:\n",
    "```\n",
    "today of the second the company and a second the company ...\n",
    "```\n",
    "\n",
    "Gayunpaman, kung titingnan natin ang distribusyon ng posibilidad para sa susunod na karakter, maaaring ang pagkakaiba sa pagitan ng ilang pinakamataas na posibilidad ay hindi ganoon kalaki, halimbawa, ang isang karakter ay maaaring may posibilidad na 0.2, habang ang isa pa ay 0.19, at iba pa. Halimbawa, kapag naghahanap ng susunod na karakter sa pagkakasunod-sunod na '*play*', ang susunod na karakter ay maaaring pantay na maging espasyo o **e** (tulad ng sa salitang *player*).\n",
    "\n",
    "Ito ay nagdadala sa atin sa konklusyon na hindi palaging \"makatarungan\" na piliin ang karakter na may mas mataas na posibilidad, dahil ang pagpili sa pangalawang pinakamataas ay maaari pa ring magresulta sa makabuluhang teksto. Mas matalino ang **pag-sample** ng mga karakter mula sa distribusyon ng posibilidad na ibinigay ng output ng network.\n",
    "\n",
    "Ang pag-sample na ito ay maaaring gawin gamit ang `multinomial` na function na nagpapatupad ng tinatawag na **multinomial distribution**. Ang isang function na nagpapatupad ng ganitong **malambot** na pagbuo ng teksto ay nakasaad sa ibaba:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temperature = 0.3\n",
      "Today and a company and complete an all the land the restrational the as a security and has provers the pay to and a report and the computer in the stand has filities and working the law the stations for a company and with the company and the final the first company and refight of the state and and workin\n",
      "\n",
      "--- Temperature = 0.8\n",
      "Today he oniis its first to Aus bomblaties the marmation a to manan  boogot that pirate assaid a relaid their that goverfin the the Cappets Ecrotional Assonia Cition targets it annight the w scyments Blamity #39;s TVeer Diercheg Reserals fran envyuil that of ster said access what succers of Dour-provelith\n",
      "\n",
      "--- Temperature = 1.0\n",
      "Today holy they a 11 will meda a toket subsuaties, engins for Chanos, they's has stainger past to opening orital his thempting new Nattona was al innerforder advan-than #36;s night year his religuled talitatian what the but with Wednesday to Justment will wemen of Mark CCC Camp as Timed Nae wome a leaders\n",
      "\n",
      "--- Temperature = 1.3\n",
      "Today gpone 2.5 fech atcusion poor cocles toparsdorM.cht Line Pamage put 43 his calt lowed to the book, that has authh-the silia rruch ailing to'ory andhes beutirsimi- Aefffive heading offil an auf eacklets is charged evis, Gunymy oy) Mony has it after-sloythyor loveId out filme, the Natabl -Najuntaxiggs \n",
      "\n",
      "--- Temperature = 1.8\n",
      "Today plary, P.slan chly\\401 mardregationly #39;t 8.1Mide) closes ,filtcon alfly playin roven!\\grea.-QFBEP: Iss onfarchQ/itilia CCf Zivesigntwasta orce.-Peul-aw.uicrin of fuglinfsut aftaningwo, MIEX awayew Aice Woiduar Corvagiugge oppo esig ThusBratourid canthly-RyI.co lagitems\\eexciaishes.conBabntusmor I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_soft(net,size=100,start='today ',temperature=1.0):\n",
    "        chars = list(start)\n",
    "        out, s = net(enc(chars).view(1,-1).to(device))\n",
    "        for i in range(size):\n",
    "            #nc = torch.argmax(out[0][-1])\n",
    "            out_dist = out[0][-1].div(temperature).exp()\n",
    "            nc = torch.multinomial(out_dist,1)[0]\n",
    "            chars.append(vocab.get_itos()[nc])\n",
    "            out, s = net(nc.view(1,-1),s)\n",
    "        return ''.join(chars)\n",
    "    \n",
    "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
    "    print(f\"--- Temperature = {i}\\n{generate_soft(net,size=300,start='Today ',temperature=i)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inilunsad namin ang isa pang parameter na tinatawag na **temperature**, na ginagamit upang ipakita kung gaano kahigpit dapat tayong sumunod sa pinakamataas na posibilidad. Kung ang temperature ay 1.0, gumagawa tayo ng patas na multinomial sampling, at kapag ang temperature ay umabot sa infinity - nagiging pantay-pantay ang lahat ng posibilidad, at random tayong pumipili ng susunod na karakter. Sa halimbawa sa ibaba, mapapansin natin na nagiging walang kahulugan ang teksto kapag masyadong tinaasan ang temperature, at nagmumukhang \"cycled\" na mahigpit na binuong teksto kapag mas malapit ito sa 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Paunawa**:  \nAng dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7673cd150d96c74c6d6011460094efb4",
   "translation_date": "2025-08-28T04:10:22+00:00",
   "source_file": "lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb",
   "language_code": "tl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}