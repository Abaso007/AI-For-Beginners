{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagsasanay ng RL para sa Pagbalanse ng Cartpole\n",
    "\n",
    "Ang notebook na ito ay bahagi ng [AI for Beginners Curriculum](http://aka.ms/ai-beginners). Ito ay inspirasyon mula sa [opisyal na PyTorch tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) at [Cartpole PyTorch implementation na ito](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Sa halimbawang ito, gagamit tayo ng RL upang sanayin ang isang modelo na magbalanse ng poste sa isang cart na maaaring gumalaw pakaliwa at pakanan sa isang pahalang na sukat. Gagamitin natin ang [OpenAI Gym](https://www.gymlibrary.ml/) na kapaligiran upang gayahin ang poste.\n",
    "\n",
    "> **Note**: Maaari mong patakbuhin ang code ng araling ito nang lokal (hal. mula sa Visual Studio Code), kung saan magbubukas ang simulation sa isang bagong window. Kapag pinapatakbo ang code online, maaaring kailanganin mong gumawa ng ilang pagbabago sa code, tulad ng inilarawan [dito](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Magsisimula tayo sa pamamagitan ng pagtiyak na naka-install ang Gym:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, gumawa tayo ng CartPole environment at tingnan kung paano ito gamitin. Ang isang environment ay may mga sumusunod na katangian:\n",
    "\n",
    "* **Action space** ay ang hanay ng mga posibleng aksyon na maaari nating gawin sa bawat hakbang ng simulation  \n",
    "* **Observation space** ay ang hanay ng mga obserbasyon na maaari nating makita  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tingnan natin kung paano gumagana ang simulation. Ang sumusunod na loop ay nagpapatakbo ng simulation hanggang sa ang `env.step` ay hindi magbalik ng termination flag na `done`. Pipili tayo ng mga aksyon nang random gamit ang `env.action_space.sample()`, na nangangahulugang malamang na mabigo agad ang eksperimento (ang CartPole environment ay nagtatapos kapag ang bilis ng CartPole, ang posisyon nito, o anggulo ay lumampas sa mga tiyak na limitasyon).\n",
    "\n",
    "> Magbubukas ang simulation sa bagong window. Maaari mong patakbuhin ang code nang ilang beses at obserbahan kung paano ito kumikilos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapapansin mo na ang mga obserbasyon ay binubuo ng 4 na numero. Ang mga ito ay:\n",
    "- Posisyon ng kariton\n",
    "- Bilis ng kariton\n",
    "- Anggulo ng poste\n",
    "- Bilis ng pag-ikot ng poste\n",
    "\n",
    "Ang `rew` ay ang gantimpala na natatanggap natin sa bawat hakbang. Makikita mo na sa CartPole na kapaligiran, ikaw ay ginagantimpalaan ng 1 puntos para sa bawat hakbang ng simulasyon, at ang layunin ay pataasin ang kabuuang gantimpala, ibig sabihin, ang oras na kayang balansehin ng CartPole nang hindi bumabagsak.\n",
    "\n",
    "Sa reinforcement learning, ang layunin natin ay sanayin ang isang **patakaran** $\\pi$, na para sa bawat estado $s$ ay magsasabi sa atin kung aling aksyon $a$ ang dapat gawin, kaya sa esensya $a = \\pi(s)$.\n",
    "\n",
    "Kung nais mo ng probabilistikong solusyon, maaari mong isipin ang patakaran bilang pagbibigay ng hanay ng mga probabilidad para sa bawat aksyon, ibig sabihin, ang $\\pi(a|s)$ ay nangangahulugan ng probabilidad na dapat nating gawin ang aksyon $a$ sa estado $s$.\n",
    "\n",
    "## Paraan ng Policy Gradient\n",
    "\n",
    "Sa pinakasimpleng RL algorithm, na tinatawag na **Policy Gradient**, magsasanay tayo ng isang neural network upang mahulaan ang susunod na aksyon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aating sasanayin ang network sa pamamagitan ng pagsasagawa ng maraming eksperimento, at ia-update ang network pagkatapos ng bawat takbo. Magde-define tayo ng isang function na magsasagawa ng eksperimento at magbabalik ng mga resulta (tinatawag na **trace**) - lahat ng estado, aksyon (at ang kanilang inirerekomendang mga probabilidad), at mga gantimpala:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaari kang magpatakbo ng isang episode gamit ang hindi sanay na network at mapansin na ang kabuuang gantimpala (kilala rin bilang haba ng episode) ay napakababa:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isa sa mga mahirap na aspeto ng policy gradient algorithm ay ang paggamit ng **discounted rewards**. Ang ideya ay kinukuwenta natin ang vector ng kabuuang rewards sa bawat hakbang ng laro, at sa prosesong ito ay dinidiskwento natin ang mga maagang rewards gamit ang isang coefficient $gamma$. Ina-normalize din natin ang resulting vector, dahil gagamitin natin ito bilang timbang upang makaapekto sa ating training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, magsimula na tayo sa aktwal na pagsasanay! Tatakbo tayo ng 300 episodes, at sa bawat episode ay gagawin natin ang mga sumusunod:\n",
    "\n",
    "1. Patakbuhin ang eksperimento at kolektahin ang trace.\n",
    "2. Kalkulahin ang pagkakaiba (`gradients`) sa pagitan ng mga ginawang aksyon at ng mga hinulaang probabilidad. Kapag mas maliit ang pagkakaiba, mas sigurado tayo na tama ang aksyong ginawa.\n",
    "3. Kalkulahin ang mga discounted rewards at imultiply ang gradients sa mga discounted rewards - titiyakin nito na ang mga hakbang na may mas mataas na gantimpala ay magkakaroon ng mas malaking epekto sa panghuling resulta kumpara sa mga hakbang na may mas mababang gantimpala.\n",
    "4. Ang mga inaasahang target na aksyon para sa ating neural network ay bahagyang manggagaling sa mga hinulaang probabilidad habang tumatakbo, at bahagyang mula sa mga kalkuladong gradients. Gagamitin natin ang parameter na `alpha` upang matukoy kung hanggang saan isasaalang-alang ang gradients at rewards - ito ay tinatawag na *learning rate* ng reinforcement algorithm.\n",
    "5. Sa wakas, sasanayin natin ang ating network sa mga estado at inaasahang aksyon, at uulitin ang proseso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, patakbuhin natin ang episode na may rendering upang makita ang resulta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sana napansin mo na ang poste ay kaya nang magbalanse nang maayos!\n",
    "\n",
    "## Actor-Critic Model\n",
    "\n",
    "Ang Actor-Critic model ay isang mas advanced na bersyon ng policy gradients, kung saan gumagawa tayo ng neural network upang matutunan ang parehong policy at tinatayang mga gantimpala. Ang network ay magkakaroon ng dalawang output (o maaari mo itong tingnan bilang dalawang magkahiwalay na network):\n",
    "* **Actor** ang magrerekomenda ng aksyon na dapat gawin sa pamamagitan ng pagbibigay sa atin ng state probability distribution, tulad ng sa policy gradient model.\n",
    "* **Critic** ang magtatantiya kung ano ang magiging gantimpala mula sa mga aksyong iyon. Ibabalik nito ang kabuuang tinatayang gantimpala sa hinaharap sa ibinigay na estado.\n",
    "\n",
    "Tukuyin natin ang ganitong modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kailangan nating bahagyang baguhin ang ating mga `discounted_rewards` at `run_episode` na mga function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon, tatakbuhin natin ang pangunahing training loop. Gagamitin natin ang manual na proseso ng pagsasanay ng network sa pamamagitan ng pagkalkula ng tamang loss functions at pag-update ng mga parameter ng network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mga Mahalagang Punto\n",
    "\n",
    "Nakita natin ang dalawang RL algorithm sa demo na ito: simple policy gradient at ang mas sopistikadong actor-critic. Makikita mo na ang mga algorithm na ito ay gumagana gamit ang mga abstraktong konsepto ng estado, aksyon, at gantimpala - kaya maaari silang i-apply sa iba't ibang uri ng kapaligiran.\n",
    "\n",
    "Ang reinforcement learning ay nagbibigay-daan sa atin na matutunan ang pinakamainam na estratehiya para lutasin ang problema sa pamamagitan lamang ng pagtingin sa huling gantimpala. Ang katotohanan na hindi natin kailangan ng mga labelled dataset ay nagbibigay-daan sa atin na ulitin ang mga simulation nang maraming beses upang ma-optimize ang ating mga modelo. Gayunpaman, marami pa ring mga hamon sa RL, na maaari mong matutunan kung magpapasya kang magpokus nang higit pa sa kawili-wiling larangang ito ng AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Paunawa**:  \nAng dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T02:53:55+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "tl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}