<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-28T19:36:50+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "fi"
}
-->
# Generatiiviset vastakkaisasettelumallit

Edellisess√§ osiossa opimme **generatiivisista malleista**: malleista, jotka voivat luoda uusia kuvia, jotka muistuttavat harjoitusaineistossa olevia kuvia. VAE oli hyv√§ esimerkki generatiivisesta mallista.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Jos kuitenkin yrit√§mme luoda jotain todella merkityksellist√§, kuten maalauksen kohtuullisella resoluutiolla, VAE:lla huomaamme, ett√§ koulutus ei konvergoi hyvin. T√§t√§ k√§ytt√∂tapausta varten meid√§n tulisi oppia toinen arkkitehtuuri, joka on erityisesti suunnattu generatiivisille malleille - **Generatiiviset vastakkaisasettelumallit**, eli GANit.

GANin p√§√§idea on k√§ytt√§√§ kahta neuroverkkoa, jotka koulutetaan toisiaan vastaan:

<img src="images/gan_architecture.png" width="70%"/>

> Kuva: [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Pieni sanasto:
> * **Generaattori** on verkko, joka ottaa satunnaisen vektorin ja tuottaa kuvan tuloksena.
> * **Diskriminaattori** on verkko, joka ottaa kuvan ja yritt√§√§ m√§√§ritt√§√§, onko se oikea kuva (harjoitusaineistosta) vai generaattorin luoma. Se on k√§yt√§nn√∂ss√§ kuvaluokitin.

### Diskriminaattori

Diskriminaattorin arkkitehtuuri ei eroa tavallisesta kuvaluokitusverkosta. Yksinkertaisimmillaan se voi olla t√§ysin yhdistetty luokitin, mutta todenn√§k√∂isesti se on [konvoluutioneuroverkko](../07-ConvNets/README.md).

> ‚úÖ GAN, joka perustuu konvoluutioneuroverkkoihin, tunnetaan nimell√§ [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

CNN-diskriminaattori koostuu seuraavista kerroksista: useita konvoluutioita ja poolauksia (joilla pienennet√§√§n spatiaalista kokoa) sek√§ yksi tai useampi t√§ysin yhdistetty kerros, joilla saadaan "piirrevektori", ja lopuksi bin√§√§riluokitin.

> ‚úÖ 'Poolaus' t√§ss√§ yhteydess√§ on tekniikka, joka pienent√§√§ kuvan kokoa. "Poolauskerrokset pienent√§v√§t datan ulottuvuuksia yhdist√§m√§ll√§ yhden kerroksen neuroniryhmien tulokset yhdeksi neuroniksi seuraavassa kerroksessa." - [l√§hde](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generaattori

Generaattori on hieman monimutkaisempi. Voit ajatella sen olevan k√§√§nteinen diskriminaattori. Se alkaa latenttivektorista (piirrevektorin sijaan), sis√§lt√§√§ t√§ysin yhdistetyn kerroksen, joka muuntaa sen vaadittuun kokoon/muotoon, ja sit√§ seuraavat dekonstruktiot ja skaalaus yl√∂sp√§in. T√§m√§ on samanlainen kuin [autokooderin](../09-Autoencoders/README.md) *dekooderi*-osa.

> ‚úÖ Koska konvoluutiokerros toteutetaan lineaarisena suodattimena, joka kulkee kuvan l√§pi, dekonstruktio on pohjimmiltaan samanlainen kuin konvoluutio ja voidaan toteuttaa samalla kerroslogiikalla.

<img src="images/gan_arch_detail.png" width="70%"/>

> Kuva: [Dmitry Soshnikov](http://soshnikov.com)

### GANin kouluttaminen

GANit ovat **vastakkaisasettelullisia**, koska generaattorin ja diskriminaattorin v√§lill√§ on jatkuva kilpailu. T√§m√§n kilpailun aikana sek√§ generaattori ett√§ diskriminaattori parantavat suoritustaan, jolloin verkko oppii tuottamaan yh√§ parempia kuvia.

Koulutus tapahtuu kahdessa vaiheessa:

* **Diskriminaattorin kouluttaminen**. T√§m√§ teht√§v√§ on melko suoraviivainen: luomme generaattorin avulla er√§n kuvia, jotka merkit√§√§n 0:ksi (v√§√§rennetty kuva), ja otamme er√§n kuvia sy√∂tedatasta (merkittyn√§ 1:ksi, oikea kuva). Saamme jonkin *diskriminaattorih√§vi√∂n* ja suoritamme takapropagaation.
* **Generaattorin kouluttaminen**. T√§m√§ on hieman monimutkaisempaa, koska emme tied√§ generaattorin odotettua tulosta suoraan. Otamme koko GAN-verkon, joka koostuu generaattorista ja diskriminaattorista, sy√∂t√§mme siihen satunnaisia vektoreita ja odotamme tuloksen olevan 1 (vastaa oikeita kuvia). J√§√§dyt√§mme sitten diskriminaattorin parametrit (emme halua sen kouluttuvan t√§ss√§ vaiheessa) ja suoritamme takapropagaation.

T√§m√§n prosessin aikana sek√§ generaattorin ett√§ diskriminaattorin h√§vi√∂t eiv√§t laske merkitt√§v√§sti. Ihannetilanteessa niiden tulisi vaihdella, mik√§ vastaa molempien verkkojen suorituskyvyn paranemista.

## ‚úçÔ∏è Harjoitukset: GANit

* [GAN-muistikirja TensorFlow/Kerasilla](GANTF.ipynb)
* [GAN-muistikirja PyTorchilla](GANPyTorch.ipynb)

### GANin koulutuksen ongelmat

GANit ovat tunnetusti erityisen vaikeita kouluttaa. T√§ss√§ muutamia ongelmia:

* **Mode Collapse**. T√§m√§ tarkoittaa, ett√§ generaattori oppii tuottamaan yhden onnistuneen kuvan, joka huijaa diskriminaattoria, mutta ei monenlaisia erilaisia kuvia.
* **Herkk√§ hyperparametreille**. Usein GAN ei konvergoi lainkaan, mutta oppimisnopeuden √§killinen lasku voi johtaa konvergenssiin.
* **Tasapainon yll√§pit√§minen** generaattorin ja diskriminaattorin v√§lill√§. Monissa tapauksissa diskriminaattorin h√§vi√∂ voi pudota nollaan suhteellisen nopeasti, mik√§ est√§√§ generaattoria kouluttautumasta edelleen. T√§m√§n voittamiseksi voimme yritt√§√§ asettaa eri oppimisnopeudet generaattorille ja diskriminaattorille tai ohittaa diskriminaattorin koulutuksen, jos h√§vi√∂ on jo liian alhainen.
* **Korkean resoluution kouluttaminen**. T√§m√§ ongelma liittyy samaan kuin autokoodereissa: liian monen konvoluutiokerroksen rekonstruointi johtaa artefakteihin. T√§m√§ ongelma ratkaistaan tyypillisesti niin sanotulla **progressiivisella kasvulla**, jossa ensin muutama kerros koulutetaan matalaresoluutioisilla kuvilla, ja sitten kerroksia "avataan" tai lis√§t√§√§n. Toinen ratkaisu on lis√§t√§ ylim√§√§r√§isi√§ yhteyksi√§ kerrosten v√§lille ja kouluttaa useita resoluutioita samanaikaisesti - katso lis√§tietoja t√§st√§ [Multi-Scale Gradient GANs -paperista](https://arxiv.org/abs/1903.06048).

## Tyylinsiirto

GANit ovat loistava tapa luoda taiteellisia kuvia. Toinen mielenkiintoinen tekniikka on niin sanottu **tyylinsiirto**, jossa otetaan yksi **sis√§lt√∂kuva** ja piirret√§√§n se uudelleen eri tyylill√§, soveltaen **tyylikuvan** suodattimia.

N√§in se toimii:
* Aloitamme satunnaisesta kohinakuvaasta (tai sis√§lt√∂kuvasta, mutta ymm√§rt√§misen kannalta on helpompi aloittaa satunnaisesta kohinasta).
* Tavoitteenamme on luoda sellainen kuva, joka olisi l√§hell√§ sek√§ sis√§lt√∂kuvaa ett√§ tyylikuvaa. T√§m√§ m√§√§ritet√§√§n kahdella h√§vi√∂funktiolla:
   - **Sis√§lt√∂h√§vi√∂** lasketaan CNN:n tietyiss√§ kerroksissa nykyisest√§ kuvasta ja sis√§lt√∂kuvasta saatujen piirteiden perusteella.
   - **Tyylih√§vikk√∂** lasketaan nykyisen kuvan ja tyylikuvan v√§lill√§ √§lykk√§√§ll√§ tavalla k√§ytt√§m√§ll√§ Gram-matriiseja (lis√§tietoja [esimerkkimuistikirjassa](StyleTransfer.ipynb)).
* Kuvan tasoittamiseksi ja kohinan poistamiseksi otamme k√§ytt√∂√∂n my√∂s **Variation loss** -h√§vi√∂n, joka laskee keskim√§√§r√§isen et√§isyyden vierekk√§isten pikselien v√§lill√§.
* P√§√§optimointisilmukka s√§√§t√§√§ nykyist√§ kuvaa gradienttilaskennan (tai jonkin muun optimointialgoritmin) avulla minimoidakseen kokonaismenetyksen, joka on kaikkien kolmen h√§vi√∂n painotettu summa.

## ‚úçÔ∏è Esimerkki: [Tyylinsiirto](StyleTransfer.ipynb)

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Yhteenveto

T√§ss√§ oppitunnissa opit GANeista ja niiden kouluttamisesta. Opit my√∂s erityisist√§ haasteista, joita t√§m√§ neuroverkkojen tyyppi voi kohdata, sek√§ strategioista niiden voittamiseksi.

## üöÄ Haaste

K√§y l√§pi [tyylinsiirto-muistikirja](StyleTransfer.ipynb) k√§ytt√§en omia kuviasi.

## Kertaus ja itseopiskelu

Lis√§tietoja GANeista l√∂yd√§t n√§ist√§ l√§hteist√§:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), GAN-arkkitehtuuri, jota kannattaa harkita
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Teht√§v√§

Palaa johonkin t√§m√§n oppitunnin kahdesta muistikirjasta ja kouluta GAN omilla kuvillasi. Mit√§ voit luoda?

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Pyrimme tarkkuuteen, mutta huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§isell√§ kielell√§ tulee pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nt√§mist√§. Emme ole vastuussa t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§ aiheutuvista v√§√§rink√§sityksist√§ tai virhetulkinnoista.