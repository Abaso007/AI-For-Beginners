{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL-mallin kouluttaminen tasapainottamaan Cartpole\n",
    "\n",
    "Tämä muistikirja on osa [AI for Beginners -opetussuunnitelmaa](http://aka.ms/ai-beginners). Se on saanut inspiraationsa [virallisesta PyTorch-oppaasta](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) ja [tästä Cartpole PyTorch -toteutuksesta](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Tässä esimerkissä käytämme RL:ää (vahvistusoppimista) kouluttaaksemme mallin tasapainottamaan tankoa vaunussa, joka voi liikkua vasemmalle ja oikealle vaakasuoralla tasolla. Käytämme [OpenAI Gym](https://www.gymlibrary.ml/) -ympäristöä simuloimaan tankoa.\n",
    "\n",
    "> **Huomio**: Voit suorittaa tämän oppitunnin koodin paikallisesti (esim. Visual Studio Codessa), jolloin simulaatio avautuu uuteen ikkunaan. Jos suoritat koodin verkossa, sinun saattaa olla tarpeen tehdä joitakin muutoksia koodiin, kuten kuvattu [tässä](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Aloitamme varmistamalla, että Gym on asennettu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt luodaan CartPole-ympäristö ja katsotaan, miten sitä käytetään. Ympäristöllä on seuraavat ominaisuudet:\n",
    "\n",
    "* **Toimintotila** on joukko mahdollisia toimintoja, joita voimme suorittaa jokaisessa simulaation vaiheessa  \n",
    "* **Havaintotila** on tila, jossa voimme tehdä havaintoja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katsotaan, miten simulaatio toimii. Seuraava silmukka suorittaa simulaation, kunnes `env.step` palauttaa lopetuslipun `done`. Valitsemme toiminnot satunnaisesti käyttämällä `env.action_space.sample()`, mikä tarkoittaa, että koe epäonnistuu todennäköisesti hyvin nopeasti (CartPole-ympäristö päättyy, kun CartPolen nopeus, sijainti tai kulma ylittävät tietyt rajat).\n",
    "\n",
    "> Simulaatio avautuu uuteen ikkunaan. Voit suorittaa koodin useita kertoja ja nähdä, miten se käyttäytyy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voit huomata, että havainnot sisältävät neljä lukua. Ne ovat:\n",
    "- Kärryn sijainti\n",
    "- Kärryn nopeus\n",
    "- Tangon kulma\n",
    "- Tangon pyörimisnopeus\n",
    "\n",
    "`rew` on palkkio, jonka saamme jokaisella askeleella. CartPole-ympäristössä saat yhden pisteen jokaisesta simulaatioaskeleesta, ja tavoitteena on maksimoida kokonaispalkkio, eli aika, jonka CartPole pystyy tasapainottamaan kaatumatta.\n",
    "\n",
    "Vahvistusoppimisen aikana tavoitteemme on kouluttaa **politiikka** $\\pi$, joka jokaisessa tilassa $s$ kertoo meille, mikä toiminto $a$ tulisi valita, eli käytännössä $a = \\pi(s)$.\n",
    "\n",
    "Jos haluat todennäköisyyspohjaisen ratkaisun, voit ajatella politiikan palauttavan joukon todennäköisyyksiä jokaiselle toiminnolle, eli $\\pi(a|s)$ tarkoittaisi todennäköisyyttä, että meidän tulisi valita toiminto $a$ tilassa $s$.\n",
    "\n",
    "## Politiikkagradienttimenetelmä\n",
    "\n",
    "Yksinkertaisimmassa RL-algoritmissa, jota kutsutaan **politiikkagradientiksi**, koulutamme neuroverkon ennustamaan seuraavan toiminnon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aiomme kouluttaa verkkoa suorittamalla useita kokeita ja päivittämällä verkkomme jokaisen ajon jälkeen. Määritellään funktio, joka suorittaa kokeen ja palauttaa tulokset (niin sanottu **jälki**) - kaikki tilat, toiminnot (ja niiden suositellut todennäköisyydet) sekä palkkiot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voit ajaa yhden jakson kouluttamattomalla verkolla ja huomata, että kokonaispalkkio (eli jakson pituus) on erittäin alhainen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yksi politiikkagradienttialgoritmin hankalista puolista on käyttää **diskontattuja palkintoja**. Ajatuksena on, että laskemme pelin jokaisessa vaiheessa kokonaispalkintojen vektorin, ja tämän prosessin aikana diskonttaamme aikaiset palkinnot käyttäen jotakin kerrointa $gamma$. Normalisoimme myös tuloksena olevan vektorin, koska käytämme sitä painona vaikuttamaan koulutukseemme:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt aloitetaan varsinainen harjoittelu! Suoritamme 300 jaksoa, ja jokaisessa jaksossa teemme seuraavat vaiheet:\n",
    "\n",
    "1. Suorita koe ja kerää jälki\n",
    "1. Laske ero (`gradients`) toteutettujen toimien ja ennustettujen todennäköisyyksien välillä. Mitä pienempi ero on, sitä varmemmin olemme tehneet oikean toiminnan.\n",
    "1. Laske diskontatut palkkiot ja kerro gradientit diskontatuilla palkkioilla - tämä varmistaa, että korkeampia palkkioita sisältävät askeleet vaikuttavat lopputulokseen enemmän kuin matalampia palkkioita sisältävät.\n",
    "1. Odotetut tavoitetoimet neuroverkkoamme varten otetaan osittain ennustetuista todennäköisyyksistä kokeen aikana ja osittain lasketuista gradienteista. Käytämme `alpha`-parametria määrittämään, missä määrin gradientit ja palkkiot otetaan huomioon - tätä kutsutaan vahvistusalgoritmin *oppimisnopeudeksi*.\n",
    "1. Lopuksi koulutamme verkkoamme tiloilla ja odotetuilla toimilla, ja toistamme prosessin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt suoritetaan jakso renderöinnillä nähdäksemme tuloksen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toivottavasti huomaat, että tanko pysyy nyt melko hyvin tasapainossa!\n",
    "\n",
    "## Actor-Critic-malli\n",
    "\n",
    "Actor-Critic-malli on jatkokehitys policy gradient -menetelmästä, jossa rakennamme neuroverkon oppimaan sekä toimintapolitiikan että arvioidut palkkiot. Verkolla on kaksi ulostuloa (tai voit ajatella sen olevan kaksi erillistä verkkoa):\n",
    "* **Actor** suosittelee, mitä toimintoa tulisi käyttää, antamalla meille tilan todennäköisyysjakauman, kuten policy gradient -mallissa.\n",
    "* **Critic** arvioi, millainen palkkio näistä toimista voisi olla. Se palauttaa arvioidut kokonaispalkkiot tulevaisuudessa annetussa tilassa.\n",
    "\n",
    "Määritellään tällainen malli:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meidän täytyisi hieman muokata `discounted_rewards`- ja `run_episode`-funktioitamme:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt suoritetaan pääasiallinen koulutussilmukka. Käytämme manuaalista verkon koulutusprosessia laskemalla oikeat häviöfunktiot ja päivittämällä verkon parametrit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yhteenveto\n",
    "\n",
    "Tässä demossa olemme tutustuneet kahteen vahvistusoppimisalgoritmiin: yksinkertaiseen policy gradient -menetelmään ja kehittyneempään actor-critic -menetelmään. Voit huomata, että nämä algoritmit toimivat abstraktien tilan, toiminnan ja palkkion käsitteiden avulla – siksi niitä voidaan soveltaa hyvin erilaisiin ympäristöihin.\n",
    "\n",
    "Vahvistusoppimisen avulla voimme oppia parhaan strategian ongelman ratkaisemiseksi pelkästään tarkastelemalla lopullista palkkiota. Se, että emme tarvitse valmiiksi merkittyjä aineistoja, mahdollistaa simulaatioiden toistamisen useita kertoja malliemme optimoimiseksi. Tästä huolimatta RL:ssä on yhä monia haasteita, joihin voit perehtyä tarkemmin, jos päätät syventyä tähän mielenkiintoiseen tekoälyn osa-alueeseen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Vastuuvapauslauseke**:  \nTämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäinen asiakirja sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T20:19:27+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "fi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}