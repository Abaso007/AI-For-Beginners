<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-28T19:17:14+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "fi"
}
-->
# Syv√§vahvistusoppiminen

Vahvistusoppiminen (RL) n√§hd√§√§n yhten√§ koneoppimisen perusparadigmoista, yhdess√§ ohjatun oppimisen ja ohjaamattoman oppimisen kanssa. Siin√§ miss√§ ohjatussa oppimisessa tukeudumme tunnettuja tuloksia sis√§lt√§v√§√§n aineistoon, RL perustuu **oppimiseen tekem√§ll√§**. Esimerkiksi, kun n√§emme ensimm√§ist√§ kertaa tietokonepelin, alamme pelata sit√§, vaikka emme tied√§ s√§√§nt√∂j√§, ja pian pystymme parantamaan taitojamme pelk√§st√§√§n pelaamisen ja k√§ytt√§ytymisen s√§√§t√§misen kautta.

## [Esiluentavisa](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL:n suorittamiseen tarvitsemme:

* **Ymp√§rist√∂n** tai **simulaattorin**, joka m√§√§ritt√§√§ pelin s√§√§nn√∂t. Meid√§n tulisi pysty√§ suorittamaan kokeita simulaattorissa ja tarkkailemaan tuloksia.
* **Palkkiofunktion**, joka osoittaa, kuinka onnistunut kokeemme oli. Esimerkiksi tietokonepelin pelaamisen oppimisessa palkkio olisi lopullinen pistem√§√§r√§mme.

Palkkiofunktion perusteella meid√§n tulisi pysty√§ s√§√§t√§m√§√§n k√§ytt√§ytymist√§mme ja parantamaan taitojamme, jotta seuraavalla kerralla pelaamme paremmin. Suurin ero muiden koneoppimisen tyyppien ja RL:n v√§lill√§ on se, ett√§ RL:ss√§ emme yleens√§ tied√§, voitammeko vai h√§vi√§mme, ennen kuin peli on ohi. N√§in ollen emme voi sanoa, onko tietty siirto yksin√§√§n hyv√§ vai ei ‚Äì saamme palkkion vasta pelin lopussa.

RL:n aikana suoritamme tyypillisesti monia kokeita. Jokaisen kokeen aikana meid√§n t√§ytyy tasapainottaa t√§h√§n menness√§ oppimamme optimaalisen strategian seuraaminen (**hy√∂dynt√§minen**) ja uusien mahdollisten tilojen tutkiminen (**tutkiminen**).

## OpenAI Gym

Erinomainen ty√∂kalu RL:lle on [OpenAI Gym](https://gym.openai.com/) ‚Äì **simulaatioymp√§rist√∂**, joka voi simuloida monia erilaisia ymp√§rist√∂j√§ Atari-peleist√§ aina fysiikkaan, kuten tangon tasapainottamiseen. Se on yksi suosituimmista simulaatioymp√§rist√∂ist√§ vahvistusoppimisalgoritmien kouluttamiseen, ja sit√§ yll√§pit√§√§ [OpenAI](https://openai.com/).

> **Note**: Voit n√§hd√§ kaikki OpenAI Gymin tarjoamat ymp√§rist√∂t [t√§√§lt√§](https://gym.openai.com/envs/#classic_control).

## CartPole-tasapainotus

Olette varmasti n√§hneet moderneja tasapainotuslaitteita, kuten *Segway* tai *Gyroscooters*. Ne pystyv√§t automaattisesti tasapainottamaan itsens√§ s√§√§t√§m√§ll√§ py√∂ri√§√§n kiihtyvyysmittarin tai gyroskoopin signaalin perusteella. T√§ss√§ osiossa opimme ratkaisemaan vastaavan ongelman ‚Äì tangon tasapainottamisen. Se muistuttaa tilannetta, jossa sirkustaiteilija tasapainottaa tankoa k√§dell√§√§n ‚Äì mutta t√§ss√§ tapauksessa tasapainotus tapahtuu vain yhdess√§ ulottuvuudessa.

Yksinkertaistettu versio tasapainotuksesta tunnetaan nimell√§ **CartPole**-ongelma. CartPole-maailmassa meill√§ on vaakasuuntainen liukus√§√§din, joka voi liikkua vasemmalle tai oikealle, ja tavoitteena on tasapainottaa pystysuora tanko liukus√§√§timen p√§√§ll√§ sen liikkuessa.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

T√§m√§n ymp√§rist√∂n luomiseen ja k√§ytt√§miseen tarvitsemme muutaman rivin Python-koodia:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Jokaiseen ymp√§rist√∂√∂n p√§√§see k√§siksi samalla tavalla:
* `env.reset` aloittaa uuden kokeen
* `env.step` suorittaa simulaatioaskeleen. Se vastaanottaa **toiminnon** **toimintatilasta** ja palauttaa **havainnon** (havaintotilasta), sek√§ palkkion ja lopetuslipun.

Yll√§ olevassa esimerkiss√§ suoritamme satunnaisen toiminnon jokaisessa askeleessa, mink√§ vuoksi kokeen kesto on hyvin lyhyt:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL-algoritmin tavoitteena on kouluttaa malli ‚Äì niin kutsuttu **politiikka** œÄ ‚Äì joka palauttaa toiminnon tiettyyn tilaan vastauksena. Voimme my√∂s ajatella politiikan olevan todenn√§k√∂isyyspohjainen, eli mille tahansa tilalle *s* ja toiminnolle *a* se palauttaa todenn√§k√∂isyyden œÄ(*a*|*s*), ett√§ meid√§n tulisi valita *a* tilassa *s*.

## Politiikkagradienttialgoritmi

Ilmeisin tapa mallintaa politiikka on luoda neuroverkko, joka ottaa sy√∂tteen√§ tilat ja palauttaa vastaavat toiminnot (tai pikemminkin kaikkien toimintojen todenn√§k√∂isyydet). Tietyss√§ mieless√§ se olisi samanlainen kuin tavallinen luokitteluteht√§v√§, mutta merkitt√§v√§ ero on siin√§, ett√§ emme tied√§ etuk√§teen, mit√§ toimintoja meid√§n tulisi suorittaa kussakin vaiheessa.

Ajatuksena on arvioida n√§it√§ todenn√§k√∂isyyksi√§. Rakennamme **kumulatiivisten palkkioiden** vektorin, joka n√§ytt√§√§ kokonaispalkkiomme kussakin kokeen vaiheessa. Sovellamme my√∂s **palkkioiden diskonttausta** kertomalla aikaisemmat palkkiot kertoimella Œ≥=0.99, jotta aikaisempien palkkioiden merkitys v√§henee. Sitten vahvistamme niit√§ kokeen vaiheita, jotka tuottavat suurempia palkkioita.

> Lue lis√§√§ politiikkagradienttialgoritmista ja katso se toiminnassa [esimerkkivihkossa](CartPole-RL-TF.ipynb).

## Actor-Critic-algoritmi

Parannettu versio politiikkagradienttimenetelm√§st√§ tunnetaan nimell√§ **Actor-Critic**. Sen perusidea on, ett√§ neuroverkko koulutetaan palauttamaan kaksi asiaa:

* Politiikka, joka m√§√§ritt√§√§, mik√§ toiminto suoritetaan. T√§t√§ osaa kutsutaan **actoriksi**.
* Arvio siit√§, kuinka suuren kokonaispalkkion voimme odottaa saavuttavamme t√§ss√§ tilassa ‚Äì t√§t√§ osaa kutsutaan **criticiksi**.

Tietyss√§ mieless√§ t√§m√§ arkkitehtuuri muistuttaa [GAN:ia](../../4-ComputerVision/10-GANs/README.md), jossa meill√§ on kaksi verkkoa, jotka koulutetaan toisiaan vastaan. Actor-Critic-mallissa actor ehdottaa toimintoa, joka meid√§n tulisi suorittaa, ja critic yritt√§√§ olla kriittinen ja arvioida tuloksen. Tavoitteenamme on kuitenkin kouluttaa n√§m√§ verkot toimimaan yhdess√§.

Koska tied√§mme sek√§ todelliset kumulatiiviset palkkiot ett√§ criticin kokeen aikana palauttamat tulokset, on suhteellisen helppoa rakentaa h√§vi√∂funktio, joka minimoi niiden v√§lisen eron. T√§m√§ antaa meille **critic-h√§vi√∂n**. Voimme laskea **actor-h√§vi√∂n** k√§ytt√§m√§ll√§ samaa l√§hestymistapaa kuin politiikkagradienttialgoritmissa.

Kun olemme suorittaneet jonkin n√§ist√§ algoritmeista, voimme odottaa CartPolen k√§ytt√§ytyv√§n n√§in:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Harjoitukset: Politiikkagradientit ja Actor-Critic RL

Jatka oppimista seuraavissa vihkoissa:

* [RL TensorFlow'ssa](CartPole-RL-TF.ipynb)
* [RL PyTorchissa](CartPole-RL-PyTorch.ipynb)

## Muut RL-teht√§v√§t

Vahvistusoppiminen on nyky√§√§n nopeasti kasvava tutkimusalue. Joitakin mielenkiintoisia vahvistusoppimisen sovelluksia ovat:

* Tietokoneen opettaminen pelaamaan **Atari-pelej√§**. T√§m√§n ongelman haastava osa on, ett√§ meill√§ ei ole yksinkertaista tilaa, joka on esitetty vektorina, vaan pikemminkin kuvakaappaus ‚Äì ja meid√§n t√§ytyy k√§ytt√§√§ CNN:√§√§ muuntamaan t√§m√§ ruutukuva ominaisuusvektoriksi tai palkkiotiedon poimimiseksi. Atari-pelit ovat saatavilla Gymiss√§.
* Tietokoneen opettaminen pelaamaan lautapelej√§, kuten shakkia ja Go:ta. Viime aikoina huipputason ohjelmat, kuten **Alpha Zero**, on koulutettu alusta alkaen kahden agentin pelatessa toisiaan vastaan ja parantuessa jokaisessa vaiheessa.
* Teollisuudessa RL:√§√§ k√§ytet√§√§n ohjausj√§rjestelmien luomiseen simulaatiosta. Palvelu nimelt√§ [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) on suunniteltu erityisesti t√§t√§ varten.

## Yhteenveto

Olemme nyt oppineet kouluttamaan agentteja saavuttamaan hyvi√§ tuloksia pelk√§st√§√§n tarjoamalla heille palkkiofunktion, joka m√§√§ritt√§√§ pelin toivotun tilan, ja antamalla heille mahdollisuuden tutkia √§lykk√§√§sti hakutilaa. Olemme kokeilleet kahta algoritmia ja saavuttaneet hyvi√§ tuloksia suhteellisen lyhyess√§ ajassa. T√§m√§ on kuitenkin vasta alkua matkallasi RL:n parissa, ja sinun kannattaa ehdottomasti harkita erillisen kurssin ottamista, jos haluat syventy√§ aiheeseen.

## üöÄ Haaste

Tutustu "Muut RL-teht√§v√§t" -osiossa lueteltuihin sovelluksiin ja yrit√§ toteuttaa yksi niist√§!

## [J√§lkiluentavisa](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Kertaus ja itseopiskelu

Lue lis√§√§ klassisesta vahvistusoppimisesta [Koneoppiminen aloittelijoille -opetussuunnitelmastamme](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Katso [t√§m√§ loistava video](https://www.youtube.com/watch?v=qv6UVOQ0F44), jossa kerrotaan, kuinka tietokone voi oppia pelaamaan Super Mariota.

## Teht√§v√§: [Kouluta Mountain Car](lab/README.md)

Tavoitteenasi t√§ss√§ teht√§v√§ss√§ on kouluttaa toinen Gym-ymp√§rist√∂ ‚Äì [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.