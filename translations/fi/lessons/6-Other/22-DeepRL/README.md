<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T09:55:16+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "fi"
}
-->
# Syv√§vahvistusoppiminen

Vahvistusoppiminen (RL) n√§hd√§√§n yhten√§ koneoppimisen perusparadigmoista, yhdess√§ ohjatun oppimisen ja ohjaamattoman oppimisen kanssa. Siin√§ miss√§ ohjatussa oppimisessa tukeudumme tunnettuja tuloksia sis√§lt√§v√§√§n aineistoon, RL perustuu **oppimiseen tekem√§ll√§**. Esimerkiksi, kun n√§emme ensimm√§ist√§ kertaa tietokonepelin, alamme pelata sit√§, vaikka emme tied√§ s√§√§nt√∂j√§, ja pian pystymme parantamaan taitojamme pelk√§st√§√§n pelaamalla ja mukauttamalla k√§ytt√§ytymist√§mme.

## [Esiluentovisa](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL:n suorittamiseen tarvitsemme:

* **Ymp√§rist√∂n** tai **simulaattorin**, joka m√§√§ritt√§√§ pelin s√§√§nn√∂t. Meid√§n tulisi pysty√§ suorittamaan kokeita simulaattorissa ja tarkkailemaan tuloksia.
* **Palkkiofunktion**, joka osoittaa, kuinka onnistunut kokeemme oli. Esimerkiksi tietokonepelin pelaamisen oppimisessa palkkio olisi lopullinen pistem√§√§r√§mme.

Palkkiofunktion perusteella meid√§n tulisi pysty√§ mukauttamaan k√§ytt√§ytymist√§mme ja parantamaan taitojamme, jotta seuraavalla kerralla pelaamme paremmin. Suurin ero RL:n ja muiden koneoppimisen tyyppien v√§lill√§ on se, ett√§ RL:ss√§ emme yleens√§ tied√§, voitammeko vai h√§vi√§mme, ennen kuin peli on ohi. N√§in ollen emme voi sanoa, onko tietty siirto yksin√§√§n hyv√§ vai ei ‚Äì saamme palkkion vasta pelin lopussa.

RL:n aikana suoritamme tyypillisesti monia kokeita. Jokaisen kokeen aikana meid√§n on tasapainotettava t√§h√§n menness√§ oppimamme optimaalisen strategian noudattaminen (**hy√∂dynt√§minen**) ja uusien mahdollisten tilojen tutkiminen (**tutkiminen**).

## OpenAI Gym

Erinomainen ty√∂kalu RL:√§√§n on [OpenAI Gym](https://gym.openai.com/) - **simulaatioymp√§rist√∂**, joka voi simuloida monia erilaisia ymp√§rist√∂j√§, alkaen Atari-peleist√§ aina fysiikkaan, kuten tangon tasapainottamiseen. Se on yksi suosituimmista simulaatioymp√§rist√∂ist√§ vahvistusoppimisalgoritmien kouluttamiseen, ja sit√§ yll√§pit√§√§ [OpenAI](https://openai.com/).

> **Note**: Voit n√§hd√§ kaikki OpenAI Gymin tarjoamat ymp√§rist√∂t [t√§√§lt√§](https://gym.openai.com/envs/#classic_control).

## CartPole-tasapainotus

Olette varmasti n√§hneet moderneja tasapainolaitteita, kuten *Segway* tai *Gyroscooters*. Ne pystyv√§t automaattisesti tasapainottamaan itsens√§ s√§√§t√§m√§ll√§ py√∂ri√§√§n kiihtyvyysmittarin tai gyroskoopin signaalin perusteella. T√§ss√§ osiossa opimme ratkaisemaan vastaavan ongelman ‚Äì tangon tasapainottamisen. Se muistuttaa tilannetta, jossa sirkustaiteilija tasapainottaa tankoa k√§dell√§√§n ‚Äì mutta t√§ss√§ tapauksessa tasapainotus tapahtuu vain yhdess√§ ulottuvuudessa.

Yksinkertaistettu versio tasapainottamisesta tunnetaan nimell√§ **CartPole-ongelma**. CartPole-maailmassa meill√§ on vaakasuuntainen liukus√§√§din, joka voi liikkua vasemmalle tai oikealle, ja tavoitteena on tasapainottaa pystysuora tanko liukus√§√§timen p√§√§ll√§ sen liikkuessa.

<img alt="cartpole" src="images/cartpole.png" width="200"/>

T√§m√§n ymp√§rist√∂n luomiseen ja k√§ytt√§miseen tarvitsemme muutaman rivin Python-koodia:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Jokaiseen ymp√§rist√∂√∂n p√§√§see k√§siksi samalla tavalla:
* `env.reset` aloittaa uuden kokeen
* `env.step` suorittaa simulaatioaskeleen. Se vastaanottaa **toiminnon** **toimintatilasta** ja palauttaa **havainnon** (havaintotilasta), sek√§ palkkion ja lopetuslipun.

Yll√§ olevassa esimerkiss√§ suoritamme satunnaisen toiminnon jokaisessa askeleessa, mink√§ vuoksi kokeen kesto on hyvin lyhyt:

![tasapainoton cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL-algoritmin tavoitteena on kouluttaa malli ‚Äì niin kutsuttu **politiikka** &pi; ‚Äì joka palauttaa toiminnon tiettyyn tilaan vastauksena. Voimme my√∂s pit√§√§ politiikkaa todenn√§k√∂isyyspohjaisena, eli mille tahansa tilalle *s* ja toiminnolle *a* se palauttaa todenn√§k√∂isyyden &pi;(*a*|*s*), ett√§ meid√§n tulisi valita *a* tilassa *s*.

## Politiikkagradienttialgoritmi

Ilmeisin tapa mallintaa politiikka on luoda neuroverkko, joka ottaa sy√∂tteen√§ tilat ja palauttaa vastaavat toiminnot (tai pikemminkin kaikkien toimintojen todenn√§k√∂isyydet). Tietyss√§ mieless√§ se olisi samanlainen kuin tavallinen luokitteluteht√§v√§, mutta merkitt√§v√§ ero on siin√§, ett√§ emme tied√§ etuk√§teen, mit√§ toimintoja meid√§n tulisi suorittaa kussakin vaiheessa.

Ajatuksena on arvioida n√§it√§ todenn√§k√∂isyyksi√§. Rakennamme **kumulatiivisten palkkioiden** vektorin, joka osoittaa kokonaispalkkiomme kussakin kokeen vaiheessa. Sovellamme my√∂s **palkkioiden diskonttausta** kertomalla aikaisemmat palkkiot kertoimella &gamma;=0.99, jotta aikaisempien palkkioiden merkitys v√§henee. Sitten vahvistamme niit√§ kokeen vaiheita, jotka tuottavat suurempia palkkioita.

> Lue lis√§√§ Politiikkagradienttialgoritmista ja katso se toiminnassa [esimerkkivihkossa](CartPole-RL-TF.ipynb).

## Actor-Critic-algoritmi

Parannettu versio Politiikkagradienttimenetelm√§st√§ tunnetaan nimell√§ **Actor-Critic**. Sen perusidea on, ett√§ neuroverkko koulutetaan palauttamaan kaksi asiaa:

* Politiikka, joka m√§√§ritt√§√§, mik√§ toiminto suoritetaan. T√§t√§ osaa kutsutaan **actoriksi**.
* Arvio siit√§, kuinka suuren kokonaispalkkion voimme odottaa saavuttavamme t√§ss√§ tilassa ‚Äì t√§t√§ osaa kutsutaan **criticiksi**.

Tietyss√§ mieless√§ t√§m√§ arkkitehtuuri muistuttaa [GAN:ia](../../4-ComputerVision/10-GANs/README.md), jossa meill√§ on kaksi verkkoa, jotka koulutetaan toisiaan vastaan. Actor-Critic-mallissa actor ehdottaa toimintoa, joka meid√§n tulisi suorittaa, ja critic yritt√§√§ olla kriittinen ja arvioida tuloksen. Tavoitteenamme on kuitenkin kouluttaa n√§m√§ verkot yhdess√§.

Koska tied√§mme sek√§ todelliset kumulatiiviset palkkiot ett√§ criticin kokeen aikana palauttamat tulokset, on suhteellisen helppoa rakentaa h√§vi√∂funktio, joka minimoi niiden v√§lisen eron. T√§m√§ antaa meille **critic-h√§vi√∂n**. Voimme laskea **actor-h√§vi√∂n** k√§ytt√§m√§ll√§ samaa l√§hestymistapaa kuin politiikkagradienttialgoritmissa.

Kun olemme suorittaneet jonkin n√§ist√§ algoritmeista, voimme odottaa CartPolen k√§ytt√§ytyv√§n n√§in:

![tasapainottava cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Harjoitukset: Politiikkagradientit ja Actor-Critic RL

Jatka oppimista seuraavissa vihkoissa:

* [RL TensorFlow'ssa](CartPole-RL-TF.ipynb)
* [RL PyTorchissa](CartPole-RL-PyTorch.ipynb)

## Muut RL-teht√§v√§t

Vahvistusoppiminen on nyky√§√§n nopeasti kasvava tutkimusalue. Joitakin mielenkiintoisia vahvistusoppimisen sovelluksia ovat:

* Tietokoneen opettaminen pelaamaan **Atari-pelej√§**. T√§m√§n ongelman haastava osa on, ett√§ meill√§ ei ole yksinkertaista tilaa, joka olisi esitetty vektorina, vaan pikemminkin kuvakaappaus ‚Äì ja meid√§n on k√§ytett√§v√§ CNN:√§√§ muuntamaan t√§m√§ kuvaruutu ominaisuusvektoriksi tai palkkiotiedon poimimiseen. Atari-pelit ovat saatavilla Gymiss√§.
* Tietokoneen opettaminen pelaamaan lautapelej√§, kuten shakkia ja Go:ta. Viime aikoina huipputason ohjelmat, kuten **Alpha Zero**, on koulutettu alusta alkaen kahden agentin pelatessa toisiaan vastaan ja parantaessa joka askeleella.
* Teollisuudessa RL:√§√§ k√§ytet√§√§n ohjausj√§rjestelmien luomiseen simulaatiosta. Palvelu nimelt√§ [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) on suunniteltu erityisesti t√§h√§n tarkoitukseen.

## Yhteenveto

Olemme nyt oppineet kouluttamaan agentteja saavuttamaan hyvi√§ tuloksia pelk√§st√§√§n tarjoamalla heille palkkiofunktion, joka m√§√§ritt√§√§ pelin toivotun tilan, ja antamalla heille mahdollisuuden tutkia √§lykk√§√§sti hakutilaa. Olemme kokeilleet kahta algoritmia ja saavuttaneet hyvi√§ tuloksia suhteellisen lyhyess√§ ajassa. T√§m√§ on kuitenkin vasta alkua matkallesi RL:n parissa, ja sinun kannattaa ehdottomasti harkita erillisen kurssin suorittamista, jos haluat syventy√§ aiheeseen.

## üöÄ Haaste

Tutustu "Muut RL-teht√§v√§t" -osiossa lueteltuihin sovelluksiin ja yrit√§ toteuttaa yksi niist√§!

## [J√§lkiluentovisa](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Kertaus ja itseopiskelu

Lue lis√§√§ klassisesta vahvistusoppimisesta [Koneoppiminen aloittelijoille -opetussuunnitelmastamme](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Katso [t√§m√§ loistava video](https://www.youtube.com/watch?v=qv6UVOQ0F44), jossa kerrotaan, kuinka tietokone voi oppia pelaamaan Super Mariota.

## Teht√§v√§: [Kouluta Mountain Car](lab/README.md)

Tavoitteenasi t√§ss√§ teht√§v√§ss√§ on kouluttaa toinen Gym-ymp√§rist√∂ ‚Äì [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

