<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-28T19:46:28+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "fi"
}
-->
# Johdanto neuroverkkoihin. Monikerroksinen perceptron

Edellisess√§ osiossa opit yksinkertaisimmasta neuroverkkonmallista - yhden kerroksen perceptronista, joka on lineaarinen kahden luokan luokittelumalli.

T√§ss√§ osiossa laajennamme t√§t√§ mallia joustavammaksi kehykseksi, joka mahdollistaa:

* suorittaa **moniluokkaluokittelua** kahden luokan luokittelun lis√§ksi
* ratkaista **regressio-ongelmia** luokittelun lis√§ksi
* erottaa luokkia, jotka eiv√§t ole lineaarisesti erotettavissa

Kehit√§mme my√∂s oman modulaarisen kehyksen Pythonissa, jonka avulla voimme rakentaa erilaisia neuroverkkorakenteita.

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Koneoppimisen formalisointi

Aloitetaan koneoppimisen ongelman formalisoinnilla. Oletetaan, ett√§ meill√§ on harjoitusaineisto **X** ja sen luokat **Y**, ja meid√§n t√§ytyy rakentaa malli *f*, joka tekee mahdollisimman tarkkoja ennusteita. Ennusteiden laatua mitataan **h√§vi√∂funktiolla** ‚Ñí. Seuraavia h√§vi√∂funktioita k√§ytet√§√§n usein:

* Regressio-ongelmassa, kun meid√§n t√§ytyy ennustaa luku, voimme k√§ytt√§√§ **absoluuttista virhett√§** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| tai **neli√∂llist√§ virhett√§** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Luokittelussa k√§yt√§mme **0-1 h√§vi√∂t√§** (joka on k√§yt√§nn√∂ss√§ sama kuin mallin **tarkkuus**) tai **logistista h√§vi√∂t√§**.

Yhden kerroksen perceptronissa funktio *f* m√§√§riteltiin lineaarisena funktiona *f(x)=wx+b* (t√§ss√§ *w* on painomatriisi, *x* on sy√∂teominaisuuksien vektori ja *b* on bias-vektori). Eri neuroverkkorakenteissa t√§m√§ funktio voi olla monimutkaisempi.

> Luokittelussa on usein toivottavaa saada todenn√§k√∂isyydet vastaaville luokille verkon ulostulona. Muuntaaksemme satunnaiset luvut todenn√§k√∂isyyksiksi (esim. normalisoidaksemme ulostulon), k√§yt√§mme usein **softmax**-funktiota œÉ, ja funktiosta *f* tulee *f(x)=œÉ(wx+b)*.

Yll√§ olevassa *f*:n m√§√§ritelm√§ss√§ *w* ja *b* kutsutaan **parametreiksi** Œ∏=‚ü®*w,b*‚ü©. Kun aineisto ‚ü®**X**,**Y**‚ü© on annettu, voimme laskea kokonaisvirheen koko aineistolle parametrien Œ∏ funktiona.

> ‚úÖ **Neuroverkon koulutuksen tavoite on minimoida virhe muuttamalla parametreja Œ∏**

## Gradienttimenetelm√§ optimointiin

On olemassa tunnettu menetelm√§ funktioiden optimointiin, nimelt√§√§n **gradienttimenetelm√§**. Idea on, ett√§ voimme laskea derivaatan (moniulotteisessa tapauksessa **gradientin**) h√§vi√∂funktion suhteen parametreihin ja muuttaa parametreja siten, ett√§ virhe pienenee. T√§m√§ voidaan formalisoida seuraavasti:

* Alusta parametrit satunnaisilla arvoilla w<sup>(0)</sup>, b<sup>(0)</sup>
* Toista seuraava vaihe monta kertaa:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

Koulutuksen aikana optimointivaiheet lasketaan oletettavasti koko aineistoa k√§ytt√§en (muista, ett√§ h√§vi√∂ lasketaan summana kaikkien harjoitusn√§ytteiden l√§pi). Todellisuudessa otamme kuitenkin pieni√§ osia aineistosta, joita kutsutaan **minibatcheiksi**, ja laskemme gradientit osajoukon perusteella. Koska osajoukko valitaan satunnaisesti joka kerta, t√§t√§ menetelm√§√§ kutsutaan **stokastiseksi gradienttimenetelm√§ksi** (SGD).

## Monikerroksiset perceptronit ja takaisinkuljetus

Yhden kerroksen verkko, kuten yll√§ n√§htiin, pystyy luokittelemaan lineaarisesti erotettavia luokkia. Rikkaamman mallin rakentamiseksi voimme yhdist√§√§ useita verkon kerroksia. Matemaattisesti t√§m√§ tarkoittaisi, ett√§ funktio *f* olisi monimutkaisempi ja laskettaisiin useassa vaiheessa:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

T√§ss√§ Œ± on **ep√§lineaarinen aktivointifunktio**, œÉ on softmax-funktio, ja parametrit Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradienttimenetelm√§ pysyy samana, mutta gradienttien laskeminen on vaikeampaa. Ketjulaskus√§√§nn√∂n avulla voimme laskea derivaatat seuraavasti:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ Ketjulaskus√§√§nt√∂√§ k√§ytet√§√§n laskemaan h√§vi√∂funktion derivaatat parametrien suhteen.

Huomaa, ett√§ kaikkien n√§iden lausekkeiden vasemmanpuoleinen osa on sama, ja n√§in voimme tehokkaasti laskea derivaatat aloittaen h√§vi√∂funktiosta ja kulkemalla "taaksep√§in" laskentakaavion l√§pi. N√§in monikerroksisen perceptronin koulutusmenetelm√§√§ kutsutaan **takaisinkuljetukseksi** tai 'backpropiksi'.

<img alt="laskentakaavio" src="images/ComputeGraphGrad.png"/>

> TODO: kuvan l√§hde

> ‚úÖ K√§ymme takaisinkuljetusta paljon yksityiskohtaisemmin l√§pi esimerkkivihkossamme.  

## Yhteenveto

T√§ss√§ oppitunnissa rakensimme oman neuroverkkokirjaston ja k√§ytimme sit√§ yksinkertaiseen kaksiulotteiseen luokitteluteht√§v√§√§n.

## üöÄ Haaste

Liitetyss√§ vihkossa toteutat oman kehyksen monikerroksisten perceptronien rakentamiseen ja kouluttamiseen. N√§et yksityiskohtaisesti, miten modernit neuroverkot toimivat.

Siirry [OwnFramework](OwnFramework.ipynb) -vihkoon ja k√§y se l√§pi.

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Kertaus ja itseopiskelu

Takaisinkuljetus on yleinen algoritmi, jota k√§ytet√§√§n teko√§lyss√§ ja koneoppimisessa. Se kannattaa [tutkia tarkemmin](https://wikipedia.org/wiki/Backpropagation).

## [Teht√§v√§](lab/README.md)

T√§ss√§ laboratoriossa sinua pyydet√§√§n k√§ytt√§m√§√§n t√§ss√§ oppitunnissa rakentamaasi kehyst√§ MNIST-k√§sinkirjoitettujen numeroiden luokitteluun.

* [Ohjeet](lab/README.md)
* [Vihko](lab/MyFW_MNIST.ipynb)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.