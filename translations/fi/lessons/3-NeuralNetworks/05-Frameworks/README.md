<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-28T19:49:29+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "fi"
}
-->
# Neuroverkkojen Kehykset

Kuten olemme jo oppineet, neuroverkkojen tehokkaaseen kouluttamiseen tarvitaan kaksi asiaa:

* Kyky k√§sitell√§ tensoreita, esimerkiksi kertoa, laskea yhteen ja laskea joitakin funktioita, kuten sigmoid tai softmax
* Kyky laskea kaikkien lausekkeiden gradientit, jotta voidaan suorittaa gradienttilaskeuman optimointi

## [Esiluentavisa](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Vaikka `numpy`-kirjasto pystyy suorittamaan ensimm√§isen osan, tarvitsemme mekanismin gradienttien laskemiseen. [Oma kehyksemme](../04-OwnFramework/OwnFramework.ipynb), jonka kehitimme edellisess√§ osiossa, vaati, ett√§ kaikki derivaattafunktiot ohjelmoitiin manuaalisesti `backward`-metodissa, joka suorittaa takapropagaation. Ihanteellisesti kehys antaisi meille mahdollisuuden laskea gradientit *mille tahansa lausekkeelle*, jonka voimme m√§√§ritell√§.

Toinen t√§rke√§ asia on kyky suorittaa laskelmia GPU:lla tai muilla erikoistuneilla laskentayksik√∂ill√§, kuten [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Syvien neuroverkkojen kouluttaminen vaatii *valtavasti* laskentatehoa, ja n√§iden laskelmien rinnakkaistaminen GPU:illa on eritt√§in t√§rke√§√§.

> ‚úÖ Termi 'rinnakkaistaminen' tarkoittaa laskelmien jakamista useille laitteille.

T√§ll√§ hetkell√§ kaksi suosituinta neuroverkkokehyst√§ ovat: [TensorFlow](http://TensorFlow.org) ja [PyTorch](https://pytorch.org/). Molemmat tarjoavat matalan tason API:n tensoreiden k√§sittelyyn sek√§ CPU:lla ett√§ GPU:lla. Matalan tason API:n lis√§ksi on my√∂s korkeamman tason API, nimelt√§√§n [Keras](https://keras.io/) ja [PyTorch Lightning](https://pytorchlightning.ai/) vastaavasti.

Matalan tason API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
Korkean tason API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Matalan tason API:t** molemmissa kehyksiss√§ mahdollistavat niin sanottujen **laskentakaavioiden** rakentamisen. T√§m√§ kaavio m√§√§ritt√§√§, miten tulos (yleens√§ h√§vi√∂funktio) lasketaan annetuilla sy√∂teparametreilla, ja se voidaan siirt√§√§ laskettavaksi GPU:lle, jos sellainen on k√§ytett√§viss√§. On olemassa funktioita, jotka voivat laskea t√§m√§n laskentakaavion gradientit, joita voidaan sitten k√§ytt√§√§ mallin parametrien optimointiin.

**Korkean tason API:t** k√§sittelev√§t neuroverkkoja pitk√§lti **kerrosten sarjana**, mik√§ tekee useimpien neuroverkkojen rakentamisesta paljon helpompaa. Mallin kouluttaminen vaatii yleens√§ datan valmistelun ja sitten `fit`-funktion kutsumisen ty√∂n suorittamiseksi.

Korkean tason API mahdollistaa tyypillisten neuroverkkojen nopean rakentamisen ilman, ett√§ tarvitsee huolehtia monista yksityiskohdista. Samaan aikaan matalan tason API tarjoaa paljon enemm√§n hallintaa koulutusprosessiin, ja siksi niit√§ k√§ytet√§√§n paljon tutkimuksessa, kun k√§sitell√§√§n uusia neuroverkkorakenteita.

On my√∂s t√§rke√§√§ ymm√§rt√§√§, ett√§ molempia API:ja voidaan k√§ytt√§√§ yhdess√§. Esimerkiksi voit kehitt√§√§ oman verkkokerrosarkkitehtuurisi matalan tason API:lla ja k√§ytt√§√§ sit√§ sitten osana suurempaa verkkoa, joka on rakennettu ja koulutettu korkean tason API:lla. Tai voit m√§√§ritell√§ verkon korkean tason API:lla kerrosten sarjana ja k√§ytt√§√§ sitten omaa matalan tason koulutussilmukkaa optimointiin. Molemmat API:t perustuvat samoihin perusk√§sitteisiin, ja ne on suunniteltu toimimaan hyvin yhdess√§.

## Oppiminen

T√§ss√§ kurssissa tarjoamme suurimman osan sis√§ll√∂st√§ sek√§ PyTorchille ett√§ TensorFlow'lle. Voit valita haluamasi kehyksen ja k√§yd√§ l√§pi vain vastaavat muistikirjat. Jos et ole varma, mink√§ kehyksen valitset, lue keskusteluja internetist√§ aiheesta **PyTorch vs. TensorFlow**. Voit my√∂s tutustua molempiin kehyksiin saadaksesi paremman k√§sityksen.

Miss√§ mahdollista, k√§yt√§mme yksinkertaisuuden vuoksi korkean tason API:ja. Uskomme kuitenkin, ett√§ on t√§rke√§√§ ymm√§rt√§√§, miten neuroverkot toimivat alusta alkaen, joten aluksi aloitamme ty√∂skentelem√§ll√§ matalan tason API:n ja tensoreiden kanssa. Jos kuitenkin haluat p√§√§st√§ nopeasti alkuun etk√§ halua k√§ytt√§√§ paljon aikaa n√§iden yksityiskohtien oppimiseen, voit ohittaa ne ja siirty√§ suoraan korkean tason API-muistikirjoihin.

## ‚úçÔ∏è Harjoitukset: Kehykset

Jatka oppimista seuraavissa muistikirjoissa:

Matalan tason API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
Korkean tason API | [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Kun olet hallinnut kehykset, kerrataan ylikoulutuksen k√§site.

# Ylikoulutus

Ylikoulutus on eritt√§in t√§rke√§ k√§site koneoppimisessa, ja on eritt√§in t√§rke√§√§ ymm√§rt√§√§ se oikein!

Tarkastellaan seuraavaa ongelmaa, jossa yritet√§√§n approksimoida 5 pistett√§ (esitettyn√§ `x`-merkeill√§ alla olevissa kaavioissa):

![lineaarinen](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.fi.jpg) | ![ylikoulutus](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.fi.jpg)
-------------------------|--------------------------
**Lineaarinen malli, 2 parametria** | **Ei-lineaarinen malli, 7 parametria**
Koulutusvirhe = 5.3 | Koulutusvirhe = 0
Validointivirhe = 5.1 | Validointivirhe = 20

* Vasemmalla n√§emme hyv√§n suoran viivan approksimaation. Koska parametrien m√§√§r√§ on sopiva, malli ymm√§rt√§√§ pisteiden jakautumisen oikein.
* Oikealla malli on liian voimakas. Koska meill√§ on vain 5 pistett√§ ja mallilla on 7 parametria, se voi mukautua siten, ett√§ se kulkee kaikkien pisteiden l√§pi, jolloin koulutusvirhe on 0. T√§m√§ kuitenkin est√§√§ mallia ymm√§rt√§m√§st√§ datan oikeaa mallia, joten validointivirhe on eritt√§in korkea.

On eritt√§in t√§rke√§√§ l√∂yt√§√§ oikea tasapaino mallin monimutkaisuuden (parametrien m√§√§r√§) ja koulutusn√§ytteiden m√§√§r√§n v√§lill√§.

## Miksi ylikoulutusta tapahtuu

  * Liian v√§h√§n koulutusdataa
  * Liian voimakas malli
  * Liikaa kohinaa sy√∂tedatassa

## Miten ylikoulutus havaitaan

Kuten yll√§ olevasta kaaviosta n√§kyy, ylikoulutus voidaan havaita eritt√§in alhaisesta koulutusvirheest√§ ja korkeasta validointivirheest√§. Normaalisti koulutuksen aikana sek√§ koulutus- ett√§ validointivirheet alkavat pienenty√§, ja jossain vaiheessa validointivirhe saattaa lakata pienentym√§st√§ ja alkaa kasvaa. T√§m√§ on merkki ylikoulutuksesta ja osoitus siit√§, ett√§ koulutus tulisi todenn√§k√∂isesti lopettaa t√§ss√§ vaiheessa (tai ainakin tallentaa mallin tilanne).

![ylikoulutus](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.fi.png)

## Miten ylikoulutusta estet√§√§n

Jos huomaat, ett√§ ylikoulutusta tapahtuu, voit tehd√§ jonkin seuraavista:

 * Lis√§√§ koulutusdatan m√§√§r√§√§
 * V√§henn√§ mallin monimutkaisuutta
 * K√§yt√§ jotakin [regularisointitekniikkaa](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), kuten [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), jota k√§sittelemme my√∂hemmin.

## Ylikoulutus ja Bias-Variance-tasapaino

Ylikoulutus on itse asiassa tapaus yleisemm√§st√§ tilastollisesta ongelmasta, jota kutsutaan [Bias-Variance-tasapainoksi](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Jos tarkastelemme mallimme mahdollisia virhel√§hteit√§, voimme n√§hd√§ kahdenlaisia virheit√§:

* **Harhavirheet** johtuvat siit√§, ett√§ algoritmimme ei pysty mallintamaan koulutusdatan suhdetta oikein. T√§m√§ voi johtua siit√§, ett√§ mallimme ei ole tarpeeksi voimakas (**alikoulutus**).
* **Varianssivirheet**, jotka johtuvat siit√§, ett√§ malli mallintaa sy√∂tedatan kohinaa merkityksellisen suhteen sijaan (**ylikoulutus**).

Koulutuksen aikana harhavirhe pienenee (kun mallimme oppii approksimoimaan dataa), ja varianssivirhe kasvaa. On t√§rke√§√§ lopettaa koulutus - joko manuaalisesti (kun havaitsemme ylikoulutusta) tai automaattisesti (ottamalla k√§ytt√∂√∂n regularisointi) - ylikoulutuksen est√§miseksi.

## Yhteenveto

T√§ss√§ oppitunnissa opit kahden suosituimman teko√§lykehyksen, TensorFlow'n ja PyTorchin, eri API:en erot. Lis√§ksi opit eritt√§in t√§rke√§n aiheen, ylikoulutuksen.

## üöÄ Haaste

Mukana olevissa muistikirjoissa l√∂yd√§t 'teht√§vi√§' alareunasta; k√§y l√§pi muistikirjat ja suorita teht√§v√§t.

## [J√§lkiluentavisa](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Kertaus ja itseopiskelu

Tee tutkimusta seuraavista aiheista:

- TensorFlow
- PyTorch
- Ylikoulutus

Kysy itselt√§si seuraavat kysymykset:

- Mit√§ eroa on TensorFlow'lla ja PyTorchilla?
- Mit√§ eroa on ylikoulutuksella ja alikoulutuksella?

## [Teht√§v√§](lab/README.md)

T√§ss√§ laboratoriossa sinun tulee ratkaista kaksi luokitteluongelmaa k√§ytt√§en yksi- ja monikerroksisia t√§ysin kytkettyj√§ verkkoja joko PyTorchilla tai TensorFlow'lla.

* [Ohjeet](lab/README.md)
* [Muistikirja](lab/LabFrameworks.ipynb)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.