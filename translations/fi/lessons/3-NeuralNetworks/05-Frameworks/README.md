<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddd216f558a255260a9374008002c971",
  "translation_date": "2025-09-23T10:02:17+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "fi"
}
-->
# Neuroverkkojen Kehykset

Kuten olemme jo oppineet, tehokkaan neuroverkkojen kouluttamisen kannalta meid√§n t√§ytyy tehd√§ kaksi asiaa:

* Operoida tensoreilla, esimerkiksi kertoa, laskea yhteen ja laskea joitakin funktioita, kuten sigmoid tai softmax
* Laskea kaikkien lausekkeiden gradientit, jotta voimme suorittaa gradienttilaskeutumisoptimoinnin

## [Esiluennon kysely](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Vaikka `numpy`-kirjasto pystyy suorittamaan ensimm√§isen osan, tarvitsemme mekanismin gradienttien laskemiseen. [Oma kehyksemme](../04-OwnFramework/OwnFramework.ipynb), jonka kehitimme edellisess√§ osiossa, vaati meit√§ ohjelmoimaan kaikki derivaattafunktiot manuaalisesti `backward`-metodissa, joka suorittaa takaisinlevityksen. Ihanteellisesti kehys antaisi meille mahdollisuuden laskea gradientit *mille tahansa lausekkeelle*, jonka voimme m√§√§ritell√§.

Toinen t√§rke√§ asia on kyky suorittaa laskelmia GPU:lla tai muilla erikoistuneilla laskentayksik√∂ill√§, kuten [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Syvien neuroverkkojen kouluttaminen vaatii *paljon* laskentaa, ja laskelmien rinnakkaistaminen GPU:lla on eritt√§in t√§rke√§√§.

> ‚úÖ Termi 'rinnakkaistaminen' tarkoittaa laskelmien jakamista useille laitteille.

T√§ll√§ hetkell√§ kaksi suosituinta neuroverkkojen kehyst√§ ovat: [TensorFlow](http://TensorFlow.org) ja [PyTorch](https://pytorch.org/). Molemmat tarjoavat matalan tason API:n tensoreiden k√§sittelyyn sek√§ CPU:lla ett√§ GPU:lla. Matalan tason API:n lis√§ksi on my√∂s korkean tason API, nimelt√§√§n [Keras](https://keras.io/) ja [PyTorch Lightning](https://pytorchlightning.ai/) vastaavasti.

Matalan tason API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
Korkean tason API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Matalan tason API:t** molemmissa kehyksiss√§ mahdollistavat niin sanottujen **laskentakaavioiden** rakentamisen. T√§m√§ kaavio m√§√§ritt√§√§, miten tulos (yleens√§ h√§vi√∂funktio) lasketaan annetuilla sy√∂teparametreilla, ja sen voi siirt√§√§ laskettavaksi GPU:lle, jos se on saatavilla. Kaaviolle on olemassa funktioita, jotka laskevat sen gradientit, joita voidaan k√§ytt√§√§ mallin parametrien optimointiin.

**Korkean tason API:t** k√§sittelev√§t neuroverkkoja pitk√§lti **kerrosten sarjana**, ja tekev√§t useimpien neuroverkkojen rakentamisesta paljon helpompaa. Mallin kouluttaminen vaatii yleens√§ datan valmistelua ja sitten `fit`-funktion kutsumista ty√∂n suorittamiseksi.

Korkean tason API:n avulla voit rakentaa tyypillisi√§ neuroverkkoja eritt√§in nopeasti ilman, ett√§ sinun tarvitsee huolehtia monista yksityiskohdista. Samalla matalan tason API tarjoaa paljon enemm√§n kontrollia koulutusprosessiin, ja siksi sit√§ k√§ytet√§√§n paljon tutkimuksessa, kun k√§sitell√§√§n uusia neuroverkkoarkkitehtuureja.

On my√∂s t√§rke√§√§ ymm√§rt√§√§, ett√§ molempia API:ita voi k√§ytt√§√§ yhdess√§, esimerkiksi voit kehitt√§√§ oman verkon kerrosarkkitehtuurin matalan tason API:lla ja k√§ytt√§√§ sit√§ osana suurempaa verkkoa, joka on rakennettu ja koulutettu korkean tason API:lla. Tai voit m√§√§ritell√§ verkon korkean tason API:lla kerrosten sarjana ja k√§ytt√§√§ omaa matalan tason koulutussilmukkaa optimointiin. Molemmat API:t perustuvat samoihin perusk√§sitteisiin, ja ne on suunniteltu toimimaan hyvin yhdess√§.

## Oppiminen

T√§ss√§ kurssissa tarjoamme suurimman osan sis√§ll√∂st√§ sek√§ PyTorchille ett√§ TensorFlow'lle. Voit valita mieluisan kehyksen ja k√§yd√§ l√§pi vain vastaavat muistikirjat. Jos et ole varma, mink√§ kehyksen valitset, lue keskusteluja internetiss√§ aiheesta **PyTorch vs. TensorFlow**. Voit my√∂s tutustua molempiin kehyksiin saadaksesi paremman k√§sityksen.

Miss√§ mahdollista, k√§yt√§mme korkean tason API:ita yksinkertaisuuden vuoksi. Uskomme kuitenkin, ett√§ on t√§rke√§√§ ymm√§rt√§√§, miten neuroverkot toimivat alusta alkaen, joten aluksi aloitamme ty√∂skentelyn matalan tason API:n ja tensoreiden kanssa. Jos kuitenkin haluat p√§√§st√§ nopeasti alkuun etk√§ halua k√§ytt√§√§ paljon aikaa n√§iden yksityiskohtien oppimiseen, voit ohittaa ne ja siirty√§ suoraan korkean tason API-muistikirjoihin.

## ‚úçÔ∏è Harjoitukset: Kehykset

Jatka oppimista seuraavissa muistikirjoissa:

Matalan tason API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
Korkean tason API | [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Kun olet hallinnut kehykset, kerrataan ylikoulutuksen k√§sitett√§.

# Ylikoulutus

Ylikoulutus on eritt√§in t√§rke√§ k√§site koneoppimisessa, ja on eritt√§in t√§rke√§√§ ymm√§rt√§√§ se oikein!

Tarkastellaan seuraavaa ongelmaa, jossa pyrit√§√§n approksimoimaan 5 pistett√§ (esitettyn√§ `x`-merkeill√§ alla olevissa kaavioissa):

![lineaarinen](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.fi.jpg) | ![ylikoulutus](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.fi.jpg)
-------------------------|--------------------------
**Lineaarinen malli, 2 parametria** | **Ei-lineaarinen malli, 7 parametria**
Koulutusvirhe = 5.3 | Koulutusvirhe = 0
Validointivirhe = 5.1 | Validointivirhe = 20

* Vasemmalla n√§emme hyv√§n suoran approksimaation. Koska parametrien m√§√§r√§ on sopiva, malli ymm√§rt√§√§ pisteiden jakautumisen oikein.
* Oikealla malli on liian voimakas. Koska meill√§ on vain 5 pistett√§ ja mallilla on 7 parametria, se voi mukautua siten, ett√§ se kulkee kaikkien pisteiden l√§pi, jolloin koulutusvirhe on 0. T√§m√§ kuitenkin est√§√§ mallia ymm√§rt√§m√§st√§ datan oikeaa kaavaa, joten validointivirhe on eritt√§in korkea.

On eritt√§in t√§rke√§√§ l√∂yt√§√§ oikea tasapaino mallin monimutkaisuuden (parametrien m√§√§r√§) ja koulutusn√§ytteiden m√§√§r√§n v√§lill√§.

## Miksi ylikoulutusta tapahtuu

  * Liian v√§h√§n koulutusdataa
  * Liian voimakas malli
  * Liikaa kohinaa sy√∂tedatassa

## Miten ylikoulutus havaitaan

Kuten yll√§ olevasta kaaviosta n√§kyy, ylikoulutus voidaan havaita eritt√§in pienest√§ koulutusvirheest√§ ja suuresta validointivirheest√§. Normaalisti koulutuksen aikana n√§emme sek√§ koulutus- ett√§ validointivirheiden alkavan pienenty√§, ja jossain vaiheessa validointivirhe saattaa lakata pienentym√§st√§ ja alkaa kasvaa. T√§m√§ on merkki ylikoulutuksesta ja indikaattori siit√§, ett√§ koulutus pit√§isi todenn√§k√∂isesti lopettaa t√§ss√§ vaiheessa (tai ainakin tehd√§ mallista tilannekuva).

![ylikoulutus](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.fi.png)

## Miten ylikoulutusta estet√§√§n

Jos huomaat, ett√§ ylikoulutusta tapahtuu, voit tehd√§ jonkin seuraavista:

 * Lis√§√§ koulutusdatan m√§√§r√§√§
 * V√§henn√§ mallin monimutkaisuutta
 * K√§yt√§ jotakin [regularisointitekniikkaa](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), kuten [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), jota k√§sittelemme my√∂hemmin.

## Ylikoulutus ja Bias-Variance Tradeoff

Ylikoulutus on itse asiassa tilanne, joka liittyy yleisemp√§√§n tilastolliseen ongelmaan nimelt√§ [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Jos tarkastelemme mallimme virhel√§hteit√§, voimme n√§hd√§ kahdenlaisia virheit√§:

* **Bias-virheet** johtuvat siit√§, ett√§ algoritmimme ei pysty oikein mallintamaan koulutusdatan v√§list√§ suhdetta. T√§m√§ voi johtua siit√§, ett√§ mallimme ei ole tarpeeksi voimakas (**alikoulutus**).
* **Variance-virheet**, jotka johtuvat siit√§, ett√§ malli mallintaa kohinaa sy√∂tedatassa merkityksellisen suhteen sijaan (**ylikoulutus**).

Koulutuksen aikana bias-virhe pienenee (kun mallimme oppii approksimoimaan dataa), ja variance-virhe kasvaa. On t√§rke√§√§ lopettaa koulutus - joko manuaalisesti (kun havaitsemme ylikoulutusta) tai automaattisesti (ottamalla k√§ytt√∂√∂n regularisointi) - ylikoulutuksen est√§miseksi.

## Yhteenveto

T√§ss√§ oppitunnissa opit kahden suosituimman AI-kehyksen, TensorFlow'n ja PyTorchin, eri API:iden erot. Lis√§ksi opit eritt√§in t√§rke√§st√§ aiheesta, ylikoulutuksesta.

## üöÄ Haaste

Liitetyiss√§ muistikirjoissa l√∂yd√§t 'teht√§vi√§' lopusta; k√§y l√§pi muistikirjat ja suorita teht√§v√§t.

## [Luennon j√§lkeinen kysely](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Kertaus & Itseopiskelu

Tutki seuraavia aiheita:

- TensorFlow
- PyTorch
- Ylikoulutus

Kysy itselt√§si seuraavat kysymykset:

- Mit√§ eroa on TensorFlow'lla ja PyTorchilla?
- Mit√§ eroa on ylikoulutuksella ja alikoulutuksella?

## [Teht√§v√§](lab/README.md)

T√§ss√§ laboratoriossa sinua pyydet√§√§n ratkaisemaan kaksi luokitteluongelmaa k√§ytt√§en yksikerroksisia ja monikerroksisia t√§ysin kytkettyj√§ verkkoja PyTorchilla tai TensorFlow'lla.

* [Ohjeet](lab/README.md)
* [Muistikirja](lab/LabFrameworks.ipynb)

---

