<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbd3f73e4139f030ecb2e20387d70fee",
  "translation_date": "2025-09-23T10:05:19+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "fi"
}
-->
# Tekstin esitt√§minen tensoreina

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Tekstin luokittelu

T√§m√§n osion ensimm√§isess√§ osassa keskitymme **tekstin luokitteluteht√§v√§√§n**. K√§yt√§mme [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) -datakokonaisuutta, joka sis√§lt√§√§ uutisartikkeleita, kuten seuraavan:

* Kategoria: Tiede/Teknologia
* Otsikko: Ky. Yhti√∂ voittaa apurahan peptidien tutkimiseen (AP)
* Teksti: AP - Kemian tutkijan perustama yritys Louisvillen yliopistossa voitti apurahan kehitt√§√§kseen...

Tavoitteenamme on luokitella uutinen yhteen kategorioista tekstin perusteella.

## Tekstin esitt√§minen

Jos haluamme ratkaista luonnollisen kielen k√§sittelyn (NLP) teht√§vi√§ neuroverkoilla, meid√§n t√§ytyy l√∂yt√§√§ tapa esitt√§√§ teksti tensoreina. Tietokoneet esitt√§v√§t tekstimerkit jo numeroina, jotka vastaavat n√§yt√∂ll√§ n√§kyvi√§ fontteja, k√§ytt√§en esimerkiksi ASCII- tai UTF-8-koodauksia.

<img alt="Kuva, joka n√§ytt√§√§ kaavion, jossa merkki muunnetaan ASCII- ja binaarimuotoon" src="images/ascii-character-map.png" width="50%"/>

> [Kuvan l√§hde](https://www.seobility.net/en/wiki/ASCII)

Ihmisen√§ ymm√§rr√§mme, mit√§ kukin kirjain **edustaa**, ja miten kaikki merkit muodostavat lauseen sanat. Tietokoneet eiv√§t kuitenkaan itsess√§√§n ymm√§rr√§ t√§t√§, ja neuroverkon t√§ytyy oppia merkitys koulutuksen aikana.

Siksi voimme k√§ytt√§√§ erilaisia l√§hestymistapoja tekstin esitt√§miseen:

* **Merkki-tason esitys**, jossa teksti esitet√§√§n k√§sittelem√§ll√§ jokaista merkki√§ numerona. Jos tekstikorpuksessamme on *C* erilaista merkki√§, sana *Hello* esitet√§√§n 5x*C* tensorina. Jokainen kirjain vastaa tensorin saraketta one-hot-koodauksessa.
* **Sana-tason esitys**, jossa luomme **sanaston** kaikista tekstimme sanoista ja esittelemme sanat one-hot-koodauksella. T√§m√§ l√§hestymistapa on jossain m√§√§rin parempi, koska yksitt√§isell√§ kirjaimella ei ole paljon merkityst√§, ja k√§ytt√§m√§ll√§ korkeampia semanttisia k√§sitteit√§ - sanoja - yksinkertaistamme teht√§v√§√§ neuroverkolle. Kuitenkin suuren sanaston koon vuoksi meid√§n t√§ytyy k√§sitell√§ korkeadimensionaalisia harvoja tensoreita.

Riippumatta esitystavasta, meid√§n t√§ytyy ensin muuntaa teksti **tokenien** sarjaksi, jossa yksi token voi olla merkki, sana tai joskus jopa osa sanaa. Sitten token muunnetaan numeroksi, yleens√§ **sanaston** avulla, ja t√§m√§ numero voidaan sy√∂tt√§√§ neuroverkkoon one-hot-koodauksen avulla.

## N-grammit

Luonnollisessa kieless√§ sanojen tarkka merkitys voidaan m√§√§ritt√§√§ vain kontekstissa. Esimerkiksi *neuroverkko* ja *kalastusverkko* tarkoittavat t√§ysin eri asioita. Yksi tapa ottaa t√§m√§ huomioon on rakentaa mallimme sanaparien perusteella ja k√§sitell√§ sanaparit erillisin√§ sanaston tokeneina. T√§ll√§ tavalla lause *I like to go fishing* esitet√§√§n seuraavilla tokeneilla: *I like*, *like to*, *to go*, *go fishing*. T√§m√§n l√§hestymistavan ongelma on, ett√§ sanaston koko kasvaa merkitt√§v√§sti, ja yhdistelm√§t kuten *go fishing* ja *go shopping* esitet√§√§n eri tokeneilla, jotka eiv√§t jaa semanttista samankaltaisuutta, vaikka niiss√§ on sama verbi.

Joissakin tapauksissa voimme harkita tri-grammien - kolmen sanan yhdistelmien - k√§ytt√∂√§. N√§in ollen l√§hestymistapaa kutsutaan usein **n-grammeiksi**. Lis√§ksi n-grammeja voi olla j√§rkev√§√§ k√§ytt√§√§ merkki-tason esityksess√§, jolloin n-grammit vastaavat suunnilleen eri tavujen yhdistelmi√§.

## Bag-of-Words ja TF/IDF

Kun ratkaistaan teht√§vi√§, kuten tekstin luokittelua, meid√§n t√§ytyy pysty√§ esitt√§m√§√§n teksti yhdell√§ kiinte√§n kokoisella vektorilla, jota k√§yt√§mme sy√∂tteen√§ lopulliselle tihe√§lle luokittelijalle. Yksi yksinkertaisimmista tavoista tehd√§ t√§m√§ on yhdist√§√§ kaikki yksitt√§iset sanan esitykset, esimerkiksi lis√§√§m√§ll√§ ne yhteen. Jos lis√§√§mme jokaisen sanan one-hot-koodaukset, p√§√§dymme frekvenssivektoriin, joka n√§ytt√§√§, kuinka monta kertaa kukin sana esiintyy tekstiss√§. T√§llainen tekstin esitys kutsutaan **bag-of-words** (BoW).

<img src="images/bow.png" width="90%"/>

> Kuva: kirjoittaja

BoW esitt√§√§, mitk√§ sanat esiintyv√§t tekstiss√§ ja miss√§ m√§√§rin, mik√§ voi olla hyv√§ indikaattori tekstin sis√§ll√∂st√§. Esimerkiksi poliittinen uutisartikkeli sis√§lt√§√§ todenn√§k√∂isesti sanoja kuten *presidentti* ja *maa*, kun taas tieteellinen julkaisu saattaa sis√§lt√§√§ sanoja kuten *t√∂rm√§ytin*, *l√∂ydetty*, jne. N√§in ollen sanan frekvenssit voivat monissa tapauksissa olla hyv√§ indikaattori tekstin sis√§ll√∂st√§.

BoW:n ongelma on, ett√§ tietyt yleiset sanat, kuten *ja*, *on*, jne., esiintyv√§t useimmissa teksteiss√§ ja niill√§ on korkeimmat frekvenssit, mik√§ peitt√§√§ alleen todella t√§rke√§t sanat. Voimme v√§hent√§√§ n√§iden sanojen merkityst√§ ottamalla huomioon, kuinka usein sanat esiintyv√§t koko dokumenttikokoelmassa. T√§m√§ on TF/IDF-l√§hestymistavan p√§√§idea, joka k√§sitell√§√§n tarkemmin t√§m√§n oppitunnin liitetyiss√§ muistikirjoissa.

Kuitenkaan mik√§√§n n√§ist√§ l√§hestymistavoista ei t√§ysin huomioi tekstin **semantiikkaa**. Tarvitsemme tehokkaampia neuroverkkopohjaisia malleja t√§h√§n, joita k√§sittelemme my√∂hemmin t√§ss√§ osiossa.

## ‚úçÔ∏è Harjoitukset: Tekstin esitt√§minen

Jatka oppimista seuraavissa muistikirjoissa:

* [Tekstin esitt√§minen PyTorchilla](TextRepresentationPyTorch.ipynb)
* [Tekstin esitt√§minen TensorFlow'lla](TextRepresentationTF.ipynb)

## Yhteenveto

T√§h√§n menness√§ olemme tutkineet tekniikoita, jotka voivat lis√§t√§ frekvenssipainotusta eri sanoille. Ne eiv√§t kuitenkaan pysty esitt√§m√§√§n merkityst√§ tai j√§rjestyst√§. Kuten kuuluisa kielitieteilij√§ J. R. Firth sanoi vuonna 1935: "Sanan t√§ydellinen merkitys on aina kontekstuaalinen, eik√§ merkityksen tutkimista ilman kontekstia voida ottaa vakavasti." Opimme my√∂hemmin kurssilla, kuinka tekstist√§ voidaan saada kontekstuaalista tietoa kielimallinnuksen avulla.

## üöÄ Haaste

Kokeile muita harjoituksia k√§ytt√§en bag-of-words-menetelm√§√§ ja erilaisia datamalleja. Voit saada inspiraatiota t√§st√§ [Kaggle-kilpailusta](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Kertaus ja itseopiskelu

Harjoittele taitojasi tekstin upotuksilla ja bag-of-words-tekniikoilla [Microsoft Learnissa](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Teht√§v√§: Muistikirjat](assignment.md)

---

