<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-28T20:08:38+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "fi"
}
-->
# Tekstin esitt√§minen tensoreina

## [Ennakkovisa](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Tekstin luokittelu

T√§m√§n osion ensimm√§isess√§ osassa keskitymme **tekstin luokitteluun**. K√§yt√§mme [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) -aineistoa, joka sis√§lt√§√§ uutisartikkeleita, kuten seuraavan:

* Kategoria: Tiede/Teknologia
* Otsikko: Ky. Yhti√∂ saa apurahan peptidien tutkimiseen (AP)
* Sis√§lt√∂: AP - Kemian tutkijan perustama yritys Louisvillen yliopistosta sai apurahan kehitt√§√§kseen...

Tavoitteenamme on luokitella uutinen tekstin perusteella yhteen kategorioista.

## Tekstin esitt√§minen

Jos haluamme ratkaista luonnollisen kielen k√§sittelyn (NLP) teht√§vi√§ neuroverkoilla, meid√§n on l√∂ydett√§v√§ tapa esitt√§√§ teksti tensoreina. Tietokoneet esitt√§v√§t tekstimerkit jo numeroina, jotka vastaavat n√§yt√∂ll√§ n√§kyvi√§ fontteja, k√§ytt√§en esimerkiksi ASCII- tai UTF-8-koodauksia.

<img alt="Kuva, joka n√§ytt√§√§ kaavion, jossa merkki muunnetaan ASCII- ja binaariesitykseksi" src="images/ascii-character-map.png" width="50%"/>

> [Kuvan l√§hde](https://www.seobility.net/en/wiki/ASCII)

Ihmisin√§ ymm√§rr√§mme, mit√§ kukin kirjain **tarkoittaa**, ja miten kaikki merkit muodostavat yhdess√§ lauseen sanat. Tietokoneet eiv√§t kuitenkaan itsess√§√§n ymm√§rr√§ t√§t√§, ja neuroverkon on opittava merkitys koulutuksen aikana.

Siksi voimme k√§ytt√§√§ erilaisia l√§hestymistapoja tekstin esitt√§miseen:

* **Merkkitason esitys**, jossa teksti esitet√§√§n k√§sittelem√§ll√§ jokaista merkki√§ numerona. Jos tekstikorpuksessamme on *C* erilaista merkki√§, sana *Hello* esitet√§√§n 5x*C*-tensorina. Jokainen kirjain vastaa tensorin saraketta one-hot-koodauksessa.
* **Sanatason esitys**, jossa luomme tekstimme kaikista sanoista **sanaston** ja esittelemme sanat one-hot-koodauksella. T√§m√§ l√§hestymistapa on jossain m√§√§rin parempi, koska yksitt√§isill√§ kirjaimilla ei ole paljon merkityst√§, ja k√§ytt√§m√§ll√§ korkeampia semanttisia k√§sitteit√§ - sanoja - yksinkertaistamme teht√§v√§√§ neuroverkolle. Kuitenkin suuren sanaston koon vuoksi joudumme k√§sittelem√§√§n korkean ulottuvuuden harvoja tensoreita.

Riippumatta esitystavasta, meid√§n on ensin muunnettava teksti **tokeneiden** jaksoksi, joissa yksi token voi olla merkki, sana tai joskus jopa osa sanaa. Sitten muunnetaan token numeroksi, tyypillisesti k√§ytt√§m√§ll√§ **sanastoa**, ja t√§m√§ numero voidaan sy√∂tt√§√§ neuroverkkoon one-hot-koodauksen avulla.

## N-grammit

Luonnollisessa kieless√§ sanojen tarkka merkitys voidaan m√§√§ritt√§√§ vain kontekstissa. Esimerkiksi *neuroverkko* ja *kalastusverkko* tarkoittavat t√§ysin eri asioita. Yksi tapa ottaa t√§m√§ huomioon on rakentaa mallimme sanapareille ja k√§sitell√§ sanapareja erillisin√§ sanaston tokeneina. N√§in lause *I like to go fishing* esitet√§√§n seuraavalla tokeneiden jaksolla: *I like*, *like to*, *to go*, *go fishing*. T√§m√§n l√§hestymistavan ongelmana on, ett√§ sanaston koko kasvaa merkitt√§v√§sti, ja yhdistelm√§t kuten *go fishing* ja *go shopping* esitet√§√§n eri tokeneina, joilla ei ole mit√§√§n semanttista yhteytt√§, vaikka niiss√§ on sama verbi.

Joissakin tapauksissa voimme harkita my√∂s tri-grammien - kolmen sanan yhdistelmien - k√§ytt√∂√§. T√§m√§n vuoksi l√§hestymistapaa kutsutaan usein **n-grammeiksi**. Lis√§ksi on j√§rkev√§√§ k√§ytt√§√§ n-grammeja merkkitason esityksess√§, jolloin n-grammit vastaavat suunnilleen eri tavujen yhdistelmi√§.

## Bag-of-Words ja TF/IDF

Kun ratkaistaan teht√§vi√§, kuten tekstin luokittelua, meid√§n on pystytt√§v√§ esitt√§m√§√§n teksti yhdell√§ kiinte√§n kokoisella vektorilla, jota k√§yt√§mme sy√∂tteen√§ lopulliselle tihe√§lle luokittelijalle. Yksi yksinkertaisimmista tavoista tehd√§ t√§m√§ on yhdist√§√§ kaikki yksitt√§iset sanan esitykset, esimerkiksi lis√§√§m√§ll√§ ne yhteen. Jos lis√§√§mme jokaisen sanan one-hot-koodaukset, saamme vektorin, joka n√§ytt√§√§, kuinka monta kertaa kukin sana esiintyy tekstiss√§. T√§llainen tekstin esitys tunnetaan nimell√§ **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Kuva kirjoittajan tekem√§

BoW esitt√§√§ k√§yt√§nn√∂ss√§, mitk√§ sanat esiintyv√§t tekstiss√§ ja miss√§ m√§√§rin, mik√§ voi olla hyv√§ osoitus tekstin sis√§ll√∂st√§. Esimerkiksi poliittinen uutisartikkeli sis√§lt√§√§ todenn√§k√∂isesti sanoja kuten *presidentti* ja *maa*, kun taas tieteellinen julkaisu saattaa sis√§lt√§√§ sanoja kuten *kiihdytin*, *l√∂ydetty* jne. N√§in ollen sanan esiintymistiheydet voivat monissa tapauksissa olla hyv√§ osoitus tekstin sis√§ll√∂st√§.

BoW:n ongelmana on, ett√§ tietyt yleiset sanat, kuten *ja*, *on* jne., esiintyv√§t useimmissa teksteiss√§ ja niill√§ on korkeimmat esiintymistiheydet, mik√§ peitt√§√§ alleen sanat, jotka ovat todella t√§rkeit√§. Voimme v√§hent√§√§ n√§iden sanojen merkityst√§ ottamalla huomioon, kuinka usein sanat esiintyv√§t koko dokumenttikokoelmassa. T√§m√§ on TF/IDF-l√§hestymistavan perusidea, jota k√§sitell√§√§n tarkemmin t√§m√§n oppitunnin liitteen√§ olevissa muistikirjoissa.

Mik√§√§n n√§ist√§ l√§hestymistavoista ei kuitenkaan pysty t√§ysin huomioimaan tekstin **semantiikkaa**. Tarvitsemme tehokkaampia neuroverkkopohjaisia malleja t√§m√§n saavuttamiseksi, ja n√§it√§ k√§sitell√§√§n my√∂hemmin t√§ss√§ osiossa.

## ‚úçÔ∏è Harjoitukset: Tekstin esitt√§minen

Jatka oppimista seuraavissa muistikirjoissa:

* [Tekstin esitt√§minen PyTorchilla](TextRepresentationPyTorch.ipynb)
* [Tekstin esitt√§minen TensorFlow'lla](TextRepresentationTF.ipynb)

## Yhteenveto

T√§h√§n menness√§ olemme tutkineet tekniikoita, jotka voivat lis√§t√§ painotusta eri sanojen esiintymistiheyksille. Ne eiv√§t kuitenkaan pysty esitt√§m√§√§n merkityst√§ tai j√§rjestyst√§. Kuten kuuluisa kielitieteilij√§ J. R. Firth sanoi vuonna 1935: "Sanan t√§ydellinen merkitys on aina kontekstuaalinen, eik√§ merkityksen tutkimista ilman kontekstia voida ottaa vakavasti." Kurssin my√∂hemmiss√§ osissa opimme, kuinka tekstist√§ voidaan vangita kontekstuaalista tietoa kielimallinnuksen avulla.

## üöÄ Haaste

Kokeile muita harjoituksia k√§ytt√§en bag-of-words-menetelm√§√§ ja erilaisia datamalleja. Voit saada inspiraatiota t√§st√§ [Kaggle-kilpailusta](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [J√§lkivisa](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Kertaus ja itseopiskelu

Harjoittele taitojasi tekstin upotusten ja bag-of-words-tekniikoiden parissa [Microsoft Learnissa](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Teht√§v√§: Muistikirjat](assignment.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.