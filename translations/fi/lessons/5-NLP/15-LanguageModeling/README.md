<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T10:03:10+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "fi"
}
-->
# Kielen mallintaminen

Semanttiset upotukset, kuten Word2Vec ja GloVe, ovat itse asiassa ensimm√§inen askel kohti **kielen mallintamista** - mallien luomista, jotka jollain tavalla *ymm√§rt√§v√§t* (tai *edustavat*) kielen luonnetta.

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Kielen mallintamisen p√§√§idea on niiden kouluttaminen merkitsem√§tt√∂mill√§ aineistoilla valvomattomalla tavalla. T√§m√§ on t√§rke√§√§, koska meill√§ on valtavia m√§√§ri√§ merkitsem√§t√∂nt√§ teksti√§ saatavilla, kun taas merkitty√§ teksti√§ on aina rajallisesti sen mukaan, kuinka paljon aikaa voimme k√§ytt√§√§ sen merkitsemiseen. Useimmiten voimme rakentaa kielimalleja, jotka voivat **ennustaa puuttuvia sanoja** tekstiss√§, koska on helppoa peitt√§√§ satunnainen sana tekstiss√§ ja k√§ytt√§√§ sit√§ harjoitusn√§ytteen√§.

## Upotusten kouluttaminen

Aiemmissa esimerkeiss√§mme k√§ytimme valmiiksi koulutettuja semanttisia upotuksia, mutta on mielenkiintoista n√§hd√§, miten n√§it√§ upotuksia voidaan kouluttaa. On olemassa useita mahdollisia ideoita, joita voidaan k√§ytt√§√§:

* **N-Gram**-kielen mallintaminen, jossa ennustamme sanan katsomalla N edellist√§ sanaa (N-grammi).
* **Continuous Bag-of-Words** (CBoW), jossa ennustamme keskimm√§isen sanan $W_0$ sanajonossa $W_{-N}$, ..., $W_N$.
* **Skip-gram**, jossa ennustamme joukon naapurisanoja {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} keskimm√§isest√§ sanasta $W_0$.

![kuva paperista, jossa k√§sitell√§√§n sanojen muuntamista vektoreiksi](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fi.png)

> Kuva [t√§st√§ paperista](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Esimerkkivihkot: CBoW-mallin kouluttaminen

Jatka oppimista seuraavien vihkojen avulla:

* [CBoW Word2Vecin kouluttaminen TensorFlow'lla](CBoW-TF.ipynb)
* [CBoW Word2Vecin kouluttaminen PyTorchilla](CBoW-PyTorch.ipynb)

## Yhteenveto

Edellisess√§ oppitunnissa n√§imme, ett√§ sanaupotukset toimivat kuin taikuus! Nyt tied√§mme, ett√§ sanaupotusten kouluttaminen ei ole kovin monimutkainen teht√§v√§, ja meid√§n pit√§isi pysty√§ kouluttamaan omia sanaupotuksia erityisalojen teksteille tarvittaessa.

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Kertaus ja itseopiskelu

* [Virallinen PyTorch-opas kielen mallintamisesta](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Virallinen TensorFlow-opas Word2Vec-mallin kouluttamisesta](https://www.TensorFlow.org/tutorials/text/word2vec).
* **gensim**-kehyst√§ k√§ytt√§m√§ll√§ yleisimpien upotusten kouluttaminen muutamalla koodirivill√§ on kuvattu [t√§ss√§ dokumentaatiossa](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Teht√§v√§: Skip-Gram-mallin kouluttaminen](lab/README.md)

Laboratoriossa haastamme sinut muokkaamaan t√§m√§n oppitunnin koodia Skip-Gram-mallin kouluttamiseksi CBoW:n sijaan. [Lue yksityiskohdat](lab/README.md)

---

