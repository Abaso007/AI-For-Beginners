<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-28T19:59:54+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "fi"
}
-->
# Kielen mallintaminen

Semanttiset upotukset, kuten Word2Vec ja GloVe, ovat itse asiassa ensimm√§inen askel kohti **kielen mallintamista** - mallien luomista, jotka jollain tavalla *ymm√§rt√§v√§t* (tai *edustavat*) kielen luonnetta.

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Kielen mallintamisen p√§√§idea on niiden kouluttaminen merkitsem√§tt√∂mill√§ aineistoilla valvomattomalla tavalla. T√§m√§ on t√§rke√§√§, koska meill√§ on valtavia m√§√§ri√§ merkitsem√§t√∂nt√§ teksti√§ saatavilla, kun taas merkitty√§ teksti√§ on aina rajallisesti sen mukaan, kuinka paljon aikaa voimme k√§ytt√§√§ sen merkitsemiseen. Useimmiten voimme rakentaa kielimalleja, jotka voivat **ennustaa puuttuvia sanoja** tekstiss√§, koska on helppoa peitt√§√§ satunnainen sana tekstiss√§ ja k√§ytt√§√§ sit√§ harjoitusn√§ytteen√§.

## Upotusten kouluttaminen

Aiemmissa esimerkeiss√§mme k√§ytimme valmiiksi koulutettuja semanttisia upotuksia, mutta on mielenkiintoista n√§hd√§, miten n√§it√§ upotuksia voidaan kouluttaa. On olemassa useita mahdollisia ideoita, joita voidaan k√§ytt√§√§:

* **N-Gram**-kielen mallintaminen, jossa ennustamme sanan katsomalla N edellist√§ sanaa (N-grammi)
* **Continuous Bag-of-Words** (CBoW), jossa ennustamme keskimm√§isen sanan $W_0$ sanajonossa $W_{-N}$, ..., $W_N$.
* **Skip-gram**, jossa ennustamme joukon naapurisanoja {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} keskimm√§isest√§ sanasta $W_0$.

![kuva paperista, jossa k√§sitell√§√§n sanojen muuntamista vektoreiksi](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fi.png)

> Kuva [t√§st√§ paperista](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Esimerkkivihkot: CBoW-mallin kouluttaminen

Jatka oppimistasi seuraavissa vihkoissa:

* [CBoW Word2Vecin kouluttaminen TensorFlow'lla](CBoW-TF.ipynb)
* [CBoW Word2Vecin kouluttaminen PyTorchilla](CBoW-PyTorch.ipynb)

## Yhteenveto

Edellisess√§ oppitunnissa n√§imme, ett√§ sanaupotukset toimivat kuin taikuutta! Nyt tied√§mme, ett√§ sanaupotusten kouluttaminen ei ole kovin monimutkainen teht√§v√§, ja meid√§n pit√§isi pysty√§ kouluttamaan omia sanaupotuksia erityisalojen teksteille tarvittaessa.

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Kertaus ja itseopiskelu

* [Virallinen PyTorch-opas kielen mallintamisesta](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Virallinen TensorFlow-opas Word2Vec-mallin kouluttamisesta](https://www.TensorFlow.org/tutorials/text/word2vec).
* **gensim**-kehyst√§ k√§ytt√§m√§ll√§ yleisimpien upotusten kouluttaminen muutamalla koodirivill√§ on kuvattu [t√§ss√§ dokumentaatiossa](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Teht√§v√§: Skip-Gram-mallin kouluttaminen](lab/README.md)

Laboratoriossa haastamme sinut muokkaamaan t√§m√§n oppitunnin koodia Skip-Gram-mallin kouluttamiseksi CBoW:n sijaan. [Lue lis√§tiedot](lab/README.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.