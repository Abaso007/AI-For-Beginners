<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-28T20:05:37+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "fi"
}
-->
# Toistuvat neuroverkot

## [Ennakkokysely](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

Aiemmissa osioissa olemme k√§ytt√§neet tekstin semanttisia esityksi√§ ja yksinkertaista lineaarista luokitinta upotusten p√§√§ll√§. T√§m√§ arkkitehtuuri pyrkii vangitsemaan sanojen yhdistetyn merkityksen lauseessa, mutta se ei ota huomioon sanojen **j√§rjestyst√§**, koska upotusten p√§√§lle tehty yhdist√§misoperaatio poistaa t√§m√§n tiedon alkuper√§isest√§ tekstist√§. Koska n√§m√§ mallit eiv√§t pysty mallintamaan sanojen j√§rjestyst√§, ne eiv√§t voi ratkaista monimutkaisempia tai ep√§selvempi√§ teht√§vi√§, kuten tekstin generointia tai kysymysten vastaamista.

Jotta voimme ymm√§rt√§√§ tekstin sekvenssin merkityksen, meid√§n t√§ytyy k√§ytt√§√§ toista neuroverkkoarkkitehtuuria, jota kutsutaan **toistuvaksi neuroverkoksi** eli RNN:ksi. RNN:ss√§ sy√∂t√§mme lauseen verkon l√§pi yksi symboli kerrallaan, ja verkko tuottaa jonkin **tilan**, jonka sy√∂t√§mme verkkoon uudelleen seuraavan symbolin kanssa.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.fi.png)

> Kuva: kirjoittaja

Kun sy√∂tteen√§ on tokenien sekvenssi X<sub>0</sub>,...,X<sub>n</sub>, RNN luo neuroverkkolohkojen sekvenssin ja kouluttaa t√§m√§n sekvenssin p√§√§st√§ p√§√§h√§n takaisinlevityksen avulla. Jokainen verkkolohko ottaa sy√∂tteen√§ parin (X<sub>i</sub>,S<sub>i</sub>) ja tuottaa tuloksena S<sub>i+1</sub>. Lopullinen tila S<sub>n</sub> (tai tulos Y<sub>n</sub>) sy√∂tet√§√§n lineaariseen luokittimeen tuloksen tuottamiseksi. Kaikilla verkkolohkoilla on samat painot, ja ne koulutetaan p√§√§st√§ p√§√§h√§n yhden takaisinlevityskierroksen avulla.

Koska tilavektorit S<sub>0</sub>,...,S<sub>n</sub> kulkevat verkon l√§pi, se pystyy oppimaan sanojen v√§lisi√§ sekventiaalisia riippuvuuksia. Esimerkiksi, kun sana *not* esiintyy jossain kohtaa sekvenssi√§, verkko voi oppia kumoamaan tiettyj√§ elementtej√§ tilavektorissa, mik√§ johtaa negaatioon.

> ‚úÖ Koska kaikkien RNN-lohkojen painot yll√§ olevassa kuvassa ovat jaettuja, sama kuva voidaan esitt√§√§ yhten√§ lohkona (oikealla) toistuvalla palautesilmukalla, joka sy√∂tt√§√§ verkon ulostulotilan takaisin sy√∂tteeksi.

## RNN-solun anatomia

Katsotaan, miten yksinkertainen RNN-solu on j√§rjestetty. Se ottaa sy√∂tteen√§ edellisen tilan S<sub>i-1</sub> ja nykyisen symbolin X<sub>i</sub>, ja sen t√§ytyy tuottaa ulostulotila S<sub>i</sub> (ja joskus olemme kiinnostuneita my√∂s jostain muusta ulostulosta Y<sub>i</sub>, kuten generatiivisissa verkoissa).

Yksinkertaisessa RNN-solussa on kaksi painomatriisia: yksi muuntaa sy√∂tesymbolin (kutsutaan sit√§ W:ksi) ja toinen muuntaa sy√∂tetilan (H). T√§ss√§ tapauksessa verkon ulostulo lasketaan kaavalla œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b), miss√§ œÉ on aktivointifunktio ja b on lis√§bias.

<img alt="RNN-solun anatomia" src="images/rnn-anatomy.png" width="50%"/>

> Kuva: kirjoittaja

Monissa tapauksissa sy√∂tetokenit kulkevat upotuskerroksen l√§pi ennen RNN:√§√§n sy√∂tt√§mist√§, jotta dimensioita voidaan pienent√§√§. T√§ss√§ tapauksessa, jos sy√∂tevektorien dimensio on *emb_size* ja tilavektorin dimensio on *hid_size*, W:n koko on *emb_size*√ó*hid_size* ja H:n koko on *hid_size*√ó*hid_size*.

## Pitk√§t lyhytaikaiset muistiverkot (LSTM)

Yksi klassisten RNN:ien suurimmista ongelmista on niin sanottu **h√§vi√§vien gradienttien** ongelma. Koska RNN:t koulutetaan p√§√§st√§ p√§√§h√§n yhdess√§ takaisinlevityskierroksessa, niill√§ on vaikeuksia virheen propagoinnissa verkon ensimm√§isiin kerroksiin, eik√§ verkko n√§in ollen pysty oppimaan kaukaisten tokenien v√§lisi√§ suhteita. Yksi tapa v√§ltt√§√§ t√§m√§ ongelma on ottaa k√§ytt√∂√∂n **eksplisiittinen tilanhallinta** k√§ytt√§m√§ll√§ niin sanottuja **portteja**. T√§llaisia arkkitehtuureja on kaksi tunnettua: **Pitk√§t lyhytaikaiset muistiverkot** (LSTM) ja **Gated Relay Unit** (GRU).

![Kuva esimerkki pitk√§n lyhytaikaisen muistisolun rakenteesta](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Kuval√§hde TBD

LSTM-verkko on j√§rjestetty samalla tavalla kuin RNN, mutta kerrosten v√§lill√§ kulkee kaksi tilaa: varsinainen tila C ja piilotettu vektori H. Jokaisessa yksik√∂ss√§ piilotettu vektori H<sub>i</sub> yhdistet√§√§n sy√∂tteeseen X<sub>i</sub>, ja ne ohjaavat, mit√§ tilassa C tapahtuu **porttien** avulla. Jokainen portti on neuroverkko, jossa on sigmoid-aktivointi (ulostulo v√§lill√§ [0,1]), ja sit√§ voidaan ajatella bittimaskina, kun se kerrotaan tilavektorilla. Kuvassa vasemmalta oikealle on seuraavat portit:

* **Unohtamisportti** ottaa piilotetun vektorin ja m√§√§ritt√§√§, mitk√§ komponentit vektorista C t√§ytyy unohtaa ja mitk√§ v√§litt√§√§ eteenp√§in.
* **Sy√∂tt√∂portti** ottaa tietoa sy√∂tteest√§ ja piilotetuista vektoreista ja lis√§√§ sen tilaan.
* **Ulostuloportti** muuntaa tilan lineaarisen kerroksen kautta, jossa on *tanh*-aktivointi, ja valitsee sen komponentteja piilotetun vektorin H<sub>i</sub> avulla tuottaakseen uuden tilan C<sub>i+1</sub>.

Tilan C komponentteja voidaan ajatella lippuina, jotka voidaan kytke√§ p√§√§lle ja pois. Esimerkiksi, kun kohtaamme nimen *Alice* sekvenssiss√§, voimme olettaa, ett√§ se viittaa naispuoliseen hahmoon, ja nostaa tilassa lipun, joka kertoo, ett√§ lauseessa on naispuolinen substantiivi. Kun my√∂hemmin kohtaamme fraasin *and Tom*, nostamme lipun, joka kertoo, ett√§ lauseessa on monikollinen substantiivi. N√§in manipuloimalla tilaa voimme mahdollisesti seurata lauseen osien kieliopillisia ominaisuuksia.

> ‚úÖ Erinomainen resurssi LSTM:n sis√§isen toiminnan ymm√§rt√§miseen on t√§m√§ loistava artikkeli [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) kirjoittanut Christopher Olah.

## Kaksisuuntaiset ja monikerroksiset RNN:t

Olemme k√§sitelleet toistuvia verkkoja, jotka toimivat yhteen suuntaan, sekvenssin alusta loppuun. T√§m√§ vaikuttaa luonnolliselta, koska se muistuttaa tapaa, jolla luemme ja kuuntelemme puhetta. Kuitenkin, koska monissa k√§yt√§nn√∂n tapauksissa meill√§ on satunnainen p√§√§sy sy√∂tteen sekvenssiin, voi olla j√§rkev√§√§ suorittaa toistuva laskenta molempiin suuntiin. T√§llaisia verkkoja kutsutaan **kaksisuuntaisiksi** RNN:iksi. Kaksisuuntaisessa verkossa tarvitsemme kaksi piilotettua tilavektoria, yhden kumpaankin suuntaan.

Toistuva verkko, joko yksisuuntainen tai kaksisuuntainen, vangitsee tiettyj√§ kuvioita sekvenssiss√§ ja voi tallentaa ne tilavektoriin tai v√§litt√§√§ ulostuloon. Kuten konvoluutioverkoissa, voimme rakentaa toisen toistuvan kerroksen ensimm√§isen p√§√§lle vangitaksemme korkeamman tason kuvioita ja rakentaa matalan tason kuvioista, jotka ensimm√§inen kerros on poiminut. T√§m√§ johtaa k√§sitteeseen **monikerroksinen RNN**, joka koostuu kahdesta tai useammasta toistuvasta verkosta, joissa edellisen kerroksen ulostulo sy√∂tet√§√§n seuraavaan kerrokseen.

![Kuva monikerroksisesta pitk√§n lyhytaikaisen muistiverkosta](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.fi.jpg)

*Kuva [t√§st√§ upeasta postauksesta](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) kirjoittanut Fernando L√≥pez*

## ‚úçÔ∏è Harjoitukset: Upotukset

Jatka oppimista seuraavissa muistikirjoissa:

* [RNN:t PyTorchilla](RNNPyTorch.ipynb)
* [RNN:t TensorFlow'lla](RNNTF.ipynb)

## Yhteenveto

T√§ss√§ osiossa olemme n√§hneet, ett√§ RNN:it√§ voidaan k√§ytt√§√§ sekvenssiluokitteluun, mutta itse asiassa ne voivat k√§sitell√§ monia muita teht√§vi√§, kuten tekstin generointia, konek√§√§nn√∂st√§ ja paljon muuta. Tarkastelemme n√§it√§ teht√§vi√§ seuraavassa osiossa.

## üöÄ Haaste

Lue kirjallisuutta LSTM:ist√§ ja pohdi niiden sovelluksia:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [J√§lkikysely](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## Kertaus ja itseopiskelu

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) kirjoittanut Christopher Olah.

## [Teht√§v√§: Muistikirjat](assignment.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.