<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-28T19:58:30+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "fi"
}
-->
# Generatiiviset verkot

## [Esiluentakysely](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Toistuvat neuroverkot (Recurrent Neural Networks, RNN) ja niiden porttimalliset solumuunnelmat, kuten Long Short Term Memory Cells (LSTM) ja Gated Recurrent Units (GRU), tarjoavat mekanismin kielen mallintamiseen, koska ne voivat oppia sanojen j√§rjestyksen ja ennustaa seuraavan sanan sekvenssiss√§. T√§m√§ mahdollistaa RNN:ien k√§yt√∂n **generatiivisissa teht√§viss√§**, kuten tavallisessa tekstin generoinnissa, konek√§√§nn√∂ksess√§ ja jopa kuvatekstien luomisessa.

> ‚úÖ Mieti kaikkia niit√§ kertoja, kun olet hy√∂tynyt generatiivisista teht√§vist√§, kuten tekstin t√§ydennyksest√§ kirjoittaessasi. Tutki suosikkisovelluksiasi ja selvit√§, ovatko ne hy√∂dynt√§neet RNN:ej√§.

Edellisess√§ yksik√∂ss√§ k√§sitellyss√§ RNN-arkkitehtuurissa jokainen RNN-yksikk√∂ tuotti seuraavan piilotilan ulostulona. Voimme kuitenkin lis√§t√§ toisen ulostulon jokaiseen toistuvaan yksikk√∂√∂n, mik√§ mahdollistaa **sekvenssin** tuottamisen (joka on yht√§ pitk√§ kuin alkuper√§inen sekvenssi). Lis√§ksi voimme k√§ytt√§√§ RNN-yksik√∂it√§, jotka eiv√§t ota sy√∂tett√§ jokaisessa vaiheessa, vaan ottavat vain alkuper√§isen tilavektorin ja tuottavat sitten ulostulon sekvenssin.

T√§m√§ mahdollistaa erilaiset neuroverkkoarkkitehtuurit, jotka on esitetty alla olevassa kuvassa:

![Kuva, joka n√§ytt√§√§ yleisi√§ toistuvien neuroverkkojen malleja.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.fi.jpg)

> Kuva blogikirjoituksesta [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) kirjoittanut [Andrej Karpaty](http://karpathy.github.io/)

* **Yksi-yhteen** on perinteinen neuroverkko, jossa on yksi sy√∂te ja yksi ulostulo
* **Yksi-moneen** on generatiivinen arkkitehtuuri, joka ottaa yhden sy√∂tearvon ja tuottaa sekvenssin ulostuloarvoja. Esimerkiksi, jos haluamme kouluttaa **kuvatekstien luontiverkon**, joka tuottaa tekstikuvauksen kuvasta, voimme sy√∂tt√§√§ kuvan, k√§sitell√§ sen CNN:ll√§ saadaksemme piilotilan ja k√§ytt√§√§ sitten toistuvaa ketjua tuottamaan kuvatekstin sana sanalta
* **Moneen-yksi** vastaa edellisess√§ yksik√∂ss√§ kuvattuja RNN-arkkitehtuureja, kuten tekstiluokittelua
* **Moneen-moneen**, tai **sekvenssi-sekvenssi**, vastaa teht√§vi√§, kuten **konek√§√§nn√∂s**, jossa ensimm√§inen RNN ker√§√§ kaiken tiedon sy√∂tesekvenssist√§ piilotilaan, ja toinen RNN-ketju purkaa t√§m√§n tilan ulostulosekvenssiksi.

T√§ss√§ yksik√∂ss√§ keskitymme yksinkertaisiin generatiivisiin malleihin, jotka auttavat meit√§ tuottamaan teksti√§. Yksinkertaisuuden vuoksi k√§yt√§mme merkkitason tokenisointia.

Koulutamme t√§m√§n RNN:n tuottamaan teksti√§ askel askeleelta. Jokaisessa vaiheessa otamme merkkisekvenssin, jonka pituus on `nchars`, ja pyyd√§mme verkkoa tuottamaan seuraavan ulostulomerkin jokaiselle sy√∂temerkille:

![Kuva, joka n√§ytt√§√§ esimerkin RNN:n sanan 'HELLO' generoinnista.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.fi.png)

Kun tuotamme teksti√§ (inference-vaiheessa), aloitamme jollakin **aloitussy√∂tteell√§**, joka sy√∂tet√§√§n RNN-soluihin tuottamaan niiden v√§limuisti, ja t√§st√§ tilasta alkaa generointi. Tuotamme yhden merkin kerrallaan ja sy√∂t√§mme tilan ja tuotetun merkin seuraavaan RNN-soluun tuottamaan seuraavan merkin, kunnes olemme tuottaneet tarpeeksi merkkej√§.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Kuva kirjoittajalta

## ‚úçÔ∏è Harjoitukset: Generatiiviset verkot

Jatka oppimista seuraavissa muistikirjoissa:

* [Generatiiviset verkot PyTorchilla](GenerativePyTorch.ipynb)
* [Generatiiviset verkot TensorFlow'lla](GenerativeTF.ipynb)

## Pehme√§ tekstin generointi ja l√§mp√∂tila

Jokaisen RNN-solun ulostulo on merkkien todenn√§k√∂isyysjakauma. Jos aina valitsemme merkin, jolla on korkein todenn√§k√∂isyys seuraavaksi merkiksi generoidussa tekstiss√§, teksti voi usein "j√§√§d√§ kiert√§m√§√§n" samoja merkkisekvenssej√§ uudelleen ja uudelleen, kuten t√§ss√§ esimerkiss√§:

```
today of the second the company and a second the company ...
```

Jos kuitenkin tarkastelemme seuraavan merkin todenn√§k√∂isyysjakaumaa, voi olla, ett√§ muutaman korkeimman todenn√§k√∂isyyden ero ei ole suuri, esimerkiksi yksi merkki voi saada todenn√§k√∂isyyden 0,2 ja toinen 0,19. Esimerkiksi, kun etsit√§√§n seuraavaa merkki√§ sekvenssiss√§ '*play*', seuraava merkki voi yht√§ hyvin olla joko v√§lily√∂nti tai **e** (kuten sanassa *player*).

T√§m√§ johtaa siihen johtop√§√§t√∂kseen, ett√§ ei ole aina "oikeudenmukaista" valita merkki√§, jolla on korkein todenn√§k√∂isyys, koska toisen korkeimman valitseminen voi silti johtaa mielekk√§√§seen tekstiin. On viisaampaa **n√§ytteist√§√§** merkkej√§ verkon ulostulon antamasta todenn√§k√∂isyysjakaumasta. Voimme my√∂s k√§ytt√§√§ parametria, **l√§mp√∂tila**, joka tasoittaa todenn√§k√∂isyysjakaumaa, jos haluamme lis√§t√§ satunnaisuutta, tai tekee siit√§ jyrkemm√§n, jos haluamme pit√§yty√§ enemm√§n korkeimman todenn√§k√∂isyyden merkeiss√§.

Tutki, miten t√§m√§ pehme√§ tekstin generointi on toteutettu yll√§ linkitetyiss√§ muistikirjoissa.

## Yhteenveto

Vaikka tekstin generointi voi olla hy√∂dyllist√§ itsess√§√§n, suurimmat hy√∂dyt tulevat kyvyst√§ tuottaa teksti√§ RNN:ien avulla jostakin alkuper√§isest√§ piirrevektorista. Esimerkiksi tekstin generointia k√§ytet√§√§n osana konek√§√§nn√∂st√§ (sekvenssi-sekvenssi, t√§ss√§ tapauksessa *enkooderin* tilavektoria k√§ytet√§√§n tuottamaan tai *dekoodaamaan* k√§√§nnetty viesti) tai kuvan tekstikuvauksen luomista (jolloin piirrevektori tulee CNN-uutteesta).

## üöÄ Haaste

Ota oppitunteja Microsoft Learn -alustalta t√§st√§ aiheesta

* Tekstin generointi [PyTorchilla](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow'lla](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [J√§lkiluentakysely](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Kertaus ja itseopiskelu

T√§ss√§ on joitakin artikkeleita tiet√§myksesi laajentamiseksi

* Eri l√§hestymistapoja tekstin generointiin Markovin ketjuilla, LSTM:ll√§ ja GPT-2:lla: [blogikirjoitus](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Tekstin generoinnin esimerkki [Keras-dokumentaatiossa](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Teht√§v√§](lab/README.md)

Olemme n√§hneet, kuinka teksti√§ voidaan generoida merkki kerrallaan. Laboratoriossa tutkit sanatasoista tekstin generointia.

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.