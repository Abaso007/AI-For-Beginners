{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huomiomekanismit ja transformerit\n",
    "\n",
    "Yksi toistoverkkojen suurimmista heikkouksista on, että kaikki sanat sekvenssissä vaikuttavat tulokseen samalla tavalla. Tämä johtaa heikkoon suorituskykyyn tavanomaisilla LSTM-enkooderi-dekooderi-malleilla sekvenssistä sekvenssiin -tehtävissä, kuten nimettyjen entiteettien tunnistuksessa ja konekäännöksessä. Todellisuudessa tietyillä syötteen sanoilla on usein suurempi vaikutus peräkkäisiin tulosteisiin kuin toisilla.\n",
    "\n",
    "Ajatellaan sekvenssistä sekvenssiin -mallia, kuten konekäännöstä. Se toteutetaan kahdella toistoverkolla, joissa yksi verkko (**enkooderi**) tiivistää syötteen piilotilaan ja toinen verkko (**dekooderi**) purkaa tämän piilotilan käännetyksi tulokseksi. Tämän lähestymistavan ongelmana on, että verkon lopputilalla on vaikeuksia muistaa lauseen alkua, mikä heikentää mallin laatua pitkissä lauseissa.\n",
    "\n",
    "**Huomiomekanismit** tarjoavat tavan painottaa kunkin syötevektorin kontekstuaalista vaikutusta RNN:n jokaisessa tulosennusteessa. Tämä toteutetaan luomalla oikopolkuja syötteen RNN:n välitilojen ja tulos-RNN:n välille. Näin ollen, kun tuotetaan tulossymbolia $y_t$, otamme huomioon kaikki syötteen piilotilat $h_i$, eri painokertoimilla $\\alpha_{t,i}$. \n",
    "\n",
    "![Kuva, joka näyttää enkooderi/dekooderi-mallin additiivisella huomiokerroksella](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.fi.png)\n",
    "*Enkooderi-dekooderi-malli additiivisella huomiomekanismilla [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), lainattu [tästä blogikirjoituksesta](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Huomiomatriisi $\\{\\alpha_{i,j}\\}$ edustaa sitä, kuinka paljon tietyt syötteen sanat vaikuttavat tietyn sanan muodostumiseen tulossekvenssissä. Alla on esimerkki tällaisesta matriisista:\n",
    "\n",
    "![Kuva, joka näyttää esimerkkikohdistuksen RNNsearch-50:llä, otettu Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.fi.png)\n",
    "\n",
    "*Kuva otettu [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Kuva 3)*\n",
    "\n",
    "Huomiomekanismit ovat vastuussa suuresta osasta nykyistä tai lähes nykyistä huipputasoa luonnollisen kielen käsittelyssä. Huomion lisääminen kuitenkin kasvattaa merkittävästi mallin parametrien määrää, mikä aiheutti skaalausongelmia RNN:ien kanssa. RNN:ien skaalaamisen keskeinen rajoite on, että mallien toistuva luonne tekee koulutuksen eräajosta ja rinnakkaistamisesta haastavaa. RNN:ssä jokainen sekvenssin elementti täytyy käsitellä järjestyksessä, mikä tarkoittaa, ettei sitä voida helposti rinnakkaistaa.\n",
    "\n",
    "Huomiomekanismien käyttöönotto yhdessä tämän rajoitteen kanssa johti nykyisten huipputason Transformer-mallien luomiseen, joita käytämme tänään, kuten BERT ja OpenGPT3.\n",
    "\n",
    "## Transformer-mallit\n",
    "\n",
    "Sen sijaan, että jokaisen edellisen ennusteen konteksti välitettäisiin seuraavaan arviointivaiheeseen, **transformer-mallit** käyttävät **paikkakoodauksia** ja huomiota tallentaakseen annetun syötteen kontekstin annetussa tekstin ikkunassa. Alla oleva kuva näyttää, kuinka paikkakoodaukset ja huomio voivat tallentaa kontekstin annetussa ikkunassa.\n",
    "\n",
    "![Animoitu GIF, joka näyttää, kuinka arvioinnit suoritetaan transformer-malleissa.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Koska jokainen syötteen sijainti kartoitetaan itsenäisesti jokaiseen tulosteen sijaintiin, transformerit voivat rinnakkaistaa paremmin kuin RNN:t, mikä mahdollistaa paljon suuremmat ja ilmaisukykyisemmät kielimallit. Jokainen huomiointipää voi oppia erilaisia suhteita sanojen välillä, mikä parantaa luonnollisen kielen käsittelyn tehtäviä.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) on erittäin suuri monikerroksinen transformer-verkko, jossa on 12 kerrosta *BERT-base*-mallissa ja 24 kerrosta *BERT-large*-mallissa. Malli esikoulutetaan ensin suurella tekstikorpuksella (Wikipedia + kirjat) käyttämällä valvomattua koulutusta (ennustamalla peitettyjä sanoja lauseessa). Esikoulutuksen aikana malli omaksuu merkittävän määrän kielen ymmärrystä, jota voidaan hyödyntää muilla aineistoilla hienosäädön avulla. Tätä prosessia kutsutaan **siirto-oppimiseksi**. \n",
    "\n",
    "![Kuva osoitteesta http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.fi.png)\n",
    "\n",
    "Transformer-arkkitehtuureista on monia variaatioita, kuten BERT, DistilBERT, BigBird, OpenGPT3 ja muita, joita voidaan hienosäätää. [HuggingFace-paketti](https://github.com/huggingface/) tarjoaa kirjaston monien näiden arkkitehtuurien kouluttamiseen PyTorchilla. \n",
    "\n",
    "## BERT:n käyttäminen tekstiluokitteluun\n",
    "\n",
    "Katsotaanpa, kuinka voimme käyttää esikoulutettua BERT-mallia perinteisen tehtävämme ratkaisemiseen: sekvenssiluokitteluun. Luokittelemme alkuperäisen AG News -aineistomme.\n",
    "\n",
    "Ensiksi ladataan HuggingFace-kirjasto ja aineistomme:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koska käytämme valmiiksi koulutettua BERT-mallia, meidän täytyy käyttää sille tarkoitettua tokenisoijaa. Ensiksi lataamme tokenisoijan, joka liittyy valmiiksi koulutettuun BERT-malliin.\n",
    "\n",
    "HuggingFace-kirjasto sisältää valmiiksi koulutettujen mallien arkiston, jota voit käyttää yksinkertaisesti määrittämällä mallin nimen `from_pretrained`-funktioiden argumenttina. Kaikki mallin tarvitsemat binääritiedostot ladataan automaattisesti.\n",
    "\n",
    "Joissain tilanteissa saatat kuitenkin joutua lataamaan omia mallejasi. Tällöin voit määrittää hakemiston, joka sisältää kaikki tarvittavat tiedostot, kuten tokenisoijan parametrit, `config.json`-tiedoston mallin parametreilla, binääripainot jne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer`-objekti sisältää `encode`-funktion, jota voidaan käyttää suoraan tekstin koodaamiseen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sitten luodaan iteraattorit, joita käytämme koulutuksen aikana datan käsittelyyn. Koska BERT käyttää omaa koodausfunktiotaan, meidän täytyy määritellä täyttöfunktio, joka on samanlainen kuin aiemmin määrittelemämme `padify`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tapauksessamme käytämme valmiiksi koulutettua BERT-mallia nimeltä `bert-base-uncased`. Ladataan malli käyttämällä `BertForSequenceClassification`-pakettia. Tämä varmistaa, että mallillamme on jo tarvittava luokitteluun tarkoitettu arkkitehtuuri, mukaan lukien lopullinen luokitin. Näet varoitusviestin, joka ilmoittaa, että lopullisen luokittimen painot eivät ole alustettuja, ja malli vaatisi esikoulutusta - tämä on täysin normaalia, sillä juuri sitä olemme tekemässä!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt olemme valmiita aloittamaan koulutuksen! Koska BERT on jo valmiiksi esikoulutettu, haluamme aloittaa melko pienellä oppimisnopeudella, jotta emme tuhoa alkuperäisiä painoja.\n",
    "\n",
    "Kaiken raskaan työn hoitaa `BertForSequenceClassification`-malli. Kun kutsumme mallia koulutusdatan kanssa, se palauttaa sekä häviön että verkon ulostulon syöteminierälle. Käytämme häviötä parametrien optimointiin (`loss.backward()` suorittaa taaksepäin kulkevan vaiheen) ja `out`-arvoa koulutustarkkuuden laskemiseen vertaamalla saatuja luokkia `labs` (laskettu käyttäen `argmax`) odotettuihin `labels`-arvoihin.\n",
    "\n",
    "Prosessin hallitsemiseksi keräämme häviön ja tarkkuuden useiden iteraatioiden aikana ja tulostamme ne jokaisen `report_freq`-koulutussyklin jälkeen.\n",
    "\n",
    "Tämä koulutusprosessi kestää todennäköisesti melko kauan, joten rajoitamme iteraatioiden määrää.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voit huomata (varsinkin, jos lisäät iteraatioiden määrää ja odotat tarpeeksi kauan), että BERT-luokittelu antaa meille varsin hyvän tarkkuuden! Tämä johtuu siitä, että BERT ymmärtää jo valmiiksi kielen rakenteen melko hyvin, ja meidän tarvitsee vain hienosäätää lopullista luokittelijaa. Kuitenkin, koska BERT on suuri malli, koko koulutusprosessi vie paljon aikaa ja vaatii huomattavaa laskentatehoa! (GPU, ja mieluiten useampi kuin yksi).\n",
    "\n",
    "> **Huom:** Esimerkissämme olemme käyttäneet yhtä pienimmistä valmiiksi koulutetuista BERT-malleista. On olemassa suurempia malleja, jotka todennäköisesti tuottavat parempia tuloksia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mallin suorituskyvyn arviointi\n",
    "\n",
    "Nyt voimme arvioida mallimme suorituskykyä testidatalla. Arviointisilmukka on hyvin samanlainen kuin harjoitussilmukka, mutta meidän ei pidä unohtaa vaihtaa mallia arviointitilaan kutsumalla `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tärkeimmät asiat\n",
    "\n",
    "Tässä osiossa olemme nähneet, kuinka helppoa on ottaa valmiiksi koulutettu kielimalli **transformers**-kirjastosta ja mukauttaa se tekstiluokittelutehtäväämme. Samalla tavalla BERT-malleja voidaan käyttää entiteettien tunnistamiseen, kysymyksiin vastaamiseen ja muihin NLP-tehtäviin.\n",
    "\n",
    "Transformer-mallit edustavat NLP:n nykyistä huipputasoa, ja useimmissa tapauksissa niiden pitäisi olla ensimmäinen ratkaisu, jota kokeilet, kun toteutat räätälöityjä NLP-ratkaisuja. Kuitenkin tämän moduulin käsittelemien toistuvien neuroverkkojen perusperiaatteiden ymmärtäminen on äärimmäisen tärkeää, jos haluat rakentaa edistyneitä neuroverkkomalleja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Vastuuvapauslauseke**:  \nTämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäinen asiakirja sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T21:38:33+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "fi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}