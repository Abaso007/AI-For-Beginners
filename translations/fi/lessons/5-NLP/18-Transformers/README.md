<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f335dfcb4a993920504c387973a36957",
  "translation_date": "2025-09-23T10:03:21+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "fi"
}
-->
# Huomiomekanismit ja Transformerit

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Yksi NLP:n t√§rkeimmist√§ ongelmista on **konek√§√§nn√∂s**, olennainen teht√§v√§, joka on pohjana esimerkiksi Google K√§√§nt√§j√§lle. T√§ss√§ osiossa keskitymme konek√§√§nn√∂kseen tai yleisemmin mihin tahansa *sekvenssi-sekvenssi*-teht√§v√§√§n (jota kutsutaan my√∂s **lauseen muunnokseksi**).

RNN:ien avulla sekvenssi-sekvenssi toteutetaan kahdella toistuvalla verkolla, joissa yksi verkko, **enkooderi**, tiivist√§√§ sy√∂tteen piilotilaan, kun taas toinen verkko, **dekooderi**, purkaa t√§m√§n piilotilan k√§√§nnetyksi tulokseksi. T√§ss√§ l√§hestymistavassa on kuitenkin muutamia ongelmia:

* Enkooderin verkon lopputila ei muista hyvin lauseen alkua, mik√§ heikent√§√§ mallin laatua pitkien lauseiden kohdalla.
* Kaikilla sekvenssin sanoilla on sama vaikutus tulokseen. Todellisuudessa tietyill√§ sanoilla sy√∂tteess√§ on usein suurempi vaikutus kuin toisilla.

**Huomiomekanismit** tarjoavat tavan painottaa kunkin sy√∂tevektorin kontekstuaalista vaikutusta RNN:n kunkin ennusteen kohdalla. T√§m√§ toteutetaan luomalla oikoteit√§ sy√∂tteen RNN:n v√§litilojen ja tuloksen RNN:n v√§lille. N√§in ollen, kun tuotetaan ulostulosymbolia y<sub>t</sub>, otamme huomioon kaikki sy√∂tteen piilotilat h<sub>i</sub>, eri painokertoimilla &alpha;<sub>t,i</sub>.

![Kuva, joka n√§ytt√§√§ enkooderi/dekooderi-mallin additiivisella huomiokerroksella](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.fi.png)

> Enkooderi-dekooderi-malli additiivisella huomiomekanismilla [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), lainattu [t√§st√§ blogikirjoituksesta](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Huomiomatriisi {&alpha;<sub>i,j</sub>} edustaa sit√§, kuinka paljon tietyt sy√∂tteen sanat vaikuttavat tietyn sanan tuottamiseen ulostulosekvenssiss√§. Alla on esimerkki t√§llaisesta matriisista:

![Kuva, joka n√§ytt√§√§ esimerkkikohdistuksen RNNsearch-50:ll√§, otettu Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.fi.png)

> Kuva [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Huomiomekanismit ovat vastuussa monista nykyisist√§ tai l√§hes nykyisist√§ huipputason NLP-malleista. Huomion lis√§√§minen kuitenkin kasvattaa merkitt√§v√§sti mallin parametrien m√§√§r√§√§, mik√§ johti RNN:ien skaalausongelmiin. RNN:ien keskeinen rajoite on, ett√§ mallien toistuva luonne tekee koulutuksen er√§ajosta ja rinnakkaistamisesta haastavaa. RNN:ss√§ jokainen sekvenssin elementti t√§ytyy k√§sitell√§ j√§rjestyksess√§, mik√§ tarkoittaa, ett√§ sit√§ ei voi helposti rinnakkaistaa.

![Enkooderi Dekooderi Huomiolla](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Kuva [Googlen blogista](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Huomiomekanismien k√§ytt√∂√∂notto yhdess√§ t√§m√§n rajoitteen kanssa johti nykyisten huipputason Transformer-mallien luomiseen, kuten BERT ja Open-GPT3.

## Transformer-mallit

Yksi transformereiden keskeisist√§ ideoista on v√§ltt√§√§ RNN:ien sekventiaalinen luonne ja luoda malli, joka on rinnakkaistettavissa koulutuksen aikana. T√§m√§ saavutetaan kahdella idealla:

* positionaalinen koodaus
* itsehuomiomekanismin k√§ytt√∂ kuvioiden tunnistamiseen RNN:ien (tai CNN:ien) sijaan (siksi paperi, joka esittelee transformerit, on nimelt√§√§n *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Positionaalinen koodaus/Upotus

Positionaalisen koodauksen idea on seuraava. 
1. RNN:ien k√§yt√∂ss√§ tokenien suhteellinen sijainti edustetaan askelten m√§√§r√§ll√§, eik√§ sit√§ tarvitse eksplisiittisesti esitt√§√§. 
2. Kun siirryt√§√§n huomioon, tokenien suhteelliset sijainnit sekvenssiss√§ t√§ytyy tiet√§√§. 
3. Positionaalisen koodauksen saamiseksi t√§ydenn√§mme token-sekvenssi√§ sekvenssin token-sijaintien sekvenssill√§ (esim. numerot 0,1, ...).
4. Sekoitetaan token-sijainti tokenin upotusvektoriin. Position (kokonaisluku) voidaan muuntaa vektoriksi eri tavoilla:

* Koulutettava upotus, kuten token-upotus. T√§m√§ on l√§hestymistapa, jota tarkastelemme t√§ss√§. Sovellamme upotuskerroksia sek√§ tokeneihin ett√§ niiden sijainteihin, jolloin saadaan samankokoiset upotusvektorit, jotka sitten lis√§t√§√§n yhteen.
* Kiinte√§ positionaalinen koodausfunktio, kuten alkuper√§isess√§ paperissa ehdotettiin.

<img src="images/pos-embedding.png" width="50%"/>

> Kuva kirjoittajalta

Tuloksena saadaan positionaalinen upotus, joka upottaa sek√§ alkuper√§isen tokenin ett√§ sen sijainnin sekvenssiss√§.

### Monip√§inen itsehuomio

Seuraavaksi t√§ytyy tunnistaa kuvioita sekvenssiss√§. T√§t√§ varten transformerit k√§ytt√§v√§t **itsehuomiomekanismia**, joka on k√§yt√§nn√∂ss√§ huomio, joka kohdistetaan samaan sekvenssiin sy√∂tteen√§ ja tuloksena. Itsehuomion soveltaminen mahdollistaa **kontekstin** huomioimisen lauseessa ja sen, mitk√§ sanat liittyv√§t toisiinsa. Esimerkiksi se auttaa tunnistamaan, mihin sanat kuten *se* viittaavat, ja ottaa kontekstin huomioon:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.fi.png)

> Kuva [Googlen blogista](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Transformereissa k√§ytet√§√§n **monip√§ist√§ huomiota**, jotta verkko pystyy tunnistamaan useita erilaisia riippuvuuksia, kuten pitk√§aikaisia vs. lyhytaikaisia sanasuhteita, korrelaatioita vs. jotain muuta jne.

[TensorFlow Notebook](TransformersTF.ipynb) sis√§lt√§√§ lis√§√§ yksityiskohtia transformer-kerrosten toteutuksesta.

### Enkooderi-Dekooderi Huomio

Transformereissa huomiota k√§ytet√§√§n kahdessa paikassa:

* Kuvioiden tunnistamiseen sy√∂tetekstiss√§ itsehuomion avulla
* Sekvenssien k√§√§nt√§miseen - se on huomio-kerros enkooderin ja dekooderin v√§lill√§.

Enkooderi-dekooderi huomio on hyvin samanlainen kuin RNN:ien huomiomekanismi, kuten t√§m√§n osion alussa kuvattiin. T√§m√§ animoitu diagrammi selitt√§√§ enkooderi-dekooderi huomion roolin.

![Animoitu GIF, joka n√§ytt√§√§, miten arvioinnit suoritetaan transformer-malleissa.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Koska jokainen sy√∂tteen sijainti kartoitetaan itsen√§isesti jokaiseen tuloksen sijaintiin, transformerit voivat rinnakkaistaa paremmin kuin RNN:t, mik√§ mahdollistaa paljon suuremmat ja ilmaisukykyisemm√§t kielimallit. Jokainen huomiop√§√§ voidaan k√§ytt√§√§ oppimaan erilaisia sanasuhteita, jotka parantavat NLP-teht√§vi√§.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) on eritt√§in suuri monikerroksinen transformer-verkko, jossa on 12 kerrosta *BERT-base*-mallissa ja 24 kerrosta *BERT-large*-mallissa. Malli esikoulutetaan ensin suurella tekstikorpuksella (Wikipedia + kirjat) k√§ytt√§m√§ll√§ valvomatonta koulutusta (ennustamalla peitettyj√§ sanoja lauseessa). Esikoulutuksen aikana malli omaksuu merkitt√§v√§n m√§√§r√§n kielen ymm√§rryst√§, jota voidaan hy√∂dynt√§√§ muilla aineistoilla hienos√§√§d√∂n avulla. T√§t√§ prosessia kutsutaan **siirto-oppimiseksi**.

![kuva osoitteesta http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.fi.png)

> Kuva [l√§hde](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Harjoitukset: Transformerit

Jatka oppimista seuraavissa muistikirjoissa:

* [Transformerit PyTorchilla](TransformersPyTorch.ipynb)
* [Transformerit TensorFlow'lla](TransformersTF.ipynb)

## Yhteenveto

T√§ss√§ oppitunnissa opit Transformereista ja Huomiomekanismeista, jotka ovat olennaisia ty√∂kaluja NLP:n ty√∂kalupakissa. Transformer-arkkitehtuureista on monia variaatioita, kuten BERT, DistilBERT, BigBird, OpenGPT3 ja muita, joita voidaan hienos√§√§t√§√§. [HuggingFace-paketti](https://github.com/huggingface/) tarjoaa alustan monien n√§iden arkkitehtuurien kouluttamiseen sek√§ PyTorchilla ett√§ TensorFlow'lla.

## üöÄ Haaste

## [J√§lkikysely](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Kertaus & Itseopiskelu

* [Blogikirjoitus](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), joka selitt√§√§ klassisen [Attention is all you need](https://arxiv.org/abs/1706.03762) -paperin transformereista.
* [Blogikirjoitusten sarja](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) transformereista, joka selitt√§√§ arkkitehtuurin yksityiskohtaisesti.

## [Teht√§v√§](assignment.md)

---

