<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-28T20:00:57+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "fi"
}
-->
# Huomiomekanismit ja Transformerit

## [Ennakkovisa](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Yksi NLP:n t√§rkeimmist√§ ongelmista on **konek√§√§nn√∂s**, joka on olennainen teht√§v√§ esimerkiksi Google K√§√§nt√§j√§n kaltaisille ty√∂kaluille. T√§ss√§ osiossa keskitymme konek√§√§nn√∂kseen tai yleisemmin mihin tahansa *sekvenssist√§ sekvenssiin* -teht√§v√§√§n (jota kutsutaan my√∂s **lauseen muunnokseksi**).

RNN:ien avulla sekvenssist√§ sekvenssiin -teht√§v√§ toteutetaan kahdella rekurrentilla verkolla, joissa toinen verkko, **enkooderi**, tiivist√§√§ sy√∂tesekvenssin piilotilaan, kun taas toinen verkko, **dekooderi**, purkaa t√§m√§n piilotilan k√§√§nnetyksi tulokseksi. T√§ss√§ l√§hestymistavassa on kuitenkin muutamia ongelmia:

* Enkooderin verkon lopputila ei muista hyvin lauseen alkua, mik√§ heikent√§√§ mallin laatua pitkien lauseiden kohdalla.
* Kaikilla sekvenssin sanoilla on sama vaikutus tulokseen. Todellisuudessa kuitenkin tietyill√§ sanoilla sy√∂tesekvenssiss√§ on usein suurempi vaikutus kuin toisilla.

**Huomiomekanismit** tarjoavat keinon painottaa kunkin sy√∂tevektorin kontekstuaalista vaikutusta RNN:n kunkin ennusteen kohdalla. T√§m√§ toteutetaan luomalla oikopolkuja sy√∂te-RNN:n v√§litilojen ja tuloste-RNN:n v√§lille. N√§in ollen, kun tuotetaan tulostesymbolia y<sub>t</sub>, otamme huomioon kaikki sy√∂tteen piilotilat h<sub>i</sub>, eri painokertoimilla Œ±<sub>t,i</sub>.

![Kuva, joka n√§ytt√§√§ enkooderi-dekooderi-mallin additiivisella huomiokerroksella](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.fi.png)

> Enkooderi-dekooderi-malli additiivisella huomiomekanismilla [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), lainattu [t√§st√§ blogikirjoituksesta](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Huomiomatriisi {Œ±<sub>i,j</sub>} edustaa sit√§, kuinka paljon tietyt sy√∂tesanat vaikuttavat tietyn sanan muodostumiseen tulostesekvenssiss√§. Alla on esimerkki t√§llaisesta matriisista:

![Kuva, joka n√§ytt√§√§ esimerkkikohdistuksen RNNsearch-50:ll√§, otettu Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.fi.png)

> Kuva [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Kuva 3)

Huomiomekanismit ovat vastuussa monista nykyisist√§ tai l√§hes nykyisist√§ huipputuloksista NLP:ss√§. Huomion lis√§√§minen kuitenkin kasvattaa huomattavasti mallin parametrien m√§√§r√§√§, mik√§ aiheutti skaalausongelmia RNN:ien kanssa. Yksi RNN:ien skaalaamisen keskeisist√§ rajoitteista on, ett√§ mallien rekurrentti luonne tekee koulutuksen er√§ajosta ja rinnakkaistamisesta haastavaa. RNN:ss√§ jokainen sekvenssin elementti t√§ytyy k√§sitell√§ j√§rjestyksess√§, mik√§ est√§√§ helpon rinnakkaistamisen.

![Enkooderi-dekooderi huomiolla](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Kuva [Googlen blogista](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Huomiomekanismien k√§ytt√∂√∂notto yhdess√§ t√§m√§n rajoitteen kanssa johti nykyisten huipputason Transformer-mallien, kuten BERT ja Open-GPT3, kehitt√§miseen.

## Transformer-mallit

Yksi transformereiden keskeisist√§ ideoista on v√§ltt√§√§ RNN:ien sekventiaalinen luonne ja luoda malli, joka on rinnakkaistettavissa koulutuksen aikana. T√§m√§ saavutetaan kahdella idealla:

* positionaalinen koodaus
* itsereferenssihuomion k√§ytt√∂ mallintamaan kuvioita RNN:ien (tai CNN:ien) sijaan (t√§st√§ syyst√§ transformereita esittelev√§ artikkeli on nimelt√§√§n *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Positionaalinen koodaus/embedding

Positionaalisen koodauksen idea on seuraava:  
1. RNN:ien k√§yt√∂ss√§ tokenien suhteellinen sijainti esitet√§√§n askelten m√§√§r√§ll√§, eik√§ sit√§ tarvitse eksplisiittisesti esitt√§√§.  
2. Kun siirryt√§√§n huomioon, meid√§n t√§ytyy tiet√§√§ tokenien suhteelliset sijainnit sekvenssiss√§.  
3. Positionaalisen koodauksen saamiseksi laajennamme token-sekvenssi√§mme sekvenssin token-sijaintien sekvenssill√§ (esim. numeroilla 0, 1, ...).  
4. Sitten yhdist√§mme token-sijainnin ja tokenin embedding-vektorin. Sijainnin (kokonaisluku) muuntamiseksi vektoriksi voimme k√§ytt√§√§ erilaisia l√§hestymistapoja:

* Koulutettava embedding, kuten token-embedding. T√§m√§ on l√§hestymistapa, jota tarkastelemme t√§ss√§. Sovellamme embedding-kerroksia sek√§ tokeneihin ett√§ niiden sijainteihin, jolloin saadaan saman ulottuvuuden embedding-vektorit, jotka sitten summataan yhteen.
* Kiinte√§ positionaalinen koodausfunktio, kuten alkuper√§isess√§ artikkelissa ehdotettiin.

<img src="images/pos-embedding.png" width="50%"/>

> Kuva kirjoittajalta

Positionaalisen embeddingin tuloksena saadaan embedding, joka sis√§lt√§√§ sek√§ alkuper√§isen tokenin ett√§ sen sijainnin sekvenssiss√§.

### Monip√§inen itsereferenssihuomio

Seuraavaksi meid√§n t√§ytyy tunnistaa kuvioita sekvenssiss√§mme. T√§t√§ varten transformerit k√§ytt√§v√§t **itsereferenssihuomiota**, joka on k√§yt√§nn√∂ss√§ huomio, jota sovelletaan samaan sekvenssiin sek√§ sy√∂tteen√§ ett√§ tulosteena. Itsereferenssihuomion soveltaminen mahdollistaa **kontekstin** huomioimisen lauseessa ja sen, mitk√§ sanat liittyv√§t toisiinsa. Esimerkiksi se auttaa tunnistamaan, mihin sanat, kuten *se*, viittaavat, ja ottaa my√∂s kontekstin huomioon:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.fi.png)

> Kuva [Googlen blogista](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Transformereissa k√§yt√§mme **monip√§ist√§ huomiota**, jotta verkko pystyy tunnistamaan useita erilaisia riippuvuuksia, kuten pitk√§aikaisia vs. lyhytaikaisia sanasuhteita, yhteisviittauksia vs. muita suhteita jne.

[TensorFlow Notebook](TransformersTF.ipynb) sis√§lt√§√§ lis√§√§ yksityiskohtia transformer-kerrosten toteutuksesta.

### Enkooderi-dekooderi-huomio

Transformereissa huomiota k√§ytet√§√§n kahdessa paikassa:

* Tunnistamaan kuvioita sy√∂tetekstiss√§ itsereferenssihuomion avulla
* Suorittamaan sekvenssin k√§√§nn√∂s - t√§m√§ on huomio-kerros enkooderin ja dekooderin v√§lill√§.

Enkooderi-dekooderi-huomio on hyvin samanlainen kuin RNN:ien huomio, kuten t√§m√§n osion alussa kuvattiin. T√§m√§ animoitu kaavio selitt√§√§ enkooderi-dekooderi-huomion roolin.

![Animoitu GIF, joka n√§ytt√§√§, miten arvioinnit suoritetaan transformer-malleissa.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Koska jokainen sy√∂tepositio kartoitetaan itsen√§isesti jokaiseen tulostepositioon, transformerit voivat rinnakkaistaa paremmin kuin RNN:t, mik√§ mahdollistaa paljon suuremmat ja ilmaisukykyisemm√§t kielimallit. Jokainen huomiop√§√§ voi oppia erilaisia suhteita sanojen v√§lill√§, mik√§ parantaa NLP-teht√§vien suorituskyky√§.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) on eritt√§in suuri monikerroksinen transformer-verkko, jossa on 12 kerrosta *BERT-base*-mallissa ja 24 kerrosta *BERT-large*-mallissa. Malli koulutetaan ensin suurella tekstikorpuksella (Wikipedia + kirjat) k√§ytt√§m√§ll√§ valvomatonta koulutusta (ennustamalla peitettyj√§ sanoja lauseessa). Esikoulutuksen aikana malli omaksuu merkitt√§v√§n m√§√§r√§n kielen ymm√§rryst√§, jota voidaan hy√∂dynt√§√§ muilla aineistoilla hienos√§√§d√∂n avulla. T√§t√§ prosessia kutsutaan **siirto-oppimiseksi**.

![kuva osoitteesta http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.fi.png)

> Kuva [l√§hde](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Harjoitukset: Transformerit

Jatka oppimista seuraavissa muistikirjoissa:

* [Transformerit PyTorchilla](TransformersPyTorch.ipynb)
* [Transformerit TensorFlow'lla](TransformersTF.ipynb)

## Yhteenveto

T√§ss√§ oppitunnissa opit Transformereista ja Huomiomekanismeista, jotka ovat olennaisia ty√∂kaluja NLP:n ty√∂kalupakissa. Transformer-arkkitehtuureista on olemassa monia variaatioita, kuten BERT, DistilBERT, BigBird, OpenGPT3 ja muita, joita voidaan hienos√§√§t√§√§. [HuggingFace-paketti](https://github.com/huggingface/) tarjoaa kirjaston monien n√§iden arkkitehtuurien kouluttamiseen sek√§ PyTorchilla ett√§ TensorFlow'lla.

## üöÄ Haaste

## [J√§lkivisa](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Kertaus ja itseopiskelu

* [Blogikirjoitus](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), joka selitt√§√§ klassisen [Attention is all you need](https://arxiv.org/abs/1706.03762) -artikkelin transformereista.
* [Blogikirjoitussarja](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) transformereista, joka selitt√§√§ arkkitehtuurin yksityiskohtaisesti.

## [Teht√§v√§](assignment.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.