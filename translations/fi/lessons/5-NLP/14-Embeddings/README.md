<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-28T20:07:22+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "fi"
}
-->
# Upotukset

## [Ennakkokysely](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Kun koulutimme luokittimia BoW:n tai TF/IDF:n perusteella, k√§ytimme korkean ulottuvuuden bag-of-words-vektoreita, joiden pituus oli `vocab_size`, ja muunsimme eksplisiittisesti matalan ulottuvuuden paikkavektoreista harvoiksi yksiulotteisiksi vektoreiksi. T√§m√§ yksiulotteinen esitys ei kuitenkaan ole muistin kannalta tehokas. Lis√§ksi jokaista sanaa k√§sitell√§√§n toisistaan riippumattomana, eli yksiulotteiset vektorit eiv√§t ilmaise mit√§√§n semanttista samankaltaisuutta sanojen v√§lill√§.

**Upotuksen** idea on edustaa sanoja matalamman ulottuvuuden tiheill√§ vektoreilla, jotka jollain tavalla heijastavat sanan semanttista merkityst√§. My√∂hemmin keskustelemme siit√§, miten rakentaa merkityksellisi√§ sanaupotuksia, mutta toistaiseksi voimme ajatella upotuksia tapana pienent√§√§ sanavektorin ulottuvuutta.

Upotuskerros ottaa sanan sy√∂tteen√§ ja tuottaa ulostulovektorin, jonka pituus on m√§√§ritelty `embedding_size`. Tietyss√§ mieless√§ se on hyvin samanlainen kuin `Linear`-kerros, mutta sen sijaan, ett√§ se ottaisi yksiulotteisen vektorin, se voi ottaa sanan numeron sy√∂tteen√§, jolloin voimme v√§ltt√§√§ suurten yksiulotteisten vektorien luomisen.

K√§ytt√§m√§ll√§ upotuskerrosta luokittajaverkkomme ensimm√§isen√§ kerroksena voimme siirty√§ bag-of-words-mallista **embedding bag** -malliin, jossa ensin muutamme tekstimme jokaisen sanan vastaavaksi upotukseksi ja laskemme sitten jonkin aggregaattifunktion n√§iden upotusten yli, kuten `sum`, `average` tai `max`.  

![Kuva, joka n√§ytt√§√§ upotusluokittimen viidelle sanalle sekvenssiss√§.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.fi.png)

> Kuva kirjoittajalta

## ‚úçÔ∏è Harjoitukset: Upotukset

Jatka oppimista seuraavissa muistikirjoissa:
* [Upotukset PyTorchilla](EmbeddingsPyTorch.ipynb)
* [Upotukset TensorFlowlla](EmbeddingsTF.ipynb)

## Semanttiset upotukset: Word2Vec

Vaikka upotuskerros oppi kartoittamaan sanat vektoriedustukseen, t√§m√§ edustus ei v√§ltt√§m√§tt√§ sis√§lt√§nyt paljon semanttista merkityst√§. Olisi hy√∂dyllist√§ oppia vektoriedustus, jossa samankaltaiset sanat tai synonyymit vastaavat vektoreita, jotka ovat l√§hell√§ toisiaan jonkin vektoriet√§isyyden (esim. euklidisen et√§isyyden) suhteen.

T√§m√§n saavuttamiseksi meid√§n t√§ytyy esikouluttaa upotusmallimme suurella tekstikokoelmalla erityisell√§ tavalla. Yksi tapa kouluttaa semanttisia upotuksia on nimelt√§√§n [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Se perustuu kahteen p√§√§arkkitehtuuriin, joita k√§ytet√§√§n sanojen hajautetun edustuksen tuottamiseen:

 - **Jatkuva bag-of-words** (CBoW) ‚Äî t√§ss√§ arkkitehtuurissa koulutamme mallin ennustamaan sanan ymp√§r√∂iv√§st√§ kontekstista. Annetulla ngrammilla $(W_{-2},W_{-1},W_0,W_1,W_2)$ mallin tavoitteena on ennustaa $W_0$ k√§ytt√§en $(W_{-2},W_{-1},W_1,W_2)$.
 - **Jatkuva skip-gram** on CBoW:n vastakohta. Malli k√§ytt√§√§ ymp√§r√∂iv√§√§ kontekstisanan ikkunaa ennustaakseen nykyisen sanan.

CBoW on nopeampi, kun taas skip-gram on hitaampi, mutta se edustaa harvinaisia sanoja paremmin.

![Kuva, joka n√§ytt√§√§ sek√§ CBoW- ett√§ Skip-Gram-algoritmit sanojen muuntamiseksi vektoreiksi.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fi.png)

> Kuva [t√§st√§ artikkelista](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vec-esikoulutetut upotukset (samoin kuin muut vastaavat mallit, kuten GloVe) voidaan my√∂s k√§ytt√§√§ upotuskerroksen sijasta neuroverkoissa. Meid√§n t√§ytyy kuitenkin k√§sitell√§ sanastoja, koska Word2Vec/GloVe:n esikoulutuksessa k√§ytetty sanasto eroaa todenn√§k√∂isesti tekstikorpuksemme sanastosta. Tutustu yll√§ oleviin muistikirjoihin n√§hd√§ksesi, miten t√§m√§ ongelma voidaan ratkaista.

## Kontekstuaaliset upotukset

Yksi perinteisten esikoulutettujen upotusten, kuten Word2Vecin, keskeisist√§ rajoituksista on sanan merkityksen erottelun ongelma. Vaikka esikoulutetut upotukset voivat vangita osan sanojen merkityksest√§ kontekstissa, jokainen mahdollinen sanan merkitys koodataan samaan upotukseen. T√§m√§ voi aiheuttaa ongelmia jatkomalleissa, koska monet sanat, kuten sana 'play', voivat tarkoittaa eri asioita riippuen kontekstista, jossa niit√§ k√§ytet√§√§n.

Esimerkiksi sana 'play' n√§iss√§ kahdessa eri lauseessa tarkoittaa hyvin eri asioita:

- K√§vin teatterissa katsomassa **n√§ytelm√§√§**.
- John haluaa **leikki√§** yst√§viens√§ kanssa.

Yll√§ olevat esikoulutetut upotukset edustavat molempia n√§ytelm√§n merkityksi√§ samassa upotuksessa. T√§m√§n rajoituksen voittamiseksi meid√§n t√§ytyy rakentaa upotuksia **kielimallin** perusteella, joka on koulutettu suurella tekstikorpuksella ja *tiet√§√§*, miten sanoja voidaan yhdist√§√§ eri konteksteissa. Kontekstuaalisten upotusten k√§sittely on t√§m√§n oppitunnin ulkopuolella, mutta palaamme niihin my√∂hemmin kurssilla, kun puhumme kielimalleista.

## Yhteenveto

T√§ss√§ oppitunnissa opit rakentamaan ja k√§ytt√§m√§√§n upotuskerroksia TensorFlowssa ja Pytorchissa, jotta sanojen semanttiset merkitykset heijastuisivat paremmin.

## üöÄ Haaste

Word2Veci√§ on k√§ytetty joissakin mielenkiintoisissa sovelluksissa, kuten laululyriikan ja runouden luomisessa. Tutustu [t√§h√§n artikkeliin](https://www.politetype.com/blog/word2vec-color-poems), jossa kirjoittaja k√§y l√§pi, miten h√§n k√§ytti Word2Veci√§ runojen luomiseen. Katso my√∂s [t√§m√§ Dan Shiffmannin video](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), jossa selitet√§√§n t√§t√§ tekniikkaa eri tavalla. Kokeile sitten soveltaa n√§it√§ tekniikoita omaan tekstikorpukseesi, ehk√§ Kagglesta hankittuun.

## [J√§lkikysely](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## Kertaus ja itseopiskelu

Lue t√§m√§ Word2Vec-artikkeli: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Teht√§v√§: Muistikirjat](assignment.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.