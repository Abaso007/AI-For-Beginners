{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upotukset\n",
    "\n",
    "Edellisessä esimerkissä käsittelimme korkeulotteisia bag-of-words-vektoreita, joiden pituus oli `vocab_size`, ja muunsimme matalalitteiset sijaintiesitykset eksplisiittisesti harvoiksi yksi-ykkösesityksiksi. Tämä yksi-ykkösesitys ei ole muistin kannalta tehokas. Lisäksi jokaista sanaa käsitellään toisistaan riippumattomana, joten yksi-ykkösenkoodatut vektorit eivät ilmaise sanojen semanttisia samankaltaisuuksia.\n",
    "\n",
    "Tässä osiossa jatkamme **News AG** -aineiston tutkimista. Aloitetaan lataamalla data ja hakemalla joitakin määritelmiä edellisestä osasta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mikä on upotus?\n",
    "\n",
    "**Upotuksen** idea on edustaa sanoja matalampidimensionaalisilla tiheillä vektoreilla, jotka heijastavat sanan semanttista merkitystä. Myöhemmin käsittelemme, miten rakentaa merkityksellisiä sanaupotuksia, mutta toistaiseksi voimme ajatella upotuksia tapana vähentää sanavektorin dimensioita.\n",
    "\n",
    "Upotuskerros ottaa sanan syötteenä ja tuottaa ulostulovektorin, jonka koko on määritelty `embedding_size`. Tietyssä mielessä se on hyvin samanlainen kuin `Dense`-kerros, mutta sen sijaan, että se ottaisi syötteenä yksi-kuuma-koodatun vektorin, se pystyy ottamaan sanan numeron.\n",
    "\n",
    "Kun käytämme upotuskerrosta verkkomme ensimmäisenä kerroksena, voimme siirtyä sanojen pussi -mallista **upotuspussi**-malliin, jossa ensin muutamme tekstimme jokaisen sanan vastaavaksi upotukseksi ja sitten laskemme jonkin aggregaattifunktion näiden upotusten yli, kuten `sum`, `average` tai `max`.\n",
    "\n",
    "![Kuva, joka näyttää upotusluokittelijan viidelle sekvenssisanalle.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.fi.png)\n",
    "\n",
    "Luokittelijaneuroverkkomme koostuu seuraavista kerroksista:\n",
    "\n",
    "* `TextVectorization`-kerros, joka ottaa syötteenä merkkijonon ja tuottaa tensorin token-numeroista. Määrittelemme kohtuullisen sanaston koon `vocab_size` ja jätämme vähemmän käytetyt sanat huomiotta. Syötteen muoto on 1, ja ulostulon muoto on $n$, koska saamme tulokseksi $n$ tokenia, joista jokainen sisältää numeroita välillä 0–`vocab_size`.\n",
    "* `Embedding`-kerros, joka ottaa $n$ numeroa ja pienentää jokaisen numeron tiheäksi vektoriksi, jonka pituus on määritelty (esimerkissämme 100). Näin ollen syötetensorin muoto $n$ muuttuu $n\\times 100$ tensoriksi.\n",
    "* Aggregointikerros, joka laskee tämän tensorin keskiarvon ensimmäisen akselin yli, eli se laskee kaikkien $n$ syötetensorien keskiarvon, jotka vastaavat eri sanoja. Toteutamme tämän kerroksen käyttämällä `Lambda`-kerrosta ja välitämme siihen funktion keskiarvon laskemiseksi. Ulostulon muoto on 100, ja se on koko syötesekvenssin numeerinen esitys.\n",
    "* Lopullinen `Dense` lineaarinen luokittelija.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary`-tulosteessa, **output shape** -sarakkeessa, ensimmäinen tensorin dimensio `None` vastaa minibatchin kokoa, ja toinen vastaa token-sekvenssin pituutta. Kaikilla minibatchin token-sekvensseillä on eri pituudet. Keskustelemme, miten käsitellä tätä seuraavassa osiossa.\n",
    "\n",
    "Nyt harjoitellaan verkkoa:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Huomaa** että rakennamme vektoroijaa tietojoukon osajoukon perusteella. Tämä tehdään prosessin nopeuttamiseksi, ja se saattaa johtaa tilanteeseen, jossa kaikkia tekstimme tokenoita ei ole sanastossa. Tällöin nämä tokenit jätetään huomiotta, mikä voi johtaa hieman alhaisempaan tarkkuuteen. Kuitenkin todellisessa elämässä tekstin osajoukko antaa usein hyvän arvion sanastosta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Käsitellään muuttuvia sekvenssikokoja\n",
    "\n",
    "Ymmärretään, miten koulutus tapahtuu pienissä erissä. Yllä olevassa esimerkissä syöte-tenzorin ulottuvuus on 1, ja käytämme 128:n kokoisia pieneriä, jolloin tensorin todellinen koko on $128 \\times 1$. Kuitenkin jokaisessa lauseessa olevien tokenien määrä vaihtelee. Jos sovellamme `TextVectorization`-kerrosta yhteen syötteeseen, palautettujen tokenien määrä vaihtelee sen mukaan, miten teksti on tokenisoitu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kuitenkin, kun sovellamme vektoroijaa useisiin sekvensseihin, sen täytyy tuottaa suorakulmainen tensorimuoto, joten se täyttää käyttämättömät elementit PAD-tokenilla (joka meidän tapauksessamme on nolla):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tässä näemme upotukset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Huom**: Jotta täydennystä voidaan minimoida, joissakin tapauksissa on järkevää järjestää kaikki tietojoukon sekvenssit pituuden mukaan nousevaan järjestykseen (tai tarkemmin sanottuna tokenien lukumäärän mukaan). Tämä varmistaa, että kukin minibatch sisältää samankaltaisen pituisia sekvenssejä.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semanttiset upotukset: Word2Vec\n",
    "\n",
    "Edellisessä esimerkissämme upotuskerros oppi kartoittamaan sanat vektoriesityksiksi, mutta näillä esityksillä ei ollut semanttista merkitystä. Olisi hyödyllistä oppia vektoriesitys siten, että samankaltaiset sanat tai synonyymit vastaavat vektoreita, jotka ovat lähellä toisiaan jonkin vektorietäisyyden (esimerkiksi euklidisen etäisyyden) perusteella.\n",
    "\n",
    "Tämän saavuttamiseksi meidän täytyy esikouluttaa upotusmallimme suurella tekstikokoelmalla käyttämällä tekniikkaa, kuten [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Se perustuu kahteen pääarkkitehtuuriin, joita käytetään sanojen hajautettujen esitysten tuottamiseen:\n",
    "\n",
    " - **Jatkuva sanapussimalli** (CBoW), jossa mallia koulutetaan ennustamaan sana ympäröivän kontekstin perusteella. Annettuna ngrammi $(W_{-2},W_{-1},W_0,W_1,W_2)$, mallin tavoitteena on ennustaa $W_0$ käyttäen $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Jatkuva skip-gram** on CBoW:n vastakohta. Malli käyttää ympäröivää kontekstisanan ikkunaa ennustaakseen nykyisen sanan.\n",
    "\n",
    "CBoW on nopeampi, kun taas skip-gram on hitaampi, mutta se edustaa harvinaisia sanoja paremmin.\n",
    "\n",
    "![Kuva, joka näyttää sekä CBoW- että Skip-Gram-algoritmit sanojen muuntamiseksi vektoreiksi.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fi.png)\n",
    "\n",
    "Kokeillaksemme Word2Vec-upotusta, joka on esikoulutettu Google News -aineistolla, voimme käyttää **gensim**-kirjastoa. Alla etsimme sanoja, jotka ovat lähimpänä sanaa 'neural'.\n",
    "\n",
    "> **Huom:** Kun luot sanavektoreita ensimmäistä kertaa, niiden lataaminen voi kestää jonkin aikaa!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voimme myös poimia sanasta vektoriesityksen, jota voidaan käyttää luokittelumallin kouluttamisessa. Vektoriesityksessä on 300 komponenttia, mutta tässä näytämme selkeyden vuoksi vain vektorin ensimmäiset 20 komponenttia:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hienoa semanttisissa upotuksissa on se, että voit manipuloida vektorisalausta semantiikan perusteella. Esimerkiksi voimme pyytää löytämään sanan, jonka vektoriedustus on mahdollisimman lähellä sanoja *kuningas* ja *nainen*, ja mahdollisimman kaukana sanasta *mies*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Esimerkki yllä käyttää sisäistä GenSym-taikuutta, mutta taustalla oleva logiikka on itse asiassa melko yksinkertainen. Mielenkiintoinen asia upotuksissa on, että voit suorittaa normaaleja vektorioperaatioita upotusvektoreilla, ja tämä heijastaa operaatioita sanojen **merkityksissä**. Esimerkki yllä voidaan ilmaista vektorioperaatioiden avulla: laskemme vektorin, joka vastaa **KING-MAN+WOMAN** (operaatiot `+` ja `-` suoritetaan vastaavien sanojen vektoriedustuksilla), ja sitten etsimme sanakirjasta lähimmän sanan kyseiseen vektoriin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Jouduimme lisäämään pienet kertoimet *man*- ja *woman*-vektoreihin – kokeile poistaa ne ja katso, mitä tapahtuu.\n",
    "\n",
    "Lähimmän vektorin löytämiseksi käytämme TensorFlow-työkaluja laskemaan etäisyysvektorin oman vektorimme ja sanaston kaikkien vektorien välillä, ja sitten löydämme pienimmän sanan indeksin käyttämällä `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaikka Word2Vec vaikuttaa hyvältä tavalta ilmaista sanojen semantiikkaa, sillä on monia haittoja, mukaan lukien seuraavat:\n",
    "\n",
    "* Sekä CBoW- että skip-gram-mallit ovat **ennustavia upotuksia**, ja ne ottavat huomioon vain paikallisen kontekstin. Word2Vec ei hyödynnä globaalia kontekstia.\n",
    "* Word2Vec ei ota huomioon sanojen **morfologiaa**, eli sitä, että sanan merkitys voi riippua sanan eri osista, kuten juuresta.\n",
    "\n",
    "**FastText** pyrkii voittamaan toisen rajoituksen ja rakentaa Word2Vecin pohjalta oppimalla vektoriedustuksia jokaiselle sanalle ja kunkin sanan sisältämille merkkien n-grammeille. Näiden edustusten arvot keskiarvotetaan yhdeksi vektoriksi jokaisessa harjoitusvaiheessa. Vaikka tämä lisää paljon lisälaskentaa esikoulutukseen, se mahdollistaa sana-upotusten koodaavan osasanatietoa.\n",
    "\n",
    "Toinen menetelmä, **GloVe**, käyttää erilaista lähestymistapaa sana-upotuksiin, perustuen sana-konteksti-matriisin faktorisointiin. Ensin se rakentaa suuren matriisin, joka laskee sanojen esiintymiskerrat eri konteksteissa, ja sitten se yrittää esittää tämän matriisin pienemmissä ulottuvuuksissa tavalla, joka minimoi rekonstruointitappion.\n",
    "\n",
    "Gensim-kirjasto tukee näitä sana-upotuksia, ja voit kokeilla niitä muuttamalla yllä olevaa mallin latauskoodia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esikoulutettujen upotusten käyttö Kerasissa\n",
    "\n",
    "Voimme muokata yllä olevaa esimerkkiä esitäyttääksemme upotuskerroksemme matriisin semanttisilla upotuksilla, kuten Word2Vec. Esikoulutetun upotuksen ja tekstikorpuksen sanastot eivät todennäköisesti vastaa toisiaan, joten meidän on valittava yksi. Tässä tutkimme kahta mahdollista vaihtoehtoa: tokenisoijan sanaston käyttöä ja Word2Vec-upotusten sanaston käyttöä.\n",
    "\n",
    "### Tokenisoijan sanaston käyttö\n",
    "\n",
    "Kun käytämme tokenisoijan sanastoa, osalla sanaston sanoista on vastaavat Word2Vec-upotukset, mutta osa puuttuu. Koska sanastomme koko on `vocab_size` ja Word2Vec-upotuksen vektoripituus on `embed_size`, upotuskerros esitetään painomatriisina, jonka muoto on `vocab_size`$\\times$`embed_size`. Täytämme tämän matriisin käymällä sanaston läpi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanat, joita ei löydy Word2Vec-sanakirjasta, voidaan joko jättää nolliksi tai luoda niille satunnaisvektori.\n",
    "\n",
    "Nyt voimme määritellä upotuskerroksen esikoulutetuilla painoilla:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Huomio**: Huomaa, että asetamme `trainable=False` luodessamme `Embedding`-kerroksen, mikä tarkoittaa, että emme uudelleenkouluta Embedding-kerrosta. Tämä saattaa hieman heikentää tarkkuutta, mutta nopeuttaa koulutusta.\n",
    "\n",
    "### Embedding-sanakirjan käyttö\n",
    "\n",
    "Yksi ongelma aiemmassa lähestymistavassa on, että TextVectorization- ja Embedding-kerrosten käyttämät sanakirjat ovat erilaisia. Tämän ongelman ratkaisemiseksi voimme käyttää jotakin seuraavista ratkaisuista:\n",
    "* Uudelleenkouluta Word2Vec-malli käyttämällä omaa sanakirjaamme.\n",
    "* Lataa datasetti käyttämällä Word2Vec-mallin esikoulutettua sanakirjaa. Datasetin lataamiseen käytettävät sanakirjat voidaan määrittää latauksen yhteydessä.\n",
    "\n",
    "Jälkimmäinen lähestymistapa vaikuttaa helpommalta, joten toteutetaan se. Ensimmäiseksi luomme `TextVectorization`-kerroksen määritetyllä sanakirjalla, joka on otettu Word2Vec-embeddingeistä:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim-sanan upotuskirjasto sisältää kätevän funktion, `get_keras_embeddings`, joka luo automaattisesti vastaavan Keras-upotuskerroksen sinulle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yksi syy siihen, miksi emme näe korkeampaa tarkkuutta, on se, että jotkut sanamme datasta puuttuvat esikoulutetusta GloVe-sanakirjasta, ja siksi ne käytännössä jätetään huomiotta. Tämän voittamiseksi voimme kouluttaa omat upotuksemme perustuen omaan dataamme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontekstuaaliset upotukset\n",
    "\n",
    "Yksi perinteisten esikoulutettujen upotusten, kuten Word2Vecin, keskeisistä rajoituksista on se, että vaikka ne voivat vangita jonkin verran sanan merkitystä, ne eivät pysty erottamaan eri merkityksiä toisistaan. Tämä voi aiheuttaa ongelmia jatkomalleissa.\n",
    "\n",
    "Esimerkiksi sana 'play' tarkoittaa eri asioita näissä kahdessa lauseessa:\n",
    "- Kävin teatterissa katsomassa **näytelmän**.\n",
    "- John haluaa **leikkiä** ystäviensä kanssa.\n",
    "\n",
    "Esikoulutetut upotukset, joista puhuimme, edustavat molempia sanan 'play' merkityksiä samalla upotuksella. Tämän rajoituksen voittamiseksi meidän täytyy rakentaa upotuksia, jotka perustuvat **kielimalliin**, joka on koulutettu suurella tekstikorpuksella ja *ymmärtää*, miten sanoja voidaan yhdistää eri konteksteissa. Kontekstuaalisten upotusten käsittely on tämän tutoriaalin ulkopuolella, mutta palaamme niihin, kun puhumme kielimalleista seuraavassa osiossa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Vastuuvapauslauseke**:  \nTämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäinen asiakirja sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T21:52:05+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "fi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}