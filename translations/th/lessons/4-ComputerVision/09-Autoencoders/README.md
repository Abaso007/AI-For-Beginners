<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-29T08:55:37+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "th"
}
-->
# ออโต้เอนโค้ดเดอร์ (Autoencoders)

เมื่อฝึกสอน CNNs หนึ่งในปัญหาคือเราต้องการข้อมูลที่มีการติดป้ายกำกับจำนวนมาก ในกรณีของการจำแนกภาพ เราจำเป็นต้องแยกภาพออกเป็นคลาสต่างๆ ซึ่งเป็นงานที่ต้องทำด้วยมือ

## [แบบทดสอบก่อนเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/17)

อย่างไรก็ตาม เราอาจต้องการใช้ข้อมูลดิบ (ที่ไม่มีป้ายกำกับ) สำหรับการฝึกสอนตัวดึงคุณลักษณะของ CNN ซึ่งเรียกว่า **การเรียนรู้แบบกึ่งควบคุมตนเอง (self-supervised learning)** แทนที่จะใช้ป้ายกำกับ เราจะใช้ภาพฝึกสอนเป็นทั้งอินพุตและเอาต์พุตของเครือข่าย แนวคิดหลักของ **ออโต้เอนโค้ดเดอร์** คือเราจะมี **เครือข่ายเอนโค้ดเดอร์** ที่แปลงภาพอินพุตไปยัง **พื้นที่แฝง (latent space)** (โดยปกติจะเป็นเวกเตอร์ขนาดเล็กกว่า) จากนั้น **เครือข่ายดีโค้ดเดอร์** จะมีเป้าหมายในการสร้างภาพต้นฉบับขึ้นมาใหม่

> ✅ [ออโต้เอนโค้ดเดอร์](https://wikipedia.org/wiki/Autoencoder) คือ "ประเภทของโครงข่ายประสาทเทียมที่ใช้ในการเรียนรู้การเข้ารหัสข้อมูลที่ไม่มีป้ายกำกับอย่างมีประสิทธิภาพ"

เนื่องจากเรากำลังฝึกสอนออโต้เอนโค้ดเดอร์เพื่อจับข้อมูลจากภาพต้นฉบับให้ได้มากที่สุดเพื่อการสร้างใหม่ที่แม่นยำ เครือข่ายจึงพยายามค้นหา **การฝังตัว (embedding)** ที่ดีที่สุดของภาพอินพุตเพื่อจับความหมายของภาพ

![AutoEncoder Diagram](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.th.jpg)

> ภาพจาก [บล็อก Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## สถานการณ์ที่ใช้ออโต้เอนโค้ดเดอร์

แม้ว่าการสร้างภาพต้นฉบับขึ้นมาใหม่อาจดูเหมือนไม่มีประโยชน์ในตัวเอง แต่มีบางสถานการณ์ที่ออโต้เอนโค้ดเดอร์มีประโยชน์อย่างยิ่ง:

* **ลดมิติของภาพเพื่อการแสดงผล** หรือ **ฝึกฝังตัวภาพ** โดยปกติออโต้เอนโค้ดเดอร์จะให้ผลลัพธ์ที่ดีกว่า PCA เพราะมันคำนึงถึงลักษณะเชิงพื้นที่ของภาพและคุณลักษณะเชิงลำดับชั้น
* **การลบสัญญาณรบกวน (Denoising)** เช่น การลบสัญญาณรบกวนออกจากภาพ เนื่องจากสัญญาณรบกวนมีข้อมูลที่ไม่สำคัญจำนวนมาก ออโต้เอนโค้ดเดอร์ไม่สามารถใส่ข้อมูลทั้งหมดลงในพื้นที่แฝงที่ค่อนข้างเล็กได้ ดังนั้นมันจึงจับเฉพาะส่วนที่สำคัญของภาพ เมื่อฝึกสอนตัวลบสัญญาณรบกวน เราเริ่มต้นด้วยภาพต้นฉบับและใช้ภาพที่เพิ่มสัญญาณรบกวนเทียมเป็นอินพุตสำหรับออโต้เอนโค้ดเดอร์
* **การเพิ่มความละเอียด (Super-resolution)** เพิ่มความละเอียดของภาพ เราเริ่มต้นด้วยภาพความละเอียดสูงและใช้ภาพที่มีความละเอียดต่ำกว่าเป็นอินพุตของออโต้เอนโค้ดเดอร์
* **โมเดลสร้างสรรค์ (Generative models)** เมื่อเราฝึกสอนออโต้เอนโค้ดเดอร์แล้ว ส่วนดีโค้ดเดอร์สามารถใช้สร้างวัตถุใหม่โดยเริ่มจากเวกเตอร์แฝงแบบสุ่ม

## ออโต้เอนโค้ดเดอร์เชิงแปรผัน (Variational Autoencoders - VAE)

ออโต้เอนโค้ดเดอร์แบบดั้งเดิมลดมิติของข้อมูลอินพุตโดยการค้นหาคุณลักษณะที่สำคัญของภาพอินพุต อย่างไรก็ตาม เวกเตอร์แฝงมักไม่มีความหมายมากนัก กล่าวอีกนัยหนึ่ง หากใช้ชุดข้อมูล MNIST เป็นตัวอย่าง การค้นหาว่าตัวเลขใดสอดคล้องกับเวกเตอร์แฝงต่างๆ ไม่ใช่เรื่องง่าย เพราะเวกเตอร์แฝงที่ใกล้กันอาจไม่ได้สอดคล้องกับตัวเลขเดียวกันเสมอไป

ในทางกลับกัน หากต้องการฝึกโมเดล *สร้างสรรค์* จะดีกว่าหากมีความเข้าใจเกี่ยวกับพื้นที่แฝง แนวคิดนี้นำเราไปสู่ **ออโต้เอนโค้ดเดอร์เชิงแปรผัน (VAE)**

VAE คือออโต้เอนโค้ดเดอร์ที่เรียนรู้การทำนาย *การแจกแจงทางสถิติ* ของพารามิเตอร์แฝง หรือที่เรียกว่า **การแจกแจงแฝง (latent distribution)** ตัวอย่างเช่น เราอาจต้องการให้เวกเตอร์แฝงมีการแจกแจงแบบปกติที่มีค่าเฉลี่ย z<sub>mean</sub> และส่วนเบี่ยงเบนมาตรฐาน z<sub>sigma</sub> (ทั้งค่าเฉลี่ยและส่วนเบี่ยงเบนมาตรฐานเป็นเวกเตอร์ที่มีมิติ d) เอนโค้ดเดอร์ใน VAE จะเรียนรู้การทำนายพารามิเตอร์เหล่านี้ และดีโค้ดเดอร์จะใช้เวกเตอร์สุ่มจากการแจกแจงนี้เพื่อสร้างวัตถุขึ้นมาใหม่

สรุปได้ว่า:

 * จากเวกเตอร์อินพุต เราทำนาย `z_mean` และ `z_log_sigma` (แทนที่จะทำนายส่วนเบี่ยงเบนมาตรฐานโดยตรง เราทำนายลอการิทึมของมัน)
 * เราสุ่มตัวอย่างเวกเตอร์ `sample` จากการแจกแจง N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * ดีโค้ดเดอร์พยายามถอดรหัสภาพต้นฉบับโดยใช้ `sample` เป็นเวกเตอร์อินพุต

 <img src="images/vae.png" width="50%">

> ภาพจาก [บล็อกโพสต์นี้](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) โดย Isaak Dykeman

ออโต้เอนโค้ดเดอร์เชิงแปรผันใช้ฟังก์ชันการสูญเสียที่ซับซ้อนซึ่งประกอบด้วยสองส่วน:

* **การสูญเสียจากการสร้างใหม่ (Reconstruction loss)** เป็นฟังก์ชันการสูญเสียที่แสดงให้เห็นว่าภาพที่สร้างใหม่ใกล้เคียงกับเป้าหมายเพียงใด (อาจเป็น Mean Squared Error หรือ MSE) ซึ่งเป็นฟังก์ชันการสูญเสียเดียวกับในออโต้เอนโค้ดเดอร์ปกติ
* **การสูญเสีย KL (KL loss)** ซึ่งทำให้มั่นใจว่าการแจกแจงตัวแปรแฝงยังคงใกล้เคียงกับการแจกแจงแบบปกติ โดยอิงตามแนวคิดของ [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) ซึ่งเป็นเมตริกที่ใช้ประเมินความคล้ายคลึงกันระหว่างการแจกแจงทางสถิติสองแบบ

ข้อได้เปรียบสำคัญของ VAE คือช่วยให้เราสร้างภาพใหม่ได้ง่ายขึ้น เพราะเรารู้ว่าควรสุ่มตัวอย่างเวกเตอร์แฝงจากการแจกแจงใด ตัวอย่างเช่น หากเราฝึก VAE ด้วยเวกเตอร์แฝง 2 มิติบน MNIST เราสามารถปรับเปลี่ยนองค์ประกอบของเวกเตอร์แฝงเพื่อสร้างตัวเลขต่างๆ:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> ภาพโดย [Dmitry Soshnikov](http://soshnikov.com)

สังเกตว่าภาพค่อยๆ ผสมกันเมื่อเราเริ่มได้เวกเตอร์แฝงจากส่วนต่างๆ ของพื้นที่พารามิเตอร์แฝง นอกจากนี้ เรายังสามารถแสดงภาพพื้นที่นี้ใน 2 มิติ:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> ภาพโดย [Dmitry Soshnikov](http://soshnikov.com)

## ✍️ แบบฝึกหัด: ออโต้เอนโค้ดเดอร์

เรียนรู้เพิ่มเติมเกี่ยวกับออโต้เอนโค้ดเดอร์ในสมุดบันทึกต่อไปนี้:

* [ออโต้เอนโค้ดเดอร์ใน TensorFlow](AutoencodersTF.ipynb)
* [ออโต้เอนโค้ดเดอร์ใน PyTorch](AutoEncodersPyTorch.ipynb)

## คุณสมบัติของออโต้เอนโค้ดเดอร์

* **เฉพาะข้อมูล** - ทำงานได้ดีเฉพาะกับประเภทของภาพที่ได้รับการฝึกสอนมา ตัวอย่างเช่น หากเราฝึกเครือข่ายเพิ่มความละเอียดบนดอกไม้ มันจะทำงานได้ไม่ดีบนภาพบุคคล เพราะเครือข่ายสามารถสร้างภาพความละเอียดสูงขึ้นได้โดยใช้รายละเอียดที่ได้จากชุดข้อมูลฝึกสอน
* **สูญเสียข้อมูลบางส่วน** - ภาพที่สร้างใหม่ไม่เหมือนกับภาพต้นฉบับ ลักษณะของการสูญเสียถูกกำหนดโดย *ฟังก์ชันการสูญเสีย* ที่ใช้ระหว่างการฝึกสอน
* ทำงานกับ **ข้อมูลที่ไม่มีป้ายกำกับ**

## [แบบทดสอบหลังเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## สรุป

ในบทเรียนนี้ คุณได้เรียนรู้เกี่ยวกับประเภทต่างๆ ของออโต้เอนโค้ดเดอร์ที่นักวิทยาศาสตร์ AI สามารถใช้ได้ คุณได้เรียนรู้วิธีสร้างและใช้งานเพื่อสร้างภาพขึ้นมาใหม่ นอกจากนี้ คุณยังได้เรียนรู้เกี่ยวกับ VAE และวิธีใช้มันเพื่อสร้างภาพใหม่

## 🚀 ความท้าทาย

ในบทเรียนนี้ คุณได้เรียนรู้เกี่ยวกับการใช้ออโต้เอนโค้ดเดอร์สำหรับภาพ แต่พวกมันยังสามารถใช้กับดนตรีได้ด้วย! ลองดูโครงการ MusicVAE ของ Magenta [MusicVAE](https://magenta.tensorflow.org/music-vae) ซึ่งใช้ออโต้เอนโค้ดเดอร์เพื่อเรียนรู้การสร้างดนตรีขึ้นมาใหม่ ลองทำ [การทดลอง](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) กับไลบรารีนี้เพื่อดูว่าคุณสามารถสร้างอะไรได้บ้าง

## [แบบทดสอบหลังเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## การทบทวนและการศึกษาด้วยตนเอง

สำหรับการอ้างอิง อ่านเพิ่มเติมเกี่ยวกับออโต้เอนโค้ดเดอร์ในแหล่งข้อมูลเหล่านี้:

* [การสร้างออโต้เอนโค้ดเดอร์ใน Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [บล็อกโพสต์บน NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [การอธิบายออโต้เอนโค้ดเดอร์เชิงแปรผัน](https://kvfrans.com/variational-autoencoders-explained/)
* [ออโต้เอนโค้ดเดอร์เชิงแปรผันแบบมีเงื่อนไข](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## การบ้าน

ในตอนท้ายของ [สมุดบันทึกนี้ที่ใช้ TensorFlow](AutoencodersTF.ipynb) คุณจะพบ 'งาน' - ใช้สิ่งนี้เป็นการบ้านของคุณ

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาต้นทางควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้