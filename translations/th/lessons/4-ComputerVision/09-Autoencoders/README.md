<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1b8d9e1b3a6f1daa864b1ff3dfc3076d",
  "translation_date": "2025-09-23T09:05:59+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "th"
}
-->
# ออโตเอนโคเดอร์

เมื่อฝึก CNN หนึ่งในปัญหาคือเราต้องการข้อมูลที่มีการติดป้ายกำกับจำนวนมาก ในกรณีของการจำแนกภาพ เราต้องแยกภาพออกเป็นคลาสต่าง ๆ ซึ่งเป็นงานที่ต้องทำด้วยมือ

## [แบบทดสอบก่อนเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/17)

อย่างไรก็ตาม เราอาจต้องการใช้ข้อมูลดิบ (ที่ไม่มีการติดป้ายกำกับ) เพื่อฝึกตัวดึงคุณลักษณะของ CNN ซึ่งเรียกว่า **การเรียนรู้แบบกึ่งกำกับดูแล** แทนที่จะใช้ป้ายกำกับ เราจะใช้ภาพฝึกเป็นทั้งอินพุตและเอาต์พุตของเครือข่าย แนวคิดหลักของ **ออโตเอนโคเดอร์** คือเราจะมี **เครือข่ายเอนโคเดอร์** ที่แปลงภาพอินพุตไปเป็น **พื้นที่แฝง** (โดยปกติจะเป็นเวกเตอร์ที่มีขนาดเล็กกว่า) จากนั้น **เครือข่ายดีโคเดอร์** ซึ่งมีเป้าหมายในการสร้างภาพต้นฉบับขึ้นมาใหม่

> ✅ [ออโตเอนโคเดอร์](https://wikipedia.org/wiki/Autoencoder) คือ "ประเภทของเครือข่ายประสาทเทียมที่ใช้เรียนรู้การเข้ารหัสข้อมูลที่มีประสิทธิภาพจากข้อมูลที่ไม่มีการติดป้ายกำกับ"

เนื่องจากเรากำลังฝึกออโตเอนโคเดอร์เพื่อจับข้อมูลจากภาพต้นฉบับให้ได้มากที่สุดเพื่อการสร้างใหม่ที่แม่นยำ เครือข่ายจึงพยายามค้นหา **การฝังตัว** ที่ดีที่สุดของภาพอินพุตเพื่อจับความหมายของภาพ

![AutoEncoder Diagram](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.th.jpg)

> ภาพจาก [บล็อก Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## สถานการณ์ที่ใช้ Autoencoders

แม้ว่าการสร้างภาพต้นฉบับขึ้นมาใหม่อาจดูเหมือนไม่มีประโยชน์ในตัวเอง แต่มีบางสถานการณ์ที่ออโตเอนโคเดอร์มีประโยชน์อย่างยิ่ง:

* **ลดมิติของภาพเพื่อการแสดงผล** หรือ **ฝึกการฝังตัวของภาพ** โดยทั่วไปออโตเอนโคเดอร์ให้ผลลัพธ์ที่ดีกว่า PCA เพราะมันคำนึงถึงลักษณะเชิงพื้นที่ของภาพและคุณลักษณะเชิงลำดับชั้น
* **การลบสัญญาณรบกวน** เช่น การลบเสียงรบกวนออกจากภาพ เนื่องจากเสียงรบกวนมีข้อมูลที่ไม่มีประโยชน์มากมาย ออโตเอนโคเดอร์ไม่สามารถใส่ข้อมูลทั้งหมดลงในพื้นที่แฝงที่ค่อนข้างเล็กได้ ดังนั้นมันจึงจับเฉพาะส่วนสำคัญของภาพ เมื่อฝึกตัวลบสัญญาณรบกวน เราเริ่มต้นด้วยภาพต้นฉบับ และใช้ภาพที่มีการเพิ่มเสียงรบกวนเทียมเป็นอินพุตสำหรับออโตเอนโคเดอร์
* **การเพิ่มความละเอียดของภาพ** โดยเริ่มต้นจากภาพที่มีความละเอียดสูง และใช้ภาพที่มีความละเอียดต่ำกว่าเป็นอินพุตของออโตเอนโคเดอร์
* **โมเดลการสร้าง** เมื่อเราฝึกออโตเอนโคเดอร์แล้ว ส่วนดีโคเดอร์สามารถใช้สร้างวัตถุใหม่โดยเริ่มจากเวกเตอร์แฝงแบบสุ่ม

## ออโตเอนโคเดอร์แบบแปรผัน (VAE)

ออโตเอนโคเดอร์แบบดั้งเดิมลดมิติของข้อมูลอินพุตโดยการค้นหาคุณลักษณะที่สำคัญของภาพอินพุต อย่างไรก็ตาม เวกเตอร์แฝงมักไม่มีความหมายมากนัก กล่าวคือ หากใช้ชุดข้อมูล MNIST เป็นตัวอย่าง การค้นหาว่าตัวเลขใดที่สอดคล้องกับเวกเตอร์แฝงต่าง ๆ ไม่ใช่เรื่องง่าย เพราะเวกเตอร์แฝงที่ใกล้กันไม่ได้หมายความว่าจะสอดคล้องกับตัวเลขเดียวกันเสมอไป

ในทางกลับกัน หากต้องการฝึก *โมเดลการสร้าง* จะดีกว่าหากมีความเข้าใจเกี่ยวกับพื้นที่แฝง แนวคิดนี้นำเราไปสู่ **ออโตเอนโคเดอร์แบบแปรผัน** (VAE)

VAE คือออโตเอนโคเดอร์ที่เรียนรู้การทำนาย *การแจกแจงทางสถิติ* ของพารามิเตอร์แฝง หรือที่เรียกว่า **การแจกแจงแฝง** ตัวอย่างเช่น เราอาจต้องการให้เวกเตอร์แฝงมีการแจกแจงแบบปกติที่มีค่าเฉลี่ย z<sub>mean</sub> และส่วนเบี่ยงเบนมาตรฐาน z<sub>sigma</sub> (ทั้งค่าเฉลี่ยและส่วนเบี่ยงเบนมาตรฐานเป็นเวกเตอร์ที่มีมิติ d) เอนโคเดอร์ใน VAE เรียนรู้การทำนายพารามิเตอร์เหล่านี้ และดีโคเดอร์จะใช้เวกเตอร์สุ่มจากการแจกแจงนี้เพื่อสร้างวัตถุขึ้นมาใหม่

สรุปได้ว่า:

* จากเวกเตอร์อินพุต เราทำนาย `z_mean` และ `z_log_sigma` (แทนที่จะทำนายส่วนเบี่ยงเบนมาตรฐานโดยตรง เราทำนายลอการิทึมของมัน)
* เราสุ่มเวกเตอร์ `sample` จากการแจกแจง N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
* ดีโคเดอร์พยายามถอดรหัสภาพต้นฉบับโดยใช้ `sample` เป็นเวกเตอร์อินพุต

<img src="images/vae.png" width="50%">

> ภาพจาก [บล็อกโพสต์นี้](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) โดย Isaak Dykeman

ออโตเอนโคเดอร์แบบแปรผันใช้ฟังก์ชันการสูญเสียที่ซับซ้อนซึ่งประกอบด้วยสองส่วน:

* **การสูญเสียจากการสร้างใหม่** เป็นฟังก์ชันการสูญเสียที่แสดงให้เห็นว่าภาพที่สร้างขึ้นใหม่ใกล้เคียงกับเป้าหมายมากแค่ไหน (อาจเป็น Mean Squared Error หรือ MSE) ซึ่งเป็นฟังก์ชันการสูญเสียเดียวกับในออโตเอนโคเดอร์แบบปกติ
* **การสูญเสีย KL** ซึ่งทำให้แน่ใจว่าการแจกแจงตัวแปรแฝงยังคงใกล้เคียงกับการแจกแจงแบบปกติ โดยอิงจากแนวคิดของ [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) ซึ่งเป็นเมตริกที่ใช้ประเมินว่าการแจกแจงทางสถิติสองแบบมีความคล้ายคลึงกันมากแค่ไหน

ข้อดีสำคัญของ VAE คือมันช่วยให้เราสร้างภาพใหม่ได้ง่ายขึ้น เพราะเรารู้ว่าควรสุ่มเวกเตอร์แฝงจากการแจกแจงใด ตัวอย่างเช่น หากเราฝึก VAE ด้วยเวกเตอร์แฝง 2 มิติบน MNIST เราสามารถเปลี่ยนแปลงองค์ประกอบของเวกเตอร์แฝงเพื่อให้ได้ตัวเลขต่าง ๆ:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> ภาพโดย [Dmitry Soshnikov](http://soshnikov.com)

สังเกตว่าภาพผสมกันอย่างไร เมื่อเราเริ่มได้เวกเตอร์แฝงจากส่วนต่าง ๆ ของพื้นที่พารามิเตอร์แฝง เราสามารถแสดงภาพพื้นที่นี้ใน 2 มิติได้เช่นกัน:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> ภาพโดย [Dmitry Soshnikov](http://soshnikov.com)

## ✍️ แบบฝึกหัด: ออโตเอนโคเดอร์

เรียนรู้เพิ่มเติมเกี่ยวกับออโตเอนโคเดอร์ในโน้ตบุ๊กต่อไปนี้:

* [Autoencoders ใน TensorFlow](AutoencodersTF.ipynb)
* [Autoencoders ใน PyTorch](AutoEncodersPyTorch.ipynb)

## คุณสมบัติของออโตเอนโคเดอร์

* **เฉพาะข้อมูล** - มันทำงานได้ดีเฉพาะกับประเภทของภาพที่ได้รับการฝึกมาเท่านั้น ตัวอย่างเช่น หากเราฝึกเครือข่ายเพิ่มความละเอียดบนภาพดอกไม้ มันจะไม่ทำงานได้ดีบนภาพบุคคล เพราะเครือข่ายสามารถสร้างภาพที่มีความละเอียดสูงขึ้นโดยการนำรายละเอียดที่ละเอียดมาจากคุณลักษณะที่เรียนรู้จากชุดข้อมูลฝึก
* **สูญเสียข้อมูลบางส่วน** - ภาพที่สร้างขึ้นใหม่ไม่เหมือนกับภาพต้นฉบับ ลักษณะของการสูญเสียถูกกำหนดโดย *ฟังก์ชันการสูญเสีย* ที่ใช้ระหว่างการฝึก
* ทำงานกับ **ข้อมูลที่ไม่มีการติดป้ายกำกับ**

## [แบบทดสอบหลังเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## สรุป

ในบทเรียนนี้ คุณได้เรียนรู้เกี่ยวกับประเภทต่าง ๆ ของออโตเอนโคเดอร์ที่มีให้สำหรับนักวิทยาศาสตร์ AI คุณได้เรียนรู้วิธีสร้างและวิธีใช้มันเพื่อสร้างภาพขึ้นมาใหม่ นอกจากนี้คุณยังได้เรียนรู้เกี่ยวกับ VAE และวิธีใช้มันเพื่อสร้างภาพใหม่

## 🚀 ความท้าทาย

ในบทเรียนนี้ คุณได้เรียนรู้เกี่ยวกับการใช้ออโตเอนโคเดอร์สำหรับภาพ แต่พวกมันยังสามารถใช้กับดนตรีได้ด้วย! ลองดูโครงการ [MusicVAE](https://magenta.tensorflow.org/music-vae) ของ Magenta ซึ่งใช้ออโตเอนโคเดอร์เพื่อเรียนรู้การสร้างดนตรีขึ้นมาใหม่ ลองทำ [การทดลอง](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) กับไลบรารีนี้เพื่อดูว่าคุณสามารถสร้างอะไรได้บ้าง

## [แบบทดสอบหลังเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## ทบทวนและศึกษาด้วยตนเอง

สำหรับการอ้างอิง อ่านเพิ่มเติมเกี่ยวกับออโตเอนโคเดอร์ในแหล่งข้อมูลเหล่านี้:

* [การสร้างออโตเอนโคเดอร์ใน Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [บล็อกโพสต์บน NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Variational Autoencoders Explained](https://kvfrans.com/variational-autoencoders-explained/)
* [Conditional Variational Autoencoders](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## งานที่มอบหมาย

ในตอนท้ายของ [โน้ตบุ๊กนี้ที่ใช้ TensorFlow](AutoencodersTF.ipynb) คุณจะพบ 'งาน' - ใช้สิ่งนี้เป็นงานที่มอบหมายของคุณ

---

