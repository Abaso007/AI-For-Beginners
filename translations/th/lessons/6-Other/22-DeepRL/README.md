<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T09:02:41+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "th"
}
-->
# การเรียนรู้เสริมกำลังเชิงลึก (Deep Reinforcement Learning)

การเรียนรู้เสริมกำลัง (Reinforcement Learning หรือ RL) ถือเป็นหนึ่งในแนวทางพื้นฐานของการเรียนรู้ของเครื่อง (Machine Learning) ควบคู่ไปกับการเรียนรู้แบบมีผู้สอน (Supervised Learning) และการเรียนรู้แบบไม่มีผู้สอน (Unsupervised Learning) ในขณะที่การเรียนรู้แบบมีผู้สอนอาศัยชุดข้อมูลที่มีผลลัพธ์ที่ทราบล่วงหน้า RL นั้นอิงกับแนวคิด **เรียนรู้จากการลงมือทำ** ตัวอย่างเช่น เมื่อเราเล่นเกมคอมพิวเตอร์ครั้งแรก เราเริ่มเล่นโดยที่ยังไม่รู้กฎเกณฑ์ แต่ไม่นานเราก็สามารถพัฒนาทักษะได้จากการเล่นและปรับเปลี่ยนพฤติกรรมของเรา

## [แบบทดสอบก่อนเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/43)

ในการทำ RL เราจำเป็นต้องมี:

* **สภาพแวดล้อม** หรือ **ตัวจำลอง** ที่กำหนดกฎของเกม เราควรสามารถรันการทดลองในตัวจำลองและสังเกตผลลัพธ์ได้
* **ฟังก์ชันรางวัล** ซึ่งบ่งบอกถึงความสำเร็จของการทดลอง ในกรณีของการเรียนรู้การเล่นเกมคอมพิวเตอร์ รางวัลอาจเป็นคะแนนสุดท้ายของเรา

จากฟังก์ชันรางวัลนี้ เราควรสามารถปรับพฤติกรรมของเราและพัฒนาทักษะ เพื่อให้ครั้งต่อไปเราเล่นได้ดีขึ้น ความแตกต่างหลักระหว่าง RL กับการเรียนรู้ของเครื่องประเภทอื่นคือ ใน RL เรามักไม่ทราบว่าเราชนะหรือแพ้จนกว่าจะจบเกม ดังนั้น เราไม่สามารถบอกได้ว่าการเคลื่อนไหวใดๆ เพียงอย่างเดียวดีหรือไม่ดี - เราจะได้รับรางวัลก็ต่อเมื่อจบเกมเท่านั้น

ในระหว่างการทำ RL เรามักจะทำการทดลองหลายครั้ง ในแต่ละการทดลอง เราต้องสมดุลระหว่างการใช้กลยุทธ์ที่ดีที่สุดที่เราเรียนรู้มาแล้ว (**การใช้ประโยชน์**) และการสำรวจสถานะใหม่ๆ ที่เป็นไปได้ (**การสำรวจ**)

## OpenAI Gym

เครื่องมือที่ยอดเยี่ยมสำหรับ RL คือ [OpenAI Gym](https://gym.openai.com/) - **สภาพแวดล้อมจำลอง** ซึ่งสามารถจำลองสภาพแวดล้อมที่หลากหลาย ตั้งแต่เกม Atari ไปจนถึงฟิสิกส์ของการทรงตัวของเสา เป็นหนึ่งในสภาพแวดล้อมจำลองที่ได้รับความนิยมมากที่สุดสำหรับการฝึกอัลกอริทึมการเรียนรู้เสริมกำลัง และได้รับการดูแลโดย [OpenAI](https://openai.com/)

> **Note**: คุณสามารถดูสภาพแวดล้อมทั้งหมดที่มีใน OpenAI Gym ได้ [ที่นี่](https://gym.openai.com/envs/#classic_control)

## การทรงตัวของ CartPole

คุณอาจเคยเห็นอุปกรณ์ทรงตัวสมัยใหม่ เช่น *Segway* หรือ *Gyroscooters* ซึ่งสามารถทรงตัวได้โดยอัตโนมัติผ่านการปรับล้อให้ตอบสนองต่อสัญญาณจากเครื่องวัดความเร่งหรือไจโรสโคป ในส่วนนี้ เราจะเรียนรู้วิธีแก้ปัญหาที่คล้ายกัน - การทรงตัวของเสา ซึ่งคล้ายกับสถานการณ์ที่นักแสดงละครสัตว์ต้องทรงตัวเสาบนมือของเขา - แต่การทรงตัวของเสานี้เกิดขึ้นในมิติเดียว (1D)

เวอร์ชันที่เรียบง่ายของการทรงตัวนี้เรียกว่า **ปัญหา CartPole** ในโลกของ CartPole เรามีแถบเลื่อนแนวนอนที่สามารถเคลื่อนที่ไปทางซ้ายหรือขวาได้ และเป้าหมายคือการทรงตัวของเสาตั้งตรงบนแถบเลื่อนในขณะที่มันเคลื่อนที่

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

ในการสร้างและใช้สภาพแวดล้อมนี้ เราต้องใช้โค้ด Python เพียงไม่กี่บรรทัด:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

แต่ละสภาพแวดล้อมสามารถเข้าถึงได้ในลักษณะเดียวกัน:
* `env.reset` เริ่มการทดลองใหม่
* `env.step` ทำการจำลองในแต่ละขั้นตอน โดยรับ **action** จาก **action space** และส่งคืน **observation** (จาก observation space) พร้อมกับรางวัลและธงสิ้นสุดการทดลอง

ในตัวอย่างข้างต้น เราทำการสุ่ม action ในแต่ละขั้นตอน ซึ่งทำให้การทดลองมีอายุสั้นมาก:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

เป้าหมายของอัลกอริทึม RL คือการฝึกโมเดล - หรือที่เรียกว่า **policy** &pi; - ซึ่งจะส่งคืน action เพื่อตอบสนองต่อสถานะที่กำหนด เราสามารถพิจารณา policy ว่าเป็นแบบสุ่มได้ เช่น สำหรับสถานะ *s* และ action *a* มันจะส่งคืนความน่าจะเป็น &pi;(*a*|*s*) ที่เราควรเลือก *a* ในสถานะ *s*

## อัลกอริทึม Policy Gradients

วิธีที่ชัดเจนที่สุดในการสร้าง policy คือการสร้างโครงข่ายประสาทเทียม (Neural Network) ที่รับสถานะเป็นอินพุต และส่งคืน action ที่สอดคล้องกัน (หรือความน่าจะเป็นของ action ทั้งหมด) ในแง่นี้ มันจะคล้ายกับงานการจำแนกประเภททั่วไป แต่มีความแตกต่างสำคัญ - เราไม่ทราบล่วงหน้าว่า action ใดที่ควรเลือกในแต่ละขั้นตอน

แนวคิดคือการประมาณค่าความน่าจะเป็นเหล่านั้น เราสร้างเวกเตอร์ของ **รางวัลสะสม** ซึ่งแสดงรางวัลรวมของเราในแต่ละขั้นตอนของการทดลอง นอกจากนี้เรายังใช้ **การลดค่ารางวัล** โดยการคูณรางวัลก่อนหน้าด้วยค่าสัมประสิทธิ์ &gamma;=0.99 เพื่อลดบทบาทของรางวัลก่อนหน้า จากนั้นเราจะเสริมขั้นตอนเหล่านั้นตามเส้นทางการทดลองที่ให้รางวัลมากกว่า

> เรียนรู้เพิ่มเติมเกี่ยวกับอัลกอริทึม Policy Gradient และดูตัวอย่างใน [สมุดบันทึกตัวอย่าง](CartPole-RL-TF.ipynb)

## อัลกอริทึม Actor-Critic

เวอร์ชันที่ปรับปรุงของแนวทาง Policy Gradients เรียกว่า **Actor-Critic** แนวคิดหลักคือโครงข่ายประสาทเทียมจะถูกฝึกให้ส่งคืนสองสิ่ง:

* Policy ซึ่งกำหนด action ที่ควรเลือก ส่วนนี้เรียกว่า **actor**
* การประมาณค่ารางวัลรวมที่เราคาดว่าจะได้รับในสถานะนี้ - ส่วนนี้เรียกว่า **critic**

ในแง่หนึ่ง สถาปัตยกรรมนี้คล้ายกับ [GAN](../../4-ComputerVision/10-GANs/README.md) ซึ่งเรามีโครงข่ายสองตัวที่ฝึกซึ่งกันและกัน ในโมเดล Actor-Critic, actor จะเสนอ action ที่เราต้องทำ และ critic จะพยายามวิจารณ์และประมาณผลลัพธ์ อย่างไรก็ตาม เป้าหมายของเราคือการฝึกโครงข่ายเหล่านี้ให้ทำงานร่วมกัน

เนื่องจากเราทราบทั้งรางวัลสะสมจริงและผลลัพธ์ที่ critic ส่งคืนระหว่างการทดลอง จึงค่อนข้างง่ายที่จะสร้างฟังก์ชันการสูญเสีย (loss function) ที่จะลดความแตกต่างระหว่างพวกมัน นั่นจะให้เราได้ **critic loss** เราสามารถคำนวณ **actor loss** โดยใช้แนวทางเดียวกับในอัลกอริทึม Policy Gradient

หลังจากรันอัลกอริทึมเหล่านี้ เราสามารถคาดหวังให้ CartPole ของเราทำงานได้ดังนี้:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ แบบฝึกหัด: Policy Gradients และ Actor-Critic RL

เรียนรู้เพิ่มเติมในสมุดบันทึกต่อไปนี้:

* [RL ใน TensorFlow](CartPole-RL-TF.ipynb)
* [RL ใน PyTorch](CartPole-RL-PyTorch.ipynb)

## งาน RL อื่นๆ

ปัจจุบัน Reinforcement Learning เป็นสาขาการวิจัยที่เติบโตอย่างรวดเร็ว ตัวอย่างที่น่าสนใจของการเรียนรู้เสริมกำลัง ได้แก่:

* การสอนคอมพิวเตอร์ให้เล่น **เกม Atari** ความท้าทายในปัญหานี้คือเราไม่มีสถานะง่ายๆ ที่แสดงเป็นเวกเตอร์ แต่เป็นภาพหน้าจอ - และเราจำเป็นต้องใช้ CNN เพื่อแปลงภาพหน้าจอนี้เป็นเวกเตอร์คุณลักษณะ หรือดึงข้อมูลรางวัลออกมา เกม Atari มีให้ใน Gym
* การสอนคอมพิวเตอร์ให้เล่นเกมกระดาน เช่น หมากรุกและโกะ โปรแกรมที่ล้ำสมัยอย่าง **Alpha Zero** ได้รับการฝึกฝนจากศูนย์โดยให้ตัวแทนสองตัวเล่นกันเองและพัฒนาขึ้นในแต่ละขั้นตอน
* ในอุตสาหกรรม RL ถูกใช้เพื่อสร้างระบบควบคุมจากการจำลอง บริการที่เรียกว่า [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) ได้รับการออกแบบมาโดยเฉพาะสำหรับสิ่งนี้

## สรุป

เราได้เรียนรู้วิธีฝึกตัวแทนให้บรรลุผลลัพธ์ที่ดีเพียงแค่ให้ฟังก์ชันรางวัลที่กำหนดสถานะที่ต้องการของเกม และให้โอกาสพวกเขาสำรวจพื้นที่ค้นหาอย่างชาญฉลาด เราได้ลองใช้อัลกอริทึมสองตัวและประสบความสำเร็จในระยะเวลาที่ค่อนข้างสั้น อย่างไรก็ตาม นี่เป็นเพียงจุดเริ่มต้นของการเดินทางของคุณใน RL และคุณควรพิจารณาเรียนหลักสูตรแยกต่างหากหากคุณต้องการเจาะลึก

## 🚀 ความท้าทาย

สำรวจแอปพลิเคชันที่ระบุไว้ในส่วน 'งาน RL อื่นๆ' และลองนำไปใช้ดู!

## [แบบทดสอบหลังเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## การทบทวนและการศึกษาด้วยตนเอง

เรียนรู้เพิ่มเติมเกี่ยวกับการเรียนรู้เสริมกำลังแบบคลาสสิกใน [หลักสูตร Machine Learning for Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)

ชม [วิดีโอที่ยอดเยี่ยมนี้](https://www.youtube.com/watch?v=qv6UVOQ0F44) ที่พูดถึงวิธีที่คอมพิวเตอร์สามารถเรียนรู้การเล่น Super Mario

## งานที่ได้รับมอบหมาย: [ฝึก Mountain Car](lab/README.md)

เป้าหมายของคุณในงานนี้คือการฝึกสภาพแวดล้อม Gym อื่น - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)

---

