<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-29T08:41:05+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "th"
}
-->
# การเรียนรู้เสริมกำลังเชิงลึก

การเรียนรู้เสริมกำลัง (Reinforcement Learning หรือ RL) ถือเป็นหนึ่งในกระบวนทัศน์พื้นฐานของการเรียนรู้ของเครื่อง (Machine Learning) ควบคู่ไปกับการเรียนรู้แบบมีผู้สอน (Supervised Learning) และการเรียนรู้แบบไม่มีผู้สอน (Unsupervised Learning) ในขณะที่การเรียนรู้แบบมีผู้สอนอาศัยชุดข้อมูลที่มีผลลัพธ์ที่ทราบล่วงหน้า RL จะอิงกับ **การเรียนรู้จากการลงมือทำ** ตัวอย่างเช่น เมื่อเราเล่นเกมคอมพิวเตอร์ครั้งแรก เราเริ่มเล่นโดยที่ไม่รู้กฎเกณฑ์ใด ๆ และในไม่ช้าเราก็สามารถพัฒนาทักษะของเราได้จากกระบวนการเล่นและปรับพฤติกรรมของเราเอง

## [แบบทดสอบก่อนเรียน](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

ในการทำ RL เราจำเป็นต้องมี:

* **สภาพแวดล้อม** หรือ **ตัวจำลอง** ที่กำหนดกฎของเกม เราควรสามารถรันการทดลองในตัวจำลองและสังเกตผลลัพธ์ได้
* **ฟังก์ชันรางวัล** ซึ่งบ่งบอกถึงความสำเร็จของการทดลองของเรา ในกรณีของการเรียนรู้การเล่นเกมคอมพิวเตอร์ รางวัลอาจเป็นคะแนนสุดท้ายของเรา

จากฟังก์ชันรางวัลนี้ เราควรสามารถปรับพฤติกรรมของเราและพัฒนาทักษะ เพื่อให้ครั้งต่อไปเราเล่นได้ดีขึ้น ความแตกต่างหลักระหว่างการเรียนรู้ของเครื่องประเภทอื่น ๆ และ RL คือใน RL เรามักจะไม่ทราบว่าเราชนะหรือแพ้จนกว่าจะจบเกม ดังนั้น เราไม่สามารถบอกได้ว่าการเคลื่อนไหวใด ๆ เพียงอย่างเดียวดีหรือไม่ดี - เราจะได้รับรางวัลก็ต่อเมื่อจบเกมเท่านั้น

ในระหว่างการทำ RL เรามักจะทำการทดลองหลายครั้ง ในแต่ละการทดลอง เราต้องสร้างสมดุลระหว่างการใช้กลยุทธ์ที่ดีที่สุดที่เราได้เรียนรู้มาแล้ว (**การใช้ประโยชน์**) และการสำรวจสถานะใหม่ ๆ ที่เป็นไปได้ (**การสำรวจ**)

## OpenAI Gym

เครื่องมือที่ยอดเยี่ยมสำหรับ RL คือ [OpenAI Gym](https://gym.openai.com/) - **สภาพแวดล้อมจำลอง** ซึ่งสามารถจำลองสภาพแวดล้อมที่หลากหลาย ตั้งแต่เกม Atari ไปจนถึงฟิสิกส์เบื้องหลังการทรงตัวของเสา OpenAI Gym เป็นหนึ่งในสภาพแวดล้อมจำลองที่ได้รับความนิยมมากที่สุดสำหรับการฝึกอัลกอริทึมการเรียนรู้เสริมกำลัง และได้รับการดูแลโดย [OpenAI](https://openai.com/)

> **Note**: คุณสามารถดูสภาพแวดล้อมทั้งหมดที่มีใน OpenAI Gym ได้ [ที่นี่](https://gym.openai.com/envs/#classic_control)

## การทรงตัวของ CartPole

คุณอาจเคยเห็นอุปกรณ์ทรงตัวสมัยใหม่ เช่น *Segway* หรือ *Gyroscooters* อุปกรณ์เหล่านี้สามารถทรงตัวได้โดยอัตโนมัติ โดยการปรับล้อให้ตอบสนองต่อสัญญาณจากเครื่องวัดความเร่งหรือไจโรสโคป ในส่วนนี้ เราจะเรียนรู้วิธีแก้ปัญหาที่คล้ายกัน - การทรงตัวของเสา ซึ่งคล้ายกับสถานการณ์ที่นักแสดงละครสัตว์ต้องทรงตัวเสาบนมือของเขา - แต่การทรงตัวของเสานี้เกิดขึ้นในมิติเดียว (1D) เท่านั้น

เวอร์ชันที่เรียบง่ายของการทรงตัวนี้เรียกว่า **ปัญหา CartPole** ในโลกของ CartPole เรามีแถบเลื่อนแนวนอนที่สามารถเคลื่อนไปทางซ้ายหรือขวาได้ และเป้าหมายคือการทรงตัวของเสาตั้งตรงบนแถบเลื่อนในขณะที่มันเคลื่อนที่

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

ในการสร้างและใช้สภาพแวดล้อมนี้ เราต้องใช้โค้ด Python เพียงไม่กี่บรรทัด:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

แต่ละสภาพแวดล้อมสามารถเข้าถึงได้ในลักษณะเดียวกัน:
* `env.reset` เริ่มการทดลองใหม่
* `env.step` ทำการจำลองในแต่ละขั้นตอน โดยรับ **action** จาก **action space** และส่งคืน **observation** (จาก observation space) รวมถึงรางวัลและธงสิ้นสุดการทดลอง

ในตัวอย่างข้างต้น เราทำการสุ่ม action ในแต่ละขั้นตอน ซึ่งเป็นเหตุผลว่าทำไมการทดลองจึงมีอายุสั้น:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

เป้าหมายของอัลกอริทึม RL คือการฝึกโมเดล - หรือที่เรียกว่า **policy** π - ซึ่งจะส่งคืน action เพื่อตอบสนองต่อสถานะที่กำหนด เราสามารถพิจารณา policy ว่าเป็นแบบสุ่มได้ เช่น สำหรับสถานะ *s* และ action *a* ใด ๆ policy จะส่งคืนความน่าจะเป็น π(*a*|*s*) ที่เราควรเลือก *a* ในสถานะ *s*

## อัลกอริทึม Policy Gradients

วิธีที่ชัดเจนที่สุดในการสร้าง policy คือการสร้างโครงข่ายประสาทเทียม (Neural Network) ที่รับสถานะเป็นอินพุต และส่งคืน action ที่สอดคล้องกัน (หรือความน่าจะเป็นของ action ทั้งหมด) ในแง่นี้ มันจะคล้ายกับงานการจำแนกประเภททั่วไป แต่มีความแตกต่างสำคัญ - เราไม่ทราบล่วงหน้าว่า action ใดที่เราควรเลือกในแต่ละขั้นตอน

แนวคิดคือการประมาณค่าความน่าจะเป็นเหล่านั้น เราสร้างเวกเตอร์ของ **รางวัลสะสม** ซึ่งแสดงรางวัลรวมของเราในแต่ละขั้นตอนของการทดลอง นอกจากนี้ เรายังใช้ **การลดค่ารางวัล** โดยการคูณรางวัลก่อนหน้าด้วยค่าสัมประสิทธิ์ γ=0.99 เพื่อลดบทบาทของรางวัลก่อนหน้า จากนั้น เราเสริมความแข็งแกร่งให้กับขั้นตอนเหล่านั้นตามเส้นทางการทดลองที่ให้รางวัลมากกว่า

> เรียนรู้เพิ่มเติมเกี่ยวกับอัลกอริทึม Policy Gradient และดูตัวอย่างใน [สมุดบันทึกตัวอย่าง](CartPole-RL-TF.ipynb)

## อัลกอริทึม Actor-Critic

เวอร์ชันที่ปรับปรุงของวิธี Policy Gradients เรียกว่า **Actor-Critic** แนวคิดหลักคือโครงข่ายประสาทเทียมจะถูกฝึกให้ส่งคืนสองสิ่ง:

* Policy ซึ่งกำหนด action ที่ควรทำ ส่วนนี้เรียกว่า **actor**
* การประมาณรางวัลรวมที่เราคาดว่าจะได้รับในสถานะนี้ - ส่วนนี้เรียกว่า **critic**

ในแง่หนึ่ง สถาปัตยกรรมนี้คล้ายกับ [GAN](../../4-ComputerVision/10-GANs/README.md) ซึ่งเรามีโครงข่ายสองตัวที่ฝึกซึ่งกันและกัน ในโมเดล Actor-Critic, actor จะเสนอ action ที่เราต้องทำ และ critic จะพยายามวิจารณ์และประมาณผลลัพธ์ อย่างไรก็ตาม เป้าหมายของเราคือการฝึกโครงข่ายเหล่านั้นให้ทำงานร่วมกัน

เนื่องจากเราทราบทั้งรางวัลสะสมจริงและผลลัพธ์ที่ critic ส่งคืนระหว่างการทดลอง จึงค่อนข้างง่ายที่จะสร้างฟังก์ชันความสูญเสีย (Loss Function) ที่จะลดความแตกต่างระหว่างพวกมัน ซึ่งจะให้ **critic loss** เราสามารถคำนวณ **actor loss** โดยใช้วิธีเดียวกับในอัลกอริทึม Policy Gradient

หลังจากรันอัลกอริทึมเหล่านี้ เราสามารถคาดหวังให้ CartPole ของเราทำงานได้ดังนี้:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ แบบฝึกหัด: Policy Gradients และ Actor-Critic RL

เรียนรู้เพิ่มเติมในสมุดบันทึกต่อไปนี้:

* [RL ใน TensorFlow](CartPole-RL-TF.ipynb)
* [RL ใน PyTorch](CartPole-RL-PyTorch.ipynb)

## งาน RL อื่น ๆ

ปัจจุบัน การเรียนรู้เสริมกำลังเป็นสาขาการวิจัยที่เติบโตอย่างรวดเร็ว ตัวอย่างที่น่าสนใจของการเรียนรู้เสริมกำลัง ได้แก่:

* การสอนคอมพิวเตอร์ให้เล่น **เกม Atari** ความท้าทายในปัญหานี้คือเราไม่มีสถานะง่าย ๆ ที่แสดงเป็นเวกเตอร์ แต่เป็นภาพหน้าจอ - และเราจำเป็นต้องใช้ CNN เพื่อแปลงภาพหน้าจอนี้เป็นเวกเตอร์คุณลักษณะ หรือดึงข้อมูลรางวัลออกมา เกม Atari มีให้ใน Gym
* การสอนคอมพิวเตอร์ให้เล่นเกมกระดาน เช่น หมากรุกและโกะ โปรแกรมที่ล้ำสมัย เช่น **Alpha Zero** ได้รับการฝึกฝนจากศูนย์โดยตัวแทนสองตัวเล่นกันเองและพัฒนาขึ้นในแต่ละขั้นตอน
* ในอุตสาหกรรม RL ถูกใช้เพื่อสร้างระบบควบคุมจากการจำลอง บริการที่เรียกว่า [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) ได้รับการออกแบบมาโดยเฉพาะสำหรับสิ่งนี้

## สรุป

เราได้เรียนรู้วิธีฝึกตัวแทนให้บรรลุผลลัพธ์ที่ดีเพียงแค่ให้ฟังก์ชันรางวัลที่กำหนดสถานะที่ต้องการของเกม และให้โอกาสพวกเขาสำรวจพื้นที่ค้นหาอย่างชาญฉลาด เราได้ลองใช้อัลกอริทึมสองตัวและได้ผลลัพธ์ที่ดีในระยะเวลาอันสั้น อย่างไรก็ตาม นี่เป็นเพียงจุดเริ่มต้นของการเดินทางของคุณใน RL และคุณควรพิจารณาเรียนหลักสูตรแยกต่างหากหากคุณต้องการเจาะลึกลงไป

## 🚀 ความท้าทาย

สำรวจแอปพลิเคชันที่ระบุไว้ในส่วน 'งาน RL อื่น ๆ' และลองนำไปใช้ดู!

## [แบบทดสอบหลังเรียน](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## การทบทวนและการศึกษาด้วยตนเอง

เรียนรู้เพิ่มเติมเกี่ยวกับการเรียนรู้เสริมกำลังแบบคลาสสิกใน [หลักสูตร Machine Learning for Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)

ดู [วิดีโอที่ยอดเยี่ยมนี้](https://www.youtube.com/watch?v=qv6UVOQ0F44) ที่พูดถึงวิธีที่คอมพิวเตอร์สามารถเรียนรู้การเล่น Super Mario

## การบ้าน: [ฝึก Mountain Car](lab/README.md)

เป้าหมายของคุณในงานนี้คือการฝึกสภาพแวดล้อม Gym อื่น - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้