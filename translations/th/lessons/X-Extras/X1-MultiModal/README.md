<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9c592c26aca16ca085d268c732284187",
  "translation_date": "2025-08-29T08:44:41+00:00",
  "source_file": "lessons/X-Extras/X1-MultiModal/README.md",
  "language_code": "th"
}
-->
# เครือข่ายแบบหลายโมดอล

หลังจากความสำเร็จของโมเดล Transformer ในการแก้ปัญหางาน NLP สถาปัตยกรรมเดียวกันหรือคล้ายกันก็ถูกนำมาใช้กับงานด้านคอมพิวเตอร์วิทัศน์ (Computer Vision) มีความสนใจเพิ่มขึ้นในการสร้างโมเดลที่สามารถ *รวม* ความสามารถด้านภาพและภาษาธรรมชาติ หนึ่งในความพยายามดังกล่าวคือโมเดล CLIP และ DALL.E ที่พัฒนาโดย OpenAI

## การฝึกอบรมภาพแบบเปรียบเทียบ (CLIP)

แนวคิดหลักของ CLIP คือการเปรียบเทียบข้อความ (text prompts) กับภาพ และประเมินว่าภาพนั้นสอดคล้องกับข้อความมากน้อยเพียงใด

![สถาปัตยกรรม CLIP](../../../../../translated_images/clip-arch.b3dbf20b4e8ed8be1c38e2bc6100fd3cc257c33cda4692b301be91f791b13ea7.th.png)

> *ภาพจาก [บทความนี้](https://openai.com/blog/clip/)*

โมเดลนี้ถูกฝึกด้วยภาพที่ได้จากอินเทอร์เน็ตและคำบรรยายภาพ สำหรับแต่ละชุดข้อมูล เราจะนำคู่ (ภาพ, ข้อความ) จำนวน N คู่ และแปลงเป็นเวกเตอร์ตัวแทน I และ T เวกเตอร์เหล่านี้จะถูกจับคู่กัน ฟังก์ชันการสูญเสีย (loss function) ถูกกำหนดให้เพิ่มความคล้ายคลึงของโคไซน์ (cosine similarity) ระหว่างเวกเตอร์ที่เป็นคู่เดียวกัน (เช่น I และ T) และลดความคล้ายคลึงของโคไซน์ระหว่างคู่ที่เหลือทั้งหมด นี่คือเหตุผลที่วิธีการนี้เรียกว่า **contrastive**

ไลบรารี CLIP สามารถใช้งานได้จาก [GitHub ของ OpenAI](https://github.com/openai/CLIP) วิธีการนี้อธิบายไว้ใน [บทความนี้](https://openai.com/blog/clip/) และในรายละเอียดเพิ่มเติมใน [เอกสารนี้](https://arxiv.org/pdf/2103.00020.pdf)

เมื่อโมเดลนี้ถูกฝึกเสร็จแล้ว เราสามารถให้ชุดภาพและข้อความแก่โมเดล และโมเดลจะคืนค่าเป็นเทนเซอร์ที่มีความน่าจะเป็น CLIP สามารถใช้ในหลายงาน เช่น:

**การจำแนกภาพ (Image Classification)**

สมมติว่าเราต้องการจำแนกภาพระหว่างแมว สุนัข และมนุษย์ ในกรณีนี้ เราสามารถให้โมเดลรับภาพและข้อความ เช่น "*ภาพของแมว*", "*ภาพของสุนัข*", "*ภาพของมนุษย์*" ในเวกเตอร์ผลลัพธ์ที่มีความน่าจะเป็น 3 ค่า เราเพียงแค่เลือกดัชนีที่มีค่ามากที่สุด

![CLIP สำหรับการจำแนกภาพ](../../../../../translated_images/clip-class.3af42ef0b2b19369a633df5f20ddf4f5a01d6c8ffa181e9d3a0572c19f919f72.th.png)

> *ภาพจาก [บทความนี้](https://openai.com/blog/clip/)*

**การค้นหาภาพด้วยข้อความ (Text-Based Image Search)**

เรายังสามารถทำในทางกลับกันได้ หากเรามีชุดภาพ เราสามารถส่งชุดภาพนี้ไปยังโมเดลพร้อมข้อความ และโมเดลจะคืนค่าภาพที่คล้ายกับข้อความมากที่สุด

## ✍️ ตัวอย่าง: [การใช้ CLIP สำหรับการจำแนกภาพและการค้นหาภาพ](Clip.ipynb)

เปิดโน้ตบุ๊ก [Clip.ipynb](Clip.ipynb) เพื่อดูการทำงานของ CLIP

## การสร้างภาพด้วย VQGAN+CLIP

CLIP ยังสามารถใช้สำหรับ **การสร้างภาพ** จากข้อความได้อีกด้วย ในการทำเช่นนี้ เราจำเป็นต้องมี **โมเดลตัวสร้าง (generator model)** ที่สามารถสร้างภาพจากเวกเตอร์อินพุตได้ หนึ่งในโมเดลดังกล่าวคือ [VQGAN](https://compvis.github.io/taming-transformers/) (Vector-Quantized GAN)

แนวคิดหลักของ VQGAN ที่แตกต่างจาก [GAN](../../4-ComputerVision/10-GANs/README.md) ทั่วไปคือ:
* ใช้สถาปัตยกรรม Transformer แบบ autoregressive เพื่อสร้างลำดับของส่วนภาพที่มีบริบทซึ่งประกอบกันเป็นภาพ ส่วนภาพเหล่านี้ถูกเรียนรู้โดย [CNN](../../4-ComputerVision/07-ConvNets/README.md)
* ใช้ตัวแยกแยะภาพย่อย (sub-image discriminator) เพื่อตรวจจับว่าส่วนของภาพเป็น "จริง" หรือ "ปลอม" (แทนที่จะใช้วิธี "ทั้งหมดหรือไม่มีเลย" ใน GAN ทั่วไป)

เรียนรู้เพิ่มเติมเกี่ยวกับ VQGAN ได้ที่เว็บไซต์ [Taming Transformers](https://compvis.github.io/taming-transformers/)

ความแตกต่างสำคัญระหว่าง VQGAN และ GAN ทั่วไปคือ GAN สามารถสร้างภาพที่ดีจากเวกเตอร์อินพุตใด ๆ ได้ แต่ VQGAN อาจสร้างภาพที่ไม่สอดคล้องกัน ดังนั้นเราจำเป็นต้องมีการชี้นำเพิ่มเติมในกระบวนการสร้างภาพ ซึ่งสามารถทำได้โดยใช้ CLIP

![สถาปัตยกรรม VQGAN+CLIP](../../../../../translated_images/vqgan.5027fe05051dfa3101950cfa930303f66e6478b9bd273e83766731796e462d9b.th.png)

ในการสร้างภาพที่สอดคล้องกับข้อความ เราเริ่มต้นด้วยเวกเตอร์การเข้ารหัสแบบสุ่มที่ถูกส่งผ่าน VQGAN เพื่อสร้างภาพ จากนั้นใช้ CLIP เพื่อสร้างฟังก์ชันการสูญเสียที่แสดงว่าภาพสอดคล้องกับข้อความมากน้อยเพียงใด เป้าหมายคือการลดค่าฟังก์ชันการสูญเสียนี้โดยใช้การถ่ายทอดย้อนกลับ (back propagation) เพื่อปรับพารามิเตอร์ของเวกเตอร์อินพุต

ไลบรารีที่ยอดเยี่ยมที่นำ VQGAN+CLIP มาใช้งานคือ [Pixray](http://github.com/pixray/pixray)

![ภาพที่สร้างโดย Pixray](../../../../../translated_images/a_closeup_watercolor_portrait_of_young_male_teacher_of_literature_with_a_book.2384968e9db8a0d09dc96de938b9f95bde8a7e1c721f48f286a7795bf16d56c7.th.png) |  ![ภาพที่สร้างโดย Pixray](../../../../../translated_images/a_closeup_oil_portrait_of_young_female_teacher_of_computer_science_with_a_computer.e0b6495f210a439077e1c32cc8afdf714e634fe24dc78dc5aa45fd2f560b0ed5.th.png) | ![ภาพที่สร้างโดย Pixray](../../../../../translated_images/a_closeup_oil_portrait_of_old_male_teacher_of_math.5362e67aa7fc2683b9d36a613b364deb7454760cd39205623fc1e3938fa133c0.th.png)
----|----|----
ภาพที่สร้างจากข้อความ *ภาพเหมือนสีน้ำของครูหนุ่มสอนวรรณกรรมพร้อมหนังสือ* | ภาพที่สร้างจากข้อความ *ภาพเหมือนสีน้ำมันของครูสาวสอนวิทยาการคอมพิวเตอร์พร้อมคอมพิวเตอร์* | ภาพที่สร้างจากข้อความ *ภาพเหมือนสีน้ำมันของครูชายสูงวัยสอนคณิตศาสตร์หน้ากระดานดำ*

> ภาพจากคอลเลกชัน **Artificial Teachers** โดย [Dmitry Soshnikov](http://soshnikov.com)

## DALL-E
### [DALL-E 1](https://openai.com/research/dall-e)
DALL-E เป็นเวอร์ชันของ GPT-3 ที่ถูกฝึกให้สร้างภาพจากข้อความ มีพารามิเตอร์ 12 พันล้านตัว

แตกต่างจาก CLIP ตรงที่ DALL-E รับทั้งข้อความและภาพเป็นสตรีมเดียวของโทเค็นสำหรับทั้งภาพและข้อความ ดังนั้นจากข้อความหลาย ๆ ข้อความ คุณสามารถสร้างภาพตามข้อความได้

### [DALL-E 2](https://openai.com/dall-e-2)
ความแตกต่างหลักระหว่าง DALL-E 1 และ 2 คือ DALL-E 2 สามารถสร้างภาพและงานศิลปะที่สมจริงมากขึ้น

ตัวอย่างการสร้างภาพด้วย DALL-E:
![ภาพที่สร้างโดย DALL-E](../../../../../translated_images/DALL·E%202023-06-20%2015.56.56%20-%20a%20closeup%20watercolor%20portrait%20of%20young%20male%20teacher%20of%20literature%20with%20a%20book.6c235e8271d9ed10ce985d86aeb241a58518958647973af136912116b9518fce.th.png) |  ![ภาพที่สร้างโดย DALL-E](../../../../../translated_images/DALL·E%202023-06-20%2015.57.43%20-%20a%20closeup%20oil%20portrait%20of%20young%20female%20teacher%20of%20computer%20science%20with%20a%20computer.f21dc4166340b6c8b4d1cb57efd1e22127407f9b28c9ac7afe11344065369e64.th.png) | ![ภาพที่สร้างโดย DALL-E](../../../../../translated_images/DALL·E%202023-06-20%2015.58.42%20-%20%20a%20closeup%20oil%20portrait%20of%20old%20male%20teacher%20of%20mathematics%20in%20front%20of%20blackboard.d331c2dfbdc3f7c46aa65c0809066f5e7ed4b49609cd259852e760df21051e4a.th.png)
----|----|----
ภาพที่สร้างจากข้อความ *ภาพเหมือนสีน้ำของครูหนุ่มสอนวรรณกรรมพร้อมหนังสือ* | ภาพที่สร้างจากข้อความ *ภาพเหมือนสีน้ำมันของครูสาวสอนวิทยาการคอมพิวเตอร์พร้อมคอมพิวเตอร์* | ภาพที่สร้างจากข้อความ *ภาพเหมือนสีน้ำมันของครูชายสูงวัยสอนคณิตศาสตร์หน้ากระดานดำ*

## แหล่งข้อมูลอ้างอิง

* เอกสาร VQGAN: [Taming Transformers for High-Resolution Image Synthesis](https://compvis.github.io/taming-transformers/paper/paper.pdf)
* เอกสาร CLIP: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ แนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราจะไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้