{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การฝังข้อมูล\n",
    "\n",
    "ในตัวอย่างก่อนหน้านี้ เราได้ทำงานกับเวกเตอร์แบบถุงคำ (bag-of-words) ที่มีมิติสูงและมีความยาวเท่ากับ `vocab_size` และเราได้แปลงเวกเตอร์ที่แสดงตำแหน่งในมิติต่ำให้เป็นการแสดงผลแบบ one-hot ที่มีความเบาบาง การแสดงผลแบบ one-hot นี้ไม่ประหยัดหน่วยความจำ นอกจากนี้ แต่ละคำยังถูกพิจารณาแยกจากกัน ทำให้เวกเตอร์แบบ one-hot ไม่สามารถแสดงความคล้ายคลึงทางความหมายระหว่างคำได้\n",
    "\n",
    "ในหน่วยนี้ เราจะสำรวจชุดข้อมูล **News AG** ต่อไป เพื่อเริ่มต้น เรามาโหลดข้อมูลและนำคำจำกัดความบางส่วนจากหน่วยก่อนหน้ามาใช้กัน\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### อะไรคือ Embedding?\n",
    "\n",
    "แนวคิดของ **embedding** คือการแทนคำด้วยเวกเตอร์ที่มีมิติที่ต่ำกว่าและหนาแน่น ซึ่งสะท้อนถึงความหมายเชิงความหมายของคำ เราจะพูดถึงวิธีการสร้าง word embeddings ที่มีความหมายในภายหลัง แต่ตอนนี้ให้คิดว่า embedding เป็นวิธีลดมิติของเวกเตอร์คำ\n",
    "\n",
    "ดังนั้น embedding layer จะรับคำเป็น input และสร้าง output เป็นเวกเตอร์ที่มีขนาด `embedding_size` ในแง่หนึ่งมันคล้ายกับ `Dense` layer แต่แทนที่จะรับเวกเตอร์แบบ one-hot encoded เป็น input มันสามารถรับหมายเลขคำได้\n",
    "\n",
    "เมื่อใช้ embedding layer เป็นเลเยอร์แรกในเครือข่ายของเรา เราสามารถเปลี่ยนจาก bag-of-words ไปเป็นโมเดล **embedding bag** โดยที่เราจะแปลงแต่ละคำในข้อความของเราให้เป็น embedding ที่สอดคล้องกัน และคำนวณฟังก์ชันรวมบางอย่างจาก embeddings เหล่านั้น เช่น `sum`, `average` หรือ `max`\n",
    "\n",
    "![ภาพแสดงตัวอย่าง embedding classifier สำหรับคำในลำดับห้าคำ](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.th.png)\n",
    "\n",
    "เครือข่ายประสาทสำหรับการจำแนกของเราประกอบด้วยเลเยอร์ดังต่อไปนี้:\n",
    "\n",
    "* เลเยอร์ `TextVectorization` ซึ่งรับสตริงเป็น input และสร้างเทนเซอร์ของหมายเลขโทเค็น เราจะกำหนดขนาดคำศัพท์ `vocab_size` ที่เหมาะสม และละเว้นคำที่ใช้น้อยกว่า รูปร่างของ input จะเป็น 1 และรูปร่างของ output จะเป็น $n$ เนื่องจากเราจะได้ $n$ โทเค็นเป็นผลลัพธ์ โดยแต่ละโทเค็นจะมีหมายเลขตั้งแต่ 0 ถึง `vocab_size`\n",
    "* เลเยอร์ `Embedding` ซึ่งรับ $n$ หมายเลข และลดแต่ละหมายเลขลงเป็นเวกเตอร์หนาแน่นที่มีความยาวที่กำหนด (100 ในตัวอย่างของเรา) ดังนั้นเทนเซอร์ input ที่มีรูปร่าง $n$ จะถูกแปลงเป็นเทนเซอร์ $n\\times 100$\n",
    "* เลเยอร์การรวม (Aggregation layer) ซึ่งคำนวณค่าเฉลี่ยของเทนเซอร์นี้ตามแกนแรก กล่าวคือมันจะคำนวณค่าเฉลี่ยของเทนเซอร์ input $n$ ทั้งหมดที่สอดคล้องกับคำต่าง ๆ เพื่อสร้างเลเยอร์นี้ เราจะใช้เลเยอร์ `Lambda` และส่งฟังก์ชันที่คำนวณค่าเฉลี่ยเข้าไป รูปร่างของ output จะเป็น 100 และมันจะเป็นตัวแทนเชิงตัวเลขของลำดับ input ทั้งหมด\n",
    "* ตัวจำแนกเชิงเส้น `Dense` สุดท้าย\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ใน `summary` ที่แสดงผล ในคอลัมน์ **output shape** มิติแรกของเทนเซอร์ `None` หมายถึงขนาดของมินิแบตช์ และมิติที่สองหมายถึงความยาวของลำดับโทเค็น ลำดับโทเค็นทั้งหมดในมินิแบตช์มีความยาวที่แตกต่างกัน เราจะพูดถึงวิธีจัดการกับสิ่งนี้ในส่วนถัดไป\n",
    "\n",
    "ตอนนี้มาเริ่มฝึกเครือข่ายกัน:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **หมายเหตุ** เรากำลังสร้างเวกเตอไรเซอร์โดยใช้ข้อมูลเพียงบางส่วนเท่านั้น การทำเช่นนี้เพื่อเพิ่มความเร็วในกระบวนการ และอาจส่งผลให้บางโทเค็นจากข้อความของเราไม่ปรากฏในคลังคำศัพท์ ในกรณีนี้ โทเค็นเหล่านั้นจะถูกละเว้น ซึ่งอาจทำให้ความแม่นยำลดลงเล็กน้อย อย่างไรก็ตาม ในชีวิตจริง ข้อความบางส่วนมักให้การประมาณคลังคำศัพท์ที่ดี\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การจัดการกับขนาดลำดับตัวแปร\n",
    "\n",
    "มาทำความเข้าใจว่าการฝึกอบรมในรูปแบบมินิแบตช์เกิดขึ้นได้อย่างไร ในตัวอย่างข้างต้น เทนเซอร์อินพุตมีมิติเท่ากับ 1 และเราใช้มินิแบตช์ที่มีความยาว 128 ดังนั้นขนาดจริงของเทนเซอร์คือ $128 \\times 1$ อย่างไรก็ตาม จำนวนโทเค็นในแต่ละประโยคจะแตกต่างกันออกไป หากเราใช้เลเยอร์ `TextVectorization` กับอินพุตเดียว จำนวนโทเค็นที่ได้จะไม่เท่ากัน ขึ้นอยู่กับวิธีการที่ข้อความถูกแปลงเป็นโทเค็น:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "อย่างไรก็ตาม เมื่อเราใช้เวคเตอไรเซอร์กับลำดับหลายชุด มันจะต้องสร้างเทนเซอร์ที่มีรูปทรงสี่เหลี่ยม ดังนั้นมันจึงเติมองค์ประกอบที่ไม่ได้ใช้งานด้วยโทเค็น PAD (ซึ่งในกรณีของเราคือศูนย์):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ที่นี่เราสามารถเห็นการฝังข้อมูล:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การฝังความหมาย: Word2Vec\n",
    "\n",
    "ในตัวอย่างก่อนหน้านี้ เลเยอร์การฝัง (embedding layer) ได้เรียนรู้การแปลงคำให้เป็นเวกเตอร์ แต่เวกเตอร์เหล่านั้นยังไม่มีความหมายในเชิงความหมาย (semantic meaning) จะดีกว่าหากเราสามารถเรียนรู้การแปลงคำให้เป็นเวกเตอร์ที่คำที่มีความหมายคล้ายกันหรือคำพ้องความหมายมีเวกเตอร์ที่อยู่ใกล้กันในแง่ของระยะทางเวกเตอร์บางประเภท (เช่น ระยะทางแบบยุคลิด)\n",
    "\n",
    "เพื่อทำเช่นนั้น เราจำเป็นต้องฝึกโมเดลการฝังของเราล่วงหน้าด้วยชุดข้อความขนาดใหญ่ โดยใช้เทคนิคอย่าง [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) ซึ่งอิงตามสถาปัตยกรรมหลักสองแบบที่ใช้ในการสร้างการแสดงผลแบบกระจายของคำ:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW) ซึ่งเราฝึกโมเดลให้ทำนายคำจากบริบทโดยรอบ โดยให้ ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ เป้าหมายของโมเดลคือการทำนาย $W_0$ จาก $(W_{-2},W_{-1},W_1,W_2)$\n",
    " - **Continuous skip-gram** ตรงข้ามกับ CBoW โมเดลจะใช้บริบทของคำรอบข้างเพื่อทำนายคำปัจจุบัน\n",
    "\n",
    "CBoW ทำงานได้เร็วกว่า ในขณะที่ skip-gram แม้จะช้ากว่า แต่สามารถแสดงคำที่พบได้น้อยได้ดีกว่า\n",
    "\n",
    "![ภาพแสดงอัลกอริธึม CBoW และ Skip-Gram สำหรับแปลงคำเป็นเวกเตอร์](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.th.png)\n",
    "\n",
    "เพื่อทดลองใช้การฝัง Word2Vec ที่ฝึกไว้ล่วงหน้าด้วยชุดข้อมูล Google News เราสามารถใช้ไลบรารี **gensim** ด้านล่างนี้เป็นตัวอย่างการค้นหาคำที่คล้ายกับ 'neural' มากที่สุด\n",
    "\n",
    "> **Note:** เมื่อคุณสร้างเวกเตอร์คำครั้งแรก การดาวน์โหลดอาจใช้เวลาสักครู่!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เรายังสามารถดึงเวกเตอร์ฝังตัวจากคำมาใช้ในการฝึกโมเดลการจำแนกได้ เวกเตอร์ฝังตัวมี 300 องค์ประกอบ แต่ที่นี่เราจะแสดงเฉพาะ 20 องค์ประกอบแรกของเวกเตอร์เพื่อความชัดเจน:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "ตัวอย่างข้างต้นใช้เวทมนตร์ภายในของ GenSym บางอย่าง แต่ตรรกะพื้นฐานนั้นค่อนข้างง่าย สิ่งที่น่าสนใจเกี่ยวกับการฝังตัวคือคุณสามารถดำเนินการเวกเตอร์ปกติบนเวกเตอร์การฝังตัว และนั่นจะสะท้อนถึงการดำเนินการบน **ความหมาย** ของคำ ตัวอย่างข้างต้นสามารถแสดงออกในแง่ของการดำเนินการเวกเตอร์: เราคำนวณเวกเตอร์ที่สอดคล้องกับ **KING-MAN+WOMAN** (การดำเนินการ `+` และ `-` ถูกดำเนินการบนตัวแทนเวกเตอร์ของคำที่เกี่ยวข้อง) และจากนั้นค้นหาคำที่ใกล้เคียงที่สุดในพจนานุกรมกับเวกเตอร์นั้น:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: เราต้องเพิ่มค่าสัมประสิทธิ์เล็กน้อยให้กับเวกเตอร์ *man* และ *woman* - ลองลบออกดูเพื่อดูว่าจะเกิดอะไรขึ้น\n",
    "\n",
    "ในการหาเวกเตอร์ที่ใกล้ที่สุด เราใช้เครื่องมือของ TensorFlow เพื่อคำนวณเวกเตอร์ของระยะห่างระหว่างเวกเตอร์ของเราและเวกเตอร์ทั้งหมดในคำศัพท์ จากนั้นจึงหาอินเด็กซ์ของคำที่มีค่าน้อยที่สุดโดยใช้ `argmin`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในขณะที่ Word2Vec ดูเหมือนจะเป็นวิธีที่ยอดเยี่ยมในการแสดงความหมายของคำ แต่ก็มีข้อเสียหลายประการ รวมถึง:\n",
    "\n",
    "* ทั้งโมเดล CBoW และ skip-gram เป็น **predictive embeddings** ซึ่งพิจารณาเพียงบริบทในพื้นที่เท่านั้น Word2Vec ไม่ได้ใช้ประโยชน์จากบริบทในภาพรวม\n",
    "* Word2Vec ไม่ได้คำนึงถึง **morphology** ของคำ เช่น ความหมายของคำที่อาจขึ้นอยู่กับส่วนต่าง ๆ ของคำ เช่น รากศัพท์  \n",
    "\n",
    "**FastText** พยายามแก้ไขข้อจำกัดในข้อที่สอง โดยพัฒนาต่อยอดจาก Word2Vec ด้วยการเรียนรู้การแสดงผลแบบเวกเตอร์สำหรับแต่ละคำและ n-grams ของตัวอักษรที่พบในแต่ละคำ จากนั้นค่าของการแสดงผลเหล่านี้จะถูกเฉลี่ยเป็นเวกเตอร์เดียวในแต่ละขั้นตอนการฝึก แม้ว่าวิธีนี้จะเพิ่มการคำนวณเพิ่มเติมในขั้นตอนการฝึกเบื้องต้น แต่มันช่วยให้ word embeddings สามารถเข้ารหัสข้อมูลในระดับย่อยของคำได้\n",
    "\n",
    "อีกวิธีหนึ่งคือ **GloVe** ซึ่งใช้แนวทางที่แตกต่างในการสร้าง word embeddings โดยอิงจากการแยกตัวประกอบของเมทริกซ์ word-context ขั้นแรกจะสร้างเมทริกซ์ขนาดใหญ่ที่นับจำนวนการปรากฏของคำในบริบทต่าง ๆ จากนั้นพยายามแสดงเมทริกซ์นี้ในมิติที่ต่ำกว่าในลักษณะที่ลดการสูญเสียจากการสร้างใหม่ให้น้อยที่สุด\n",
    "\n",
    "ไลบรารี gensim รองรับ word embeddings เหล่านี้ และคุณสามารถทดลองใช้งานได้โดยการเปลี่ยนโค้ดการโหลดโมเดลด้านบน\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การใช้ pretrained embeddings ใน Keras\n",
    "\n",
    "เราสามารถปรับตัวอย่างข้างต้นเพื่อเติมข้อมูลในเมทริกซ์ของ embedding layer ด้วย semantic embeddings เช่น Word2Vec ได้ คำศัพท์ของ pretrained embedding และ text corpus อาจไม่ตรงกัน ดังนั้นเราจำเป็นต้องเลือกหนึ่งในสองตัวเลือก ที่นี่เราจะสำรวจสองตัวเลือกที่เป็นไปได้: การใช้คำศัพท์จาก tokenizer และการใช้คำศัพท์จาก Word2Vec embeddings\n",
    "\n",
    "### การใช้คำศัพท์จาก tokenizer\n",
    "\n",
    "เมื่อใช้คำศัพท์จาก tokenizer บางคำในคำศัพท์จะมี Word2Vec embeddings ที่สอดคล้องกัน และบางคำจะไม่มี โดยที่ขนาดของคำศัพท์ของเราคือ `vocab_size` และความยาวของเวกเตอร์ Word2Vec embedding คือ `embed_size` embedding layer จะถูกแทนด้วยเมทริกซ์น้ำหนักที่มีรูปร่าง `vocab_size`$\\times$`embed_size` เราจะเติมข้อมูลในเมทริกซ์นี้โดยการไล่ผ่านคำศัพท์:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สำหรับคำที่ไม่มีอยู่ในคลังคำศัพท์ของ Word2Vec เราสามารถเลือกที่จะปล่อยให้เป็นค่าเป็นศูนย์ หรือสร้างเวกเตอร์แบบสุ่มขึ้นมาแทน\n",
    "\n",
    "ตอนนี้เราสามารถกำหนดชั้น embedding ที่มีน้ำหนักที่ถูกฝึกไว้ล่วงหน้าได้:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **หมายเหตุ**: สังเกตว่าเราได้ตั้งค่า `trainable=False` เมื่อสร้าง `Embedding` ซึ่งหมายความว่าเราไม่ได้ฝึกฝนเลเยอร์ Embedding ใหม่ การทำเช่นนี้อาจทำให้ความแม่นยำลดลงเล็กน้อย แต่จะช่วยให้การฝึกฝนเร็วขึ้น\n",
    "\n",
    "### การใช้คำศัพท์ของ embedding\n",
    "\n",
    "ปัญหาหนึ่งของวิธีการก่อนหน้านี้คือคำศัพท์ที่ใช้ใน TextVectorization และ Embedding นั้นแตกต่างกัน เพื่อแก้ปัญหานี้ เราสามารถใช้หนึ่งในวิธีการต่อไปนี้:\n",
    "* ฝึกฝนโมเดล Word2Vec ใหม่โดยใช้คำศัพท์ของเรา\n",
    "* โหลดชุดข้อมูลของเราด้วยคำศัพท์จากโมเดล Word2Vec ที่ผ่านการฝึกฝนมาแล้ว คำศัพท์ที่ใช้ในการโหลดชุดข้อมูลสามารถระบุได้ในระหว่างการโหลด\n",
    "\n",
    "วิธีที่สองดูเหมือนจะง่ายกว่า ดังนั้นเรามาลองทำกัน ก่อนอื่นเราจะสร้างเลเยอร์ `TextVectorization` โดยใช้คำศัพท์ที่ระบุ ซึ่งนำมาจาก Word2Vec embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ไลบรารี gensim word embeddings มีฟังก์ชันที่สะดวกชื่อ `get_keras_embeddings` ซึ่งจะสร้างเลเยอร์ Keras embeddings ที่สอดคล้องกันให้คุณโดยอัตโนมัติ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หนึ่งในเหตุผลที่เราไม่เห็นความแม่นยำที่สูงขึ้นเป็นเพราะคำบางคำจากชุดข้อมูลของเราหายไปในคลังคำศัพท์ GloVe ที่ผ่านการฝึกมาแล้ว และดังนั้นคำเหล่านั้นจึงถูกมองข้ามไป เพื่อแก้ปัญหานี้ เราสามารถฝึกฝนการฝังตัวคำของเราเองโดยอ้างอิงจากชุดข้อมูลของเรา\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การฝังบริบท\n",
    "\n",
    "ข้อจำกัดสำคัญของการฝังคำแบบดั้งเดิมที่ผ่านการฝึกอบรมล่วงหน้า เช่น Word2Vec คือ แม้ว่ามันจะสามารถจับความหมายบางส่วนของคำได้ แต่ก็ไม่สามารถแยกแยะความหมายที่แตกต่างกันได้ ซึ่งอาจทำให้เกิดปัญหาในโมเดลที่ใช้งานต่อเนื่อง\n",
    "\n",
    "ตัวอย่างเช่น คำว่า 'play' มีความหมายต่างกันในสองประโยคนี้:\n",
    "- ฉันไปดู **ละคร** ที่โรงละคร\n",
    "- จอห์นอยาก **เล่น** กับเพื่อนของเขา\n",
    "\n",
    "การฝังคำที่ผ่านการฝึกอบรมล่วงหน้าที่เราได้พูดถึงนั้นจะเป็นตัวแทนของทั้งสองความหมายของคำว่า 'play' ในการฝังเดียวกัน เพื่อแก้ไขข้อจำกัดนี้ เราจำเป็นต้องสร้างการฝังคำที่อิงตาม **โมเดลภาษา** ซึ่งได้รับการฝึกอบรมจากชุดข้อความขนาดใหญ่ และ *เข้าใจ* ว่าคำสามารถนำมารวมกันในบริบทที่แตกต่างกันได้อย่างไร การพูดคุยเกี่ยวกับการฝังบริบทอยู่นอกเหนือขอบเขตของบทเรียนนี้ แต่เราจะกลับมาพูดถึงเรื่องนี้อีกครั้งเมื่อพูดถึงโมเดลภาษาในหน่วยถัดไป\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-29T10:56:45+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}