<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-29T09:26:34+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "th"
}
-->
# การฝังข้อมูล (Embeddings)

## [แบบทดสอบก่อนเรียน](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

เมื่อเราฝึกโมเดลจำแนกประเภทโดยใช้ BoW หรือ TF/IDF เราจะทำงานกับเวกเตอร์ bag-of-words ที่มีมิติสูงซึ่งมีความยาวเท่ากับ `vocab_size` และเราจะเปลี่ยนจากเวกเตอร์ที่มีมิติต่ำในรูปแบบตำแหน่งไปเป็นเวกเตอร์ one-hot ที่มีความหนาแน่นต่ำ อย่างไรก็ตาม การแสดงผลแบบ one-hot นี้ไม่ประหยัดหน่วยความจำ นอกจากนี้ แต่ละคำยังถูกพิจารณาแยกจากกัน กล่าวคือ เวกเตอร์ one-hot ไม่ได้แสดงถึงความคล้ายคลึงทางความหมายระหว่างคำ

แนวคิดของ **การฝังข้อมูล (embedding)** คือการแทนคำด้วยเวกเตอร์ที่มีมิติต่ำและหนาแน่น ซึ่งสะท้อนถึงความหมายของคำในเชิงความหมาย เราจะพูดถึงวิธีการสร้างการฝังคำที่มีความหมายในภายหลัง แต่ตอนนี้ให้คิดว่าการฝังข้อมูลเป็นวิธีลดมิติของเวกเตอร์คำ

ดังนั้น เลเยอร์ embedding จะรับคำเป็นอินพุต และสร้างเวกเตอร์เอาต์พุตที่มีขนาด `embedding_size` ในแง่หนึ่ง มันคล้ายกับเลเยอร์ `Linear` แต่แทนที่จะรับเวกเตอร์ one-hot เป็นอินพุต มันสามารถรับหมายเลขคำเป็นอินพุตได้ ทำให้เราไม่ต้องสร้างเวกเตอร์ one-hot ที่มีขนาดใหญ่

โดยการใช้เลเยอร์ embedding เป็นเลเยอร์แรกในเครือข่ายจำแนกประเภทของเรา เราสามารถเปลี่ยนจาก bag-of-words ไปเป็นโมเดล **embedding bag** ซึ่งเราจะแปลงแต่ละคำในข้อความของเราให้เป็น embedding ที่สอดคล้องกัน และคำนวณฟังก์ชันรวมบางอย่าง เช่น `sum`, `average` หรือ `max` บน embedding เหล่านั้นทั้งหมด  

![ภาพแสดงตัวอย่างตัวจำแนก embedding สำหรับคำในลำดับห้าคำ](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.th.png)

> ภาพโดยผู้เขียน

## ✍️ แบบฝึกหัด: การฝังข้อมูล

เรียนรู้เพิ่มเติมในโน้ตบุ๊กต่อไปนี้:
* [Embeddings with PyTorch](EmbeddingsPyTorch.ipynb)
* [Embeddings TensorFlow](EmbeddingsTF.ipynb)

## การฝังข้อมูลเชิงความหมาย: Word2Vec

แม้ว่าเลเยอร์ embedding จะเรียนรู้การแมปคำไปยังเวกเตอร์ แต่การแสดงผลนี้อาจไม่ได้มีความหมายเชิงความหมายมากนัก จะดีกว่าหากเราเรียนรู้การแสดงผลเวกเตอร์ที่คำที่คล้ายกันหรือคำพ้องความหมายมีเวกเตอร์ที่อยู่ใกล้กันในแง่ของระยะทางเวกเตอร์บางประเภท (เช่น ระยะทาง Euclidean)

เพื่อทำเช่นนั้น เราจำเป็นต้องฝึกโมเดล embedding ของเราล่วงหน้าบนชุดข้อความขนาดใหญ่ในวิธีเฉพาะ วิธีหนึ่งในการฝึกการฝังข้อมูลเชิงความหมายเรียกว่า [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) ซึ่งมีพื้นฐานมาจากสถาปัตยกรรมหลักสองแบบที่ใช้ในการสร้างการแสดงผลแบบกระจายของคำ:

 - **Continuous bag-of-words** (CBoW) — ในสถาปัตยกรรมนี้ เราฝึกโมเดลให้ทำนายคำจากบริบทโดยรอบ โดยให้ ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ เป้าหมายของโมเดลคือการทำนาย $W_0$ จาก $(W_{-2},W_{-1},W_1,W_2)$
 - **Continuous skip-gram** ตรงข้ามกับ CBoW โมเดลจะใช้บริบทคำรอบข้างเพื่อทำนายคำปัจจุบัน

CBoW ทำงานได้เร็วกว่า ในขณะที่ skip-gram ช้ากว่า แต่ทำงานได้ดีกว่าในการแสดงคำที่พบไม่บ่อย

![ภาพแสดงอัลกอริทึม CBoW และ Skip-Gram สำหรับการแปลงคำเป็นเวกเตอร์](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.th.png)

> ภาพจาก [เอกสารนี้](https://arxiv.org/pdf/1301.3781.pdf)

การฝังข้อมูลที่ผ่านการฝึก Word2Vec ล่วงหน้า (รวมถึงโมเดลที่คล้ายกัน เช่น GloVe) สามารถใช้แทนเลเยอร์ embedding ในเครือข่ายประสาทเทียมได้ อย่างไรก็ตาม เราจำเป็นต้องจัดการกับคำศัพท์ เนื่องจากคำศัพท์ที่ใช้ในการฝึก Word2Vec/GloVe ล่วงหน้ามักจะแตกต่างจากคำศัพท์ในชุดข้อความของเรา ลองดูในโน้ตบุ๊กด้านบนเพื่อดูวิธีแก้ปัญหานี้

## การฝังข้อมูลเชิงบริบท

ข้อจำกัดสำคัญประการหนึ่งของการฝังข้อมูลที่ผ่านการฝึกแบบดั้งเดิม เช่น Word2Vec คือปัญหาการแยกความหมายของคำ (word sense disambiguation) แม้ว่าการฝังข้อมูลที่ผ่านการฝึกจะสามารถจับความหมายบางส่วนของคำในบริบทได้ แต่ความหมายทุกแบบของคำจะถูกเข้ารหัสใน embedding เดียวกัน สิ่งนี้อาจทำให้เกิดปัญหาในโมเดลขั้นต่อไป เนื่องจากคำหลายคำ เช่น คำว่า 'play' มีความหมายต่างกันขึ้นอยู่กับบริบทที่ใช้

ตัวอย่างเช่น คำว่า 'play' ในสองประโยคนี้มีความหมายที่แตกต่างกันอย่างมาก:

- ฉันไปดู **ละคร** ที่โรงละคร
- จอห์นอยากจะ **เล่น** กับเพื่อนของเขา

การฝังข้อมูลที่ผ่านการฝึกข้างต้นแสดงถึงความหมายทั้งสองของคำว่า 'play' ใน embedding เดียวกัน เพื่อเอาชนะข้อจำกัดนี้ เราจำเป็นต้องสร้างการฝังข้อมูลโดยอิงจาก **โมเดลภาษา** ซึ่งได้รับการฝึกบนชุดข้อความขนาดใหญ่ และ *รู้* ว่าคำสามารถนำมารวมกันในบริบทต่าง ๆ ได้อย่างไร การพูดถึงการฝังข้อมูลเชิงบริบทอยู่นอกขอบเขตของบทเรียนนี้ แต่เราจะกลับมาพูดถึงเมื่อพูดถึงโมเดลภาษาในภายหลังในคอร์สนี้

## สรุป

ในบทเรียนนี้ คุณได้เรียนรู้วิธีสร้างและใช้เลเยอร์ embedding ใน TensorFlow และ PyTorch เพื่อสะท้อนความหมายเชิงความหมายของคำได้ดียิ่งขึ้น

## 🚀 ความท้าทาย

Word2Vec ถูกนำไปใช้ในแอปพลิเคชันที่น่าสนใจ เช่น การสร้างเนื้อเพลงและบทกวี ลองดู [บทความนี้](https://www.politetype.com/blog/word2vec-color-poems) ซึ่งอธิบายวิธีที่ผู้เขียนใช้ Word2Vec เพื่อสร้างบทกวี และดู [วิดีโอนี้โดย Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) เพื่อค้นพบคำอธิบายที่แตกต่างของเทคนิคนี้ จากนั้นลองนำเทคนิคเหล่านี้ไปใช้กับชุดข้อความของคุณเอง อาจมาจาก Kaggle

## [แบบทดสอบหลังเรียน](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## ทบทวน & ศึกษาด้วยตนเอง

อ่านเอกสารนี้เกี่ยวกับ Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [งานที่ได้รับมอบหมาย: โน้ตบุ๊ก](assignment.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้