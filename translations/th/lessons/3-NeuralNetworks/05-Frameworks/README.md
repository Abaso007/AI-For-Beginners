<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddd216f558a255260a9374008002c971",
  "translation_date": "2025-09-23T09:11:23+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "th"
}
-->
# เฟรมเวิร์กของโครงข่ายประสาทเทียม

จากที่เราได้เรียนรู้ไปแล้ว การฝึกโครงข่ายประสาทเทียมให้มีประสิทธิภาพนั้นต้องทำสองสิ่งนี้:

* การทำงานกับเทนเซอร์ เช่น การคูณ การบวก และการคำนวณฟังก์ชันบางอย่าง เช่น sigmoid หรือ softmax
* การคำนวณกราเดียนของทุกสมการ เพื่อใช้ในการปรับค่าพารามิเตอร์ด้วยวิธี gradient descent optimization

## [แบบทดสอบก่อนเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/9)

แม้ว่าไลบรารี `numpy` จะสามารถทำงานในส่วนแรกได้ แต่เรายังต้องการกลไกในการคำนวณกราเดียน ใน [เฟรมเวิร์กของเรา](../04-OwnFramework/OwnFramework.ipynb) ที่พัฒนาขึ้นในส่วนก่อนหน้า เราต้องเขียนฟังก์ชันอนุพันธ์ทั้งหมดด้วยตนเองในเมธอด `backward` ซึ่งทำหน้าที่ backpropagation อุดมคติแล้ว เฟรมเวิร์กควรให้เราสามารถคำนวณกราเดียนของ *สมการใดๆ* ที่เรากำหนดได้

อีกสิ่งที่สำคัญคือความสามารถในการคำนวณบน GPU หรือหน่วยประมวลผลเฉพาะทางอื่นๆ เช่น [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) การฝึกโครงข่ายประสาทเทียมเชิงลึกต้องใช้การคำนวณจำนวนมาก และการกระจายการคำนวณเหล่านี้บน GPU เป็นสิ่งสำคัญมาก

> ✅ คำว่า 'parallelize' หมายถึงการกระจายการคำนวณไปยังอุปกรณ์หลายตัว

ปัจจุบัน เฟรมเวิร์กโครงข่ายประสาทเทียมที่ได้รับความนิยมมากที่สุดสองตัวคือ: [TensorFlow](http://TensorFlow.org) และ [PyTorch](https://pytorch.org/) ทั้งสองมี API ระดับต่ำที่ช่วยให้ทำงานกับเทนเซอร์ได้ทั้งบน CPU และ GPU นอกจากนี้ยังมี API ระดับสูงที่เรียกว่า [Keras](https://keras.io/) และ [PyTorch Lightning](https://pytorchlightning.ai/) ตามลำดับ

Low-Level API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**API ระดับต่ำ** ในทั้งสองเฟรมเวิร์กช่วยให้คุณสร้าง **กราฟการคำนวณ** ซึ่งกราฟนี้กำหนดวิธีการคำนวณผลลัพธ์ (โดยปกติคือฟังก์ชัน loss) จากพารามิเตอร์อินพุตที่กำหนด และสามารถส่งไปคำนวณบน GPU ได้หากมี ฟังก์ชันใน API นี้ยังช่วยให้สามารถคำนวณกราเดียนของกราฟการคำนวณและนำไปใช้ปรับค่าพารามิเตอร์ของโมเดลได้

**API ระดับสูง** มองโครงข่ายประสาทเทียมเป็น **ลำดับของเลเยอร์** และช่วยให้การสร้างโครงข่ายประสาทเทียมส่วนใหญ่ทำได้ง่ายขึ้น การฝึกโมเดลมักต้องเตรียมข้อมูลและเรียกใช้ฟังก์ชัน `fit` เพื่อดำเนินการ

API ระดับสูงช่วยให้คุณสร้างโครงข่ายประสาทเทียมทั่วไปได้อย่างรวดเร็วโดยไม่ต้องกังวลกับรายละเอียดมากมาย ในขณะเดียวกัน API ระดับต่ำให้การควบคุมกระบวนการฝึกที่มากกว่า และมักใช้ในงานวิจัยเมื่อคุณต้องการจัดการกับสถาปัตยกรรมโครงข่ายประสาทเทียมใหม่ๆ

นอกจากนี้ยังสำคัญที่ต้องเข้าใจว่าคุณสามารถใช้ API ทั้งสองร่วมกันได้ เช่น คุณสามารถพัฒนาสถาปัตยกรรมเลเยอร์ของโครงข่ายของคุณเองโดยใช้ API ระดับต่ำ และนำไปใช้ในโครงข่ายขนาดใหญ่ที่สร้างและฝึกด้วย API ระดับสูง หรือคุณสามารถกำหนดโครงข่ายโดยใช้ API ระดับสูงเป็นลำดับของเลเยอร์ และใช้ลูปการฝึกของคุณเองที่สร้างด้วย API ระดับต่ำเพื่อทำการปรับค่าพารามิเตอร์ ทั้งสอง API ใช้แนวคิดพื้นฐานเดียวกันและออกแบบมาให้ทำงานร่วมกันได้ดี

## การเรียนรู้

ในคอร์สนี้ เรามีเนื้อหาสำหรับทั้ง PyTorch และ TensorFlow คุณสามารถเลือกเฟรมเวิร์กที่คุณชอบและเรียนเฉพาะโน้ตบุ๊กที่เกี่ยวข้อง หากคุณยังไม่แน่ใจว่าจะเลือกเฟรมเวิร์กใด ลองอ่านการอภิปรายในอินเทอร์เน็ตเกี่ยวกับ **PyTorch vs. TensorFlow** คุณยังสามารถลองดูทั้งสองเฟรมเวิร์กเพื่อทำความเข้าใจเพิ่มเติม

เมื่อเป็นไปได้ เราจะใช้ API ระดับสูงเพื่อความง่าย อย่างไรก็ตาม เราเชื่อว่าสำคัญที่ต้องเข้าใจว่าโครงข่ายประสาทเทียมทำงานอย่างไรตั้งแต่พื้นฐาน ดังนั้นในตอนเริ่มต้นเราจะเริ่มจากการทำงานกับ API ระดับต่ำและเทนเซอร์ แต่หากคุณต้องการเริ่มต้นอย่างรวดเร็วและไม่ต้องการใช้เวลามากในการเรียนรู้รายละเอียดเหล่านี้ คุณสามารถข้ามไปยังโน้ตบุ๊ก API ระดับสูงได้เลย

## ✍️ แบบฝึกหัด: เฟรมเวิร์ก

เรียนรู้เพิ่มเติมในโน้ตบุ๊กต่อไปนี้:

Low-Level API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

หลังจากเชี่ยวชาญเฟรมเวิร์กแล้ว มาทบทวนแนวคิดเรื่อง overfitting กัน

# Overfitting

Overfitting เป็นแนวคิดที่สำคัญมากในแมชชีนเลิร์นนิง และเป็นสิ่งที่ต้องเข้าใจให้ถูกต้อง!

ลองพิจารณาปัญหาการประมาณค่าจุด 5 จุด (แสดงด้วย `x` ในกราฟด้านล่าง):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.th.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.th.jpg)
-------------------------|--------------------------
**โมเดลเชิงเส้น, 2 พารามิเตอร์** | **โมเดลไม่เชิงเส้น, 7 พารามิเตอร์**
Training error = 5.3 | Training error = 0
Validation error = 5.1 | Validation error = 20

* ทางซ้าย เราเห็นการประมาณค่าด้วยเส้นตรงที่ดี เนื่องจากจำนวนพารามิเตอร์เหมาะสม โมเดลจึงเข้าใจการกระจายของจุดได้ถูกต้อง
* ทางขวา โมเดลมีความซับซ้อนเกินไป เนื่องจากเรามีเพียง 5 จุด แต่โมเดลมี 7 พารามิเตอร์ โมเดลจึงสามารถปรับให้ผ่านทุกจุดได้ ทำให้ training error เป็น 0 อย่างไรก็ตาม สิ่งนี้ทำให้โมเดลไม่สามารถเข้าใจรูปแบบที่ถูกต้องของข้อมูลได้ validation error จึงสูงมาก

สิ่งสำคัญคือต้องหาสมดุลที่เหมาะสมระหว่างความซับซ้อนของโมเดล (จำนวนพารามิเตอร์) และจำนวนตัวอย่างการฝึก

## ทำไม overfitting ถึงเกิดขึ้น

  * ข้อมูลการฝึกไม่เพียงพอ
  * โมเดลมีความซับซ้อนเกินไป
  * ข้อมูลอินพุตมีสัญญาณรบกวนมากเกินไป

## วิธีตรวจจับ overfitting

จากกราฟด้านบน เราสามารถตรวจจับ overfitting ได้จาก training error ที่ต่ำมาก และ validation error ที่สูง โดยปกติระหว่างการฝึก เราจะเห็นทั้ง training และ validation error ลดลง แต่ในบางจุด validation error อาจหยุดลดลงและเริ่มเพิ่มขึ้น นี่เป็นสัญญาณของ overfitting และเป็นตัวบ่งชี้ว่าเราควรหยุดการฝึกในจุดนี้ (หรืออย่างน้อยควรบันทึกสถานะของโมเดล)

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.th.png)

## วิธีป้องกัน overfitting

หากคุณเห็นว่าเกิด overfitting คุณสามารถทำสิ่งต่อไปนี้:

 * เพิ่มจำนวนข้อมูลการฝึก
 * ลดความซับซ้อนของโมเดล
 * ใช้ [เทคนิคการปรับค่า](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md) เช่น [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout) ซึ่งเราจะพิจารณาในภายหลัง

## Overfitting และ Bias-Variance Tradeoff

Overfitting เป็นกรณีหนึ่งของปัญหาทั่วไปในสถิติที่เรียกว่า [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) หากเราพิจารณาแหล่งที่มาของข้อผิดพลาดในโมเดล เราจะเห็นข้อผิดพลาดสองประเภท:

* **Bias errors** เกิดจากอัลกอริทึมของเราไม่สามารถจับความสัมพันธ์ในข้อมูลการฝึกได้อย่างถูกต้อง อาจเกิดจากโมเดลของเราไม่ทรงพลังพอ (**underfitting**)
* **Variance errors** เกิดจากโมเดลจับสัญญาณรบกวนในข้อมูลอินพุตแทนที่จะจับความสัมพันธ์ที่มีความหมาย (**overfitting**)

ระหว่างการฝึก Bias error จะลดลง (เนื่องจากโมเดลเรียนรู้ที่จะประมาณค่าข้อมูล) และ Variance error จะเพิ่มขึ้น สิ่งสำคัญคือต้องหยุดการฝึก - ไม่ว่าจะด้วยวิธีการแบบแมนนวล (เมื่อเราตรวจพบ overfitting) หรืออัตโนมัติ (โดยการใช้เทคนิคการปรับค่า) - เพื่อป้องกัน overfitting

## สรุป

ในบทเรียนนี้ คุณได้เรียนรู้เกี่ยวกับความแตกต่างระหว่าง API ต่างๆ ของสองเฟรมเวิร์ก AI ที่ได้รับความนิยมมากที่สุด TensorFlow และ PyTorch นอกจากนี้ คุณยังได้เรียนรู้เกี่ยวกับหัวข้อที่สำคัญมากอย่าง overfitting

## 🚀 ความท้าทาย

ในโน้ตบุ๊กที่แนบมานี้ คุณจะพบ 'งาน' ที่อยู่ด้านล่าง ให้ทำงานในโน้ตบุ๊กและทำงานให้เสร็จ

## [แบบทดสอบหลังเรียน](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## การทบทวนและการศึกษาด้วยตนเอง

ทำการค้นคว้าในหัวข้อต่อไปนี้:

- TensorFlow
- PyTorch
- Overfitting

ถามตัวเองด้วยคำถามต่อไปนี้:

- ความแตกต่างระหว่าง TensorFlow และ PyTorch คืออะไร?
- ความแตกต่างระหว่าง overfitting และ underfitting คืออะไร?

## [การบ้าน](lab/README.md)

ในห้องปฏิบัติการนี้ คุณจะถูกขอให้แก้ปัญหาการจำแนกประเภทสองปัญหาโดยใช้โครงข่าย fully-connected แบบชั้นเดียวและหลายชั้นโดยใช้ PyTorch หรือ TensorFlow

* [คำแนะนำ](lab/README.md)
* [โน้ตบุ๊ก](lab/LabFrameworks.ipynb)

---

