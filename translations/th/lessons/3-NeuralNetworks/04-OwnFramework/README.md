<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-29T09:08:21+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "th"
}
-->
# การแนะนำเกี่ยวกับเครือข่ายประสาทเทียม: Multi-Layered Perceptron

ในส่วนก่อนหน้านี้ คุณได้เรียนรู้เกี่ยวกับโมเดลเครือข่ายประสาทเทียมที่ง่ายที่สุด - one-layered perceptron ซึ่งเป็นโมเดลการจำแนกประเภทสองคลาสแบบเชิงเส้น

ในส่วนนี้ เราจะขยายโมเดลนี้ให้เป็นกรอบการทำงานที่ยืดหยุ่นมากขึ้น ซึ่งจะช่วยให้เรา:

* ทำการ **จำแนกประเภทหลายคลาส** นอกเหนือจากสองคลาส
* แก้ปัญหา **การถดถอย** นอกเหนือจากการจำแนกประเภท
* แยกคลาสที่ไม่สามารถแยกได้แบบเชิงเส้น

เรายังจะพัฒนากรอบการทำงานแบบโมดูลาร์ใน Python ที่ช่วยให้เราสร้างสถาปัตยกรรมเครือข่ายประสาทเทียมที่แตกต่างกันได้

## [แบบทดสอบก่อนการบรรยาย](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## การทำให้ปัญหา Machine Learning เป็นทางการ

เริ่มต้นด้วยการทำให้ปัญหา Machine Learning เป็นทางการ สมมติว่าเรามีชุดข้อมูลการฝึกอบรม **X** พร้อมป้ายกำกับ **Y** และเราต้องสร้างโมเดล *f* ที่จะให้การทำนายที่แม่นยำที่สุด คุณภาพของการทำนายจะถูกวัดโดย **Loss function** ℒ ฟังก์ชัน loss ที่มักใช้มีดังนี้:

* สำหรับปัญหาการถดถอย เมื่อเราต้องการทำนายตัวเลข เราสามารถใช้ **absolute error** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| หรือ **squared error** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* สำหรับการจำแนกประเภท เราใช้ **0-1 loss** (ซึ่งโดยพื้นฐานแล้วเหมือนกับ **accuracy** ของโมเดล) หรือ **logistic loss**

สำหรับ one-level perceptron ฟังก์ชัน *f* ถูกกำหนดเป็นฟังก์ชันเชิงเส้น *f(x)=wx+b* (ที่นี่ *w* คือเมทริกซ์น้ำหนัก *x* คือเวกเตอร์ของคุณลักษณะอินพุต และ *b* คือเวกเตอร์ bias) สำหรับสถาปัตยกรรมเครือข่ายประสาทเทียมที่แตกต่างกัน ฟังก์ชันนี้สามารถมีรูปแบบที่ซับซ้อนมากขึ้นได้

> ในกรณีของการจำแนกประเภท มักจะต้องการให้ผลลัพธ์ของเครือข่ายเป็นความน่าจะเป็นของคลาสที่เกี่ยวข้อง เพื่อแปลงตัวเลขใดๆ ให้เป็นความน่าจะเป็น (เช่น การทำให้ผลลัพธ์เป็นปกติ) เรามักใช้ฟังก์ชัน **softmax** σ และฟังก์ชัน *f* จะกลายเป็น *f(x)=σ(wx+b)*

ในคำจำกัดความของ *f* ข้างต้น *w* และ *b* ถูกเรียกว่า **parameters** θ=⟨*w,b*⟩ เมื่อได้รับชุดข้อมูล ⟨**X**,**Y**⟩ เราสามารถคำนวณข้อผิดพลาดโดยรวมในชุดข้อมูลทั้งหมดเป็นฟังก์ชันของพารามิเตอร์ θ

> ✅ **เป้าหมายของการฝึกอบรมเครือข่ายประสาทเทียมคือการลดข้อผิดพลาดโดยการปรับเปลี่ยนพารามิเตอร์ θ**

## การปรับแต่งด้วย Gradient Descent

มีวิธีการปรับแต่งฟังก์ชันที่เป็นที่รู้จักกันดีเรียกว่า **gradient descent** แนวคิดคือเราสามารถคำนวณอนุพันธ์ (ในกรณีหลายมิติเรียกว่า **gradient**) ของฟังก์ชัน loss โดยสัมพันธ์กับพารามิเตอร์ และปรับเปลี่ยนพารามิเตอร์ในลักษณะที่ข้อผิดพลาดลดลง สิ่งนี้สามารถทำให้เป็นทางการได้ดังนี้:

* กำหนดค่าเริ่มต้นให้กับพารามิเตอร์ด้วยค่าที่สุ่ม w<sup>(0)</sup>, b<sup>(0)</sup>
* ทำขั้นตอนต่อไปนี้ซ้ำหลายครั้ง:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

ในระหว่างการฝึกอบรม ขั้นตอนการปรับแต่งควรคำนวณโดยพิจารณาจากชุดข้อมูลทั้งหมด (จำไว้ว่าฟังก์ชัน loss ถูกคำนวณเป็นผลรวมผ่านตัวอย่างการฝึกอบรมทั้งหมด) อย่างไรก็ตาม ในชีวิตจริง เราใช้ส่วนเล็กๆ ของชุดข้อมูลที่เรียกว่า **minibatches** และคำนวณ gradients โดยอิงจากชุดข้อมูลย่อย เนื่องจากชุดข้อมูลย่อยถูกสุ่มเลือกในแต่ละครั้ง วิธีนี้จึงเรียกว่า **stochastic gradient descent** (SGD)

## Multi-Layered Perceptrons และ Backpropagation

เครือข่ายแบบชั้นเดียว ตามที่เราได้เห็นข้างต้น สามารถจำแนกคลาสที่แยกได้แบบเชิงเส้นได้ เพื่อสร้างโมเดลที่มีความหลากหลายมากขึ้น เราสามารถรวมหลายชั้นของเครือข่ายเข้าด้วยกัน ในทางคณิตศาสตร์หมายความว่าฟังก์ชัน *f* จะมีรูปแบบที่ซับซ้อนมากขึ้น และจะถูกคำนวณในหลายขั้นตอน:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

ที่นี่ α คือ **ฟังก์ชันการกระตุ้นแบบไม่เชิงเส้น** σ คือฟังก์ชัน softmax และพารามิเตอร์ θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>

อัลกอริทึม gradient descent จะยังคงเหมือนเดิม แต่การคำนวณ gradients จะยากขึ้น ด้วยกฎการหาอนุพันธ์แบบลูกโซ่ เราสามารถคำนวณอนุพันธ์ได้ดังนี้:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ กฎการหาอนุพันธ์แบบลูกโซ่ถูกใช้ในการคำนวณอนุพันธ์ของฟังก์ชัน loss โดยสัมพันธ์กับพารามิเตอร์

โปรดทราบว่าส่วนที่อยู่ทางซ้ายสุดของนิพจน์ทั้งหมดเหมือนกัน และดังนั้นเราสามารถคำนวณอนุพันธ์ได้อย่างมีประสิทธิภาพโดยเริ่มจากฟังก์ชัน loss และย้อนกลับผ่านกราฟการคำนวณ วิธีการฝึกอบรม multi-layered perceptron นี้เรียกว่า **backpropagation** หรือ 'backprop'

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: การอ้างอิงภาพ

> ✅ เราจะครอบคลุม backprop ในรายละเอียดเพิ่มเติมในตัวอย่าง notebook ของเรา  

## สรุป

ในบทเรียนนี้ เราได้สร้างไลบรารีเครือข่ายประสาทเทียมของเราเอง และเราได้ใช้มันสำหรับงานการจำแนกประเภทสองมิติที่ง่าย

## 🚀 ความท้าทาย

ใน notebook ที่มาพร้อมกัน คุณจะได้สร้างกรอบการทำงานของคุณเองสำหรับการสร้างและฝึกอบรม multi-layered perceptrons คุณจะสามารถเห็นรายละเอียดว่าเครือข่ายประสาทเทียมสมัยใหม่ทำงานอย่างไร

ไปที่ [OwnFramework](OwnFramework.ipynb) notebook และทำงานผ่านมัน

## [แบบทดสอบหลังการบรรยาย](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## การทบทวนและการศึกษาด้วยตนเอง

Backpropagation เป็นอัลกอริทึมทั่วไปที่ใช้ใน AI และ ML ซึ่งควรศึกษา [เพิ่มเติม](https://wikipedia.org/wiki/Backpropagation)

## [งานที่ได้รับมอบหมาย](lab/README.md)

ในห้องปฏิบัติการนี้ คุณจะถูกขอให้ใช้กรอบการทำงานที่คุณสร้างขึ้นในบทเรียนนี้เพื่อแก้ปัญหาการจำแนกตัวเลขที่เขียนด้วยมือ MNIST

* [คำแนะนำ](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้