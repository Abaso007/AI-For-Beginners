<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-24T08:57:36+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "pt"
}
-->
# Autoencoders

Ao treinar CNNs, um dos problemas √© que precisamos de muitos dados rotulados. No caso da classifica√ß√£o de imagens, √© necess√°rio separar as imagens em diferentes classes, o que exige um esfor√ßo manual.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/17)

No entanto, podemos querer usar dados brutos (n√£o rotulados) para treinar extratores de caracter√≠sticas de CNN, o que √© chamado de **aprendizagem auto-supervisionada**. Em vez de r√≥tulos, usaremos as imagens de treino como entrada e sa√≠da da rede. A ideia principal do **autoencoder** √© que teremos uma **rede codificadora** que converte a imagem de entrada em algum **espa√ßo latente** (normalmente √© apenas um vetor de tamanho reduzido), e uma **rede decodificadora**, cujo objetivo ser√° reconstruir a imagem original.

> ‚úÖ Um [autoencoder](https://wikipedia.org/wiki/Autoencoder) √© "um tipo de rede neural artificial usada para aprender codifica√ß√µes eficientes de dados n√£o rotulados."

Como estamos a treinar um autoencoder para capturar o m√°ximo de informa√ß√£o poss√≠vel da imagem original para uma reconstru√ß√£o precisa, a rede tenta encontrar a melhor **representa√ß√£o** das imagens de entrada para capturar o seu significado.

![Diagrama do Autoencoder](../../../../../lessons/4-ComputerVision/09-Autoencoders/images/autoencoder_schema.jpg)

> Imagem do [blog Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Cen√°rios para usar Autoencoders

Embora reconstruir imagens originais possa n√£o parecer √∫til por si s√≥, existem alguns cen√°rios em que os autoencoders s√£o especialmente √∫teis:

* **Reduzir a dimens√£o das imagens para visualiza√ß√£o** ou **treinar representa√ß√µes de imagens**. Normalmente, os autoencoders oferecem melhores resultados do que PCA, porque levam em considera√ß√£o a natureza espacial das imagens e as caracter√≠sticas hier√°rquicas.
* **Remo√ß√£o de ru√≠do**, ou seja, eliminar o ru√≠do da imagem. Como o ru√≠do cont√©m muita informa√ß√£o in√∫til, o autoencoder n√£o consegue encaixar tudo no espa√ßo latente relativamente pequeno e, assim, captura apenas a parte importante da imagem. Ao treinar removedores de ru√≠do, come√ßamos com imagens originais e usamos imagens com ru√≠do artificialmente adicionado como entrada para o autoencoder.
* **Super-resolu√ß√£o**, aumentar a resolu√ß√£o da imagem. Come√ßamos com imagens de alta resolu√ß√£o e usamos a imagem com resolu√ß√£o mais baixa como entrada do autoencoder.
* **Modelos generativos**. Depois de treinar o autoencoder, a parte decodificadora pode ser usada para criar novos objetos a partir de vetores latentes aleat√≥rios.

## Autoencoders Variacionais (VAE)

Os autoencoders tradicionais reduzem a dimens√£o dos dados de entrada de alguma forma, identificando as caracter√≠sticas importantes das imagens de entrada. No entanto, os vetores latentes muitas vezes n√£o fazem muito sentido. Por outras palavras, usando o conjunto de dados MNIST como exemplo, identificar quais d√≠gitos correspondem a diferentes vetores latentes n√£o √© uma tarefa f√°cil, porque vetores latentes pr√≥ximos n√£o correspondem necessariamente aos mesmos d√≠gitos.

Por outro lado, para treinar modelos *generativos*, √© melhor ter algum entendimento do espa√ßo latente. Esta ideia leva-nos ao **autoencoder variacional** (VAE).

O VAE √© um autoencoder que aprende a prever a *distribui√ß√£o estat√≠stica* dos par√¢metros latentes, chamada de **distribui√ß√£o latente**. Por exemplo, podemos querer que os vetores latentes sejam distribu√≠dos normalmente com uma m√©dia z<sub>mean</sub> e desvio padr√£o z<sub>sigma</sub> (tanto a m√©dia quanto o desvio padr√£o s√£o vetores de alguma dimensionalidade d). O codificador no VAE aprende a prever esses par√¢metros, e o decodificador utiliza um vetor aleat√≥rio dessa distribui√ß√£o para reconstruir o objeto.

Resumindo:

 * A partir do vetor de entrada, prevemos `z_mean` e `z_log_sigma` (em vez de prever o desvio padr√£o diretamente, prevemos o seu logaritmo)
 * Amostramos um vetor `sample` da distribui√ß√£o N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * O decodificador tenta decodificar a imagem original usando `sample` como vetor de entrada

 <img src="images/vae.png" width="50%">

> Imagem deste [post no blog](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) por Isaak Dykeman

Os autoencoders variacionais utilizam uma fun√ß√£o de perda complexa que consiste em duas partes:

* **Perda de reconstru√ß√£o**, que √© a fun√ß√£o de perda que mostra qu√£o pr√≥xima a imagem reconstru√≠da est√° do alvo (pode ser o Erro Quadr√°tico M√©dio, ou MSE). √â a mesma fun√ß√£o de perda usada em autoencoders normais.
* **Perda KL**, que garante que as distribui√ß√µes das vari√°veis latentes permane√ßam pr√≥ximas da distribui√ß√£o normal. Baseia-se no conceito de [diverg√™ncia de Kullback-Leibler](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - uma m√©trica para estimar qu√£o semelhantes s√£o duas distribui√ß√µes estat√≠sticas.

Uma vantagem importante dos VAEs √© que permitem gerar novas imagens de forma relativamente f√°cil, porque sabemos de qual distribui√ß√£o amostrar os vetores latentes. Por exemplo, se treinarmos um VAE com vetor latente 2D no MNIST, podemos variar os componentes do vetor latente para obter diferentes d√≠gitos:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Imagem por [Dmitry Soshnikov](http://soshnikov.com)

Observe como as imagens se misturam umas com as outras, √† medida que come√ßamos a obter vetores latentes de diferentes partes do espa√ßo de par√¢metros latentes. Tamb√©m podemos visualizar este espa√ßo em 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Imagem por [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Exerc√≠cios: Autoencoders

Aprenda mais sobre autoencoders nestes notebooks correspondentes:

* [Autoencoders em TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [Autoencoders em PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Propriedades dos Autoencoders

* **Espec√≠ficos para os dados** - funcionam bem apenas com o tipo de imagens em que foram treinados. Por exemplo, se treinarmos uma rede de super-resolu√ß√£o em flores, ela n√£o funcionar√° bem em retratos. Isso ocorre porque a rede pode produzir imagens de maior resolu√ß√£o ao usar detalhes finos aprendidos a partir do conjunto de dados de treino.
* **Com perdas** - a imagem reconstru√≠da n√£o √© exatamente igual √† imagem original. A natureza da perda √© definida pela *fun√ß√£o de perda* usada durante o treino.
* Funciona com **dados n√£o rotulados**

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Conclus√£o

Nesta li√ß√£o, aprendeu sobre os v√°rios tipos de autoencoders dispon√≠veis para o cientista de IA. Aprendeu como constru√≠-los e como us√°-los para reconstruir imagens. Tamb√©m aprendeu sobre o VAE e como us√°-lo para gerar novas imagens.

## üöÄ Desafio

Nesta li√ß√£o, aprendeu a usar autoencoders para imagens. Mas eles tamb√©m podem ser usados para m√∫sica! Explore o projeto [MusicVAE](https://magenta.tensorflow.org/music-vae) do Magenta, que utiliza autoencoders para aprender a reconstruir m√∫sica. Fa√ßa alguns [experimentos](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) com esta biblioteca para ver o que consegue criar.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Revis√£o e Autoestudo

Para refer√™ncia, leia mais sobre autoencoders nestes recursos:

* [Construindo Autoencoders em Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Post no blog NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Autoencoders Variacionais Explicados](https://kvfrans.com/variational-autoencoders-explained/)
* [Autoencoders Variacionais Condicionais](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Tarefa

No final deste [notebook usando TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb), encontrar√° uma 'tarefa' - use-a como sua tarefa.

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.