<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1b8d9e1b3a6f1daa864b1ff3dfc3076d",
  "translation_date": "2025-09-23T13:41:50+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "pt"
}
-->
# Autoencoders

Ao treinar redes neuronais convolucionais (CNNs), um dos problemas √© que precisamos de muitos dados etiquetados. No caso da classifica√ß√£o de imagens, √© necess√°rio separar as imagens em diferentes classes, o que exige esfor√ßo manual.

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/17)

No entanto, podemos querer usar dados brutos (n√£o etiquetados) para treinar extratores de caracter√≠sticas de CNN, o que √© chamado de **aprendizagem auto-supervisionada**. Em vez de etiquetas, utilizamos as imagens de treino como entrada e sa√≠da da rede. A ideia principal de um **autoencoder** √© que teremos uma **rede codificadora** que converte a imagem de entrada num **espa√ßo latente** (normalmente √© apenas um vetor de tamanho reduzido), e uma **rede descodificadora**, cujo objetivo ser√° reconstruir a imagem original.

> ‚úÖ Um [autoencoder](https://wikipedia.org/wiki/Autoencoder) √© "um tipo de rede neuronal artificial usada para aprender codifica√ß√µes eficientes de dados n√£o etiquetados."

Como estamos a treinar um autoencoder para capturar o m√°ximo de informa√ß√£o poss√≠vel da imagem original para uma reconstru√ß√£o precisa, a rede tenta encontrar a melhor **representa√ß√£o** das imagens de entrada para captar o seu significado.

![Diagrama de Autoencoder](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.pt.jpg)

> Imagem retirada do [blog da Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Cen√°rios para usar Autoencoders

Embora a reconstru√ß√£o de imagens originais possa n√£o parecer √∫til por si s√≥, existem alguns cen√°rios em que os autoencoders s√£o especialmente √∫teis:

* **Redu√ß√£o da dimens√£o de imagens para visualiza√ß√£o** ou **treino de representa√ß√µes de imagens**. Normalmente, os autoencoders produzem melhores resultados do que a PCA, porque consideram a natureza espacial das imagens e as caracter√≠sticas hier√°rquicas.
* **Remo√ß√£o de ru√≠do**, ou seja, eliminar o ru√≠do da imagem. Como o ru√≠do cont√©m muita informa√ß√£o in√∫til, o autoencoder n√£o consegue encaixar tudo no espa√ßo latente relativamente pequeno, capturando apenas a parte importante da imagem. Ao treinar removedores de ru√≠do, come√ßamos com imagens originais e usamos imagens com ru√≠do artificialmente adicionado como entrada para o autoencoder.
* **Super-resolu√ß√£o**, ou seja, aumentar a resolu√ß√£o da imagem. Come√ßamos com imagens de alta resolu√ß√£o e usamos imagens de resolu√ß√£o mais baixa como entrada para o autoencoder.
* **Modelos generativos**. Depois de treinar o autoencoder, a parte descodificadora pode ser usada para criar novos objetos a partir de vetores latentes aleat√≥rios.

## Autoencoders Variacionais (VAE)

Os autoencoders tradicionais reduzem a dimens√£o dos dados de entrada de alguma forma, identificando as caracter√≠sticas importantes das imagens de entrada. No entanto, os vetores latentes muitas vezes n√£o fazem muito sentido. Por outras palavras, usando o conjunto de dados MNIST como exemplo, identificar quais d√≠gitos correspondem a diferentes vetores latentes n√£o √© uma tarefa f√°cil, porque vetores latentes pr√≥ximos n√£o correspondem necessariamente aos mesmos d√≠gitos.

Por outro lado, para treinar modelos *generativos*, √© melhor ter algum entendimento do espa√ßo latente. Esta ideia leva-nos ao **autoencoder variacional** (VAE).

O VAE √© um autoencoder que aprende a prever a *distribui√ß√£o estat√≠stica* dos par√¢metros latentes, chamada de **distribui√ß√£o latente**. Por exemplo, podemos querer que os vetores latentes sejam distribu√≠dos normalmente com uma m√©dia z<sub>mean</sub> e um desvio padr√£o z<sub>sigma</sub> (tanto a m√©dia quanto o desvio padr√£o s√£o vetores de alguma dimensionalidade d). O codificador no VAE aprende a prever esses par√¢metros, e o descodificador utiliza um vetor aleat√≥rio dessa distribui√ß√£o para reconstruir o objeto.

Resumindo:

 * A partir do vetor de entrada, prevemos `z_mean` e `z_log_sigma` (em vez de prever o desvio padr√£o diretamente, prevemos o seu logaritmo)
 * Amostramos um vetor `sample` da distribui√ß√£o N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * O descodificador tenta reconstruir a imagem original usando `sample` como vetor de entrada

 <img src="images/vae.png" width="50%">

> Imagem retirada deste [artigo](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) por Isaak Dykeman

Os autoencoders variacionais utilizam uma fun√ß√£o de perda complexa composta por duas partes:

* **Perda de reconstru√ß√£o**, que √© a fun√ß√£o de perda que indica qu√£o pr√≥xima a imagem reconstru√≠da est√° do alvo (pode ser o Erro Quadr√°tico M√©dio, ou MSE). √â a mesma fun√ß√£o de perda usada em autoencoders normais.
* **Perda KL**, que garante que as distribui√ß√µes das vari√°veis latentes permane√ßam pr√≥ximas da distribui√ß√£o normal. Baseia-se no conceito de [diverg√™ncia de Kullback-Leibler](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - uma m√©trica para estimar a semelhan√ßa entre duas distribui√ß√µes estat√≠sticas.

Uma vantagem importante dos VAEs √© que permitem gerar novas imagens de forma relativamente f√°cil, porque sabemos de qual distribui√ß√£o amostrar os vetores latentes. Por exemplo, se treinarmos um VAE com um vetor latente 2D no MNIST, podemos variar os componentes do vetor latente para obter diferentes d√≠gitos:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Imagem por [Dmitry Soshnikov](http://soshnikov.com)

Observe como as imagens se fundem umas nas outras, √† medida que come√ßamos a obter vetores latentes de diferentes partes do espa√ßo de par√¢metros latentes. Tamb√©m podemos visualizar este espa√ßo em 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Imagem por [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Exerc√≠cios: Autoencoders

Saiba mais sobre autoencoders nestes notebooks correspondentes:

* [Autoencoders em TensorFlow](AutoencodersTF.ipynb)
* [Autoencoders em PyTorch](AutoEncodersPyTorch.ipynb)

## Propriedades dos Autoencoders

* **Espec√≠ficos aos Dados** - funcionam bem apenas com o tipo de imagens em que foram treinados. Por exemplo, se treinarmos uma rede de super-resolu√ß√£o em flores, ela n√£o funcionar√° bem em retratos. Isto acontece porque a rede pode produzir imagens de maior resolu√ß√£o ao extrair detalhes finos das caracter√≠sticas aprendidas no conjunto de dados de treino.
* **Com perdas** - a imagem reconstru√≠da n√£o √© exatamente igual √† imagem original. A natureza da perda √© definida pela *fun√ß√£o de perda* usada durante o treino.
* Funciona com **dados n√£o etiquetados**

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Conclus√£o

Nesta li√ß√£o, aprendeste sobre os v√°rios tipos de autoencoders dispon√≠veis para o cientista de IA. Aprendeste como constru√≠-los e como us√°-los para reconstruir imagens. Tamb√©m aprendeste sobre o VAE e como us√°-lo para gerar novas imagens.

## üöÄ Desafio

Nesta li√ß√£o, aprendeste sobre o uso de autoencoders para imagens. Mas eles tamb√©m podem ser usados para m√∫sica! Explora o projeto [MusicVAE](https://magenta.tensorflow.org/music-vae) do Magenta, que utiliza autoencoders para aprender a reconstruir m√∫sica. Faz algumas [experi√™ncias](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) com esta biblioteca para ver o que consegues criar.

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Revis√£o e Autoestudo

Para refer√™ncia, l√™ mais sobre autoencoders nestes recursos:

* [Construindo Autoencoders em Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Artigo no NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Autoencoders Variacionais Explicados](https://kvfrans.com/variational-autoencoders-explained/)
* [Autoencoders Variacionais Condicionais](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Tarefa

No final deste [notebook usando TensorFlow](AutoencodersTF.ipynb), encontrar√°s uma 'tarefa' - usa-a como tua tarefa.

---

