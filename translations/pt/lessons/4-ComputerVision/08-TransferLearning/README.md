<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "178c0b5ee5395733eb18aec51e71a0a9",
  "translation_date": "2025-09-23T13:40:20+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/README.md",
  "language_code": "pt"
}
-->
# Redes Pr√©-treinadas e Aprendizagem por Transfer√™ncia

Treinar CNNs pode levar muito tempo e requer uma grande quantidade de dados. No entanto, grande parte do tempo √© gasto aprendendo os melhores filtros de baixo n√≠vel que uma rede pode usar para extrair padr√µes de imagens. Surge ent√£o uma quest√£o natural: podemos usar uma rede neural treinada num conjunto de dados e adapt√°-la para classificar imagens diferentes sem precisar de um processo completo de treino?

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/15)

Esta abordagem √© chamada de **aprendizagem por transfer√™ncia**, porque transferimos algum conhecimento de um modelo de rede neural para outro. Na aprendizagem por transfer√™ncia, normalmente come√ßamos com um modelo pr√©-treinado, que foi treinado num grande conjunto de dados de imagens, como o **ImageNet**. Esses modelos j√° conseguem extrair diferentes caracter√≠sticas de imagens gen√©ricas, e em muitos casos, apenas construir um classificador em cima dessas caracter√≠sticas extra√≠das pode gerar bons resultados.

> ‚úÖ Aprendizagem por Transfer√™ncia √© um termo que tamb√©m aparece em outros campos acad√©micos, como a Educa√ß√£o. Refere-se ao processo de aplicar conhecimentos de um dom√≠nio a outro.

## Modelos Pr√©-treinados como Extratores de Caracter√≠sticas

As redes convolucionais que discutimos na se√ß√£o anterior cont√™m v√°rias camadas, cada uma destinada a extrair caracter√≠sticas da imagem, come√ßando por combina√ß√µes de pixels de baixo n√≠vel (como linhas horizontais/verticais ou tra√ßos), at√© combina√ß√µes de caracter√≠sticas de n√≠vel superior, correspondendo a coisas como o olho de uma chama. Se treinarmos uma CNN num conjunto de dados suficientemente grande e diversificado, a rede deve aprender a extrair essas caracter√≠sticas comuns.

Tanto o Keras como o PyTorch possuem fun√ß√µes para carregar facilmente pesos de redes neurais pr√©-treinadas para algumas arquiteturas comuns, a maioria das quais foi treinada com imagens do ImageNet. As mais utilizadas est√£o descritas na p√°gina [Arquiteturas de CNN](../07-ConvNets/CNN_Architectures.md) da li√ß√£o anterior. Em particular, pode considerar usar uma das seguintes:

* **VGG-16/VGG-19**, que s√£o modelos relativamente simples e ainda assim oferecem boa precis√£o. Usar o VGG como primeira tentativa √© uma boa escolha para ver como a aprendizagem por transfer√™ncia funciona.
* **ResNet**, uma fam√≠lia de modelos proposta pela Microsoft Research em 2015. Estes t√™m mais camadas e, portanto, requerem mais recursos.
* **MobileNet**, uma fam√≠lia de modelos com tamanho reduzido, adequada para dispositivos m√≥veis. Use-os se tiver poucos recursos e puder sacrificar um pouco de precis√£o.

Aqui est√£o caracter√≠sticas extra√≠das de uma imagem de um gato pela rede VGG-16:

![Caracter√≠sticas extra√≠das pela VGG-16](../../../../../translated_images/features.6291f9c7ba3a0b951af88fc9864632b9115365410765680680d30c927dd67354.pt.png)

## Conjunto de Dados de Gatos vs. C√£es

Neste exemplo, usaremos um conjunto de dados de [Gatos e C√£es](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), que √© muito pr√≥ximo de um cen√°rio real de classifica√ß√£o de imagens.

## ‚úçÔ∏è Exerc√≠cio: Aprendizagem por Transfer√™ncia

Vamos ver a aprendizagem por transfer√™ncia em a√ß√£o nos notebooks correspondentes:

* [Aprendizagem por Transfer√™ncia - PyTorch](TransferLearningPyTorch.ipynb)
* [Aprendizagem por Transfer√™ncia - TensorFlow](TransferLearningTF.ipynb)

## Visualizando o Gato Adversarial

Uma rede neural pr√©-treinada cont√©m diferentes padr√µes no seu *c√©rebro*, incluindo no√ß√µes de **gato ideal** (bem como c√£o ideal, zebra ideal, etc.). Seria interessante **visualizar esta imagem**. No entanto, isso n√£o √© simples, porque os padr√µes est√£o espalhados pelos pesos da rede e organizados numa estrutura hier√°rquica.

Uma abordagem que podemos adotar √© come√ßar com uma imagem aleat√≥ria e tentar usar a t√©cnica de **otimiza√ß√£o por descida de gradiente** para ajustar essa imagem de forma que a rede comece a pensar que √© um gato.

![Ciclo de Otimiza√ß√£o de Imagem](../../../../../translated_images/ideal-cat-loop.999fbb8ff306e044f997032f4eef9152b453e6a990e449bbfb107de2493cc37e.pt.png)

No entanto, se fizermos isso, obteremos algo muito semelhante a um ru√≠do aleat√≥rio. Isso acontece porque *existem muitas maneiras de fazer a rede pensar que a imagem de entrada √© um gato*, incluindo algumas que n√£o fazem sentido visualmente. Embora essas imagens contenham muitos padr√µes t√≠picos de um gato, n√£o h√° nada que as obrigue a serem visualmente distintas.

Para melhorar o resultado, podemos adicionar outro termo √† fun√ß√£o de perda, chamado **perda de varia√ß√£o**. √â uma m√©trica que mostra qu√£o semelhantes s√£o os pixels vizinhos da imagem. Minimizar a perda de varia√ß√£o torna a imagem mais suave e elimina o ru√≠do, revelando padr√µes mais visualmente apelativos. Aqui est√° um exemplo de imagens "ideais", classificadas como gato e zebra com alta probabilidade:

![Gato Ideal](../../../../../translated_images/ideal-cat.203dd4597643d6b0bd73038b87f9c0464322725e3a06ab145d25d4a861c70592.pt.png) | ![Zebra Ideal](../../../../../translated_images/ideal-zebra.7f70e8b54ee15a7a314000bb5df38a6cfe086ea04d60df4d3ef313d046b98a2b.pt.png)
-----|-----
 *Gato Ideal* | *Zebra Ideal*

Uma abordagem semelhante pode ser usada para realizar os chamados **ataques adversariais** numa rede neural. Suponha que queremos enganar uma rede neural e fazer com que um c√£o pare√ßa um gato. Se pegarmos na imagem de um c√£o, que √© reconhecida pela rede como um c√£o, podemos ajust√°-la ligeiramente usando otimiza√ß√£o por descida de gradiente at√© que a rede comece a classific√°-la como um gato:

![Imagem de um C√£o](../../../../../translated_images/original-dog.8f68a67d2fe0911f33041c0f7fce8aa4ea919f9d3917ec4b468298522aeb6356.pt.png) | ![Imagem de um c√£o classificada como gato](../../../../../translated_images/adversarial-dog.d9fc7773b0142b89752539bfbf884118de845b3851c5162146ea0b8809fc820f.pt.png)
-----|-----
*Imagem original de um c√£o* | *Imagem de um c√£o classificada como gato*

Veja o c√≥digo para reproduzir os resultados acima no seguinte notebook:

* [Gato Ideal e Adversarial - TensorFlow](AdversarialCat_TF.ipynb)

## Conclus√£o

Usando a aprendizagem por transfer√™ncia, √© poss√≠vel montar rapidamente um classificador para uma tarefa de classifica√ß√£o de objetos personalizada e alcan√ßar alta precis√£o. Pode perceber que tarefas mais complexas que estamos a resolver agora requerem maior poder computacional e n√£o podem ser facilmente resolvidas no CPU. Na pr√≥xima unidade, tentaremos usar uma implementa√ß√£o mais leve para treinar o mesmo modelo usando menos recursos computacionais, o que resulta numa ligeira redu√ß√£o de precis√£o.

## üöÄ Desafio

Nos notebooks que acompanham, h√° notas no final sobre como o conhecimento transferido funciona melhor com dados de treino algo semelhantes (um novo tipo de animal, talvez). Experimente com tipos de imagens completamente novos para ver como os seus modelos de conhecimento transferido se comportam.

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Revis√£o e Autoestudo

Leia o documento [TrainingTricks.md](TrainingTricks.md) para aprofundar o seu conhecimento sobre outras formas de treinar os seus modelos.

## [Tarefa](lab/README.md)

Neste laborat√≥rio, usaremos o conjunto de dados real [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) de animais de estima√ß√£o, com 35 ra√ßas de gatos e c√£es, e construiremos um classificador de aprendizagem por transfer√™ncia.

---

