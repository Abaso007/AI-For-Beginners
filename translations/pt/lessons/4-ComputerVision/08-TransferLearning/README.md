<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "717775c4050ccbffbe0c961ad8bf7bf7",
  "translation_date": "2025-08-24T09:00:55+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/README.md",
  "language_code": "pt"
}
-->
# Redes Pr√©-treinadas e Aprendizagem por Transfer√™ncia

Treinar CNNs pode levar muito tempo e requer uma grande quantidade de dados. No entanto, grande parte do tempo √© gasto aprendendo os melhores filtros de baixo n√≠vel que uma rede pode usar para extrair padr√µes de imagens. Surge uma quest√£o natural: podemos usar uma rede neural treinada em um conjunto de dados e adapt√°-la para classificar imagens diferentes sem precisar de um processo completo de treino?

## [Question√°rio pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/108)

Essa abordagem √© chamada de **aprendizagem por transfer√™ncia**, porque transferimos algum conhecimento de um modelo de rede neural para outro. Na aprendizagem por transfer√™ncia, normalmente come√ßamos com um modelo pr√©-treinado, que foi treinado em um grande conjunto de dados de imagens, como o **ImageNet**. Esses modelos j√° conseguem extrair diferentes caracter√≠sticas de imagens gen√©ricas, e, em muitos casos, apenas construir um classificador em cima dessas caracter√≠sticas extra√≠das pode gerar bons resultados.

> ‚úÖ Aprendizagem por Transfer√™ncia √© um termo que tamb√©m aparece em outros campos acad√™micos, como Educa√ß√£o. Refere-se ao processo de aplicar conhecimento de um dom√≠nio a outro.

## Modelos Pr√©-treinados como Extratores de Caracter√≠sticas

As redes convolucionais que discutimos na se√ß√£o anterior cont√™m v√°rias camadas, cada uma delas destinada a extrair caracter√≠sticas da imagem, come√ßando por combina√ß√µes de pixels de baixo n√≠vel (como linhas horizontais/verticais ou tra√ßos), at√© combina√ß√µes de caracter√≠sticas de n√≠vel mais alto, correspondendo a coisas como o olho de uma chama. Se treinarmos uma CNN em um conjunto de dados suficientemente grande e diversificado, a rede deve aprender a extrair essas caracter√≠sticas comuns.

Tanto o Keras quanto o PyTorch possuem fun√ß√µes para carregar facilmente pesos de redes neurais pr√©-treinadas para algumas arquiteturas comuns, a maioria das quais foi treinada em imagens do ImageNet. As mais utilizadas est√£o descritas na p√°gina [Arquiteturas de CNN](../07-ConvNets/CNN_Architectures.md) da li√ß√£o anterior. Em particular, pode ser interessante considerar uma das seguintes:

* **VGG-16/VGG-19**, que s√£o modelos relativamente simples, mas ainda assim oferecem boa precis√£o. Usar o VGG como uma primeira tentativa √© uma boa escolha para ver como a aprendizagem por transfer√™ncia funciona.
* **ResNet**, uma fam√≠lia de modelos proposta pela Microsoft Research em 2015. Eles possuem mais camadas e, portanto, exigem mais recursos.
* **MobileNet**, uma fam√≠lia de modelos com tamanho reduzido, adequada para dispositivos m√≥veis. Use-os se estiver com poucos recursos e puder sacrificar um pouco de precis√£o.

Aqui est√£o exemplos de caracter√≠sticas extra√≠das de uma imagem de um gato pela rede VGG-16:

![Caracter√≠sticas extra√≠das pela VGG-16](../../../../../lessons/4-ComputerVision/08-TransferLearning/images/features.png)

## Conjunto de Dados de Gatos vs. C√£es

Neste exemplo, usaremos um conjunto de dados de [Gatos e C√£es](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), que √© muito pr√≥ximo de um cen√°rio real de classifica√ß√£o de imagens.

## ‚úçÔ∏è Exerc√≠cio: Aprendizagem por Transfer√™ncia

Vamos ver a aprendizagem por transfer√™ncia em a√ß√£o nos notebooks correspondentes:

* [Transfer Learning - PyTorch](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb)
* [Transfer Learning - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningTF.ipynb)

## Visualizando o Gato Adversarial

Uma rede neural pr√©-treinada cont√©m diferentes padr√µes dentro do seu *c√©rebro*, incluindo no√ß√µes de **gato ideal** (assim como c√£o ideal, zebra ideal, etc.). Seria interessante **visualizar essa imagem** de alguma forma. No entanto, isso n√£o √© simples, porque os padr√µes est√£o espalhados pelos pesos da rede e organizados em uma estrutura hier√°rquica.

Uma abordagem que podemos adotar √© come√ßar com uma imagem aleat√≥ria e, em seguida, usar a t√©cnica de **otimiza√ß√£o por descida de gradiente** para ajustar essa imagem de forma que a rede comece a pensar que √© um gato.

![Loop de Otimiza√ß√£o de Imagem](../../../../../lessons/4-ComputerVision/08-TransferLearning/images/ideal-cat-loop.png)

No entanto, se fizermos isso, obteremos algo muito semelhante a um ru√≠do aleat√≥rio. Isso ocorre porque *existem muitas maneiras de fazer a rede pensar que a imagem de entrada √© um gato*, incluindo algumas que n√£o fazem sentido visualmente. Embora essas imagens contenham muitos padr√µes t√≠picos de um gato, n√£o h√° nada que as restrinja a serem visualmente distintas.

Para melhorar o resultado, podemos adicionar outro termo √† fun√ß√£o de perda, chamado **perda de varia√ß√£o**. √â uma m√©trica que mostra qu√£o semelhantes s√£o os pixels vizinhos da imagem. Minimizar a perda de varia√ß√£o torna a imagem mais suave e elimina o ru√≠do, revelando padr√µes mais visualmente atraentes. Aqui est√° um exemplo de imagens "ideais", classificadas como gato e zebra com alta probabilidade:

![Gato Ideal](../../../../../lessons/4-ComputerVision/08-TransferLearning/images/ideal-cat.png) | ![Zebra Ideal](../../../../../lessons/4-ComputerVision/08-TransferLearning/images/ideal-zebra.png)
-----|-----
 *Gato Ideal* | *Zebra Ideal*

Uma abordagem semelhante pode ser usada para realizar os chamados **ataques adversariais** em uma rede neural. Suponha que queremos enganar uma rede neural e fazer um c√£o parecer um gato. Se pegarmos a imagem de um c√£o, que √© reconhecida pela rede como um c√£o, podemos ajust√°-la um pouco usando otimiza√ß√£o por descida de gradiente at√© que a rede comece a classific√°-la como um gato:

![Imagem de um C√£o](../../../../../lessons/4-ComputerVision/08-TransferLearning/images/original-dog.png) | ![Imagem de um c√£o classificada como gato](../../../../../lessons/4-ComputerVision/08-TransferLearning/images/adversarial-dog.png)
-----|-----
*Imagem original de um c√£o* | *Imagem de um c√£o classificada como gato*

Veja o c√≥digo para reproduzir os resultados acima no seguinte notebook:

* [Ideal e Adversarial Cat - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/AdversarialCat_TF.ipynb)

## Conclus√£o

Usando a aprendizagem por transfer√™ncia, √© poss√≠vel montar rapidamente um classificador para uma tarefa de classifica√ß√£o de objetos personalizada e alcan√ßar alta precis√£o. √â poss√≠vel perceber que tarefas mais complexas que estamos resolvendo agora exigem maior poder computacional e n√£o podem ser facilmente resolvidas no CPU. Na pr√≥xima unidade, tentaremos usar uma implementa√ß√£o mais leve para treinar o mesmo modelo usando menos recursos computacionais, o que resulta em uma precis√£o ligeiramente menor.

## üöÄ Desafio

Nos notebooks que acompanham, h√° notas no final sobre como o conhecimento transferido funciona melhor com dados de treino relativamente semelhantes (um novo tipo de animal, talvez). Fa√ßa algumas experi√™ncias com tipos completamente novos de imagens para ver como os modelos de conhecimento transferido funcionam bem ou mal.

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## Revis√£o e Estudo Individual

Leia o documento [TrainingTricks.md](TrainingTricks.md) para aprofundar seu conhecimento sobre outras formas de treinar seus modelos.

## [Tarefa](lab/README.md)

Neste laborat√≥rio, usaremos o conjunto de dados real [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) de animais de estima√ß√£o, com 35 ra√ßas de gatos e c√£es, e construiremos um classificador de aprendizagem por transfer√™ncia.

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original no seu idioma nativo deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas resultantes do uso desta tradu√ß√£o.