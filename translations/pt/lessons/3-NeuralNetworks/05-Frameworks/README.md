<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-24T09:04:22+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "pt"
}
-->
# Frameworks de Redes Neuronais

Como j√° aprendemos, para treinar redes neuronais de forma eficiente, precisamos fazer duas coisas:

* Operar com tensores, por exemplo, multiplicar, somar e calcular algumas fun√ß√µes como sigmoid ou softmax.
* Calcular os gradientes de todas as express√µes, para realizar a otimiza√ß√£o por descida de gradiente.

## [Question√°rio pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Embora a biblioteca `numpy` consiga realizar a primeira parte, precisamos de algum mecanismo para calcular gradientes. No [nosso framework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) que desenvolvemos na se√ß√£o anterior, tivemos que programar manualmente todas as fun√ß√µes derivadas dentro do m√©todo `backward`, que realiza a retropropaga√ß√£o. Idealmente, um framework deveria nos permitir calcular os gradientes de *qualquer express√£o* que possamos definir.

Outro ponto importante √© a capacidade de realizar c√°lculos em GPU ou outras unidades de computa√ß√£o especializadas, como [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). O treino de redes neuronais profundas exige *muitos* c√°lculos, e poder paralelizar esses c√°lculos em GPUs √© fundamental.

> ‚úÖ O termo 'paralelizar' significa distribuir os c√°lculos por v√°rios dispositivos.

Atualmente, os dois frameworks de redes neuronais mais populares s√£o: [TensorFlow](http://TensorFlow.org) e [PyTorch](https://pytorch.org/). Ambos oferecem uma API de baixo n√≠vel para operar com tensores tanto em CPU quanto em GPU. Al√©m da API de baixo n√≠vel, tamb√©m existe uma API de alto n√≠vel, chamada [Keras](https://keras.io/) e [PyTorch Lightning](https://pytorchlightning.ai/), respectivamente.

API de Baixo N√≠vel | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------------|-------------------------------------|--------------------------------
API de Alto N√≠vel   | [Keras](https://keras.io/)         | [PyTorch Lightning](https://pytorchlightning.ai/)

**APIs de baixo n√≠vel** em ambos os frameworks permitem construir os chamados **grafos computacionais**. Este grafo define como calcular o resultado (geralmente a fun√ß√£o de perda) com os par√¢metros de entrada fornecidos e pode ser enviado para c√°lculo em GPU, se dispon√≠vel. Existem fun√ß√µes para diferenciar este grafo computacional e calcular gradientes, que podem ser usados para otimizar os par√¢metros do modelo.

**APIs de alto n√≠vel** consideram redes neuronais como uma **sequ√™ncia de camadas**, facilitando muito a constru√ß√£o da maioria das redes neuronais. Treinar o modelo geralmente requer preparar os dados e, em seguida, chamar uma fun√ß√£o `fit` para realizar o trabalho.

A API de alto n√≠vel permite construir redes neuronais t√≠picas rapidamente, sem se preocupar com muitos detalhes. Por outro lado, a API de baixo n√≠vel oferece muito mais controle sobre o processo de treino e, por isso, √© amplamente utilizada em pesquisas, quando se trabalha com novas arquiteturas de redes neuronais.

Tamb√©m √© importante entender que ambas as APIs podem ser usadas juntas. Por exemplo, pode-se desenvolver a pr√≥pria arquitetura de camada de rede usando a API de baixo n√≠vel e, em seguida, utiliz√°-la dentro de uma rede maior constru√≠da e treinada com a API de alto n√≠vel. Ou pode-se definir uma rede usando a API de alto n√≠vel como uma sequ√™ncia de camadas e, em seguida, usar o pr√≥prio loop de treino de baixo n√≠vel para realizar a otimiza√ß√£o. Ambas as APIs utilizam os mesmos conceitos b√°sicos subjacentes e s√£o projetadas para funcionar bem juntas.

## Aprendizagem

Neste curso, oferecemos a maior parte do conte√∫do tanto para PyTorch quanto para TensorFlow. Pode escolher o framework preferido e seguir apenas os notebooks correspondentes. Se n√£o tiver certeza de qual framework escolher, leia algumas discuss√µes na internet sobre **PyTorch vs. TensorFlow**. Tamb√©m pode explorar ambos os frameworks para obter uma melhor compreens√£o.

Sempre que poss√≠vel, utilizaremos APIs de alto n√≠vel para simplificar. No entanto, acreditamos que √© importante entender como as redes neuronais funcionam desde o in√≠cio, por isso come√ßamos trabalhando com a API de baixo n√≠vel e tensores. Contudo, se quiser avan√ßar rapidamente e n√£o gastar muito tempo aprendendo esses detalhes, pode pular essa parte e ir diretamente para os notebooks de API de alto n√≠vel.

## ‚úçÔ∏è Exerc√≠cios: Frameworks

Continue a aprendizagem nos seguintes notebooks:

API de Baixo N√≠vel | [Notebook TensorFlow+Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------------|-------------------------------------|--------------------------------
API de Alto N√≠vel   | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb)         | *PyTorch Lightning*

Depois de dominar os frameworks, vamos recapitular o conceito de overfitting.

# Overfitting

Overfitting √© um conceito extremamente importante em aprendizagem autom√°tica, e √© essencial compreend√™-lo corretamente!

Considere o seguinte problema de aproximar 5 pontos (representados por `x` nos gr√°ficos abaixo):

![linear](../../../../../lessons/3-NeuralNetworks/images/overfit1.jpg) | ![overfit](../../../../../lessons/3-NeuralNetworks/images/overfit2.jpg)
-------------------------|--------------------------
**Modelo linear, 2 par√¢metros** | **Modelo n√£o-linear, 7 par√¢metros**
Erro de treino = 5.3 | Erro de treino = 0
Erro de valida√ß√£o = 5.1 | Erro de valida√ß√£o = 20

* √Ä esquerda, vemos uma boa aproxima√ß√£o com uma linha reta. Como o n√∫mero de par√¢metros √© adequado, o modelo compreende corretamente a distribui√ß√£o dos pontos.
* √Ä direita, o modelo √© demasiado poderoso. Como temos apenas 5 pontos e o modelo possui 7 par√¢metros, ele ajusta-se de forma a passar por todos os pontos, fazendo com que o erro de treino seja 0. No entanto, isso impede o modelo de entender o padr√£o correto dos dados, resultando num erro de valida√ß√£o muito alto.

√â muito importante encontrar o equil√≠brio correto entre a complexidade do modelo (n√∫mero de par√¢metros) e o n√∫mero de amostras de treino.

## Por que ocorre o overfitting

  * Poucos dados de treino
  * Modelo demasiado poderoso
  * Muito ru√≠do nos dados de entrada

## Como detectar o overfitting

Como pode ver no gr√°fico acima, o overfitting pode ser detectado por um erro de treino muito baixo e um erro de valida√ß√£o elevado. Normalmente, durante o treino, vemos tanto o erro de treino quanto o de valida√ß√£o come√ßarem a diminuir, e ent√£o, em algum momento, o erro de valida√ß√£o pode parar de diminuir e come√ßar a aumentar. Este ser√° um sinal de overfitting e um indicador de que provavelmente devemos parar o treino nesse ponto (ou pelo menos fazer um snapshot do modelo).

![overfitting](../../../../../lessons/3-NeuralNetworks/images/Overfitting.png)

## Como prevenir o overfitting

Se perceber que o overfitting est√° a ocorrer, pode fazer o seguinte:

 * Aumentar a quantidade de dados de treino
 * Reduzir a complexidade do modelo
 * Utilizar alguma [t√©cnica de regulariza√ß√£o](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), como [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), que ser√° abordada mais tarde.

## Overfitting e o Tradeoff Vi√©s-Vari√¢ncia

O overfitting √©, na verdade, um caso de um problema mais gen√©rico em estat√≠stica chamado [Tradeoff Vi√©s-Vari√¢ncia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Se considerarmos as poss√≠veis fontes de erro no nosso modelo, podemos identificar dois tipos de erros:

* **Erros de vi√©s** s√£o causados pelo nosso algoritmo n√£o conseguir capturar corretamente a rela√ß√£o entre os dados de treino. Isso pode ocorrer porque o modelo n√£o √© suficientemente poderoso (**underfitting**).
* **Erros de vari√¢ncia**, que s√£o causados pelo modelo ao aproximar o ru√≠do nos dados de entrada em vez de uma rela√ß√£o significativa (**overfitting**).

Durante o treino, o erro de vi√©s diminui (√† medida que o modelo aprende a aproximar os dados) e o erro de vari√¢ncia aumenta. √â importante parar o treino - seja manualmente (quando detectamos overfitting) ou automaticamente (introduzindo regulariza√ß√£o) - para evitar o overfitting.

## Conclus√£o

Nesta li√ß√£o, aprendeu sobre as diferen√ßas entre as v√°rias APIs dos dois frameworks de IA mais populares, TensorFlow e PyTorch. Al√©m disso, aprendeu sobre um t√≥pico muito importante: overfitting.

## üöÄ Desafio

Nos notebooks associados, encontrar√° 'tarefas' no final; trabalhe nos notebooks e complete as tarefas.

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Revis√£o e Autoestudo

Pesquise sobre os seguintes t√≥picos:

- TensorFlow
- PyTorch
- Overfitting

Pergunte a si mesmo:

- Qual √© a diferen√ßa entre TensorFlow e PyTorch?
- Qual √© a diferen√ßa entre overfitting e underfitting?

## [Trabalho](lab/README.md)

Neste laborat√≥rio, ser√° solicitado que resolva dois problemas de classifica√ß√£o usando redes totalmente conectadas de uma ou v√°rias camadas, utilizando PyTorch ou TensorFlow.

* [Instru√ß√µes](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, tenha em aten√ß√£o que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes da utiliza√ß√£o desta tradu√ß√£o.