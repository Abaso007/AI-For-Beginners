<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T13:39:21+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "pt"
}
-->
# Aprendizagem por Refor√ßo Profundo

A aprendizagem por refor√ßo (RL) √© considerada um dos paradigmas b√°sicos de aprendizagem autom√°tica, ao lado da aprendizagem supervisionada e n√£o supervisionada. Enquanto na aprendizagem supervisionada dependemos de um conjunto de dados com resultados conhecidos, a RL baseia-se em **aprender fazendo**. Por exemplo, quando vemos um jogo de computador pela primeira vez, come√ßamos a jogar, mesmo sem conhecer as regras, e rapidamente conseguimos melhorar as nossas habilidades apenas pelo processo de jogar e ajustar o nosso comportamento.

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Para realizar RL, precisamos de:

* Um **ambiente** ou **simulador** que define as regras do jogo. Devemos ser capazes de realizar experi√™ncias no simulador e observar os resultados.
* Uma **fun√ß√£o de recompensa**, que indica o qu√£o bem-sucedida foi a nossa experi√™ncia. No caso de aprender a jogar um jogo de computador, a recompensa seria a nossa pontua√ß√£o final.

Com base na fun√ß√£o de recompensa, devemos ser capazes de ajustar o nosso comportamento e melhorar as nossas habilidades, para que na pr√≥xima vez joguemos melhor. A principal diferen√ßa entre outros tipos de aprendizagem autom√°tica e RL √© que na RL normalmente n√£o sabemos se ganhamos ou perdemos at√© terminarmos o jogo. Assim, n√£o podemos dizer se um determinado movimento isolado √© bom ou n√£o - s√≥ recebemos uma recompensa no final do jogo.

Durante a RL, normalmente realizamos muitas experi√™ncias. Em cada experi√™ncia, precisamos equilibrar entre seguir a estrat√©gia √≥tima que aprendemos at√© agora (**explora√ß√£o**) e explorar novos estados poss√≠veis (**explora√ß√£o**).

## OpenAI Gym

Uma ferramenta excelente para RL √© o [OpenAI Gym](https://gym.openai.com/) - um **ambiente de simula√ß√£o**, que pode simular muitos ambientes diferentes, desde jogos Atari at√© a f√≠sica por tr√°s do equil√≠brio de um poste. √â um dos ambientes de simula√ß√£o mais populares para treinar algoritmos de aprendizagem por refor√ßo e √© mantido pela [OpenAI](https://openai.com/).

> **Nota**: Pode ver todos os ambientes dispon√≠veis no OpenAI Gym [aqui](https://gym.openai.com/envs/#classic_control).

## Equil√≠brio do CartPole

Provavelmente j√° viu dispositivos modernos de equil√≠brio, como o *Segway* ou *Gyroscooters*. Eles conseguem equilibrar-se automaticamente ajustando as suas rodas em resposta a um sinal de um aceler√≥metro ou girosc√≥pio. Nesta se√ß√£o, vamos aprender a resolver um problema semelhante - equilibrar um poste. √â semelhante a uma situa√ß√£o em que um artista de circo precisa equilibrar um poste na sua m√£o - mas este equil√≠brio de poste ocorre apenas em 1D.

Uma vers√£o simplificada de equil√≠brio √© conhecida como o problema **CartPole**. No mundo do CartPole, temos um slider horizontal que pode mover-se para a esquerda ou para a direita, e o objetivo √© equilibrar um poste vertical no topo do slider enquanto ele se move.

<img alt="um cartpole" src="images/cartpole.png" width="200"/>

Para criar e usar este ambiente, precisamos de algumas linhas de c√≥digo Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Cada ambiente pode ser acessado exatamente da mesma forma:
* `env.reset` inicia uma nova experi√™ncia
* `env.step` realiza um passo de simula√ß√£o. Recebe uma **a√ß√£o** do **espa√ßo de a√ß√µes** e retorna uma **observa√ß√£o** (do espa√ßo de observa√ß√£o), bem como uma recompensa e um sinal de t√©rmino.

No exemplo acima, realizamos uma a√ß√£o aleat√≥ria em cada passo, raz√£o pela qual a vida da experi√™ncia √© muito curta:

![cartpole sem equil√≠brio](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

O objetivo de um algoritmo de RL √© treinar um modelo - a chamada **pol√≠tica** &pi; - que retornar√° a a√ß√£o em resposta a um estado dado. Tamb√©m podemos considerar a pol√≠tica como probabil√≠stica, ou seja, para qualquer estado *s* e a√ß√£o *a*, ela retornar√° a probabilidade &pi;(*a*|*s*) de que devemos tomar *a* no estado *s*.

## Algoritmo de Gradientes de Pol√≠tica

A maneira mais √≥bvia de modelar uma pol√≠tica √© criar uma rede neural que receba estados como entrada e retorne a√ß√µes correspondentes (ou, mais precisamente, as probabilidades de todas as a√ß√µes). De certa forma, seria semelhante a uma tarefa de classifica√ß√£o normal, com uma grande diferen√ßa - n√£o sabemos de antem√£o quais a√ß√µes devemos tomar em cada um dos passos.

A ideia aqui √© estimar essas probabilidades. Constru√≠mos um vetor de **recompensas acumuladas**, que mostra a nossa recompensa total em cada passo da experi√™ncia. Tamb√©m aplicamos **desconto de recompensa** multiplicando recompensas anteriores por algum coeficiente &gamma;=0.99, para diminuir o papel das recompensas anteriores. Em seguida, refor√ßamos os passos ao longo do caminho da experi√™ncia que geram maiores recompensas.

> Saiba mais sobre o algoritmo de Gradientes de Pol√≠tica e veja-o em a√ß√£o no [notebook de exemplo](CartPole-RL-TF.ipynb).

## Algoritmo Ator-Cr√≠tico

Uma vers√£o melhorada da abordagem de Gradientes de Pol√≠tica √© chamada de **Ator-Cr√≠tico**. A ideia principal por tr√°s disso √© que a rede neural seria treinada para retornar duas coisas:

* A pol√≠tica, que determina qual a√ß√£o tomar. Esta parte √© chamada de **ator**.
* A estimativa da recompensa total que podemos esperar obter neste estado - esta parte √© chamada de **cr√≠tico**.

De certa forma, esta arquitetura assemelha-se a um [GAN](../../4-ComputerVision/10-GANs/README.md), onde temos duas redes que s√£o treinadas uma contra a outra. No modelo ator-cr√≠tico, o ator prop√µe a a√ß√£o que precisamos tomar, e o cr√≠tico tenta ser cr√≠tico e estimar o resultado. No entanto, o nosso objetivo √© treinar essas redes em conjunto.

Como sabemos tanto as recompensas acumuladas reais quanto os resultados retornados pelo cr√≠tico durante a experi√™ncia, √© relativamente f√°cil construir uma fun√ß√£o de perda que minimize a diferen√ßa entre eles. Isso nos daria a **perda do cr√≠tico**. Podemos calcular a **perda do ator** usando a mesma abordagem do algoritmo de gradientes de pol√≠tica.

Depois de executar um desses algoritmos, podemos esperar que o nosso CartPole se comporte assim:

![um cartpole equilibrado](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Exerc√≠cios: Gradientes de Pol√≠tica e RL Ator-Cr√≠tico

Continue a sua aprendizagem nos seguintes notebooks:

* [RL em TensorFlow](CartPole-RL-TF.ipynb)
* [RL em PyTorch](CartPole-RL-PyTorch.ipynb)

## Outras Tarefas de RL

A aprendizagem por refor√ßo √© atualmente um campo de pesquisa em r√°pido crescimento. Alguns exemplos interessantes de aprendizagem por refor√ßo s√£o:

* Ensinar um computador a jogar **Jogos Atari**. A parte desafiadora deste problema √© que n√£o temos um estado simples representado como um vetor, mas sim uma captura de ecr√£ - e precisamos usar a CNN para converter esta imagem de ecr√£ num vetor de caracter√≠sticas ou para extrair informa√ß√µes de recompensa. Os jogos Atari est√£o dispon√≠veis no Gym.
* Ensinar um computador a jogar jogos de tabuleiro, como Xadrez e Go. Recentemente, programas de √∫ltima gera√ß√£o como **Alpha Zero** foram treinados do zero por dois agentes jogando um contra o outro e melhorando a cada passo.
* Na ind√∫stria, a RL √© usada para criar sistemas de controlo a partir de simula√ß√£o. Um servi√ßo chamado [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) foi especificamente projetado para isso.

## Conclus√£o

Aprendemos agora como treinar agentes para alcan√ßar bons resultados apenas fornecendo-lhes uma fun√ß√£o de recompensa que define o estado desejado do jogo e dando-lhes a oportunidade de explorar inteligentemente o espa√ßo de busca. Experiment√°mos com sucesso dois algoritmos e alcan√ß√°mos um bom resultado num per√≠odo de tempo relativamente curto. No entanto, este √© apenas o in√≠cio da sua jornada na RL, e deve definitivamente considerar fazer um curso separado se quiser aprofundar mais.

## üöÄ Desafio

Explore as aplica√ß√µes listadas na se√ß√£o 'Outras Tarefas de RL' e tente implementar uma!

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Revis√£o e Autoestudo

Saiba mais sobre aprendizagem por refor√ßo cl√°ssica no nosso [Curr√≠culo de Aprendizagem Autom√°tica para Iniciantes](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Assista a [este excelente v√≠deo](https://www.youtube.com/watch?v=qv6UVOQ0F44) que fala sobre como um computador pode aprender a jogar Super Mario.

## Tarefa: [Treinar um Carro na Montanha](lab/README.md)

O seu objetivo nesta tarefa ser√° treinar um ambiente diferente do Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

