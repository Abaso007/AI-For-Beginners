<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-24T09:02:46+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "pt"
}
-->
# Aprendizagem por Refor√ßo Profundo

A aprendizagem por refor√ßo (RL) √© considerada um dos paradigmas b√°sicos de aprendizagem autom√°tica, ao lado da aprendizagem supervisionada e n√£o supervisionada. Enquanto na aprendizagem supervisionada dependemos de um conjunto de dados com resultados conhecidos, a RL baseia-se em **aprender fazendo**. Por exemplo, quando vemos um jogo de computador pela primeira vez, come√ßamos a jogar, mesmo sem conhecer as regras, e logo conseguimos melhorar nossas habilidades apenas pelo processo de jogar e ajustar nosso comportamento.

## [Question√°rio pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Para realizar RL, precisamos de:

* Um **ambiente** ou **simulador** que define as regras do jogo. Devemos ser capazes de executar os experimentos no simulador e observar os resultados.
* Uma **fun√ß√£o de recompensa**, que indica o qu√£o bem-sucedido foi o nosso experimento. No caso de aprender a jogar um jogo de computador, a recompensa seria nossa pontua√ß√£o final.

Com base na fun√ß√£o de recompensa, devemos ser capazes de ajustar nosso comportamento e melhorar nossas habilidades, para que na pr√≥xima vez joguemos melhor. A principal diferen√ßa entre outros tipos de aprendizagem autom√°tica e RL √© que, na RL, normalmente n√£o sabemos se ganhamos ou perdemos at√© terminarmos o jogo. Assim, n√£o podemos dizer se um movimento espec√≠fico √© bom ou n√£o - s√≥ recebemos uma recompensa no final do jogo.

Durante a RL, normalmente realizamos muitos experimentos. Em cada experimento, precisamos equilibrar entre seguir a estrat√©gia ideal que aprendemos at√© agora (**explora√ß√£o**) e explorar novos estados poss√≠veis (**explora√ß√£o**).

## OpenAI Gym

Uma ferramenta excelente para RL √© o [OpenAI Gym](https://gym.openai.com/) - um **ambiente de simula√ß√£o**, que pode simular muitos ambientes diferentes, desde jogos Atari at√© a f√≠sica por tr√°s do equil√≠brio de um bast√£o. √â um dos ambientes de simula√ß√£o mais populares para treinar algoritmos de aprendizagem por refor√ßo e √© mantido pela [OpenAI](https://openai.com/).

> **Note**: Pode ver todos os ambientes dispon√≠veis no OpenAI Gym [aqui](https://gym.openai.com/envs/#classic_control).

## Equil√≠brio do CartPole

Provavelmente todos j√° viram dispositivos modernos de equil√≠brio, como o *Segway* ou *Gyroscooters*. Eles conseguem equilibrar-se automaticamente ajustando as rodas em resposta a um sinal de um aceler√¥metro ou girosc√≥pio. Nesta se√ß√£o, aprenderemos como resolver um problema semelhante - equilibrar um bast√£o. √â semelhante a uma situa√ß√£o em que um artista de circo precisa equilibrar um bast√£o na m√£o - mas este equil√≠brio ocorre apenas em 1D.

Uma vers√£o simplificada do equil√≠brio √© conhecida como o problema do **CartPole**. No mundo do CartPole, temos um slider horizontal que pode mover-se para a esquerda ou para a direita, e o objetivo √© equilibrar um bast√£o vertical no topo do slider enquanto ele se move.

<img alt="um cartpole" src="images/cartpole.png" width="200"/>

Para criar e usar este ambiente, precisamos de algumas linhas de c√≥digo Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Cada ambiente pode ser acessado exatamente da mesma forma:
* `env.reset` inicia um novo experimento
* `env.step` realiza um passo de simula√ß√£o. Ele recebe uma **a√ß√£o** do **espa√ßo de a√ß√µes** e retorna uma **observa√ß√£o** (do espa√ßo de observa√ß√£o), bem como uma recompensa e um sinal de t√©rmino.

No exemplo acima, realizamos uma a√ß√£o aleat√≥ria em cada passo, raz√£o pela qual a vida do experimento √© muito curta:

![cartpole sem equil√≠brio](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

O objetivo de um algoritmo de RL √© treinar um modelo - a chamada **pol√≠tica** œÄ - que retornar√° a a√ß√£o em resposta a um estado dado. Tamb√©m podemos considerar a pol√≠tica como probabil√≠stica, ou seja, para qualquer estado *s* e a√ß√£o *a*, ela retornar√° a probabilidade œÄ(*a*|*s*) de que devemos tomar *a* no estado *s*.

## Algoritmo de Gradientes de Pol√≠tica

A maneira mais √≥bvia de modelar uma pol√≠tica √© criar uma rede neural que receba estados como entrada e retorne a√ß√µes correspondentes (ou, mais precisamente, as probabilidades de todas as a√ß√µes). De certa forma, seria semelhante a uma tarefa de classifica√ß√£o normal, com uma grande diferen√ßa - n√£o sabemos de antem√£o quais a√ß√µes devemos tomar em cada passo.

A ideia aqui √© estimar essas probabilidades. Constru√≠mos um vetor de **recompensas acumuladas**, que mostra nossa recompensa total em cada passo do experimento. Tamb√©m aplicamos **desconto de recompensa**, multiplicando recompensas anteriores por um coeficiente Œ≥=0.99, para diminuir o papel das recompensas anteriores. Em seguida, refor√ßamos os passos ao longo do caminho do experimento que geram maiores recompensas.

> Saiba mais sobre o algoritmo de Gradientes de Pol√≠tica e veja-o em a√ß√£o no [notebook de exemplo](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Algoritmo Ator-Cr√≠tico

Uma vers√£o aprimorada da abordagem de Gradientes de Pol√≠tica √© chamada de **Ator-Cr√≠tico**. A ideia principal por tr√°s disso √© que a rede neural seria treinada para retornar duas coisas:

* A pol√≠tica, que determina qual a√ß√£o tomar. Esta parte √© chamada de **ator**.
* A estimativa da recompensa total que podemos esperar obter neste estado - esta parte √© chamada de **cr√≠tico**.

De certa forma, esta arquitetura se assemelha a um [GAN](../../4-ComputerVision/10-GANs/README.md), onde temos duas redes que s√£o treinadas uma contra a outra. No modelo ator-cr√≠tico, o ator prop√µe a a√ß√£o que precisamos tomar, e o cr√≠tico tenta ser cr√≠tico e estimar o resultado. No entanto, nosso objetivo √© treinar essas redes em conjunto.

Como sabemos tanto as recompensas acumuladas reais quanto os resultados retornados pelo cr√≠tico durante o experimento, √© relativamente f√°cil construir uma fun√ß√£o de perda que minimize a diferen√ßa entre eles. Isso nos daria a **perda do cr√≠tico**. Podemos calcular a **perda do ator** usando a mesma abordagem do algoritmo de gradientes de pol√≠tica.

Depois de executar um desses algoritmos, podemos esperar que nosso CartPole se comporte assim:

![um cartpole equilibrado](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Exerc√≠cios: Gradientes de Pol√≠tica e RL Ator-Cr√≠tico

Continue seu aprendizado nos seguintes notebooks:

* [RL em TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL em PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Outras Tarefas de RL

A aprendizagem por refor√ßo atualmente √© um campo de pesquisa em r√°pido crescimento. Alguns exemplos interessantes de aprendizagem por refor√ßo s√£o:

* Ensinar um computador a jogar **Jogos Atari**. A parte desafiadora deste problema √© que n√£o temos um estado simples representado como um vetor, mas sim uma captura de tela - e precisamos usar a CNN para converter esta imagem de tela em um vetor de caracter√≠sticas ou para extrair informa√ß√µes de recompensa. Os jogos Atari est√£o dispon√≠veis no Gym.
* Ensinar um computador a jogar jogos de tabuleiro, como Xadrez e Go. Recentemente, programas de √∫ltima gera√ß√£o como **Alpha Zero** foram treinados do zero por dois agentes jogando um contra o outro e melhorando a cada passo.
* Na ind√∫stria, RL √© usado para criar sistemas de controle a partir de simula√ß√£o. Um servi√ßo chamado [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) foi especificamente projetado para isso.

## Conclus√£o

Agora aprendemos como treinar agentes para alcan√ßar bons resultados apenas fornecendo-lhes uma fun√ß√£o de recompensa que define o estado desejado do jogo e dando-lhes a oportunidade de explorar inteligentemente o espa√ßo de busca. Experimentamos com sucesso dois algoritmos e alcan√ßamos um bom resultado em um per√≠odo relativamente curto de tempo. No entanto, este √© apenas o come√ßo da sua jornada em RL, e voc√™ definitivamente deve considerar fazer um curso separado se quiser aprofundar-se.

## üöÄ Desafio

Explore as aplica√ß√µes listadas na se√ß√£o 'Outras Tarefas de RL' e tente implementar uma!

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Revis√£o e Autoestudo

Saiba mais sobre aprendizagem por refor√ßo cl√°ssica em nosso [Curr√≠culo de Aprendizagem Autom√°tica para Iniciantes](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Assista a [este excelente v√≠deo](https://www.youtube.com/watch?v=qv6UVOQ0F44) que fala sobre como um computador pode aprender a jogar Super Mario.

## Tarefa: [Treine um Carro na Montanha](lab/README.md)

Seu objetivo nesta tarefa ser√° treinar um ambiente diferente do Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, √© importante notar que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.