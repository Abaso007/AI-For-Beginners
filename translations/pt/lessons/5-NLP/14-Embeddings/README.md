<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-24T08:53:41+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "pt"
}
-->
# Embeddings

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Ao treinar classificadores baseados em BoW ou TF/IDF, trabalh√°vamos com vetores de saco de palavras de alta dimens√£o com comprimento `vocab_size`, e est√°vamos a converter explicitamente de vetores de representa√ß√£o posicional de baixa dimens√£o para uma representa√ß√£o esparsa one-hot. No entanto, esta representa√ß√£o one-hot n√£o √© eficiente em termos de mem√≥ria. Al√©m disso, cada palavra √© tratada de forma independente, ou seja, os vetores codificados one-hot n√£o expressam qualquer semelhan√ßa sem√¢ntica entre palavras.

A ideia de **embedding** √© representar palavras atrav√©s de vetores densos de menor dimens√£o, que de alguma forma refletem o significado sem√¢ntico de uma palavra. Mais tarde, discutiremos como construir embeddings de palavras significativos, mas, por agora, vamos apenas pensar nos embeddings como uma forma de reduzir a dimensionalidade de um vetor de palavras.

Assim, a camada de embedding receberia uma palavra como entrada e produziria um vetor de sa√≠da com o tamanho especificado por `embedding_size`. De certa forma, √© muito semelhante a uma camada `Linear`, mas, em vez de receber um vetor codificado one-hot, ser√° capaz de receber um n√∫mero de palavra como entrada, permitindo-nos evitar a cria√ß√£o de grandes vetores codificados one-hot.

Ao usar uma camada de embedding como a primeira camada na nossa rede de classifica√ß√£o, podemos mudar de um modelo de saco de palavras para um modelo de **embedding bag**, onde primeiro convertemos cada palavra no nosso texto no embedding correspondente e, em seguida, calculamos alguma fun√ß√£o agregada sobre todos esses embeddings, como `sum`, `average` ou `max`.

![Imagem mostrando um classificador com embeddings para cinco palavras de sequ√™ncia.](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Embeddings

Continue a sua aprendizagem nos seguintes notebooks:
* [Embeddings com PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Embeddings com TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Embeddings Sem√¢nticos: Word2Vec

Embora a camada de embedding tenha aprendido a mapear palavras para representa√ß√µes vetoriais, esta representa√ß√£o n√£o necessariamente possui muito significado sem√¢ntico. Seria interessante aprender uma representa√ß√£o vetorial tal que palavras semelhantes ou sin√≥nimos correspondam a vetores pr√≥ximos entre si em termos de alguma dist√¢ncia vetorial (por exemplo, dist√¢ncia Euclidiana).

Para isso, precisamos de pr√©-treinar o nosso modelo de embedding numa grande cole√ß√£o de texto de uma forma espec√≠fica. Uma maneira de treinar embeddings sem√¢nticos √© chamada de [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Baseia-se em duas arquiteturas principais que s√£o usadas para produzir uma representa√ß√£o distribu√≠da de palavras:

 - **Continuous bag-of-words** (CBoW) ‚Äî nesta arquitetura, treinamos o modelo para prever uma palavra a partir do contexto circundante. Dado o ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, o objetivo do modelo √© prever $W_0$ a partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** √© o oposto do CBoW. O modelo usa a janela de palavras de contexto circundante para prever a palavra atual.

CBoW √© mais r√°pido, enquanto o skip-gram √© mais lento, mas faz um trabalho melhor ao representar palavras menos frequentes.

![Imagem mostrando os algoritmos CBoW e Skip-Gram para converter palavras em vetores.](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> Imagem retirada [deste artigo](https://arxiv.org/pdf/1301.3781.pdf)

Os embeddings pr√©-treinados do Word2Vec (bem como outros modelos semelhantes, como GloVe) tamb√©m podem ser usados no lugar da camada de embedding em redes neuronais. No entanto, precisamos de lidar com vocabul√°rios, porque o vocabul√°rio usado para pr√©-treinar o Word2Vec/GloVe provavelmente ser√° diferente do vocabul√°rio no nosso corpus de texto. Consulte os Notebooks acima para ver como este problema pode ser resolvido.

## Embeddings Contextuais

Uma limita√ß√£o importante das representa√ß√µes de embeddings pr√©-treinados tradicionais, como o Word2Vec, √© o problema da desambigua√ß√£o de sentidos das palavras. Embora os embeddings pr√©-treinados possam capturar algum significado das palavras no contexto, todos os poss√≠veis significados de uma palavra s√£o codificados no mesmo embedding. Isso pode causar problemas em modelos subsequentes, j√° que muitas palavras, como a palavra 'play', t√™m significados diferentes dependendo do contexto em que s√£o usadas.

Por exemplo, a palavra 'play' nas duas frases abaixo tem significados bastante diferentes:

- Fui a uma **pe√ßa** no teatro.  
- O Jo√£o quer **brincar** com os amigos.

Os embeddings pr√©-treinados acima representam ambos os significados da palavra 'play' no mesmo embedding. Para superar esta limita√ß√£o, precisamos de construir embeddings baseados no **modelo de linguagem**, que √© treinado num grande corpus de texto e *sabe* como as palavras podem ser combinadas em diferentes contextos. Discutir embeddings contextuais est√° fora do √¢mbito deste tutorial, mas voltaremos a eles ao falar sobre modelos de linguagem mais adiante no curso.

## Conclus√£o

Nesta li√ß√£o, descobriu como construir e usar camadas de embedding no TensorFlow e PyTorch para refletir melhor os significados sem√¢nticos das palavras.

## üöÄ Desafio

O Word2Vec tem sido usado para algumas aplica√ß√µes interessantes, incluindo a gera√ß√£o de letras de m√∫sicas e poesia. D√™ uma vista de olhos a [este artigo](https://www.politetype.com/blog/word2vec-color-poems), que explica como o autor usou o Word2Vec para gerar poesia. Veja tamb√©m [este v√≠deo de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) para descobrir uma explica√ß√£o diferente desta t√©cnica. Depois, tente aplicar estas t√©cnicas ao seu pr√≥prio corpus de texto, talvez obtido no Kaggle.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## Revis√£o e Autoestudo

Leia este artigo sobre o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Tarefa: Notebooks](assignment.md)

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, √© importante notar que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.