<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b708c9b85b833864c73c6281f1e6b96e",
  "translation_date": "2025-09-23T13:48:40+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "pt"
}
-->
# Embeddings

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Ao treinar classificadores baseados em BoW ou TF/IDF, trabalh√°vamos com vetores de bag-of-words de alta dimens√£o com comprimento `vocab_size`, e est√°vamos a converter explicitamente de vetores de representa√ß√£o posicional de baixa dimens√£o para uma representa√ß√£o esparsa de one-hot. No entanto, esta representa√ß√£o one-hot n√£o √© eficiente em termos de mem√≥ria. Al√©m disso, cada palavra √© tratada de forma independente, ou seja, os vetores codificados em one-hot n√£o expressam qualquer semelhan√ßa sem√¢ntica entre palavras.

A ideia de **embedding** √© representar palavras por vetores densos de menor dimens√£o, que de alguma forma refletem o significado sem√¢ntico de uma palavra. Mais tarde, discutiremos como construir embeddings de palavras significativos, mas por agora vamos apenas pensar nos embeddings como uma forma de reduzir a dimensionalidade de um vetor de palavras.

Assim, a camada de embedding receberia uma palavra como entrada e produziria um vetor de sa√≠da com o tamanho especificado `embedding_size`. De certa forma, √© muito semelhante a uma camada `Linear`, mas em vez de receber um vetor codificado em one-hot, ser√° capaz de receber um n√∫mero de palavra como entrada, permitindo-nos evitar a cria√ß√£o de grandes vetores codificados em one-hot.

Ao usar uma camada de embedding como a primeira camada na nossa rede de classifica√ß√£o, podemos mudar de um modelo de bag-of-words para um modelo de **embedding bag**, onde primeiro convertemos cada palavra no nosso texto no embedding correspondente e, em seguida, calculamos alguma fun√ß√£o agregada sobre todos esses embeddings, como `sum`, `average` ou `max`.  

![Imagem mostrando um classificador de embedding para cinco palavras de sequ√™ncia.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.pt.png)

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Embeddings

Continue a sua aprendizagem nos seguintes notebooks:
* [Embeddings com PyTorch](EmbeddingsPyTorch.ipynb)
* [Embeddings com TensorFlow](EmbeddingsTF.ipynb)

## Embeddings Sem√¢nticos: Word2Vec

Embora a camada de embedding tenha aprendido a mapear palavras para representa√ß√µes vetoriais, essa representa√ß√£o n√£o necessariamente possui muito significado sem√¢ntico. Seria interessante aprender uma representa√ß√£o vetorial tal que palavras semelhantes ou sin√≥nimos correspondam a vetores pr√≥ximos entre si em termos de alguma dist√¢ncia vetorial (por exemplo, dist√¢ncia Euclidiana).

Para isso, precisamos de pr√©-treinar o nosso modelo de embedding numa grande cole√ß√£o de texto de uma forma espec√≠fica. Uma maneira de treinar embeddings sem√¢nticos √© chamada [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Baseia-se em duas arquiteturas principais que s√£o usadas para produzir uma representa√ß√£o distribu√≠da de palavras:

 - **Continuous bag-of-words** (CBoW) ‚Äî nesta arquitetura, treinamos o modelo para prever uma palavra a partir do contexto circundante. Dado o ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, o objetivo do modelo √© prever $W_0$ a partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** √© o oposto do CBoW. O modelo usa uma janela de palavras de contexto circundantes para prever a palavra atual.

CBoW √© mais r√°pido, enquanto skip-gram √© mais lento, mas faz um trabalho melhor ao representar palavras menos frequentes.

![Imagem mostrando os algoritmos CBoW e Skip-Gram para converter palavras em vetores.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.pt.png)

> Imagem retirada [deste artigo](https://arxiv.org/pdf/1301.3781.pdf)

Os embeddings pr√©-treinados do Word2Vec (bem como outros modelos semelhantes, como GloVe) tamb√©m podem ser usados no lugar da camada de embedding em redes neurais. No entanto, precisamos de lidar com vocabul√°rios, porque o vocabul√°rio usado para pr√©-treinar o Word2Vec/GloVe provavelmente ser√° diferente do vocabul√°rio no nosso corpus de texto. Consulte os notebooks acima para ver como este problema pode ser resolvido.

## Embeddings Contextuais

Uma limita√ß√£o importante das representa√ß√µes tradicionais de embeddings pr√©-treinados, como Word2Vec, √© o problema da desambigua√ß√£o de sentidos das palavras. Embora os embeddings pr√©-treinados possam capturar algum significado das palavras no contexto, todos os poss√≠veis significados de uma palavra s√£o codificados no mesmo embedding. Isso pode causar problemas em modelos posteriores, j√° que muitas palavras, como a palavra 'play', t√™m significados diferentes dependendo do contexto em que s√£o usadas.

Por exemplo, a palavra 'play' nas duas frases abaixo tem significados bastante diferentes:

- Fui a uma **pe√ßa** no teatro.
- O Jo√£o quer **brincar** com os amigos.

Os embeddings pr√©-treinados acima representam ambos os significados da palavra 'play' no mesmo embedding. Para superar esta limita√ß√£o, precisamos de construir embeddings baseados no **modelo de linguagem**, que √© treinado num grande corpus de texto e *sabe* como as palavras podem ser combinadas em diferentes contextos. Discutir embeddings contextuais est√° fora do √¢mbito deste tutorial, mas voltaremos a eles quando falarmos sobre modelos de linguagem mais adiante no curso.

## Conclus√£o

Nesta li√ß√£o, descobriu como construir e usar camadas de embedding no TensorFlow e PyTorch para refletir melhor os significados sem√¢nticos das palavras.

## üöÄ Desafio

O Word2Vec tem sido usado em algumas aplica√ß√µes interessantes, incluindo a gera√ß√£o de letras de m√∫sicas e poesia. Veja [este artigo](https://www.politetype.com/blog/word2vec-color-poems), que explica como o autor usou o Word2Vec para gerar poesia. Assista tamb√©m a [este v√≠deo de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) para descobrir uma explica√ß√£o diferente desta t√©cnica. Depois, tente aplicar estas t√©cnicas ao seu pr√≥prio corpus de texto, talvez obtido no Kaggle.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Revis√£o & Autoestudo

Leia este artigo sobre Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Assignment: Notebooks](assignment.md)

---

