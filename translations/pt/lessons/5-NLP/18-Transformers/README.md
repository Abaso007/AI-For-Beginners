<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-24T10:17:06+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "pt"
}
-->
# Mecanismos de Aten√ß√£o e Transformers

## [Question√°rio pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Um dos problemas mais importantes no dom√≠nio do PLN (Processamento de Linguagem Natural) √© a **tradu√ß√£o autom√°tica**, uma tarefa essencial que sustenta ferramentas como o Google Tradutor. Nesta se√ß√£o, vamos focar na tradu√ß√£o autom√°tica ou, de forma mais geral, em qualquer tarefa de *sequ√™ncia para sequ√™ncia* (tamb√©m chamada de **transdu√ß√£o de frases**).

Com RNNs, a tarefa de sequ√™ncia para sequ√™ncia √© implementada por duas redes recorrentes, onde uma rede, o **codificador**, comprime uma sequ√™ncia de entrada em um estado oculto, enquanto outra rede, o **decodificador**, expande esse estado oculto em um resultado traduzido. Existem alguns problemas com essa abordagem:

* O estado final da rede codificadora tem dificuldade em lembrar o in√≠cio de uma frase, o que resulta em baixa qualidade do modelo para frases longas.
* Todas as palavras em uma sequ√™ncia t√™m o mesmo impacto no resultado. Na realidade, no entanto, palavras espec√≠ficas na sequ√™ncia de entrada frequentemente t√™m mais impacto nos resultados sequenciais do que outras.

Os **Mecanismos de Aten√ß√£o** fornecem um meio de ponderar o impacto contextual de cada vetor de entrada em cada previs√£o de sa√≠da da RNN. Isso √© implementado criando atalhos entre os estados intermedi√°rios da RNN de entrada e a RNN de sa√≠da. Dessa forma, ao gerar o s√≠mbolo de sa√≠da y<sub>t</sub>, consideramos todos os estados ocultos de entrada h<sub>i</sub>, com diferentes coeficientes de peso Œ±<sub>t,i</sub>.

![Imagem mostrando um modelo codificador/decodificador com uma camada de aten√ß√£o aditiva](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)

> O modelo codificador-decodificador com mecanismo de aten√ß√£o aditiva em [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citado deste [blog post](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

A matriz de aten√ß√£o {Œ±<sub>i,j</sub>} representaria o grau em que certas palavras de entrada influenciam a gera√ß√£o de uma palavra espec√≠fica na sequ√™ncia de sa√≠da. Abaixo est√° um exemplo de tal matriz:

![Imagem mostrando um alinhamento de exemplo encontrado pelo RNNsearch-50, retirada de Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)

> Figura de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Os mecanismos de aten√ß√£o s√£o respons√°veis por grande parte do estado da arte atual ou pr√≥ximo ao atual no PLN. No entanto, adicionar aten√ß√£o aumenta significativamente o n√∫mero de par√¢metros do modelo, o que levou a problemas de escalabilidade com RNNs. Uma limita√ß√£o chave na escalabilidade das RNNs √© que a natureza recorrente dos modelos dificulta o agrupamento e a paraleliza√ß√£o do treinamento. Em uma RNN, cada elemento de uma sequ√™ncia precisa ser processado em ordem sequencial, o que significa que n√£o pode ser facilmente paralelizado.

![Codificador Decodificador com Aten√ß√£o](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figura do [Blog do Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

A ado√ß√£o de mecanismos de aten√ß√£o, combinada com essa limita√ß√£o, levou √† cria√ß√£o dos agora modelos Transformers de estado da arte que conhecemos e usamos hoje, como BERT e Open-GPT3.

## Modelos Transformers

Uma das principais ideias por tr√°s dos transformers √© evitar a natureza sequencial das RNNs e criar um modelo que seja paraleliz√°vel durante o treinamento. Isso √© alcan√ßado implementando duas ideias:

* codifica√ß√£o posicional
* uso do mecanismo de auto-aten√ß√£o para capturar padr√µes em vez de RNNs (ou CNNs) (√© por isso que o artigo que introduz os transformers se chama *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Codifica√ß√£o/Embelezamento Posicional

A ideia da codifica√ß√£o posicional √© a seguinte:  
1. Ao usar RNNs, a posi√ß√£o relativa dos tokens √© representada pelo n√∫mero de passos, e, portanto, n√£o precisa ser representada explicitamente.  
2. No entanto, ao mudarmos para aten√ß√£o, precisamos saber as posi√ß√µes relativas dos tokens dentro de uma sequ√™ncia.  
3. Para obter a codifica√ß√£o posicional, aumentamos nossa sequ√™ncia de tokens com uma sequ√™ncia de posi√ß√µes dos tokens na sequ√™ncia (ou seja, uma sequ√™ncia de n√∫meros 0, 1, ...).  
4. Em seguida, misturamos a posi√ß√£o do token com um vetor de embelezamento do token. Para transformar a posi√ß√£o (inteiro) em um vetor, podemos usar diferentes abordagens:

* Embelezamento trein√°vel, semelhante ao embelezamento de tokens. Esta √© a abordagem que consideramos aqui. Aplicamos camadas de embelezamento tanto nos tokens quanto em suas posi√ß√µes, resultando em vetores de embelezamento com as mesmas dimens√µes, que ent√£o somamos.
* Fun√ß√£o fixa de codifica√ß√£o posicional, como proposto no artigo original.

<img src="images/pos-embedding.png" width="50%"/>

> Imagem do autor

O resultado que obtemos com o embelezamento posicional incorpora tanto o token original quanto sua posi√ß√£o dentro de uma sequ√™ncia.

### Auto-Aten√ß√£o Multi-Cabe√ßa

Em seguida, precisamos capturar alguns padr√µes dentro de nossa sequ√™ncia. Para isso, os transformers usam um mecanismo de **auto-aten√ß√£o**, que √© essencialmente aten√ß√£o aplicada √† mesma sequ√™ncia como entrada e sa√≠da. Aplicar auto-aten√ß√£o nos permite levar em conta o **contexto** dentro da frase e ver quais palavras est√£o inter-relacionadas. Por exemplo, permite-nos ver quais palavras s√£o referidas por correfer√™ncias, como *it*, e tamb√©m considerar o contexto:

![](../../../../../lessons/5-NLP/18-Transformers/images/CoreferenceResolution.png)

> Imagem do [Blog do Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Nos transformers, usamos **Aten√ß√£o Multi-Cabe√ßa** para dar √† rede o poder de capturar v√°rios tipos diferentes de depend√™ncias, como rela√ß√µes de palavras de longo prazo vs. curto prazo, correfer√™ncia vs. outra coisa, etc.

O [Notebook TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) cont√©m mais detalhes sobre a implementa√ß√£o das camadas do transformer.

### Aten√ß√£o Codificador-Decodificador

Nos transformers, a aten√ß√£o √© usada em dois lugares:

* Para capturar padr√µes dentro do texto de entrada usando auto-aten√ß√£o.
* Para realizar a tradu√ß√£o de sequ√™ncia - √© a camada de aten√ß√£o entre o codificador e o decodificador.

A aten√ß√£o codificador-decodificador √© muito semelhante ao mecanismo de aten√ß√£o usado nas RNNs, conforme descrito no in√≠cio desta se√ß√£o. Este diagrama animado explica o papel da aten√ß√£o codificador-decodificador.

![GIF animado mostrando como as avalia√ß√µes s√£o realizadas em modelos transformers.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Como cada posi√ß√£o de entrada √© mapeada de forma independente para cada posi√ß√£o de sa√≠da, os transformers podem paralelizar melhor do que as RNNs, o que permite modelos de linguagem muito maiores e mais expressivos. Cada cabe√ßa de aten√ß√£o pode ser usada para aprender diferentes rela√ß√µes entre palavras, melhorando as tarefas de Processamento de Linguagem Natural.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) √© uma rede transformer muito grande com v√°rias camadas: 12 camadas para o *BERT-base* e 24 para o *BERT-large*. O modelo √© inicialmente pr√©-treinado em um grande corpus de dados textuais (Wikipedia + livros) usando treinamento n√£o supervisionado (prevendo palavras mascaradas em uma frase). Durante o pr√©-treinamento, o modelo absorve n√≠veis significativos de compreens√£o da linguagem, que podem ser aproveitados com outros conjuntos de dados usando ajuste fino. Esse processo √© chamado de **aprendizagem por transfer√™ncia**.

![imagem de http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)

> Imagem [fonte](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Exerc√≠cios: Transformers

Continue o seu aprendizado nos seguintes notebooks:

* [Transformers em PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformers em TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Conclus√£o

Nesta li√ß√£o, voc√™ aprendeu sobre Transformers e Mecanismos de Aten√ß√£o, ferramentas essenciais na caixa de ferramentas do PLN. Existem muitas varia√ß√µes das arquiteturas de Transformers, incluindo BERT, DistilBERT, BigBird, OpenGPT3 e mais, que podem ser ajustadas. O [pacote HuggingFace](https://github.com/huggingface/) fornece um reposit√≥rio para treinar muitas dessas arquiteturas com PyTorch e TensorFlow.

## üöÄ Desafio

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Revis√£o e Autoestudo

* [Blog post](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), explicando o cl√°ssico artigo [Attention is all you need](https://arxiv.org/abs/1706.03762) sobre transformers.
* [Uma s√©rie de posts no blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sobre transformers, explicando a arquitetura em detalhe.

## [Tarefa](assignment.md)

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, √© importante ter em conta que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes da utiliza√ß√£o desta tradu√ß√£o.