<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-24T08:54:25+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "pt"
}
-->
# Representar Texto como Tensores

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Classifica√ß√£o de Texto

Na primeira parte desta sec√ß√£o, vamos focar-nos na tarefa de **classifica√ß√£o de texto**. Utilizaremos o Dataset [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), que cont√©m artigos de not√≠cias como o seguinte:

* Categoria: Ci√™ncia/Tecnologia
* T√≠tulo: Empresa de Ky. Ganha Subs√≠dio para Estudar Pept√≠deos (AP)
* Corpo: AP - Uma empresa fundada por um investigador de qu√≠mica na Universidade de Louisville ganhou um subs√≠dio para desenvolver...

O nosso objetivo ser√° classificar o artigo de not√≠cias numa das categorias com base no texto.

## Representar texto

Se quisermos resolver tarefas de Processamento de Linguagem Natural (NLP) com redes neuronais, precisamos de uma forma de representar texto como tensores. Os computadores j√° representam caracteres textuais como n√∫meros que correspondem a fontes no ecr√£ utilizando codifica√ß√µes como ASCII ou UTF-8.

<img alt="Imagem mostrando um diagrama que mapeia um car√°cter para uma representa√ß√£o ASCII e bin√°ria" src="images/ascii-character-map.png" width="50%"/>

> [Fonte da imagem](https://www.seobility.net/en/wiki/ASCII)

Como humanos, entendemos o que cada letra **representa** e como todos os caracteres se juntam para formar as palavras de uma frase. No entanto, os computadores, por si s√≥, n√£o t√™m essa compreens√£o, e a rede neuronal tem de aprender o significado durante o treino.

Por isso, podemos usar diferentes abordagens para representar texto:

* **Representa√ß√£o ao n√≠vel de caracteres**, onde representamos texto tratando cada car√°cter como um n√∫mero. Dado que temos *C* caracteres diferentes no nosso corpus de texto, a palavra *Hello* seria representada por um tensor de 5x*C*. Cada letra corresponderia a uma coluna do tensor em codifica√ß√£o one-hot.
* **Representa√ß√£o ao n√≠vel de palavras**, na qual criamos um **vocabul√°rio** de todas as palavras no nosso texto e depois representamos as palavras usando codifica√ß√£o one-hot. Esta abordagem √© de certa forma melhor, porque cada letra, por si s√≥, n√£o tem muito significado, e assim, ao usar conceitos sem√¢nticos de n√≠vel superior - palavras - simplificamos a tarefa para a rede neuronal. No entanto, dado o tamanho do dicion√°rio, precisamos de lidar com tensores esparsos de alta dimens√£o.

Independentemente da representa√ß√£o, primeiro precisamos de converter o texto numa sequ√™ncia de **tokens**, sendo um token um car√°cter, uma palavra ou, por vezes, at√© parte de uma palavra. Depois, convertemos o token num n√∫mero, normalmente usando um **vocabul√°rio**, e este n√∫mero pode ser alimentado numa rede neuronal usando codifica√ß√£o one-hot.

## N-Gramas

Na linguagem natural, o significado preciso das palavras s√≥ pode ser determinado no contexto. Por exemplo, os significados de *rede neural* e *rede de pesca* s√£o completamente diferentes. Uma das formas de levar isso em conta √© construir o nosso modelo com pares de palavras, considerando os pares de palavras como tokens separados no vocabul√°rio. Desta forma, a frase *Eu gosto de ir pescar* ser√° representada pela seguinte sequ√™ncia de tokens: *Eu gosto*, *gosto de*, *de ir*, *ir pescar*. O problema com esta abordagem √© que o tamanho do dicion√°rio cresce significativamente, e combina√ß√µes como *ir pescar* e *ir √†s compras* s√£o representadas por tokens diferentes, que n√£o partilham qualquer semelhan√ßa sem√¢ntica, apesar do mesmo verbo.  

Em alguns casos, podemos considerar usar tri-gramas -- combina√ß√µes de tr√™s palavras -- tamb√©m. Assim, esta abordagem √© frequentemente chamada de **n-gramas**. Al√©m disso, faz sentido usar n-gramas com representa√ß√£o ao n√≠vel de caracteres, caso em que os n-gramas corresponder√£o aproximadamente a diferentes s√≠labas.

## Bag-of-Words e TF/IDF

Ao resolver tarefas como classifica√ß√£o de texto, precisamos de ser capazes de representar texto por um vetor de tamanho fixo, que usaremos como entrada para o classificador denso final. Uma das formas mais simples de fazer isso √© combinar todas as representa√ß√µes individuais de palavras, por exemplo, somando-as. Se somarmos as codifica√ß√µes one-hot de cada palavra, acabaremos com um vetor de frequ√™ncias, mostrando quantas vezes cada palavra aparece no texto. Tal representa√ß√£o de texto √© chamada de **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Imagem do autor

Um BoW essencialmente representa quais palavras aparecem no texto e em que quantidades, o que pode ser uma boa indica√ß√£o do tema do texto. Por exemplo, um artigo de not√≠cias sobre pol√≠tica provavelmente cont√©m palavras como *presidente* e *pa√≠s*, enquanto uma publica√ß√£o cient√≠fica teria algo como *colisor*, *descoberto*, etc. Assim, as frequ√™ncias das palavras podem, em muitos casos, ser um bom indicador do conte√∫do do texto.

O problema com BoW √© que certas palavras comuns, como *e*, *√©*, etc., aparecem na maioria dos textos e t√™m as maiores frequ√™ncias, ocultando as palavras que s√£o realmente importantes. Podemos reduzir a import√¢ncia dessas palavras considerando a frequ√™ncia com que ocorrem em toda a cole√ß√£o de documentos. Esta √© a ideia principal por tr√°s da abordagem TF/IDF, que √© abordada em mais detalhe nos notebooks anexados a esta li√ß√£o.

No entanto, nenhuma dessas abordagens consegue levar totalmente em conta a **sem√¢ntica** do texto. Precisamos de modelos de redes neuronais mais poderosos para fazer isso, o que discutiremos mais tarde nesta sec√ß√£o.

## ‚úçÔ∏è Exerc√≠cios: Representa√ß√£o de Texto

Continue a sua aprendizagem nos seguintes notebooks:

* [Representa√ß√£o de Texto com PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [Representa√ß√£o de Texto com TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Conclus√£o

At√© agora, estud√°mos t√©cnicas que podem adicionar peso de frequ√™ncia a diferentes palavras. No entanto, estas t√©cnicas n√£o conseguem representar o significado ou a ordem. Como o famoso linguista J. R. Firth disse em 1935: "O significado completo de uma palavra √© sempre contextual, e nenhum estudo de significado fora do contexto pode ser levado a s√©rio." Mais tarde, no curso, aprenderemos como capturar informa√ß√µes contextuais do texto usando modelagem de linguagem.

## üöÄ Desafio

Experimente outros exerc√≠cios usando bag-of-words e diferentes modelos de dados. Pode inspirar-se nesta [competi√ß√£o no Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Revis√£o & Estudo Aut√≥nomo

Pratique as suas compet√™ncias com t√©cnicas de embeddings de texto e bag-of-words em [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Assignment: Notebooks](assignment.md)

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, √© importante notar que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.