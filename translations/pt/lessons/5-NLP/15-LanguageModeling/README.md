<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-24T08:54:47+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "pt"
}
-->
# Modela√ß√£o de Linguagem

Embeddings sem√¢nticos, como Word2Vec e GloVe, s√£o, na verdade, um primeiro passo em dire√ß√£o √† **modela√ß√£o de linguagem** - criar modelos que de alguma forma *compreendem* (ou *representam*) a natureza da linguagem.

## [Question√°rio pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

A ideia principal por tr√°s da modela√ß√£o de linguagem √© trein√°-los em conjuntos de dados n√£o rotulados de forma n√£o supervisionada. Isto √© importante porque temos grandes quantidades de texto n√£o rotulado dispon√≠vel, enquanto a quantidade de texto rotulado estar√° sempre limitada pelo esfor√ßo necess√°rio para rotul√°-lo. Na maioria das vezes, podemos construir modelos de linguagem que conseguem **prever palavras em falta** no texto, porque √© f√°cil ocultar uma palavra aleat√≥ria no texto e us√°-la como amostra de treino.

## Treinar Embeddings

Nos nossos exemplos anteriores, utiliz√°mos embeddings sem√¢nticos pr√©-treinados, mas √© interessante ver como esses embeddings podem ser treinados. Existem v√°rias ideias poss√≠veis que podem ser utilizadas:

* **Modela√ß√£o de linguagem N-Gram**, onde prevemos um token olhando para os N tokens anteriores (N-grama).
* **Continuous Bag-of-Words** (CBoW), onde prevemos o token central $W_0$ numa sequ√™ncia de tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, onde prevemos um conjunto de tokens vizinhos {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} a partir do token central $W_0$.

![imagem do artigo sobre convers√£o de palavras em vetores](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> Imagem retirada [deste artigo](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Notebooks de Exemplo: Treinar modelo CBoW

Continue a sua aprendizagem nos seguintes notebooks:

* [Treinar CBoW Word2Vec com TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Treinar CBoW Word2Vec com PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Conclus√£o

Na li√ß√£o anterior vimos que os embeddings de palavras funcionam como magia! Agora sabemos que treinar embeddings de palavras n√£o √© uma tarefa muito complexa, e devemos ser capazes de treinar os nossos pr√≥prios embeddings para texto espec√≠fico de dom√≠nio, se necess√°rio.

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Revis√£o & Estudo Aut√≥nomo

* [Tutorial oficial do PyTorch sobre Modela√ß√£o de Linguagem](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutorial oficial do TensorFlow sobre treino de modelo Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Utilizar o framework **gensim** para treinar os embeddings mais comuns em poucas linhas de c√≥digo √© descrito [nesta documenta√ß√£o](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Tarefa: Treinar Modelo Skip-Gram](lab/README.md)

No laborat√≥rio, desafiamos-lhe a modificar o c√≥digo desta li√ß√£o para treinar um modelo skip-gram em vez de CBoW. [Leia os detalhes](lab/README.md)

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, √© importante notar que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.