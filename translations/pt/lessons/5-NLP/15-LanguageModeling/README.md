<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T13:46:46+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "pt"
}
-->
# Modela√ß√£o de Linguagem

Embeddings sem√¢nticos, como Word2Vec e GloVe, s√£o, na verdade, um primeiro passo para a **modela√ß√£o de linguagem** - criar modelos que de alguma forma *compreendem* (ou *representam*) a natureza da linguagem.

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/29)

A ideia principal por tr√°s da modela√ß√£o de linguagem √© trein√°-los em conjuntos de dados n√£o rotulados de forma n√£o supervisionada. Isto √© importante porque temos enormes quantidades de texto n√£o rotulado dispon√≠veis, enquanto a quantidade de texto rotulado estar√° sempre limitada pelo esfor√ßo necess√°rio para rotular. Na maioria das vezes, podemos construir modelos de linguagem que conseguem **prever palavras em falta** no texto, porque √© f√°cil ocultar uma palavra aleat√≥ria no texto e us√°-la como amostra de treino.

## Treinar Embeddings

Nos nossos exemplos anteriores, utiliz√°mos embeddings sem√¢nticos pr√©-treinados, mas √© interessante ver como esses embeddings podem ser treinados. Existem v√°rias ideias poss√≠veis que podem ser utilizadas:

* **Modela√ß√£o de linguagem N-Gram**, onde prevemos um token olhando para os N tokens anteriores (N-gram).
* **Continuous Bag-of-Words** (CBoW), onde prevemos o token central $W_0$ numa sequ√™ncia de tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, onde prevemos um conjunto de tokens vizinhos {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} a partir do token central $W_0$.

![imagem do artigo sobre convers√£o de palavras em vetores](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.pt.png)

> Imagem retirada [deste artigo](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Notebooks de Exemplo: Treinar modelo CBoW

Continue a sua aprendizagem nos seguintes notebooks:

* [Treinar CBoW Word2Vec com TensorFlow](CBoW-TF.ipynb)
* [Treinar CBoW Word2Vec com PyTorch](CBoW-PyTorch.ipynb)

## Conclus√£o

Na aula anterior vimos que embeddings de palavras funcionam como magia! Agora sabemos que treinar embeddings de palavras n√£o √© uma tarefa muito complexa, e devemos ser capazes de treinar os nossos pr√≥prios embeddings para texto espec√≠fico de um dom√≠nio, se necess√°rio.

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Revis√£o & Estudo Individual

* [Tutorial oficial do PyTorch sobre Modela√ß√£o de Linguagem](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutorial oficial do TensorFlow sobre treinar modelo Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Utilizar o framework **gensim** para treinar os embeddings mais comuns em poucas linhas de c√≥digo est√° descrito [nesta documenta√ß√£o](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Tarefa: Treinar Modelo Skip-Gram](lab/README.md)

No laborat√≥rio, desafiamos voc√™ a modificar o c√≥digo desta aula para treinar um modelo skip-gram em vez de CBoW. [Leia os detalhes](lab/README.md)

---

