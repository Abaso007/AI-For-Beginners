<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "51be6057374d01d70e07dd5ec88ebc0d",
  "translation_date": "2025-09-23T13:46:27+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "pt"
}
-->
# Redes Generativas

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/33)

As Redes Neuronais Recorrentes (RNNs) e suas variantes com c√©lulas de controlo, como as Long Short Term Memory Cells (LSTMs) e Gated Recurrent Units (GRUs), proporcionaram um mecanismo para modela√ß√£o de linguagem, permitindo aprender a ordem das palavras e prever a pr√≥xima palavra numa sequ√™ncia. Isto permite-nos usar RNNs para **tarefas generativas**, como gera√ß√£o de texto comum, tradu√ß√£o autom√°tica e at√© legendagem de imagens.

> ‚úÖ Pense em todas as vezes que beneficiou de tarefas generativas, como a conclus√£o de texto enquanto escreve. Pesquise sobre as suas aplica√ß√µes favoritas para ver se utilizaram RNNs.

Na arquitetura de RNN discutida na unidade anterior, cada unidade RNN produzia o pr√≥ximo estado oculto como sa√≠da. No entanto, tamb√©m podemos adicionar outra sa√≠da a cada unidade recorrente, permitindo-nos gerar uma **sequ√™ncia** (de comprimento igual √† sequ√™ncia original). Al√©m disso, podemos usar unidades RNN que n√£o aceitam uma entrada em cada passo, mas apenas um vetor de estado inicial, e depois produzem uma sequ√™ncia de sa√≠das.

Isto permite diferentes arquiteturas neuronais, como mostrado na imagem abaixo:

![Imagem mostrando padr√µes comuns de redes neuronais recorrentes.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.pt.jpg)

> Imagem do artigo [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) por [Andrej Karpaty](http://karpathy.github.io/)

* **Um-para-um** √© uma rede neural tradicional com uma entrada e uma sa√≠da.
* **Um-para-muitos** √© uma arquitetura generativa que aceita um valor de entrada e gera uma sequ√™ncia de valores de sa√≠da. Por exemplo, se quisermos treinar uma rede de **legendagem de imagens** que produza uma descri√ß√£o textual de uma imagem, podemos usar uma imagem como entrada, pass√°-la por uma CNN para obter o estado oculto e, em seguida, usar uma cadeia recorrente para gerar a legenda palavra por palavra.
* **Muitos-para-um** corresponde √†s arquiteturas RNN descritas na unidade anterior, como classifica√ß√£o de texto.
* **Muitos-para-muitos**, ou **sequ√™ncia-para-sequ√™ncia**, corresponde a tarefas como **tradu√ß√£o autom√°tica**, onde temos uma primeira RNN que recolhe toda a informa√ß√£o da sequ√™ncia de entrada no estado oculto, e outra cadeia RNN que desenrola este estado na sequ√™ncia de sa√≠da.

Nesta unidade, vamos focar-nos em modelos generativos simples que nos ajudam a gerar texto. Para simplificar, usaremos tokeniza√ß√£o a n√≠vel de caracteres.

Vamos treinar esta RNN para gerar texto passo a passo. Em cada passo, tomaremos uma sequ√™ncia de caracteres de comprimento `nchars` e pediremos √† rede que gere o pr√≥ximo car√°cter de sa√≠da para cada car√°cter de entrada:

![Imagem mostrando um exemplo de gera√ß√£o de RNN da palavra 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.pt.png)

Ao gerar texto (durante a infer√™ncia), come√ßamos com um **prompt**, que √© passado pelas c√©lulas RNN para gerar o estado interm√©dio, e a partir deste estado come√ßa a gera√ß√£o. Geramos um car√°cter de cada vez e passamos o estado e o car√°cter gerado para outra c√©lula RNN para gerar o pr√≥ximo, at√© gerarmos caracteres suficientes.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Redes Generativas

Continue a sua aprendizagem nos seguintes notebooks:

* [Redes Generativas com PyTorch](GenerativePyTorch.ipynb)
* [Redes Generativas com TensorFlow](GenerativeTF.ipynb)

## Gera√ß√£o de texto suave e temperatura

A sa√≠da de cada c√©lula RNN √© uma distribui√ß√£o de probabilidade de caracteres. Se sempre escolhermos o car√°cter com a maior probabilidade como o pr√≥ximo car√°cter no texto gerado, o texto pode frequentemente tornar-se "c√≠clico", repetindo as mesmas sequ√™ncias de caracteres repetidamente, como neste exemplo:

```
today of the second the company and a second the company ...
```

No entanto, se olharmos para a distribui√ß√£o de probabilidade do pr√≥ximo car√°cter, pode acontecer que a diferen√ßa entre algumas das maiores probabilidades n√£o seja muito grande, por exemplo, um car√°cter pode ter probabilidade 0.2, outro 0.19, etc. Por exemplo, ao procurar o pr√≥ximo car√°cter na sequ√™ncia '*play*', o pr√≥ximo car√°cter pode ser igualmente um espa√ßo ou **e** (como na palavra *player*).

Isto leva-nos √† conclus√£o de que nem sempre √© "justo" selecionar o car√°cter com maior probabilidade, porque escolher o segundo maior ainda pode levar a texto significativo. √â mais sensato **amostrar** caracteres da distribui√ß√£o de probabilidade dada pela sa√≠da da rede. Podemos tamb√©m usar um par√¢metro, **temperatura**, que ajusta a distribui√ß√£o de probabilidade, caso queiramos adicionar mais aleatoriedade ou torn√°-la mais √≠ngreme, se quisermos aderir mais aos caracteres de maior probabilidade.

Explore como esta gera√ß√£o de texto suave √© implementada nos notebooks acima mencionados.

## Conclus√£o

Embora a gera√ß√£o de texto possa ser √∫til por si s√≥, os maiores benef√≠cios v√™m da capacidade de gerar texto usando RNNs a partir de algum vetor de caracter√≠sticas inicial. Por exemplo, a gera√ß√£o de texto √© usada como parte da tradu√ß√£o autom√°tica (sequ√™ncia-para-sequ√™ncia, neste caso o vetor de estado do *encoder* √© usado para gerar ou *decodificar* a mensagem traduzida) ou para gerar descri√ß√µes textuais de uma imagem (neste caso, o vetor de caracter√≠sticas viria de um extrator CNN).

## üöÄ Desafio

Fa√ßa algumas li√ß√µes na Microsoft Learn sobre este t√≥pico:

* Gera√ß√£o de Texto com [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Revis√£o & Autoestudo

Aqui est√£o alguns artigos para expandir o seu conhecimento:

* Diferentes abordagens para gera√ß√£o de texto com Markov Chain, LSTM e GPT-2: [artigo](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exemplo de gera√ß√£o de texto na [documenta√ß√£o do Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Tarefa](lab/README.md)

Vimos como gerar texto car√°cter por car√°cter. No laborat√≥rio, ir√° explorar a gera√ß√£o de texto a n√≠vel de palavras.

---

