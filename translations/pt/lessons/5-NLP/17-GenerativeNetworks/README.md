<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-24T08:54:05+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "pt"
}
-->
# Redes Generativas

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Redes Neuronais Recorrentes (RNNs) e suas variantes com c√©lulas controladas, como Long Short Term Memory Cells (LSTMs) e Gated Recurrent Units (GRUs), oferecem um mecanismo para modelagem de linguagem, permitindo aprender a ordem das palavras e prever a pr√≥xima palavra em uma sequ√™ncia. Isso possibilita o uso de RNNs para **tarefas generativas**, como gera√ß√£o de texto comum, tradu√ß√£o autom√°tica e at√© legendagem de imagens.

> ‚úÖ Pense em todas as vezes que beneficiou-se de tarefas generativas, como a conclus√£o de texto enquanto escreve. Pesquise sobre as suas aplica√ß√µes favoritas para descobrir se elas utilizam RNNs.

Na arquitetura de RNN discutida na unidade anterior, cada unidade RNN produzia o pr√≥ximo estado oculto como sa√≠da. No entanto, tamb√©m podemos adicionar outra sa√≠da a cada unidade recorrente, permitindo-nos gerar uma **sequ√™ncia** (com comprimento igual √† sequ√™ncia original). Al√©m disso, podemos usar unidades RNN que n√£o aceitam uma entrada em cada etapa, apenas um vetor de estado inicial, e ent√£o produzem uma sequ√™ncia de sa√≠das.

Isso permite diferentes arquiteturas neurais, como mostrado na imagem abaixo:

![Imagem mostrando padr√µes comuns de redes neurais recorrentes.](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/unreasonable-effectiveness-of-rnn.jpg)

> Imagem do artigo [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) por [Andrej Karpaty](http://karpathy.github.io/)

* **Um-para-um** √© uma rede neural tradicional com uma entrada e uma sa√≠da
* **Um-para-muitos** √© uma arquitetura generativa que aceita um valor de entrada e gera uma sequ√™ncia de valores de sa√≠da. Por exemplo, se quisermos treinar uma rede de **legendagem de imagens** que produza uma descri√ß√£o textual de uma imagem, podemos usar uma imagem como entrada, pass√°-la por uma CNN para obter seu estado oculto e, em seguida, usar uma cadeia recorrente para gerar a legenda palavra por palavra
* **Muitos-para-um** corresponde √†s arquiteturas RNN descritas na unidade anterior, como classifica√ß√£o de texto
* **Muitos-para-muitos**, ou **sequ√™ncia-para-sequ√™ncia**, corresponde a tarefas como **tradu√ß√£o autom√°tica**, onde uma primeira RNN coleta todas as informa√ß√µes da sequ√™ncia de entrada no estado oculto, e outra cadeia RNN desenrola esse estado na sequ√™ncia de sa√≠da.

Nesta unidade, focaremos em modelos generativos simples que nos ajudam a gerar texto. Para simplificar, utilizaremos tokeniza√ß√£o a n√≠vel de caracteres.

Treinaremos esta RNN para gerar texto passo a passo. Em cada etapa, tomaremos uma sequ√™ncia de caracteres de comprimento `nchars` e pediremos √† rede que gere o pr√≥ximo caractere de sa√≠da para cada caractere de entrada:

![Imagem mostrando um exemplo de gera√ß√£o de RNN da palavra 'HELLO'.](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate.png)

Ao gerar texto (durante a infer√™ncia), come√ßamos com um **prompt**, que √© passado pelas c√©lulas RNN para gerar seu estado intermedi√°rio, e ent√£o a gera√ß√£o come√ßa a partir desse estado. Geramos um caractere de cada vez e passamos o estado e o caractere gerado para outra c√©lula RNN para gerar o pr√≥ximo, at√© que tenhamos gerado caracteres suficientes.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Redes Generativas

Continue o seu aprendizado nos seguintes notebooks:

* [Redes Generativas com PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Redes Generativas com TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Gera√ß√£o de texto suave e temperatura

A sa√≠da de cada c√©lula RNN √© uma distribui√ß√£o de probabilidade de caracteres. Se sempre escolhermos o caractere com a maior probabilidade como o pr√≥ximo caractere no texto gerado, o texto pode frequentemente "ciclar" entre as mesmas sequ√™ncias de caracteres repetidamente, como neste exemplo:

```
today of the second the company and a second the company ...
```

No entanto, ao observar a distribui√ß√£o de probabilidade para o pr√≥ximo caractere, pode ser que a diferen√ßa entre algumas das maiores probabilidades n√£o seja muito grande, por exemplo, um caractere pode ter probabilidade 0.2, outro 0.19, etc. Por exemplo, ao procurar o pr√≥ximo caractere na sequ√™ncia '*play*', o pr√≥ximo caractere pode ser igualmente um espa√ßo ou **e** (como na palavra *player*).

Isso nos leva √† conclus√£o de que nem sempre √© "justo" selecionar o caractere com maior probabilidade, pois escolher o segundo maior ainda pode levar a um texto significativo. √â mais s√°bio **amostrar** caracteres da distribui√ß√£o de probabilidade fornecida pela sa√≠da da rede. Podemos tamb√©m usar um par√¢metro, **temperatura**, que ajusta a distribui√ß√£o de probabilidade, caso queiramos adicionar mais aleatoriedade ou torn√°-la mais √≠ngreme, se quisermos nos ater mais aos caracteres de maior probabilidade.

Explore como essa gera√ß√£o de texto suave √© implementada nos notebooks acima.

## Conclus√£o

Embora a gera√ß√£o de texto possa ser √∫til por si s√≥, os maiores benef√≠cios v√™m da capacidade de gerar texto usando RNNs a partir de algum vetor de caracter√≠sticas inicial. Por exemplo, a gera√ß√£o de texto √© usada como parte da tradu√ß√£o autom√°tica (sequ√™ncia-para-sequ√™ncia, neste caso o vetor de estado do *encoder* √© usado para gerar ou *decodificar* a mensagem traduzida) ou para gerar descri√ß√µes textuais de uma imagem (neste caso, o vetor de caracter√≠sticas viria de um extrator CNN).

## üöÄ Desafio

Fa√ßa algumas li√ß√µes no Microsoft Learn sobre este t√≥pico:

* Gera√ß√£o de Texto com [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Revis√£o e Autoestudo

Aqui est√£o alguns artigos para expandir o seu conhecimento:

* Diferentes abordagens para gera√ß√£o de texto com Markov Chain, LSTM e GPT-2: [artigo](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exemplo de gera√ß√£o de texto na [documenta√ß√£o do Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Tarefa](lab/README.md)

Vimos como gerar texto caractere por caractere. No laborat√≥rio, voc√™ explorar√° a gera√ß√£o de texto a n√≠vel de palavras.

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.