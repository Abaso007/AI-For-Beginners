{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melatih RL untuk Menyeimbangkan Cartpole\n",
    "\n",
    "Notebook ini adalah bagian dari [Kurikulum AI untuk Pemula](http://aka.ms/ai-beginners). Notebook ini terinspirasi dari [tutorial resmi PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) dan [implementasi Cartpole PyTorch ini](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Dalam contoh ini, kita akan menggunakan RL untuk melatih sebuah model agar dapat menyeimbangkan sebuah tiang pada kereta yang dapat bergerak ke kiri dan ke kanan pada skala horizontal. Kita akan menggunakan lingkungan [OpenAI Gym](https://www.gymlibrary.ml/) untuk mensimulasikan tiang tersebut.\n",
    "\n",
    "> **Catatan**: Anda dapat menjalankan kode pelajaran ini secara lokal (misalnya dari Visual Studio Code), di mana simulasi akan terbuka di jendela baru. Saat menjalankan kode secara online, Anda mungkin perlu melakukan beberapa penyesuaian pada kode, seperti yang dijelaskan [di sini](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Kita akan memulai dengan memastikan Gym telah terinstal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita buat lingkungan CartPole dan lihat bagaimana cara mengoperasikannya. Sebuah lingkungan memiliki properti berikut:\n",
    "\n",
    "* **Action space** adalah kumpulan aksi yang dapat kita lakukan pada setiap langkah simulasi\n",
    "* **Observation space** adalah ruang pengamatan yang dapat kita amati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita lihat bagaimana simulasi bekerja. Loop berikut menjalankan simulasi hingga `env.step` tidak mengembalikan tanda penghentian `done`. Kita akan memilih aksi secara acak menggunakan `env.action_space.sample()`, yang berarti eksperimen kemungkinan besar akan gagal dengan cepat (lingkungan CartPole berakhir ketika kecepatan CartPole, posisinya, atau sudutnya berada di luar batas tertentu).\n",
    "\n",
    "> Simulasi akan terbuka di jendela baru. Anda dapat menjalankan kode beberapa kali dan melihat bagaimana hasilnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda dapat melihat bahwa observasi mengandung 4 angka. Angka-angka tersebut adalah:\n",
    "- Posisi kereta\n",
    "- Kecepatan kereta\n",
    "- Sudut tiang\n",
    "- Laju rotasi tiang\n",
    "\n",
    "`rew` adalah hadiah yang kita terima di setiap langkah. Dalam lingkungan CartPole, Anda mendapatkan 1 poin untuk setiap langkah simulasi, dan tujuannya adalah memaksimalkan total hadiah, yaitu waktu CartPole dapat menjaga keseimbangan tanpa jatuh.\n",
    "\n",
    "Selama pembelajaran penguatan, tujuan kita adalah melatih **kebijakan** $\\pi$, yang untuk setiap keadaan $s$ akan memberi tahu kita tindakan $a$ yang harus diambil, sehingga pada dasarnya $a = \\pi(s)$.\n",
    "\n",
    "Jika Anda menginginkan solusi probabilistik, Anda dapat menganggap kebijakan sebagai pengembalian sekumpulan probabilitas untuk setiap tindakan, yaitu $\\pi(a|s)$ akan berarti probabilitas bahwa kita harus mengambil tindakan $a$ pada keadaan $s$.\n",
    "\n",
    "## Metode Policy Gradient\n",
    "\n",
    "Dalam algoritma RL yang paling sederhana, yang disebut **Policy Gradient**, kita akan melatih jaringan saraf untuk memprediksi tindakan berikutnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kami akan melatih jaringan dengan menjalankan banyak eksperimen, dan memperbarui jaringan kami setelah setiap percobaan. Mari kita definisikan sebuah fungsi yang akan menjalankan eksperimen dan mengembalikan hasilnya (yang disebut **jejak**) - semua keadaan, tindakan (dan probabilitas yang direkomendasikan), serta hadiah:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda dapat menjalankan satu episode dengan jaringan yang belum dilatih dan mengamati bahwa total hadiah (alias durasi episode) sangat rendah:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salah satu aspek rumit dari algoritma policy gradient adalah menggunakan **reward yang didiskon**. Idenya adalah kita menghitung vektor total reward pada setiap langkah permainan, dan selama proses ini kita mendiskon reward awal menggunakan beberapa koefisien $gamma$. Kita juga menormalkan vektor yang dihasilkan, karena kita akan menggunakannya sebagai bobot untuk memengaruhi pelatihan kita:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita mulai pelatihan! Kita akan menjalankan 300 episode, dan pada setiap episode kita akan melakukan hal berikut:\n",
    "\n",
    "1. Jalankan eksperimen dan kumpulkan jejaknya.\n",
    "2. Hitung perbedaan (`gradients`) antara tindakan yang diambil dan probabilitas yang diprediksi. Semakin kecil perbedaannya, semakin yakin kita bahwa tindakan yang diambil adalah tindakan yang benar.\n",
    "3. Hitung hadiah yang didiskon dan kalikan `gradients` dengan hadiah yang didiskon - ini akan memastikan bahwa langkah-langkah dengan hadiah lebih tinggi memiliki pengaruh lebih besar pada hasil akhir dibandingkan dengan langkah-langkah dengan hadiah lebih rendah.\n",
    "4. Tindakan target yang diharapkan untuk jaringan saraf kita sebagian akan diambil dari probabilitas yang diprediksi selama proses berjalan, dan sebagian dari `gradients` yang dihitung. Kita akan menggunakan parameter `alpha` untuk menentukan sejauh mana `gradients` dan hadiah diperhitungkan - ini disebut *learning rate* dari algoritma penguatan.\n",
    "5. Akhirnya, kita melatih jaringan kita pada keadaan dan tindakan yang diharapkan, lalu mengulangi prosesnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari jalankan episode dengan rendering untuk melihat hasilnya:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semoga Anda bisa melihat bahwa tiang sekarang dapat seimbang dengan cukup baik!\n",
    "\n",
    "## Model Actor-Critic\n",
    "\n",
    "Model Actor-Critic adalah pengembangan lebih lanjut dari policy gradients, di mana kita membangun jaringan neural untuk mempelajari kebijakan (policy) dan estimasi reward secara bersamaan. Jaringan ini akan memiliki dua output (atau Anda bisa melihatnya sebagai dua jaringan terpisah):\n",
    "* **Actor** akan merekomendasikan tindakan yang harus diambil dengan memberikan distribusi probabilitas keadaan, seperti pada model policy gradient.\n",
    "* **Critic** akan memperkirakan apa reward yang akan diperoleh dari tindakan tersebut. Critic mengembalikan total estimasi reward di masa depan pada keadaan tertentu.\n",
    "\n",
    "Mari kita definisikan model seperti itu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita perlu sedikit memodifikasi fungsi `discounted_rewards` dan `run_episode` kita:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita akan menjalankan loop pelatihan utama. Kita akan menggunakan proses pelatihan jaringan manual dengan menghitung fungsi kerugian yang sesuai dan memperbarui parameter jaringan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poin Penting\n",
    "\n",
    "Kita telah melihat dua algoritma RL dalam demo ini: simple policy gradient, dan actor-critic yang lebih canggih. Anda dapat melihat bahwa algoritma-algoritma tersebut bekerja dengan konsep abstrak seperti state, action, dan reward - sehingga algoritma ini dapat diterapkan pada berbagai jenis lingkungan yang sangat berbeda.\n",
    "\n",
    "Reinforcement learning memungkinkan kita mempelajari strategi terbaik untuk menyelesaikan masalah hanya dengan melihat reward akhir. Fakta bahwa kita tidak memerlukan dataset berlabel memungkinkan kita untuk mengulangi simulasi berkali-kali guna mengoptimalkan model kita. Namun, masih ada banyak tantangan dalam RL, yang mungkin akan Anda pelajari jika Anda memutuskan untuk lebih mendalami bidang AI yang menarik ini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berupaya untuk memberikan hasil yang akurat, harap diperhatikan bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang berwenang. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan manusia profesional. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T13:03:17+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "id"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}