{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mekanisme Perhatian dan Transformer\n",
    "\n",
    "Salah satu kelemahan utama jaringan berulang (recurrent networks) adalah semua kata dalam sebuah urutan memiliki dampak yang sama terhadap hasil. Hal ini menyebabkan performa yang kurang optimal pada model encoder-decoder LSTM standar untuk tugas sequence-to-sequence, seperti Named Entity Recognition dan Machine Translation. Pada kenyataannya, kata-kata tertentu dalam urutan input sering kali memiliki pengaruh lebih besar terhadap output berurutan dibandingkan yang lain.\n",
    "\n",
    "Pertimbangkan model sequence-to-sequence, seperti penerjemahan mesin. Model ini diimplementasikan dengan dua jaringan berulang, di mana satu jaringan (**encoder**) akan merangkum urutan input menjadi state tersembunyi, dan jaringan lainnya, **decoder**, akan mengurai state tersembunyi ini menjadi hasil terjemahan. Masalah dengan pendekatan ini adalah state akhir dari jaringan akan kesulitan mengingat awal kalimat, sehingga menyebabkan kualitas model yang buruk pada kalimat panjang.\n",
    "\n",
    "**Mekanisme Perhatian** menyediakan cara untuk memberikan bobot pada dampak kontekstual dari setiap vektor input terhadap setiap prediksi output dari RNN. Cara ini diimplementasikan dengan menciptakan jalur pintas antara state antara dari RNN input dan RNN output. Dengan cara ini, saat menghasilkan simbol output $y_t$, kita akan mempertimbangkan semua state tersembunyi input $h_i$, dengan koefisien bobot yang berbeda $\\alpha_{t,i}$. \n",
    "\n",
    "![Gambar menunjukkan model encoder/decoder dengan lapisan perhatian aditif](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.id.png)\n",
    "*Model encoder-decoder dengan mekanisme perhatian aditif dalam [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), dikutip dari [blog ini](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matriks perhatian $\\{\\alpha_{i,j}\\}$ akan merepresentasikan sejauh mana kata-kata tertentu dalam input berperan dalam menghasilkan kata tertentu dalam urutan output. Di bawah ini adalah contoh matriks seperti itu:\n",
    "\n",
    "![Gambar menunjukkan contoh alignment yang ditemukan oleh RNNsearch-50, diambil dari Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.id.png)\n",
    "\n",
    "*Gambar diambil dari [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Gambar 3)*\n",
    "\n",
    "Mekanisme perhatian bertanggung jawab atas banyak pencapaian terkini atau mendekati terkini dalam pemrosesan bahasa alami. Namun, menambahkan perhatian secara signifikan meningkatkan jumlah parameter model, yang menyebabkan masalah skala pada RNN. Salah satu kendala utama dalam menskalakan RNN adalah sifat berulang dari model ini yang membuat pelatihan sulit untuk dibatch dan diparalelkan. Dalam RNN, setiap elemen dari urutan harus diproses secara berurutan, yang berarti tidak dapat dengan mudah diparalelkan.\n",
    "\n",
    "Adopsi mekanisme perhatian yang dikombinasikan dengan kendala ini mengarah pada terciptanya Model Transformer yang kini menjadi State of the Art, seperti yang kita kenal dan gunakan saat ini, mulai dari BERT hingga OpenGPT3.\n",
    "\n",
    "## Model Transformer\n",
    "\n",
    "Alih-alih meneruskan konteks dari setiap prediksi sebelumnya ke langkah evaluasi berikutnya, **model transformer** menggunakan **positional encoding** dan **perhatian** untuk menangkap konteks dari input tertentu dalam jendela teks yang diberikan. Gambar di bawah ini menunjukkan bagaimana positional encoding dengan perhatian dapat menangkap konteks dalam jendela tertentu.\n",
    "\n",
    "![GIF animasi menunjukkan bagaimana evaluasi dilakukan dalam model transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Karena setiap posisi input dipetakan secara independen ke setiap posisi output, transformer dapat melakukan paralelisasi lebih baik dibandingkan RNN, yang memungkinkan model bahasa yang jauh lebih besar dan lebih ekspresif. Setiap kepala perhatian dapat digunakan untuk mempelajari hubungan yang berbeda antara kata-kata, yang meningkatkan tugas Pemrosesan Bahasa Alami di hilir.\n",
    "\n",
    "## Membangun Model Transformer Sederhana\n",
    "\n",
    "Keras tidak memiliki lapisan Transformer bawaan, tetapi kita dapat membangunnya sendiri. Seperti sebelumnya, kita akan fokus pada klasifikasi teks dari dataset AG News, tetapi perlu dicatat bahwa model Transformer menunjukkan hasil terbaik pada tugas NLP yang lebih sulit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lapisan baru dalam Keras harus merupakan subclass dari kelas `Layer`, dan mengimplementasikan metode `call`. Mari kita mulai dengan lapisan **Positional Embedding**. Kita akan menggunakan [beberapa kode dari dokumentasi resmi Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Kita akan mengasumsikan bahwa kita menyisipkan semua urutan input hingga panjang `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lapisan ini terdiri dari dua lapisan `Embedding`: untuk menyematkan token (dengan cara yang telah kita bahas sebelumnya) dan posisi token. Posisi token dibuat sebagai urutan bilangan alami dari 0 hingga `maxlen` menggunakan `tf.range`, lalu diteruskan melalui lapisan embedding. Dua vektor embedding yang dihasilkan kemudian dijumlahkan, menghasilkan representasi input yang disematkan secara posisi dengan bentuk `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Sekarang, mari kita implementasikan blok transformer. Blok ini akan menerima output dari lapisan embedding yang telah didefinisikan sebelumnya:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang, kita siap untuk mendefinisikan model transformer lengkap:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Transformer BERT\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) adalah jaringan transformer multi-layer yang sangat besar dengan 12 lapisan untuk *BERT-base*, dan 24 lapisan untuk *BERT-large*. Model ini pertama kali dilatih pada korpus teks yang sangat besar (WikiPedia + buku) menggunakan pelatihan tanpa pengawasan (memprediksi kata-kata yang disembunyikan dalam sebuah kalimat). Selama pelatihan awal, model ini menyerap pemahaman bahasa yang signifikan yang kemudian dapat dimanfaatkan dengan dataset lain melalui proses penyetelan ulang. Proses ini disebut **transfer learning**.\n",
    "\n",
    "![gambar dari http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.id.png)\n",
    "\n",
    "Ada banyak variasi arsitektur Transformer termasuk BERT, DistilBERT, BigBird, OpenGPT3, dan lainnya yang dapat disesuaikan lebih lanjut.\n",
    "\n",
    "Mari kita lihat bagaimana kita dapat menggunakan model BERT yang telah dilatih sebelumnya untuk menyelesaikan masalah klasifikasi urutan tradisional kita. Kita akan meminjam ide dan beberapa kode dari [dokumentasi resmi](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Untuk memuat model yang telah dilatih sebelumnya, kita akan menggunakan **Tensorflow hub**. Pertama, mari kita muat vektorisasi khusus BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penting untuk menggunakan vectorizer yang sama seperti yang digunakan oleh jaringan asli saat dilatih. Selain itu, vectorizer BERT mengembalikan tiga komponen:\n",
    "* `input_word_ids`, yaitu urutan nomor token untuk kalimat masukan\n",
    "* `input_mask`, menunjukkan bagian mana dari urutan yang berisi masukan sebenarnya, dan bagian mana yang merupakan padding. Ini mirip dengan mask yang dihasilkan oleh lapisan `Masking`\n",
    "* `input_type_ids` digunakan untuk tugas pemodelan bahasa, dan memungkinkan untuk menentukan dua kalimat masukan dalam satu urutan.\n",
    "\n",
    "Kemudian, kita dapat membuat instans extractor fitur BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jadi, lapisan BERT mengembalikan sejumlah hasil yang berguna:\n",
    "* `pooled_output` adalah hasil dari merata-ratakan semua token dalam urutan. Anda dapat menganggapnya sebagai embedding semantik cerdas dari seluruh jaringan. Ini setara dengan keluaran dari lapisan `GlobalAveragePooling1D` pada model kita sebelumnya.\n",
    "* `sequence_output` adalah keluaran dari lapisan transformer terakhir (sesuai dengan keluaran `TransformerBlock` pada model kita di atas).\n",
    "* `encoder_outputs` adalah keluaran dari semua lapisan transformer. Karena kita telah memuat model BERT dengan 4 lapisan (seperti yang mungkin Anda tebak dari namanya, yang mengandung `4_H`), model ini memiliki 4 tensor. Tensor terakhir sama dengan `sequence_output`.\n",
    "\n",
    "Sekarang kita akan mendefinisikan model klasifikasi end-to-end. Kita akan menggunakan *definisi model fungsional*, di mana kita mendefinisikan input model, lalu memberikan serangkaian ekspresi untuk menghitung keluarannya. Kita juga akan membuat bobot model BERT tidak dapat dilatih, dan hanya melatih pengklasifikasi akhirnya:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meskipun jumlah parameter yang dapat dilatih sedikit, prosesnya cukup lambat karena ekstraktor fitur BERT sangat berat secara komputasi. Tampaknya kita tidak berhasil mencapai akurasi yang memadai, baik karena kurangnya pelatihan maupun kurangnya parameter model.\n",
    "\n",
    "Mari kita coba membuka kunci bobot BERT dan melatihnya juga. Ini membutuhkan tingkat pembelajaran yang sangat kecil, serta strategi pelatihan yang lebih hati-hati dengan **warmup**, menggunakan pengoptimal **AdamW**. Kita akan menggunakan paket `tf-models-official` untuk membuat pengoptimal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti yang Anda lihat, pelatihan berjalan cukup lambat - tetapi Anda mungkin ingin bereksperimen dan melatih model selama beberapa epoch (5-10) dan melihat apakah Anda bisa mendapatkan hasil terbaik dibandingkan dengan pendekatan yang telah kita gunakan sebelumnya.\n",
    "\n",
    "## Perpustakaan Huggingface Transformers\n",
    "\n",
    "Cara lain yang sangat umum (dan sedikit lebih sederhana) untuk menggunakan model Transformer adalah [paket HuggingFace](https://github.com/huggingface/), yang menyediakan blok bangunan sederhana untuk berbagai tugas NLP. Paket ini tersedia baik untuk Tensorflow maupun PyTorch, kerangka kerja jaringan saraf lain yang sangat populer.\n",
    "\n",
    "> **Catatan**: Jika Anda tidak tertarik untuk melihat bagaimana perpustakaan Transformers bekerja - Anda dapat langsung melompat ke akhir notebook ini, karena Anda tidak akan melihat sesuatu yang secara substansial berbeda dari apa yang telah kita lakukan sebelumnya. Kita akan mengulangi langkah-langkah yang sama untuk melatih model BERT menggunakan perpustakaan yang berbeda dan model yang jauh lebih besar. Oleh karena itu, proses ini melibatkan pelatihan yang cukup lama, jadi Anda mungkin hanya ingin melihat kode saja.\n",
    "\n",
    "Mari kita lihat bagaimana masalah kita dapat diselesaikan menggunakan [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hal pertama yang perlu kita lakukan adalah memilih model yang akan kita gunakan. Selain beberapa model bawaan, Huggingface memiliki [repositori model online](https://huggingface.co/models), di mana Anda dapat menemukan banyak model yang telah dilatih sebelumnya oleh komunitas. Semua model tersebut dapat dimuat dan digunakan hanya dengan menyediakan nama model. Semua file biner yang diperlukan untuk model tersebut akan otomatis diunduh.\n",
    "\n",
    "Pada waktu tertentu, Anda mungkin perlu memuat model Anda sendiri. Dalam kasus ini, Anda dapat menentukan direktori yang berisi semua file terkait, termasuk parameter untuk tokenizer, file `config.json` dengan parameter model, bobot biner, dan sebagainya.\n",
    "\n",
    "Dari nama model, kita dapat menginisialisasi baik model maupun tokenizer. Mari kita mulai dengan tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objek `tokenizer` berisi fungsi `encode` yang dapat langsung digunakan untuk mengenkripsi teks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga dapat menggunakan tokenizer untuk mengenkripsi sebuah urutan dengan cara yang sesuai untuk diteruskan ke model, yaitu termasuk bidang `token_ids`, `input_mask`, dll. Kita juga dapat menentukan bahwa kita ingin tensor Tensorflow dengan memberikan argumen `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam kasus ini, kita akan menggunakan model BERT yang telah dilatih sebelumnya bernama `bert-base-uncased`. *Uncased* menunjukkan bahwa model ini tidak peka terhadap huruf besar dan kecil.\n",
    "\n",
    "Saat melatih model, kita perlu memberikan urutan token yang telah di-tokenisasi sebagai input, dan oleh karena itu kita akan merancang alur pemrosesan data. Karena `tokenizer.encode` adalah fungsi Python, kita akan menggunakan pendekatan yang sama seperti pada unit sebelumnya dengan memanggilnya menggunakan `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita dapat memuat model sebenarnya menggunakan paket `BertForSequenceClassification`. Ini memastikan bahwa model kita sudah memiliki arsitektur yang diperlukan untuk klasifikasi, termasuk classifier akhir. Anda akan melihat pesan peringatan yang menyatakan bahwa bobot classifier akhir belum diinisialisasi, dan model memerlukan pra-pelatihan - itu sepenuhnya tidak masalah, karena itulah yang akan kita lakukan!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti yang dapat Anda lihat dari `summary()`, model ini mengandung hampir 110 juta parameter! Kemungkinan, jika kita ingin tugas klasifikasi sederhana pada dataset yang relatif kecil, kita tidak ingin melatih lapisan dasar BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita siap untuk memulai pelatihan!\n",
    "\n",
    "> **Catatan**: Melatih model BERT skala penuh bisa sangat memakan waktu! Oleh karena itu, kita hanya akan melatihnya untuk 32 batch pertama. Ini hanya untuk menunjukkan bagaimana pelatihan model diatur. Jika Anda tertarik mencoba pelatihan skala penuh - cukup hapus parameter `steps_per_epoch` dan `validation_steps`, dan bersiaplah untuk menunggu!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika Anda meningkatkan jumlah iterasi dan menunggu cukup lama, serta melatih selama beberapa epoch, Anda dapat mengharapkan bahwa klasifikasi BERT memberikan akurasi terbaik! Hal ini karena BERT sudah cukup memahami struktur bahasa dengan baik, dan kita hanya perlu menyempurnakan pengklasifikasi akhirnya. Namun, karena BERT adalah model yang besar, seluruh proses pelatihan memakan waktu lama dan membutuhkan daya komputasi yang signifikan! (GPU, dan sebaiknya lebih dari satu).\n",
    "\n",
    "> **Catatan:** Dalam contoh kita, kita menggunakan salah satu model BERT pra-latih terkecil. Ada model yang lebih besar yang kemungkinan memberikan hasil yang lebih baik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poin Penting\n",
    "\n",
    "Dalam unit ini, kita telah mempelajari arsitektur model terbaru yang berbasis **transformers**. Kita telah menerapkannya untuk tugas klasifikasi teks, namun model BERT juga dapat digunakan untuk ekstraksi entitas, menjawab pertanyaan, dan tugas NLP lainnya.\n",
    "\n",
    "Model transformer mewakili teknologi terkini dalam NLP, dan dalam sebagian besar kasus, ini seharusnya menjadi solusi pertama yang Anda coba saat mengimplementasikan solusi NLP kustom. Namun, memahami prinsip dasar dari jaringan saraf berulang yang dibahas dalam modul ini sangatlah penting jika Anda ingin membangun model saraf yang lebih canggih.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diingat bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-29T16:01:12+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "id"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}