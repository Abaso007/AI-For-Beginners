<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-28T15:59:19+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "da"
}
-->
# Rekurrente Neurale Netv√¶rk

## [Quiz f√∏r forel√¶sning](https://ff-quizzes.netlify.app/en/ai/quiz/31)

I de tidligere afsnit har vi brugt rige semantiske repr√¶sentationer af tekst og en simpel line√¶r klassifikator oven p√• embeddings. Denne arkitektur fanger den samlede betydning af ordene i en s√¶tning, men tager ikke h√∏jde for **r√¶kkef√∏lgen** af ordene, fordi aggregeringsoperationen oven p√• embeddings fjerner denne information fra den oprindelige tekst. Da disse modeller ikke kan modellere ordens r√¶kkef√∏lge, kan de ikke l√∏se mere komplekse eller tvetydige opgaver som tekstgenerering eller besvarelse af sp√∏rgsm√•l.

For at fange betydningen af en tekstsekvens skal vi bruge en anden neural netv√¶rksarkitektur, som kaldes et **rekurrent neuralt netv√¶rk**, eller RNN. I et RNN sender vi vores s√¶tning gennem netv√¶rket √©t symbol ad gangen, og netv√¶rket producerer en **tilstand**, som vi derefter sender tilbage til netv√¶rket sammen med det n√¶ste symbol.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.da.png)

> Billede af forfatteren

Givet inputsekvensen af tokens X<sub>0</sub>,...,X<sub>n</sub>, skaber RNN en sekvens af neurale netv√¶rksblokke og tr√¶ner denne sekvens ende-til-ende ved hj√¶lp af backpropagation. Hver netv√¶rksblok tager et par (X<sub>i</sub>,S<sub>i</sub>) som input og producerer S<sub>i+1</sub> som resultat. Den endelige tilstand S<sub>n</sub> (eller output Y<sub>n</sub>) sendes til en line√¶r klassifikator for at producere resultatet. Alle netv√¶rksblokke deler de samme v√¶gte og tr√¶nes ende-til-ende ved hj√¶lp af √©n backpropagation-pass.

Fordi tilstandsvektorerne S<sub>0</sub>,...,S<sub>n</sub> sendes gennem netv√¶rket, er det i stand til at l√¶re de sekventielle afh√¶ngigheder mellem ordene. For eksempel, n√•r ordet *ikke* optr√¶der et sted i sekvensen, kan det l√¶re at negere visse elementer i tilstandsvektoren, hvilket resulterer i negation.

> ‚úÖ Da v√¶gtene for alle RNN-blokke p√• billedet ovenfor er delt, kan det samme billede repr√¶senteres som √©n blok (til h√∏jre) med en rekurrent feedback-loop, der sender netv√¶rkets outputtilstand tilbage til input.

## Anatomi af en RNN-celle

Lad os se, hvordan en simpel RNN-celle er organiseret. Den accepterer den tidligere tilstand S<sub>i-1</sub> og det aktuelle symbol X<sub>i</sub> som input og skal producere outputtilstanden S<sub>i</sub> (og nogle gange er vi ogs√• interesserede i et andet output Y<sub>i</sub>, som i tilf√¶ldet med generative netv√¶rk).

En simpel RNN-celle har to v√¶gtmatricer indeni: √©n transformerer et inputsymbol (lad os kalde den W), og en anden transformerer en inputtilstand (H). I dette tilf√¶lde beregnes netv√¶rkets output som œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b), hvor œÉ er aktiveringsfunktionen, og b er en yderligere bias.

<img alt="RNN Celle Anatomi" src="images/rnn-anatomy.png" width="50%"/>

> Billede af forfatteren

I mange tilf√¶lde sendes inputtokens gennem embedding-laget, f√∏r de kommer ind i RNN, for at reducere dimensionaliteten. I dette tilf√¶lde, hvis dimensionen af inputvektorerne er *emb_size*, og tilstandsvektoren er *hid_size* - er st√∏rrelsen af W *emb_size*√ó*hid_size*, og st√∏rrelsen af H er *hid_size*√ó*hid_size*.

## Long Short Term Memory (LSTM)

Et af de st√∏rste problemer med klassiske RNN'er er det s√•kaldte **forsvindende gradient-problem**. Fordi RNN'er tr√¶nes ende-til-ende i √©n backpropagation-pass, har de sv√¶rt ved at propagere fejl til de f√∏rste lag i netv√¶rket, og derfor kan netv√¶rket ikke l√¶re relationer mellem fjerne tokens. En af m√•derne at undg√• dette problem p√• er at introducere **eksplicit tilstandsh√•ndtering** ved hj√¶lp af s√•kaldte **gates**. Der er to velkendte arkitekturer af denne type: **Long Short Term Memory** (LSTM) og **Gated Relay Unit** (GRU).

![Billede, der viser et eksempel p√• en long short term memory-celle](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Billedkilde TBD

LSTM-netv√¶rket er organiseret p√• en m√•de, der ligner RNN, men der er to tilstande, der sendes fra lag til lag: den faktiske tilstand C og den skjulte vektor H. Ved hver enhed bliver den skjulte vektor H<sub>i</sub> sammenk√¶det med input X<sub>i</sub>, og de styrer, hvad der sker med tilstanden C via **gates**. Hver gate er et neuralt netv√¶rk med sigmoid-aktivering (output i omr√•det [0,1]), som kan betragtes som en bitmaske, n√•r den multipliceres med tilstandsvektoren. Der er f√∏lgende gates (fra venstre til h√∏jre p√• billedet ovenfor):

* **Forget-gaten** tager en skjult vektor og bestemmer, hvilke komponenter af vektoren C vi skal glemme, og hvilke vi skal lade passere.
* **Input-gaten** tager noget information fra input- og skjulte vektorer og inds√¶tter det i tilstanden.
* **Output-gaten** transformerer tilstanden via et line√¶rt lag med *tanh*-aktivering og v√¶lger derefter nogle af dens komponenter ved hj√¶lp af en skjult vektor H<sub>i</sub> for at producere en ny tilstand C<sub>i+1</sub>.

Komponenterne i tilstanden C kan betragtes som nogle flag, der kan t√¶ndes og slukkes. For eksempel, n√•r vi st√∏der p√• navnet *Alice* i sekvensen, kan vi antage, at det refererer til en kvindelig karakter, og h√¶ve flaget i tilstanden, der angiver, at vi har et kvindeligt substantiv i s√¶tningen. N√•r vi senere st√∏der p√• s√¶tningen *og Tom*, vil vi h√¶ve flaget, der angiver, at vi har et flertalssubstantiv. Ved at manipulere tilstanden kan vi s√•ledes holde styr p√• de grammatiske egenskaber ved s√¶tningsdele.

> ‚úÖ En fremragende ressource til at forst√• LSTM's interne funktioner er denne fantastiske artikel [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) af Christopher Olah.

## Bidirektionelle og flerlags RNN'er

Vi har diskuteret rekurrente netv√¶rk, der opererer i √©n retning, fra begyndelsen af en sekvens til slutningen. Det virker naturligt, fordi det minder om den m√•de, vi l√¶ser og lytter til tale p√•. Men da vi i mange praktiske tilf√¶lde har tilf√¶ldig adgang til inputsekvensen, kan det give mening at k√∏re rekurrent beregning i begge retninger. S√•danne netv√¶rk kaldes **bidirektionelle** RNN'er. N√•r vi arbejder med et bidirektionelt netv√¶rk, har vi brug for to skjulte tilstandsvektorer, √©n for hver retning.

Et rekurrent netv√¶rk, enten √©n-retnings- eller bidirektionelt, fanger visse m√∏nstre inden for en sekvens og kan gemme dem i en tilstandsvektor eller sende dem til output. Ligesom med konvolutionelle netv√¶rk kan vi bygge et andet rekurrent lag oven p√• det f√∏rste for at fange h√∏jere niveau m√∏nstre og bygge videre p√• lav-niveau m√∏nstre, der er udtrukket af det f√∏rste lag. Dette f√∏rer os til begrebet et **flerlags RNN**, som best√•r af to eller flere rekurrente netv√¶rk, hvor outputtet fra det forrige lag sendes til det n√¶ste lag som input.

![Billede, der viser et flerlags long-short-term-memory-RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.da.jpg)

*Billede fra [dette vidunderlige indl√¶g](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) af Fernando L√≥pez*

## ‚úçÔ∏è √òvelser: Embeddings

Forts√¶t din l√¶ring i f√∏lgende notebooks:

* [RNN'er med PyTorch](RNNPyTorch.ipynb)
* [RNN'er med TensorFlow](RNNTF.ipynb)

## Konklusion

I denne enhed har vi set, at RNN'er kan bruges til sekvensklassifikation, men de kan faktisk h√•ndtere mange flere opgaver, s√•som tekstgenerering, maskinovers√¶ttelse og mere. Vi vil overveje disse opgaver i den n√¶ste enhed.

## üöÄ Udfordring

L√¶s noget litteratur om LSTM'er og overvej deres anvendelser:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz efter forel√¶sning](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Gennemgang & Selvstudie

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) af Christopher Olah.

## [Opgave: Notebooks](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.