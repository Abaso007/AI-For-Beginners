<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-28T15:52:12+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "da"
}
-->
# Generative netv√¶rk

## [Quiz f√∏r forel√¶sning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Recurrent Neural Networks (RNNs) og deres gatede cellevarianter s√•som Long Short Term Memory Cells (LSTMs) og Gated Recurrent Units (GRUs) giver en mekanisme til sproglig modellering, da de kan l√¶re ords r√¶kkef√∏lge og give forudsigelser for det n√¶ste ord i en sekvens. Dette g√∏r det muligt at bruge RNNs til **generative opgaver**, s√•som almindelig tekstgenerering, maskinovers√¶ttelse og endda billedbeskrivelser.

> ‚úÖ T√¶nk over alle de gange, du har haft gavn af generative opgaver, s√•som tekstfuldf√∏relse, mens du skriver. Unders√∏g dine yndlingsapplikationer for at se, om de har brugt RNNs.

I RNN-arkitekturen, som vi diskuterede i den forrige enhed, producerede hver RNN-enhed den n√¶ste skjulte tilstand som output. Men vi kan ogs√• tilf√∏je et andet output til hver rekurrent enhed, hvilket giver os mulighed for at generere en **sekvens** (som er lige s√• lang som den oprindelige sekvens). Desuden kan vi bruge RNN-enheder, der ikke modtager input ved hvert trin, men blot tager en initial tilstandsvektor og derefter producerer en sekvens af outputs.

Dette muligg√∏r forskellige neurale arkitekturer, som vist p√• billedet nedenfor:

![Billede, der viser almindelige m√∏nstre for rekurrente neurale netv√¶rk.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.da.jpg)

> Billede fra blogindl√¶gget [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) af [Andrej Karpaty](http://karpathy.github.io/)

* **One-to-one** er et traditionelt neuralt netv√¶rk med √©n input og √©t output
* **One-to-many** er en generativ arkitektur, der accepterer √©n inputv√¶rdi og genererer en sekvens af outputv√¶rdier. For eksempel, hvis vi vil tr√¶ne et **billedbeskrivelsesnetv√¶rk**, der kan producere en tekstbeskrivelse af et billede, kan vi tage et billede som input, passere det gennem en CNN for at opn√• dets skjulte tilstand og derefter lade en rekurrent k√¶de generere beskrivelsen ord for ord.
* **Many-to-one** svarer til de RNN-arkitekturer, vi beskrev i den forrige enhed, s√•som tekstklassifikation.
* **Many-to-many**, eller **sequence-to-sequence**, svarer til opgaver s√•som **maskinovers√¶ttelse**, hvor vi f√∏rst har en RNN, der samler al information fra inputsekvensen i den skjulte tilstand, og en anden RNN-k√¶de, der udfolder denne tilstand til outputsekvensen.

I denne enhed vil vi fokusere p√• simple generative modeller, der hj√¶lper os med at generere tekst. For enkelhedens skyld vil vi bruge tokenisering p√• tegnniveau.

Vi vil tr√¶ne denne RNN til at generere tekst trin for trin. Ved hvert trin tager vi en sekvens af tegn med l√¶ngden `nchars` og beder netv√¶rket om at generere det n√¶ste outputtegn for hvert inputtegn:

![Billede, der viser et eksempel p√• RNN-generering af ordet 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.da.png)

N√•r vi genererer tekst (under inferens), starter vi med en **prompt**, som sendes gennem RNN-celler for at generere dens mellemliggende tilstand, og derefter starter genereringen fra denne tilstand. Vi genererer √©t tegn ad gangen og sender tilstanden og det genererede tegn til en anden RNN-celle for at generere det n√¶ste, indtil vi har genereret nok tegn.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Billede af forfatteren

## ‚úçÔ∏è √òvelser: Generative netv√¶rk

Forts√¶t din l√¶ring i f√∏lgende notebooks:

* [Generative Networks med PyTorch](GenerativePyTorch.ipynb)
* [Generative Networks med TensorFlow](GenerativeTF.ipynb)

## Bl√∏d tekstgenerering og temperatur

Outputtet fra hver RNN-celle er en sandsynlighedsfordeling af tegn. Hvis vi altid v√¶lger tegnet med den h√∏jeste sandsynlighed som det n√¶ste tegn i den genererede tekst, kan teksten ofte "cykle" mellem de samme tegnsekvenser igen og igen, som i dette eksempel:

```
today of the second the company and a second the company ...
```

Men hvis vi ser p√• sandsynlighedsfordelingen for det n√¶ste tegn, kan det v√¶re, at forskellen mellem de h√∏jeste sandsynligheder ikke er stor, f.eks. kan √©t tegn have sandsynligheden 0.2, og et andet 0.19 osv. For eksempel, n√•r vi leder efter det n√¶ste tegn i sekvensen '*play*', kan det n√¶ste tegn lige s√• godt v√¶re enten et mellemrum eller **e** (som i ordet *player*).

Dette f√∏rer os til konklusionen, at det ikke altid er "retf√¶rdigt" at v√¶lge tegnet med den h√∏jeste sandsynlighed, fordi det n√¶sth√∏jeste stadig kan f√∏re til meningsfuld tekst. Det er mere fornuftigt at **sample** tegn fra sandsynlighedsfordelingen givet af netv√¶rkets output. Vi kan ogs√• bruge en parameter, **temperatur**, der kan udj√¶vne sandsynlighedsfordelingen, hvis vi √∏nsker at tilf√∏je mere tilf√¶ldighed, eller g√∏re den stejlere, hvis vi vil holde os t√¶ttere til de tegn med h√∏jeste sandsynlighed.

Unders√∏g, hvordan denne bl√∏de tekstgenerering er implementeret i de notebooks, der er linket ovenfor.

## Konklusion

Selvom tekstgenerering kan v√¶re nyttig i sig selv, kommer de st√∏rste fordele fra evnen til at generere tekst ved hj√¶lp af RNNs fra en initial feature-vektor. For eksempel bruges tekstgenerering som en del af maskinovers√¶ttelse (sequence-to-sequence, i dette tilf√¶lde bruges tilstandsvektoren fra *encoder* til at generere eller *decode* den oversatte besked) eller til at generere tekstbeskrivelser af et billede (i hvilket tilf√¶lde feature-vektoren ville komme fra CNN-ekstraktoren).

## üöÄ Udfordring

Tag nogle lektioner p√• Microsoft Learn om dette emne

* Tekstgenerering med [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz efter forel√¶sning](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Gennemgang & Selvstudie

Her er nogle artikler til at udvide din viden

* Forskellige tilgange til tekstgenerering med Markov Chain, LSTM og GPT-2: [blogindl√¶g](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Eksempel p√• tekstgenerering i [Keras dokumentation](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Opgave](lab/README.md)

Vi har set, hvordan man genererer tekst tegn for tegn. I laboratoriet vil du udforske tekstgenerering p√• ordniveau.

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• at sikre n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.