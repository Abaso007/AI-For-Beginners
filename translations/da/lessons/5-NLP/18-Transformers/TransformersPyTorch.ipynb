{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opmærksomhedsmekanismer og transformere\n",
    "\n",
    "En stor ulempe ved rekurrente netværk er, at alle ord i en sekvens har samme indflydelse på resultatet. Dette fører til suboptimal ydeevne med standard LSTM encoder-decoder-modeller til sekvens-til-sekvens-opgaver, såsom navngiven entitetsgenkendelse og maskinoversættelse. I virkeligheden har specifikke ord i inputsekvensen ofte større indflydelse på sekventielle output end andre.\n",
    "\n",
    "Overvej en sekvens-til-sekvens-model, såsom maskinoversættelse. Den implementeres ved hjælp af to rekurrente netværk, hvor det ene netværk (**encoder**) komprimerer inputsekvensen til en skjult tilstand, og det andet, **decoder**, udfolder denne skjulte tilstand til det oversatte resultat. Problemet med denne tilgang er, at netværkets endelige tilstand har svært ved at huske begyndelsen af en sætning, hvilket resulterer i dårlig modelkvalitet for lange sætninger.\n",
    "\n",
    "**Opmærksomhedsmekanismer** giver en metode til at vægte den kontekstuelle indflydelse af hver inputvektor på hver outputforudsigelse i RNN. Dette implementeres ved at skabe genveje mellem de mellemliggende tilstande i input-RNN og output-RNN. På denne måde, når vi genererer outputsymbolet $y_t$, tager vi højde for alle skjulte inputtilstande $h_i$ med forskellige vægtkoefficienter $\\alpha_{t,i}$. \n",
    "\n",
    "![Billede, der viser en encoder/decoder-model med et additivt opmærksomhedslag](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.da.png)\n",
    "*Encoder-decoder-modellen med additiv opmærksomhedsmekanisme i [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citeret fra [denne blogpost](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Opmærksomhedsmatricen $\\{\\alpha_{i,j}\\}$ repræsenterer graden, hvormed visse inputord spiller en rolle i genereringen af et givet ord i outputsekvensen. Nedenfor er et eksempel på en sådan matrix:\n",
    "\n",
    "![Billede, der viser en prøvejustering fundet af RNNsearch-50, taget fra Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.da.png)\n",
    "\n",
    "*Figur taget fra [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Opmærksomhedsmekanismer er ansvarlige for meget af den nuværende eller næsten nuværende state-of-the-art inden for naturlig sprogbehandling. Tilføjelse af opmærksomhed øger dog antallet af modelparametre betydeligt, hvilket førte til skaleringsproblemer med RNN'er. En vigtig begrænsning ved at skalere RNN'er er, at modellernes rekurrente natur gør det udfordrende at batchbehandle og parallelisere træning. I en RNN skal hvert element i en sekvens behandles i rækkefølge, hvilket betyder, at det ikke let kan paralleliseres.\n",
    "\n",
    "Adoptionen af opmærksomhedsmekanismer kombineret med denne begrænsning førte til skabelsen af de nuværende state-of-the-art Transformer-modeller, som vi kender og bruger i dag, fra BERT til OpenGPT3.\n",
    "\n",
    "## Transformer-modeller\n",
    "\n",
    "I stedet for at videregive konteksten af hver tidligere forudsigelse til det næste evalueringsskridt bruger **transformer-modeller** **positionelle kodninger** og opmærksomhed til at fange konteksten af et givet input inden for et angivet tekstvindue. Billedet nedenfor viser, hvordan positionelle kodninger med opmærksomhed kan fange kontekst inden for et givet vindue.\n",
    "\n",
    "![Animeret GIF, der viser, hvordan evalueringer udføres i transformer-modeller.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Da hver inputposition kortlægges uafhængigt til hver outputposition, kan transformere parallelisere bedre end RNN'er, hvilket muliggør meget større og mere udtryksfulde sprogmodeller. Hver opmærksomhedshoved kan bruges til at lære forskellige relationer mellem ord, hvilket forbedrer opgaver inden for naturlig sprogbehandling.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) er et meget stort flerlagstransformernetværk med 12 lag for *BERT-base* og 24 for *BERT-large*. Modellen fortrænes først på en stor tekstkorpus (Wikipedia + bøger) ved hjælp af usuperviseret træning (forudsige maskerede ord i en sætning). Under fortræningen absorberer modellen et betydeligt niveau af sprogforståelse, som derefter kan udnyttes med andre datasæt ved hjælp af finjustering. Denne proces kaldes **transfer learning**. \n",
    "\n",
    "![Billede fra http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.da.png)\n",
    "\n",
    "Der findes mange variationer af Transformer-arkitekturer, herunder BERT, DistilBERT, BigBird, OpenGPT3 og flere, som kan finjusteres. [HuggingFace-pakken](https://github.com/huggingface/) giver et bibliotek til træning af mange af disse arkitekturer med PyTorch. \n",
    "\n",
    "## Brug af BERT til tekstklassifikation\n",
    "\n",
    "Lad os se, hvordan vi kan bruge en fortrænet BERT-model til at løse vores traditionelle opgave: sekvensklassifikation. Vi vil klassificere vores oprindelige AG News-datasæt.\n",
    "\n",
    "Først skal vi indlæse HuggingFace-biblioteket og vores datasæt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da vi skal bruge en forudtrænet BERT-model, skal vi anvende en specifik tokenizer. Først vil vi indlæse en tokenizer, der er tilknyttet den forudtrænede BERT-model.\n",
    "\n",
    "HuggingFace-biblioteket indeholder et arkiv af forudtrænede modeller, som du kan bruge ved blot at angive deres navne som argumenter til `from_pretrained`-funktionerne. Alle nødvendige binære filer til modellen vil automatisk blive downloadet.\n",
    "\n",
    "Dog vil du nogle gange have behov for at indlæse dine egne modeller. I sådanne tilfælde kan du angive den mappe, der indeholder alle relevante filer, herunder parametre til tokenizer, `config.json`-filen med modelparametre, binære vægte osv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer`-objektet indeholder `encode`-funktionen, som kan bruges direkte til at kode tekst:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lad os derefter oprette iteratorer, som vi vil bruge under træningen til at få adgang til dataene. Fordi BERT bruger sin egen kodningsfunktion, skal vi definere en padding-funktion, der ligner `padify`, som vi har defineret tidligere:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I vores tilfælde vil vi bruge den forudtrænede BERT-model kaldet `bert-base-uncased`. Lad os indlæse modellen ved hjælp af pakken `BertForSequenceClassfication`. Dette sikrer, at vores model allerede har den nødvendige arkitektur til klassifikation, inklusive den endelige klassifikator. Du vil se en advarselsmeddelelse, der angiver, at vægtene for den endelige klassifikator ikke er initialiseret, og modellen vil kræve forudtræning - det er helt i orden, fordi det er præcis, hvad vi er ved at gøre!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu er vi klar til at begynde træningen! Fordi BERT allerede er forudtrænet, vil vi starte med en ret lille læringsrate for ikke at ødelægge de oprindelige vægte.\n",
    "\n",
    "Alt det hårde arbejde udføres af `BertForSequenceClassification`-modellen. Når vi kalder modellen på træningsdataene, returnerer den både tab og netværksoutput for input-minibatch. Vi bruger tabet til parameteroptimering (`loss.backward()` udfører den baglæns passering), og `out` til at beregne træningsnøjagtighed ved at sammenligne de opnåede labels `labs` (beregnet ved hjælp af `argmax`) med de forventede `labels`.\n",
    "\n",
    "For at kontrollere processen akkumulerer vi tab og nøjagtighed over flere iterationer og udskriver dem hver `report_freq` træningscyklus.\n",
    "\n",
    "Denne træning vil sandsynligvis tage ret lang tid, så vi begrænser antallet af iterationer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan se (især hvis du øger antallet af iterationer og venter længe nok), at BERT-klassificering giver os ret god nøjagtighed! Det skyldes, at BERT allerede forstår sprogets struktur ganske godt, og vi behøver kun at finjustere den endelige klassifikator. Dog, fordi BERT er en stor model, tager hele træningsprocessen lang tid og kræver betydelige computerressourcer! (GPU, og helst mere end én).\n",
    "\n",
    "> **Note:** I vores eksempel har vi brugt en af de mindste forudtrænede BERT-modeller. Der findes større modeller, som sandsynligvis vil give bedre resultater.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluering af modellens ydeevne\n",
    "\n",
    "Nu kan vi evaluere vores models ydeevne på testdatasættet. Evalueringssløjfen ligner træningssløjfen, men vi må ikke glemme at skifte modellen til evalueringsmodus ved at kalde `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vigtig pointe\n",
    "\n",
    "I denne enhed har vi set, hvor nemt det er at tage en forudtrænet sprogmodel fra **transformers**-biblioteket og tilpasse den til vores tekstklassificeringsopgave. På samme måde kan BERT-modeller bruges til enhedsekstraktion, spørgsmål-svar og andre NLP-opgaver.\n",
    "\n",
    "Transformer-modeller repræsenterer den nuværende state-of-the-art inden for NLP, og i de fleste tilfælde bør det være den første løsning, du eksperimenterer med, når du implementerer skræddersyede NLP-løsninger. Dog er det ekstremt vigtigt at forstå de grundlæggende principper bag rekurrente neurale netværk, som blev diskuteret i dette modul, hvis du ønsker at bygge avancerede neurale modeller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi er ikke ansvarlige for eventuelle misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T17:33:13+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}