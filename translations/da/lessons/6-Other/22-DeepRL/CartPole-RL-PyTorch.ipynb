{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Træning af RL til at balancere Cartpole\n",
    "\n",
    "Denne notebook er en del af [AI for Beginners Curriculum](http://aka.ms/ai-beginners). Den er inspireret af [den officielle PyTorch-tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) og [denne Cartpole PyTorch-implementering](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "I dette eksempel vil vi bruge RL til at træne en model til at balancere en stang på en vogn, der kan bevæge sig til venstre og højre på en horisontal skala. Vi vil bruge [OpenAI Gym](https://www.gymlibrary.ml/) miljøet til at simulere stangen.\n",
    "\n",
    "> **Note**: Du kan køre koden fra denne lektion lokalt (f.eks. fra Visual Studio Code), hvor simuleringen vil åbne i et nyt vindue. Når du kører koden online, kan det være nødvendigt at lave nogle justeringer i koden, som beskrevet [her](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Vi starter med at sikre, at Gym er installeret:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lad os nu oprette CartPole-miljøet og se, hvordan vi kan arbejde med det. Et miljø har følgende egenskaber:\n",
    "\n",
    "* **Handlingsrum** er det sæt af mulige handlinger, vi kan udføre ved hvert trin i simuleringen  \n",
    "* **Observationsrum** er det rum af observationer, som vi kan foretage  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lad os se, hvordan simuleringen fungerer. Den følgende løkke kører simuleringen, indtil `env.step` ikke returnerer afslutningsflaget `done`. Vi vil tilfældigt vælge handlinger ved hjælp af `env.action_space.sample()`, hvilket betyder, at eksperimentet sandsynligvis vil fejle meget hurtigt (CartPole-miljøet afsluttes, når hastigheden af CartPole, dens position eller vinkel er uden for visse grænser).\n",
    "\n",
    "> Simuleringen åbnes i et nyt vindue. Du kan køre koden flere gange og se, hvordan den opfører sig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan bemærke, at observationerne indeholder 4 tal. Disse er:\n",
    "- Vognens position\n",
    "- Vognens hastighed\n",
    "- Stangens vinkel\n",
    "- Stangens rotationshastighed\n",
    "\n",
    "`rew` er den belønning, vi modtager ved hvert trin. Du kan se, at i CartPole-miljøet får du 1 point for hvert simuleringsskridt, og målet er at maksimere den samlede belønning, dvs. den tid, CartPole kan balancere uden at falde.\n",
    "\n",
    "Under forstærkningslæring er vores mål at træne en **politik** $\\pi$, der for hver tilstand $s$ fortæller os, hvilken handling $a$ vi skal udføre, så grundlæggende $a = \\pi(s)$.\n",
    "\n",
    "Hvis du ønsker en probabilistisk løsning, kan du tænke på politikken som noget, der returnerer et sæt sandsynligheder for hver handling, dvs. $\\pi(a|s)$ ville betyde sandsynligheden for, at vi skal udføre handling $a$ i tilstand $s$.\n",
    "\n",
    "## Policy Gradient-metoden\n",
    "\n",
    "I den simpleste RL-algoritme, kaldet **Policy Gradient**, vil vi træne et neuralt netværk til at forudsige den næste handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi vil træne netværket ved at udføre mange eksperimenter og opdatere vores netværk efter hver kørsel. Lad os definere en funktion, der vil udføre eksperimentet og returnere resultaterne (den såkaldte **trace**) - alle tilstande, handlinger (og deres anbefalede sandsynligheder) og belønninger:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan køre en episode med et utrænet netværk og observere, at den samlede belønning (AKA længden af episoden) er meget lav:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En af de vanskelige aspekter ved policy gradient-algoritmen er at bruge **diskonterede belønninger**. Ideen er, at vi beregner vektoren af samlede belønninger ved hvert trin i spillet, og under denne proces diskonterer vi de tidlige belønninger ved hjælp af en koefficient $gamma$. Vi normaliserer også den resulterende vektor, fordi vi vil bruge den som vægt til at påvirke vores træning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu skal vi i gang med den egentlige træning! Vi vil køre 300 episoder, og i hver episode vil vi gøre følgende:\n",
    "\n",
    "1. Kør eksperimentet og indsamle sporingsdata.\n",
    "2. Beregn forskellen (`gradients`) mellem de handlinger, der blev udført, og de forudsagte sandsynligheder. Jo mindre forskellen er, desto mere sikre er vi på, at vi har taget den rigtige handling.\n",
    "3. Beregn diskonterede belønninger og multiplicér gradients med de diskonterede belønninger - det sikrer, at trin med højere belønninger har større indflydelse på det endelige resultat end trin med lavere belønninger.\n",
    "4. Forventede målhandlinger for vores neurale netværk vil delvist blive taget fra de forudsagte sandsynligheder under kørslen og delvist fra de beregnede gradients. Vi vil bruge parameteren `alpha` til at bestemme, i hvilket omfang gradients og belønninger tages i betragtning - dette kaldes *læringsraten* for forstærkningsalgoritmen.\n",
    "5. Til sidst træner vi vores netværk på tilstande og forventede handlinger og gentager processen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lad os nu køre episoden med rendering for at se resultatet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forhåbentlig kan du se, at stangen nu kan balancere ret godt!\n",
    "\n",
    "## Actor-Critic Model\n",
    "\n",
    "Actor-Critic-modellen er en videreudvikling af policy gradients, hvor vi bygger et neuralt netværk til at lære både politikken og de estimerede belønninger. Netværket vil have to output (eller du kan se det som to separate netværk):\n",
    "* **Actor** vil anbefale den handling, der skal udføres, ved at give os sandsynlighedsfordelingen for tilstanden, som i policy gradient-modellen.\n",
    "* **Critic** vil estimere, hvad belønningen ville være fra disse handlinger. Den returnerer de samlede estimerede belønninger i fremtiden i den givne tilstand.\n",
    "\n",
    "Lad os definere en sådan model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ville være nødt til at ændre vores `discounted_rewards` og `run_episode` funktioner en smule:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu vil vi køre den primære træningssløjfe. Vi vil bruge en manuel netværkstræningsproces ved at beregne passende tabfunktioner og opdatere netværksparametre:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vigtig pointe\n",
    "\n",
    "Vi har set to RL-algoritmer i denne demo: simpel policy gradient og den mere sofistikerede actor-critic. Du kan se, at disse algoritmer arbejder med abstrakte begreber som tilstand, handling og belønning - hvilket gør dem anvendelige i meget forskellige miljøer.\n",
    "\n",
    "Reinforcement learning giver os mulighed for at lære den bedste strategi til at løse problemet blot ved at kigge på den endelige belønning. Det faktum, at vi ikke behøver mærkede datasæt, gør det muligt for os at gentage simuleringer mange gange for at optimere vores modeller. Der er dog stadig mange udfordringer inden for RL, som du kan lære mere om, hvis du beslutter dig for at fokusere mere på dette interessante område inden for AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for eventuelle misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T16:13:52+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}