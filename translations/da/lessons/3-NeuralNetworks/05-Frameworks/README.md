<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-28T15:40:43+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "da"
}
-->
# Neurale Netv√¶rksrammer

Som vi allerede har l√¶rt, skal vi g√∏re to ting for effektivt at tr√¶ne neurale netv√¶rk:

* Arbejde med tensorer, f.eks. multiplicere, addere og beregne funktioner som sigmoid eller softmax
* Beregne gradienter af alle udtryk for at udf√∏re gradient descent-optimering

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Mens `numpy`-biblioteket kan h√•ndtere den f√∏rste del, har vi brug for en mekanisme til at beregne gradienter. I [vores rammev√¶rk](../04-OwnFramework/OwnFramework.ipynb), som vi udviklede i det forrige afsnit, var vi n√∏dt til manuelt at programmere alle afledte funktioner i `backward`-metoden, som udf√∏rer backpropagation. Ideelt set b√∏r et rammev√¶rk give os mulighed for at beregne gradienter af *ethvert udtryk*, vi kan definere.

En anden vigtig ting er at kunne udf√∏re beregninger p√• GPU eller andre specialiserede beregningsenheder, s√•som [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Tr√¶ning af dybe neurale netv√¶rk kr√¶ver *meget* beregning, og det er meget vigtigt at kunne parallelisere disse beregninger p√• GPU'er.

> ‚úÖ Udtrykket 'parallelisere' betyder at fordele beregningerne over flere enheder.

De to mest popul√¶re neurale rammev√¶rk i √∏jeblikket er: [TensorFlow](http://TensorFlow.org) og [PyTorch](https://pytorch.org/). Begge tilbyder et lavniveau-API til at arbejde med tensorer p√• b√•de CPU og GPU. Oven p√• lavniveau-API'en findes der ogs√• h√∏jere niveau-API'er, kaldet [Keras](https://keras.io/) og [PyTorch Lightning](https://pytorchlightning.ai/) henholdsvis.

Lavniveau-API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
H√∏jniveau-API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Lavniveau-API'er** i begge rammev√¶rk giver dig mulighed for at bygge s√•kaldte **beregningsgrafer**. Denne graf definerer, hvordan outputtet (normalt tab-funktionen) beregnes med givne inputparametre og kan sendes til beregning p√• GPU, hvis den er tilg√¶ngelig. Der findes funktioner til at differentiere denne beregningsgraf og beregne gradienter, som derefter kan bruges til at optimere modelparametre.

**H√∏jniveau-API'er** betragter stort set neurale netv√¶rk som en **sekvens af lag** og g√∏r konstruktionen af de fleste neurale netv√¶rk meget lettere. Tr√¶ning af modellen kr√¶ver normalt, at man forbereder dataene og derefter kalder en `fit`-funktion for at udf√∏re arbejdet.

H√∏jniveau-API'en giver dig mulighed for hurtigt at konstruere typiske neurale netv√¶rk uden at bekymre dig om mange detaljer. Samtidig giver lavniveau-API'en meget mere kontrol over tr√¶ningsprocessen og bruges derfor ofte i forskning, n√•r man arbejder med nye neurale netv√¶rksarkitekturer.

Det er ogs√• vigtigt at forst√•, at du kan bruge begge API'er sammen, f.eks. kan du udvikle din egen netv√¶rkslagsarkitektur ved hj√¶lp af lavniveau-API'en og derefter bruge den i et st√∏rre netv√¶rk, der er konstrueret og tr√¶net med h√∏jniveau-API'en. Eller du kan definere et netv√¶rk ved hj√¶lp af h√∏jniveau-API'en som en sekvens af lag og derefter bruge din egen lavniveau-tr√¶ningssl√∏jfe til at udf√∏re optimering. Begge API'er bruger de samme grundl√¶ggende underliggende koncepter og er designet til at fungere godt sammen.

## L√¶ring

I dette kursus tilbyder vi det meste af indholdet b√•de for PyTorch og TensorFlow. Du kan v√¶lge dit foretrukne rammev√¶rk og kun gennemg√• de tilsvarende notebooks. Hvis du ikke er sikker p√•, hvilket rammev√¶rk du skal v√¶lge, kan du l√¶se nogle diskussioner p√• internettet om **PyTorch vs. TensorFlow**. Du kan ogs√• kigge p√• begge rammev√¶rk for at f√• en bedre forst√•else.

Hvor det er muligt, vil vi bruge h√∏jniveau-API'er for enkelhedens skyld. Vi mener dog, at det er vigtigt at forst√•, hvordan neurale netv√¶rk fungerer fra bunden, s√• i begyndelsen starter vi med at arbejde med lavniveau-API'er og tensorer. Hvis du dog √∏nsker at komme hurtigt i gang og ikke vil bruge meget tid p√• at l√¶re disse detaljer, kan du springe dem over og g√• direkte til notebooks med h√∏jniveau-API'er.

## ‚úçÔ∏è √òvelser: Rammev√¶rk

Forts√¶t din l√¶ring i f√∏lgende notebooks:

Lavniveau-API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
H√∏jniveau-API | [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Efter at have mestret rammev√¶rkene, lad os genopfriske begrebet overfitting.

# Overfitting

Overfitting er et ekstremt vigtigt begreb inden for maskinl√¶ring, og det er meget vigtigt at forst√• det korrekt!

Overvej f√∏lgende problem med at tiln√¶rme 5 punkter (repr√¶senteret ved `x` p√• graferne nedenfor):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.da.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.da.jpg)
-------------------------|--------------------------
**Line√¶r model, 2 parametre** | **Ikke-line√¶r model, 7 parametre**
Tr√¶ningsfejl = 5,3 | Tr√¶ningsfejl = 0
Valideringsfejl = 5,1 | Valideringsfejl = 20

* Til venstre ser vi en god ret linje-tiln√¶rmelse. Fordi antallet af parametre er passende, forst√•r modellen punktfordelingen korrekt.
* Til h√∏jre er modellen for kraftfuld. Fordi vi kun har 5 punkter, og modellen har 7 parametre, kan den justere sig, s√• den passer gennem alle punkter, hvilket g√∏r tr√¶ningsfejlen til 0. Dette forhindrer dog modellen i at forst√• det korrekte m√∏nster bag dataene, og derfor er valideringsfejlen meget h√∏j.

Det er meget vigtigt at finde den rette balance mellem modellens kompleksitet (antal parametre) og antallet af tr√¶ningspr√∏ver.

## Hvorfor opst√•r overfitting

  * Ikke nok tr√¶ningsdata
  * For kraftfuld model
  * For meget st√∏j i inputdata

## Hvordan opdager man overfitting

Som du kan se p√• grafen ovenfor, kan overfitting opdages ved en meget lav tr√¶ningsfejl og en h√∏j valideringsfejl. Normalt vil vi under tr√¶ning se b√•de tr√¶nings- og valideringsfejl begynde at falde, og derefter kan valideringsfejlen p√• et tidspunkt stoppe med at falde og begynde at stige. Dette vil v√¶re et tegn p√• overfitting og en indikator for, at vi sandsynligvis b√∏r stoppe tr√¶ningen p√• dette tidspunkt (eller i det mindste tage et snapshot af modellen).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.da.png)

## Hvordan forhindrer man overfitting

Hvis du opdager, at overfitting opst√•r, kan du g√∏re en af f√∏lgende ting:

 * √òge m√¶ngden af tr√¶ningsdata
 * Mindske modellens kompleksitet
 * Bruge en [regulariseringsteknik](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), s√•som [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), som vi vil gennemg√• senere.

## Overfitting og Bias-Variance Tradeoff

Overfitting er faktisk et tilf√¶lde af et mere generelt problem inden for statistik kaldet [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Hvis vi overvejer de mulige fejlkilder i vores model, kan vi se to typer fejl:

* **Bias-fejl** skyldes, at vores algoritme ikke er i stand til korrekt at fange forholdet mellem tr√¶ningsdataene. Det kan skyldes, at vores model ikke er kraftfuld nok (**underfitting**).
* **Varians-fejl**, som skyldes, at modellen tiln√¶rmer st√∏j i inputdataene i stedet for meningsfulde sammenh√¶nge (**overfitting**).

Under tr√¶ning falder bias-fejlen (da vores model l√¶rer at tiln√¶rme dataene), og varians-fejlen stiger. Det er vigtigt at stoppe tr√¶ningen - enten manuelt (n√•r vi opdager overfitting) eller automatisk (ved at introducere regularisering) - for at forhindre overfitting.

## Konklusion

I denne lektion l√¶rte du om forskellene mellem de forskellige API'er for de to mest popul√¶re AI-rammev√¶rk, TensorFlow og PyTorch. Derudover l√¶rte du om et meget vigtigt emne, overfitting.

## üöÄ Udfordring

I de tilh√∏rende notebooks finder du 'opgaver' nederst; gennemg√• notebooks og fuldf√∏r opgaverne.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Gennemgang & Selvstudie

Unders√∏g f√∏lgende emner:

- TensorFlow
- PyTorch
- Overfitting

Sp√∏rg dig selv f√∏lgende sp√∏rgsm√•l:

- Hvad er forskellen mellem TensorFlow og PyTorch?
- Hvad er forskellen mellem overfitting og underfitting?

## [Opgave](lab/README.md)

I denne opgave skal du l√∏se to klassifikationsproblemer ved hj√¶lp af enkelt- og flerlags fuldt forbundne netv√¶rk med PyTorch eller TensorFlow.

* [Instruktioner](lab/README.md)
* [Notebook](lab/LabFrameworks.ipynb)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• at sikre n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.