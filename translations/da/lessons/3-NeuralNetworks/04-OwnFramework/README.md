<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-28T15:37:09+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "da"
}
-->
# Introduktion til Neurale NetvÃ¦rk. Multi-Layered Perceptron

I det forrige afsnit lÃ¦rte du om den simpleste model for neurale netvÃ¦rk - enlaget perceptron, en lineÃ¦r to-klasse klassifikationsmodel.

I dette afsnit vil vi udvide denne model til en mere fleksibel ramme, der giver os mulighed for at:

* udfÃ¸re **multi-klasse klassifikation** ud over to-klasse
* lÃ¸se **regressionsproblemer** ud over klassifikation
* adskille klasser, der ikke er lineÃ¦rt separable

Vi vil ogsÃ¥ udvikle vores egen modulÃ¦re ramme i Python, som giver os mulighed for at konstruere forskellige arkitekturer for neurale netvÃ¦rk.

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalisering af MaskinlÃ¦ring

Lad os starte med at formalisere problemet med maskinlÃ¦ring. Antag, at vi har et trÃ¦ningsdatasÃ¦t **X** med labels **Y**, og vi skal bygge en model *f*, der giver de mest prÃ¦cise forudsigelser. Kvaliteten af forudsigelserne mÃ¥les ved **tab-funktionen** â„’. FÃ¸lgende tab-funktioner bruges ofte:

* For regressionsproblemer, hvor vi skal forudsige et tal, kan vi bruge **absolut fejl** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| eller **kvadreret fejl** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* For klassifikation bruger vi **0-1 tab** (som i bund og grund er det samme som **modellens nÃ¸jagtighed**) eller **logistisk tab**.

For enlaget perceptron blev funktionen *f* defineret som en lineÃ¦r funktion *f(x)=wx+b* (her er *w* vÃ¦gtmatricen, *x* er vektoren af inputfunktioner, og *b* er bias-vektoren). For forskellige arkitekturer af neurale netvÃ¦rk kan denne funktion antage en mere kompleks form.

> I tilfÃ¦lde af klassifikation er det ofte Ã¸nskeligt at fÃ¥ sandsynligheder for de tilsvarende klasser som netvÃ¦rkets output. For at konvertere vilkÃ¥rlige tal til sandsynligheder (f.eks. for at normalisere output) bruger vi ofte **softmax**-funktionen Ïƒ, og funktionen *f* bliver *f(x)=Ïƒ(wx+b)*

I definitionen af *f* ovenfor kaldes *w* og *b* **parametre** Î¸=âŸ¨*w,b*âŸ©. Givet datasÃ¦ttet âŸ¨**X**,**Y**âŸ© kan vi beregne en samlet fejl pÃ¥ hele datasÃ¦ttet som en funktion af parametrene Î¸.

> âœ… **MÃ¥let med trÃ¦ning af neurale netvÃ¦rk er at minimere fejlen ved at variere parametrene Î¸**

## Gradient Descent Optimering

Der findes en velkendt metode til optimering af funktioner kaldet **gradient descent**. Ideen er, at vi kan beregne en afledt (i det multidimensionelle tilfÃ¦lde kaldet **gradient**) af tab-funktionen med hensyn til parametrene og variere parametrene pÃ¥ en mÃ¥de, sÃ¥ fejlen mindskes. Dette kan formaliseres som fÃ¸lger:

* Initialiser parametrene med nogle tilfÃ¦ldige vÃ¦rdier w<sup>(0)</sup>, b<sup>(0)</sup>
* Gentag fÃ¸lgende trin mange gange:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Under trÃ¦ning skal optimeringstrinnene beregnes med hensyn til hele datasÃ¦ttet (husk, at tab beregnes som en sum gennem alle trÃ¦ningsprÃ¸ver). Men i praksis tager vi smÃ¥ portioner af datasÃ¦ttet kaldet **minibatches** og beregner gradienter baseret pÃ¥ en delmÃ¦ngde af data. Fordi delmÃ¦ngden vÃ¦lges tilfÃ¦ldigt hver gang, kaldes denne metode **stochastic gradient descent** (SGD).

## Multi-Layered Perceptrons og Backpropagation

Et enlaget netvÃ¦rk, som vi har set ovenfor, er i stand til at klassificere lineÃ¦rt separable klasser. For at bygge en rigere model kan vi kombinere flere lag i netvÃ¦rket. Matematisk betyder det, at funktionen *f* vil have en mere kompleks form og vil blive beregnet i flere trin:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Her er Î± en **ikke-lineÃ¦r aktiveringsfunktion**, Ïƒ er en softmax-funktion, og parametrene Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradient descent-algoritmen forbliver den samme, men det bliver mere kompliceret at beregne gradienter. Ved hjÃ¦lp af kÃ¦deregel for differentiation kan vi beregne afledte som:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… KÃ¦deregel for differentiation bruges til at beregne afledte af tab-funktionen med hensyn til parametrene.

BemÃ¦rk, at den venstre del af alle disse udtryk er den samme, og derfor kan vi effektivt beregne afledte ved at starte fra tab-funktionen og gÃ¥ "baglÃ¦ns" gennem beregningsgrafen. Derfor kaldes metoden til trÃ¦ning af en multi-layered perceptron **backpropagation**, eller 'backprop'.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: billedhenvisning

> âœ… Vi vil dÃ¦kke backpropagation meget mere detaljeret i vores notebook-eksempel.  

## Konklusion

I denne lektion har vi bygget vores egen bibliotek for neurale netvÃ¦rk, og vi har brugt det til en simpel todimensional klassifikationsopgave.

## ğŸš€ Udfordring

I den medfÃ¸lgende notebook vil du implementere din egen ramme for at bygge og trÃ¦ne multi-layered perceptrons. Du vil kunne se i detaljer, hvordan moderne neurale netvÃ¦rk fungerer.

GÃ¥ videre til [OwnFramework](OwnFramework.ipynb) notebook og arbejd dig igennem den.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Gennemgang & Selvstudie

Backpropagation er en almindelig algoritme, der bruges i AI og ML, og det er vÃ¦rd at studere [mere detaljeret](https://wikipedia.org/wiki/Backpropagation)

## [Opgave](lab/README.md)

I dette laboratorium bliver du bedt om at bruge den ramme, du har konstrueret i denne lektion, til at lÃ¸se MNIST-hÃ¥ndskrevne cifre klassifikation.

* [Instruktioner](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjÃ¦lp af AI-oversÃ¦ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestrÃ¦ber os pÃ¥ nÃ¸jagtighed, skal du vÃ¦re opmÃ¦rksom pÃ¥, at automatiserede oversÃ¦ttelser kan indeholde fejl eller unÃ¸jagtigheder. Det originale dokument pÃ¥ dets oprindelige sprog bÃ¸r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversÃ¦ttelse. Vi er ikke ansvarlige for eventuelle misforstÃ¥elser eller fejltolkninger, der opstÃ¥r som fÃ¸lge af brugen af denne oversÃ¦ttelse.