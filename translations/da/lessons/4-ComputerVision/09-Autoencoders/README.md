<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-28T15:24:03+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "da"
}
-->
# Autoencodere

N√•r man tr√¶ner CNN'er, er en af udfordringerne, at vi har brug for en stor m√¶ngde m√¶rkede data. I tilf√¶lde af billedklassifikation skal vi adskille billeder i forskellige klasser, hvilket kr√¶ver manuel indsats.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Vi kan dog √∏nske at bruge r√• (um√¶rkede) data til at tr√¶ne CNN-featureekstraktorer, hvilket kaldes **selv-supervised learning**. I stedet for labels bruger vi tr√¶ningsbilleder som b√•de netv√¶rksinput og output. Hovedideen med en **autoencoder** er, at vi har et **encoder-netv√¶rk**, der konverterer inputbilledet til et **latent rum** (normalt er det blot en vektor af mindre st√∏rrelse), og derefter et **decoder-netv√¶rk**, hvis m√•l er at rekonstruere det originale billede.

> ‚úÖ En [autoencoder](https://wikipedia.org/wiki/Autoencoder) er "en type kunstigt neuralt netv√¶rk, der bruges til at l√¶re effektive kodninger af um√¶rkede data."

Da vi tr√¶ner en autoencoder til at fange s√• meget information som muligt fra det originale billede for en pr√¶cis rekonstruktion, fors√∏ger netv√¶rket at finde den bedste **embedding** af inputbilleder for at fange meningen.

![AutoEncoder Diagram](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.da.jpg)

> Billede fra [Keras blog](https://blog.keras.io/building-autoencoders-in-keras.html)

## Scenarier for brug af Autoencodere

Selvom rekonstruktion af originale billeder ikke virker nyttigt i sig selv, er der nogle scenarier, hvor autoencodere er s√¶rligt nyttige:

* **Reducering af billeddimensioner til visualisering** eller **tr√¶ning af billede-embeddings**. Autoencodere giver normalt bedre resultater end PCA, fordi de tager h√∏jde for billedernes rumlige natur og hierarkiske features.
* **Denoising**, dvs. fjernelse af st√∏j fra billedet. Da st√∏j indeholder en masse unyttig information, kan autoencoderen ikke passe det hele ind i det relativt lille latente rum og fanger derfor kun den vigtige del af billedet. N√•r vi tr√¶ner st√∏jfjernerne, starter vi med originale billeder og bruger billeder med kunstigt tilf√∏jet st√∏j som input til autoencoderen.
* **Super-resolution**, √∏gning af billedopl√∏sning. Vi starter med billeder i h√∏j opl√∏sning og bruger billeder med lavere opl√∏sning som autoencoderens input.
* **Generative modeller**. N√•r vi har tr√¶net autoencoderen, kan decoder-delen bruges til at skabe nye objekter ud fra tilf√¶ldige latente vektorer.

## Variationsautoencodere (VAE)

Traditionelle autoencodere reducerer dimensionen af inputdata p√• en eller anden m√•de og finder de vigtige features i inputbillederne. Dog giver latente vektorer ofte ikke meget mening. Med andre ord, hvis vi tager MNIST-datas√¶ttet som eksempel, er det ikke nemt at finde ud af, hvilke cifre der svarer til forskellige latente vektorer, fordi n√¶rliggende latente vektorer ikke n√∏dvendigvis svarer til de samme cifre.

P√• den anden side er det bedre at have en forst√•else af det latente rum, n√•r man tr√¶ner *generative* modeller. Denne id√© f√∏rer os til **variationsautoencoder** (VAE).

VAE er en autoencoder, der l√¶rer at forudsige *statistisk fordeling* af de latente parametre, det s√•kaldte **latente fordeling**. For eksempel kan vi √∏nske, at latente vektorer er normalt fordelt med en middelv√¶rdi z<sub>mean</sub> og standardafvigelse z<sub>sigma</sub> (b√•de middelv√¶rdi og standardafvigelse er vektorer med en vis dimensionalitet d). Encoderen i VAE l√¶rer at forudsige disse parametre, og derefter tager decoderen en tilf√¶ldig vektor fra denne fordeling for at rekonstruere objektet.

For at opsummere:

 * Fra inputvektoren forudsiger vi `z_mean` og `z_log_sigma` (i stedet for at forudsige selve standardafvigelsen, forudsiger vi dens logaritme)
 * Vi sampler en vektor `sample` fra fordelingen N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Decoderen fors√∏ger at dekode det originale billede ved hj√¶lp af `sample` som inputvektor

 <img src="images/vae.png" width="50%">

> Billede fra [denne blogpost](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) af Isaak Dykeman

Variationsautoencodere bruger en kompleks tabfunktion, der best√•r af to dele:

* **Rekonstruktionstab** er tabfunktionen, der viser, hvor t√¶t et rekonstrueret billede er p√• m√•let (det kan v√¶re Mean Squared Error, eller MSE). Det er den samme tabfunktion som i normale autoencodere.
* **KL-tab**, som sikrer, at den latente variabels fordeling forbliver t√¶t p√• normalfordelingen. Det er baseret p√• begrebet [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - en metrik til at estimere, hvor ens to statistiske fordelinger er.

En vigtig fordel ved VAE'er er, at de g√∏r det relativt nemt at generere nye billeder, fordi vi ved, hvilken fordeling vi skal sample latente vektorer fra. For eksempel, hvis vi tr√¶ner VAE med en 2D latent vektor p√• MNIST, kan vi derefter variere komponenterne i den latente vektor for at f√• forskellige cifre:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Billede af [Dmitry Soshnikov](http://soshnikov.com)

Bem√¶rk, hvordan billederne smelter sammen, n√•r vi begynder at f√• latente vektorer fra forskellige dele af det latente parameter-rum. Vi kan ogs√• visualisere dette rum i 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Billede af [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è √òvelser: Autoencodere

L√¶r mere om autoencodere i disse tilh√∏rende notebooks:

* [Autoencodere i TensorFlow](AutoencodersTF.ipynb)
* [Autoencodere i PyTorch](AutoEncodersPyTorch.ipynb)

## Egenskaber ved Autoencodere

* **Dataspecifikke** - de fungerer kun godt med den type billeder, de er blevet tr√¶net p√•. For eksempel, hvis vi tr√¶ner et super-resolution-netv√¶rk p√• blomster, vil det ikke fungere godt p√• portr√¶tter. Dette skyldes, at netv√¶rket kan producere billeder i h√∏jere opl√∏sning ved at tage fine detaljer fra features, der er l√¶rt fra tr√¶ningsdatas√¶ttet.
* **Tabsgivende** - det rekonstruerede billede er ikke det samme som det originale billede. Naturen af tabet defineres af den *tabfunktion*, der bruges under tr√¶ningen.
* Fungerer p√• **um√¶rkede data**

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Konklusion

I denne lektion l√¶rte du om de forskellige typer autoencodere, der er tilg√¶ngelige for AI-forskeren. Du l√¶rte, hvordan man bygger dem, og hvordan man bruger dem til at rekonstruere billeder. Du l√¶rte ogs√• om VAE og hvordan man bruger det til at generere nye billeder.

## üöÄ Udfordring

I denne lektion l√¶rte du om brugen af autoencodere til billeder. Men de kan ogs√• bruges til musik! Tjek Magenta-projektets [MusicVAE](https://magenta.tensorflow.org/music-vae) projekt, som bruger autoencodere til at l√¶re at rekonstruere musik. Lav nogle [eksperimenter](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) med dette bibliotek for at se, hvad du kan skabe.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Gennemgang & Selvstudie

For reference, l√¶s mere om autoencodere i disse ressourcer:

* [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Blogpost p√• NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Variational Autoencoders Explained](https://kvfrans.com/variational-autoencoders-explained/)
* [Conditional Variational Autoencoders](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Opgave

I slutningen af [denne notebook med TensorFlow](AutoencodersTF.ipynb) finder du en 'opgave' - brug denne som din opgave.

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• at sikre n√∏jagtighed, skal det bem√¶rkes, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for eventuelle misforst√•elser eller fejltolkninger, der m√•tte opst√• som f√∏lge af brugen af denne overs√¶ttelse.