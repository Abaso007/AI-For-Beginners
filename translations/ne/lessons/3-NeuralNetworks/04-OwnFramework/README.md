<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-26T10:26:59+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "ne"
}
-->
# न्यूरल नेटवर्कको परिचय। बहु-स्तरीय परसेप्ट्रोन

अघिल्लो खण्डमा, तपाईंले सबैभन्दा साधारण न्यूरल नेटवर्क मोडेल - एक-स्तरीय परसेप्ट्रोन, जुन रेखीय दुई-वर्ग वर्गीकरण मोडेल हो, को बारेमा सिक्नुभयो।

यस खण्डमा हामी यो मोडेललाई अझ लचिलो ढाँचामा विस्तार गर्नेछौं, जसले हामीलाई निम्न कार्यहरू गर्न अनुमति दिनेछ:

* दुई-वर्गको अतिरिक्त **बहु-वर्ग वर्गीकरण** गर्न
* वर्गीकरणको अतिरिक्त **पुनरावृत्ति समस्याहरू** समाधान गर्न
* रेखीय रूपमा छुट्याउन नसकिने वर्गहरू छुट्याउन

हामी हाम्रो आफ्नै पायथनमा आधारित मोड्युलर फ्रेमवर्क पनि विकास गर्नेछौं, जसले विभिन्न न्यूरल नेटवर्क आर्किटेक्चरहरू निर्माण गर्न अनुमति दिनेछ।

## [पाठपूर्व प्रश्नोत्तरी](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## मेसिन लर्निङको औपचारिकता

हामी मेसिन लर्निङ समस्यालाई औपचारिक रूपमा परिभाषित गरेर सुरु गरौं। मानौं, हामीसँग **X** नामक प्रशिक्षण डाटासेट छ, जसको लेबलहरू **Y** छन्, र हामीले *f* नामक मोडेल निर्माण गर्नुपर्छ, जसले सबैभन्दा सही भविष्यवाणीहरू गर्नेछ। भविष्यवाणीहरूको गुणस्तर **हानि फलन** ℒ द्वारा मापन गरिन्छ। निम्न हानि फलनहरू प्रायः प्रयोग गरिन्छ:

* पुनरावृत्ति समस्याको लागि, जहाँ हामीले कुनै संख्या भविष्यवाणी गर्नुपर्छ, हामी **पूर्ण त्रुटि** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, वा **वर्ग त्रुटि** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> प्रयोग गर्न सक्छौं।
* वर्गीकरणको लागि, हामी **0-1 हानि** (जसलाई मोडेलको **सटीकता** पनि भनिन्छ), वा **लजिस्टिक हानि** प्रयोग गर्छौं।

एक-स्तरीय परसेप्ट्रोनको लागि, *f* लाई रेखीय फलन *f(x)=wx+b* को रूपमा परिभाषित गरिएको थियो (यहाँ *w* तौल म्याट्रिक्स हो, *x* इनपुट विशेषताहरूको भेक्टर हो, र *b* पूर्वाग्रह भेक्टर हो)। विभिन्न न्यूरल नेटवर्क आर्किटेक्चरहरूको लागि, यो फलन अझ जटिल रूप लिन सक्छ।

> वर्गीकरणको अवस्थामा, सम्बन्धित वर्गहरूको सम्भावनाहरू नेटवर्कको आउटपुटको रूपमा प्राप्त गर्नु वाञ्छनीय हुन्छ। मनमानी संख्याहरूलाई सम्भावनामा रूपान्तरण गर्न (जस्तै आउटपुटलाई सामान्यीकरण गर्न), हामी प्रायः **सफ्टम्याक्स** फलन σ प्रयोग गर्छौं, र फलन *f* यसरी परिभाषित हुन्छ: *f(x)=σ(wx+b)*

उपरोक्त *f* को परिभाषामा, *w* र *b* लाई **प्यारामिटरहरू** θ=⟨*w,b*⟩ भनिन्छ। डाटासेट ⟨**X**,**Y**⟩ दिइएको अवस्थामा, हामी सम्पूर्ण डाटासेटमा समग्र त्रुटि θ को फलनको रूपमा गणना गर्न सक्छौं।

> ✅ **न्यूरल नेटवर्क प्रशिक्षणको लक्ष्य भनेको θ को मान परिवर्तन गरेर त्रुटिलाई न्यूनतम बनाउनु हो।**

## ग्रेडियन्ट डिसेन्ट अप्टिमाइजेसन

फलन अप्टिमाइजेसनको एक प्रसिद्ध विधि **ग्रेडियन्ट डिसेन्ट** हो। यसको विचार यो हो कि हामी हानि फलनको डेरिभेटिभ (बहु-आयामिक अवस्थामा **ग्रेडियन्ट** भनिन्छ) पारामिटरहरूको सन्दर्भमा गणना गर्न सक्छौं, र त्रुटि घट्ने गरी पारामिटरहरू परिवर्तन गर्न सक्छौं। यसलाई निम्न रूपमा औपचारिक बनाउन सकिन्छ:

* पारामिटरहरूलाई केही अनियमित मानहरू w<sup>(0)</sup>, b<sup>(0)</sup> बाट सुरु गर्नुहोस्।
* निम्न चरणलाई धेरै पटक दोहोर्याउनुहोस्:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

प्रशिक्षणको क्रममा, अप्टिमाइजेसन चरणहरू सम्पूर्ण डाटासेटलाई ध्यानमा राखेर गणना गरिनुपर्छ (स्मरण रहोस् कि हानि सबै प्रशिक्षण नमूनाहरूको योगको रूपमा गणना गरिन्छ)। तर, वास्तविक जीवनमा हामी डाटासेटका साना भागहरू, जसलाई **मिनिब्याचहरू** भनिन्छ, लिन्छौं, र डाटाको उपसमूहको आधारमा ग्रेडियन्टहरू गणना गर्छौं। किनभने प्रत्येक पटक उपसमूह अनियमित रूपमा लिइन्छ, यस्तो विधिलाई **स्टोकास्टिक ग्रेडियन्ट डिसेन्ट** (SGD) भनिन्छ।

## बहु-स्तरीय परसेप्ट्रोन र ब्याकप्रोपोगेसन

एक-स्तरीय नेटवर्क, जस्तो हामीले माथि देख्यौं, रेखीय रूपमा छुट्याउन सकिने वर्गहरूको वर्गीकरण गर्न सक्षम छ। अझ समृद्ध मोडेल निर्माण गर्न, हामी नेटवर्कका धेरै तहहरू संयोजन गर्न सक्छौं। गणितीय रूपमा यसको अर्थ यो हुन्छ कि फलन *f* को रूप अझ जटिल हुनेछ, र यो धेरै चरणहरूमा गणना गरिनेछ:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

यहाँ, α एक **गैर-रेखीय सक्रियता फलन** हो, σ सफ्टम्याक्स फलन हो, र पारामिटरहरू θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> हुन्।

ग्रेडियन्ट डिसेन्ट एल्गोरिदम उस्तै रहनेछ, तर ग्रेडियन्टहरू गणना गर्न अझ गाह्रो हुनेछ। चेन डिफरेन्सिएसन नियमलाई ध्यानमा राख्दै, हामी डेरिभेटिभहरू निम्न रूपमा गणना गर्न सक्छौं:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ चेन डिफरेन्सिएसन नियम हानि फलनको डेरिभेटिभहरू पारामिटरहरूको सन्दर्भमा गणना गर्न प्रयोग गरिन्छ।

ध्यान दिनुहोस् कि यी सबै अभिव्यक्तिहरूको बाँया-पट्टिको भाग उस्तै छ, र यसैले हामी प्रभावकारी रूपमा डेरिभेटिभहरू हानि फलनबाट सुरु गरेर "पछाडि" कम्प्युटेसनल ग्राफ हुँदै गणना गर्न सक्छौं। यसैले बहु-स्तरीय परसेप्ट्रोनको प्रशिक्षण विधिलाई **ब्याकप्रोपोगेसन**, वा 'ब्याकप्रोप' भनिन्छ।

<img alt="कम्प्युट ग्राफ" src="images/ComputeGraphGrad.png"/>

> TODO: छवि स्रोत

> ✅ हामी ब्याकप्रोपलाई हाम्रो नोटबुक उदाहरणमा अझ विस्तृत रूपमा कभर गर्नेछौं।  

## निष्कर्ष

यस पाठमा, हामीले हाम्रो आफ्नै न्यूरल नेटवर्क पुस्तकालय निर्माण गरेका छौं, र यसलाई सरल दुई-आयामिक वर्गीकरण कार्यको लागि प्रयोग गरेका छौं।

## 🚀 चुनौती

संग्लग्न नोटबुकमा, तपाईंले बहु-स्तरीय परसेप्ट्रोनहरू निर्माण र प्रशिक्षण गर्नको लागि आफ्नो फ्रेमवर्क कार्यान्वयन गर्नुहुनेछ। तपाईं आधुनिक न्यूरल नेटवर्कहरू कसरी काम गर्छन् भन्ने विस्तृत रूपमा हेर्न सक्नुहुनेछ।

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) नोटबुकमा जानुहोस् र यसलाई पूरा गर्नुहोस्।

## [पाठपछिको प्रश्नोत्तरी](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## समीक्षा र आत्म-अध्ययन

ब्याकप्रोपोगेसन एआई र एमएलमा प्रयोग गरिने सामान्य एल्गोरिदम हो, जसलाई [अझ विस्तृत रूपमा अध्ययन](https://wikipedia.org/wiki/Backpropagation) गर्न लायक छ।

## [कार्य](lab/README.md)

यस प्रयोगशालामा, तपाईंलाई यस पाठमा निर्माण गरिएको फ्रेमवर्क प्रयोग गरेर MNIST हस्तलिखित अंक वर्गीकरण समाधान गर्न भनिएको छ।

* [निर्देशहरू](lab/README.md)
* [नोटबुक](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव शुद्धताको प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादहरूमा त्रुटि वा अशुद्धता हुन सक्छ। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्त्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।