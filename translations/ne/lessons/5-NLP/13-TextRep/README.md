<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-26T08:26:15+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "ne"
}
-->
# पाठलाई टेन्सरको रूपमा प्रतिनिधित्व गर्ने

## [पूर्व-व्याख्यान क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## पाठ वर्गीकरण

यस खण्डको पहिलो भागभरि, हामी **पाठ वर्गीकरण** कार्यमा केन्द्रित हुनेछौं। हामी [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) डेटासेट प्रयोग गर्नेछौं, जसमा निम्न जस्ता समाचार लेखहरू समावेश छन्:

* श्रेणी: विज्ञान/प्रविधि  
* शीर्षक: Ky. Company Wins Grant to Study Peptides (AP)  
* मुख्य भाग: AP - लुइसभिल विश्वविद्यालयका रसायनशास्त्र अनुसन्धानकर्ताद्वारा स्थापना गरिएको कम्पनीले विकास गर्न अनुदान जित्यो...  

हाम्रो लक्ष्य पाठको आधारमा समाचारलाई कुनै एक श्रेणीमा वर्गीकृत गर्नु हुनेछ।

## पाठको प्रतिनिधित्व

यदि हामी प्राकृतिक भाषा प्रशोधन (NLP) कार्यहरू न्यूरल नेटवर्कहरूसँग समाधान गर्न चाहन्छौं भने, पाठलाई टेन्सरको रूपमा प्रतिनिधित्व गर्ने कुनै तरिका आवश्यक छ। कम्प्युटरहरूले पहिले नै ASCII वा UTF-8 जस्ता एन्कोडिङहरू प्रयोग गरेर पाठ्य वर्णहरूलाई स्क्रिनमा फन्टसँग मिल्ने संख्याको रूपमा प्रतिनिधित्व गर्छन्।

<img alt="क्यारेक्टरलाई ASCII र बाइनरी प्रतिनिधित्वमा म्याप गर्ने चित्र" src="images/ascii-character-map.png" width="50%"/>

> [छवि स्रोत](https://www.seobility.net/en/wiki/ASCII)

मानिसका रूपमा, हामी प्रत्येक अक्षरले के **प्रतिनिधित्व** गर्छ बुझ्छौं, र सबै वर्णहरू मिलेर वाक्यका शब्दहरू कसरी बनाउँछन् भन्ने कुरा थाहा हुन्छ। तर, कम्प्युटरहरूले आफैंमा यस्तो बुझाइ राख्दैनन्, र न्यूरल नेटवर्कले प्रशिक्षणको क्रममा यसको अर्थ सिक्नुपर्छ।

त्यसैले, पाठको प्रतिनिधित्व गर्दा हामी विभिन्न दृष्टिकोणहरू प्रयोग गर्न सक्छौं:

* **क्यारेक्टर-स्तरको प्रतिनिधित्व**, जहाँ हामी प्रत्येक क्यारेक्टरलाई संख्याको रूपमा लिन्छौं। यदि हाम्रो पाठ कर्पसमा *C* फरक क्यारेक्टरहरू छन् भने, *Hello* शब्दलाई 5x*C* टेन्सरले प्रतिनिधित्व गर्नेछ। प्रत्येक अक्षरले वन-हट एन्कोडिङमा टेन्सरको स्तम्भलाई प्रतिनिधित्व गर्नेछ।  
* **शब्द-स्तरको प्रतिनिधित्व**, जसमा हामी हाम्रो पाठको सबै शब्दहरूको **शब्दकोश** बनाउँछौं, र त्यसपछि शब्दहरूलाई वन-हट एन्कोडिङ प्रयोग गरेर प्रतिनिधित्व गर्छौं। यो दृष्टिकोण केही हदसम्म राम्रो छ, किनभने प्रत्येक अक्षर आफैंमा धेरै अर्थपूर्ण हुँदैन, र उच्च-स्तरका अर्थपूर्ण अवधारणाहरू - शब्दहरू - प्रयोग गरेर हामी न्यूरल नेटवर्कको लागि कार्यलाई सरल बनाउँछौं। तर, ठूलो शब्दकोश आकारका कारण, हामी उच्च-आयामीय विरल टेन्सरहरूसँग व्यवहार गर्नुपर्छ।  

प्रतिनिधित्वको कुनै पनि तरिकामा, हामीले पहिले पाठलाई **टोकन**हरूको क्रममा रूपान्तरण गर्नुपर्छ, जहाँ एक टोकन क्यारेक्टर, शब्द, वा कहिलेकाहीँ शब्दको भाग पनि हुन सक्छ। त्यसपछि, हामी टोकनलाई संख्यामा रूपान्तरण गर्छौं, सामान्यतया **शब्दकोश** प्रयोग गरेर, र यो संख्या वन-हट एन्कोडिङ प्रयोग गरेर न्यूरल नेटवर्कमा खुवाउन सकिन्छ।

## एन-ग्रामहरू

प्राकृतिक भाषामा, शब्दहरूको सटीक अर्थ मात्र सन्दर्भमा निर्धारण गर्न सकिन्छ। उदाहरणका लागि, *neural network* र *fishing network* का अर्थहरू पूर्ण रूपमा फरक छन्। यसलाई ध्यानमा राख्नका लागि, हाम्रो मोडेललाई शब्दहरूको जोडीमा निर्माण गर्ने, र शब्द जोडीहरूलाई अलग शब्दकोश टोकनको रूपमा लिने तरिका अपनाउन सकिन्छ। यसरी, *I like to go fishing* वाक्यलाई निम्न टोकनहरूको क्रमले प्रतिनिधित्व गरिनेछ: *I like*, *like to*, *to go*, *go fishing*।  

यस दृष्टिकोणको समस्या भनेको शब्दकोशको आकार उल्लेखनीय रूपमा बढ्नु हो, र *go fishing* र *go shopping* जस्ता संयोजनहरू फरक टोकनहरूले प्रतिनिधित्व गर्छन्, जसले समान क्रियाको बावजुद कुनै पनि अर्थपूर्ण समानता साझा गर्दैन।  

केही अवस्थामा, हामी त्रि-ग्रामहरू -- तीन शब्दहरूको संयोजन -- प्रयोग गर्न विचार गर्न सक्छौं। यसरी, यो दृष्टिकोणलाई प्रायः **n-grams** भनिन्छ। साथै, क्यारेक्टर-स्तरको प्रतिनिधित्वमा n-grams प्रयोग गर्दा, n-grams प्रायः विभिन्न अक्षरसमूहहरूसँग मेल खान्छ।

## ब्याग-अफ-वर्ड्स र TF/IDF

पाठ वर्गीकरण जस्ता कार्यहरू समाधान गर्दा, हामीलाई पाठलाई एक निश्चित आकारको भेक्टरले प्रतिनिधित्व गर्न सक्षम हुनुपर्छ, जसलाई अन्तिम घना वर्गीकर्तामा इनपुटको रूपमा प्रयोग गरिनेछ। यसको सबैभन्दा सरल तरिकामध्ये एक भनेको सबै व्यक्तिगत शब्द प्रतिनिधित्वहरूलाई संयोजन गर्नु हो, जस्तै तिनीहरूलाई थपेर। यदि हामी प्रत्येक शब्दको वन-हट एन्कोडिङहरू थप्छौं भने, हामी आवृत्तिहरूको भेक्टरमा पुग्नेछौं, जसले पाठभित्र प्रत्येक शब्द कति पटक देखा पर्छ भन्ने देखाउँछ। पाठको यस्तो प्रतिनिधित्वलाई **ब्याग-अफ-वर्ड्स** (BoW) भनिन्छ।

<img src="images/bow.png" width="90%"/>

> लेखकद्वारा सिर्जित छवि

BoWले पाठमा कुन शब्दहरू देखा पर्छन् र कुन मात्रामा भन्ने कुरा प्रतिनिधित्व गर्छ, जुन वास्तवमा पाठको विषयवस्तु के हो भन्ने राम्रो संकेत हुन सक्छ। उदाहरणका लागि, राजनीतिमा आधारित समाचार लेखमा *president* र *country* जस्ता शब्दहरू हुने सम्भावना छ, जबकि वैज्ञानिक प्रकाशनमा *collider*, *discovered* जस्ता शब्दहरू हुनेछन्। यसरी, शब्द आवृत्तिहरू धेरै अवस्थामा पाठको सामग्रीको राम्रो संकेत हुन सक्छ।

BoWको समस्या भनेको केही सामान्य शब्दहरू, जस्तै *and*, *is*, आदि धेरैजसो पाठहरूमा देखा पर्छन्, र तिनीहरूको आवृत्ति उच्च हुन्छ, जसले वास्तवमै महत्त्वपूर्ण शब्दहरूलाई छायामा पार्छ। हामीले ती शब्दहरूको महत्त्व घटाउन सक्छौं, जसले गर्दा शब्दहरू सम्पूर्ण कागजात संग्रहमा कति पटक देखा पर्छ भन्ने आवृत्तिलाई ध्यानमा राख्छ। यो नै TF/IDF दृष्टिकोणको मुख्य विचार हो, जुन यस पाठसँग संलग्न नोटबुकहरूमा थप विवरणमा समेटिएको छ।

तर, यी कुनै पनि दृष्टिकोणहरूले पाठको **अर्थ**लाई पूर्ण रूपमा ध्यानमा राख्न सक्दैन। यसका लागि हामीलाई अझ शक्तिशाली न्यूरल नेटवर्क मोडेलहरू आवश्यक पर्छ, जुन हामी यस खण्डमा पछि छलफल गर्नेछौं।

## ✍️ अभ्यास: पाठको प्रतिनिधित्व

तपाईंको सिकाइलाई निम्न नोटबुकहरूमा जारी राख्नुहोस्:

* [PyTorch प्रयोग गरेर पाठको प्रतिनिधित्व](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [TensorFlow प्रयोग गरेर पाठको प्रतिनिधित्व](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## निष्कर्ष

अहिलेसम्म, हामीले विभिन्न शब्दहरूलाई आवृत्ति तौल थप्न सक्ने प्रविधिहरू अध्ययन गरेका छौं। यद्यपि, तिनीहरू अर्थ वा क्रमलाई प्रतिनिधित्व गर्न असमर्थ छन्। प्रसिद्ध भाषाविद् जे. आर. फर्थले १९३५ मा भनेका थिए, "शब्दको पूर्ण अर्थ सधैं सन्दर्भमा हुन्छ, र सन्दर्भबाहेकको अर्थको कुनै पनि अध्ययनलाई गम्भीरतापूर्वक लिन सकिँदैन।" हामी यस पाठ्यक्रममा पछि पाठबाट सन्दर्भगत जानकारी कसरी कब्जा गर्ने भन्ने कुरा भाषा मोडेलिङ प्रयोग गरेर सिक्नेछौं।

## 🚀 चुनौती

ब्याग-अफ-वर्ड्स र विभिन्न डेटा मोडेलहरू प्रयोग गरेर केही अन्य अभ्यासहरू प्रयास गर्नुहोस्। तपाईंलाई यो [Kaggle प्रतियोगिता](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) बाट प्रेरणा मिल्न सक्छ।

## [पश्च-व्याख्यान क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## समीक्षा र आत्म-अध्ययन

[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) मा पाठ एम्बेडिङ र ब्याग-अफ-वर्ड्स प्रविधिहरूको अभ्यास गर्नुहोस्।

## [असाइनमेन्ट: नोटबुकहरू](assignment.md)  

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव शुद्धताको प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुन सक्छ। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।