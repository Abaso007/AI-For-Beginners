<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-26T08:10:38+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "ne"
}
-->
# पुनरावर्ती न्यूरल नेटवर्कहरू

## [पाठ अघि क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/31)

अघिल्लो खण्डहरूमा, हामीले पाठको समृद्ध अर्थपूर्ण प्रतिनिधित्वहरू र इम्बेडिङको माथि एक साधारण रेखीय वर्गीकरणकर्ता प्रयोग गरिरहेका थियौं। यो वास्तुकलाले वाक्यमा शब्दहरूको समग्र अर्थलाई समात्छ, तर यो शब्दहरूको **क्रम**लाई ध्यानमा राख्दैन, किनभने इम्बेडिङको माथि गरिएको समग्र अपरेशनले मूल पाठबाट यो जानकारी हटाउँछ। यी मोडेलहरूले शब्दहरूको क्रमबद्धता मोडल गर्न नसक्ने हुँदा, तिनीहरूले पाठ सिर्जना वा प्रश्न उत्तरजस्ता जटिल वा अस्पष्ट कार्यहरू समाधान गर्न सक्दैनन्।

पाठ अनुक्रमको अर्थ समात्न, हामीले अर्को न्यूरल नेटवर्क वास्तुकला प्रयोग गर्न आवश्यक छ, जसलाई **पुनरावर्ती न्यूरल नेटवर्क** वा RNN भनिन्छ। RNN मा, हामी हाम्रो वाक्यलाई नेटवर्कमा एक पटकमा एक प्रतीक पास गर्छौं, र नेटवर्कले केही **राज्य** उत्पादन गर्छ, जुन हामी अर्को प्रतीकसँग नेटवर्कमा पुन: पास गर्छौं।

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.ne.png)

> लेखकद्वारा प्रदान गरिएको छवि

प्रतीकहरूको इनपुट अनुक्रम X<sub>0</sub>,...,X<sub>n</sub> दिइएको अवस्थामा, RNN ले न्यूरल नेटवर्क ब्लकहरूको अनुक्रम सिर्जना गर्छ, र यो अनुक्रमलाई अन्त्यसम्म ब्याकप्रोपोगेसन प्रयोग गरेर प्रशिक्षण दिन्छ। प्रत्येक नेटवर्क ब्लकले (X<sub>i</sub>,S<sub>i</sub>) जोडीलाई इनपुटको रूपमा लिन्छ, र परिणामस्वरूप S<sub>i+1</sub> उत्पादन गर्छ। अन्तिम राज्य S<sub>n</sub> वा (आउटपुट Y<sub>n</sub>) लाई रेखीय वर्गीकरणकर्तामा पठाइन्छ परिणाम उत्पादन गर्न। सबै नेटवर्क ब्लकहरूले समान तौल साझा गर्छन्, र एक ब्याकप्रोपोगेसन पास प्रयोग गरेर अन्त्यसम्म प्रशिक्षण गरिन्छ।

किनकि राज्य भेक्टरहरू S<sub>0</sub>,...,S<sub>n</sub> नेटवर्कमार्फत पास गरिन्छ, यसले शब्दहरू बीचको क्रमबद्ध निर्भरता सिक्न सक्षम हुन्छ। उदाहरणका लागि, जब *not* शब्द अनुक्रममा कतै देखा पर्छ, यसले राज्य भेक्टरभित्रका निश्चित तत्वहरूलाई नकार्न सिक्न सक्छ, जसले नकारात्मकता उत्पन्न गर्छ।

> ✅ माथिको चित्रमा देखाइएका सबै RNN ब्लकहरूको तौल साझा गरिएको छ, त्यसैले एउटै चित्रलाई एक ब्लक (दायाँतर्फ) को रूपमा प्रतिनिधित्व गर्न सकिन्छ, जसमा पुनरावर्ती फिडब्याक लूप हुन्छ, जसले नेटवर्कको आउटपुट राज्यलाई इनपुटमा फिर्ता पास गर्छ।

## RNN सेलको संरचना

अब हामी एक साधारण RNN सेल कसरी व्यवस्थित छ हेर्नेछौं। यसले अघिल्लो राज्य S<sub>i-1</sub> र वर्तमान प्रतीक X<sub>i</sub> लाई इनपुटको रूपमा स्वीकार्छ, र आउटपुट राज्य S<sub>i</sub> उत्पादन गर्नुपर्छ (र, कहिलेकाहीँ, हामी अन्य आउटपुट Y<sub>i</sub> मा पनि चासो राख्छौं, जस्तै जेनेरेटिभ नेटवर्कहरूको मामलामा)।

एक साधारण RNN सेलभित्र दुई तौल म्याट्रिक्स हुन्छन्: एउटा इनपुट प्रतीकलाई रूपान्तरण गर्छ (यसलाई W भनौं), र अर्को इनपुट राज्यलाई रूपान्तरण गर्छ (H)। यस अवस्थामा नेटवर्कको आउटपुट σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b) को रूपमा गणना गरिन्छ, जहाँ σ सक्रियता प्रकार्य हो र b थप पूर्वाग्रह हो।

<img alt="RNN सेलको संरचना" src="images/rnn-anatomy.png" width="50%"/>

> लेखकद्वारा प्रदान गरिएको छवि

धेरै अवस्थामा, इनपुट प्रतीकहरू RNN मा प्रवेश गर्नु अघि इम्बेडिङ तहमार्फत पास गरिन्छ ताकि आयाम घटाउन सकियोस्। यस अवस्थामा, यदि इनपुट भेक्टरहरूको आयाम *emb_size* हो, र राज्य भेक्टर *hid_size* हो - W को आकार *emb_size*×*hid_size* हुन्छ, र H को आकार *hid_size*×*hid_size* हुन्छ।

## लामो छोटो अवधि स्मृति (LSTM)

क्लासिकल RNNहरूको मुख्य समस्याहरू मध्ये एक **वेनिशिङ ग्रेडियन्ट** समस्या हो। किनकि RNNहरू एक ब्याकप्रोपोगेसन पासमा अन्त्यसम्म प्रशिक्षण गरिन्छ, यसले नेटवर्कको पहिलो तहहरूमा त्रुटि फैलाउन कठिनाइ हुन्छ, र यसैले नेटवर्कले टाढाका प्रतीकहरू बीचको सम्बन्धहरू सिक्न सक्दैन। यो समस्यालाई टार्नको लागि **स्पष्ट राज्य व्यवस्थापन** गेटहरू प्रयोग गरेर परिचय गराइन्छ। यस प्रकारका दुई प्रसिद्ध वास्तुकलाहरू छन्: **लामो छोटो अवधि स्मृति** (LSTM) र **गेटेड रिलेको इकाई** (GRU)।

![लामो छोटो अवधि स्मृति सेलको उदाहरण देखाउने छवि](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> छवि स्रोत TBD

LSTM नेटवर्क RNN जस्तै व्यवस्थित छ, तर दुई राज्यहरू छन् जुन तहबाट तहमा पास गरिन्छ: वास्तविक राज्य C, र लुकेको भेक्टर H। प्रत्येक इकाईमा, लुकेको भेक्टर H<sub>i</sub> इनपुट X<sub>i</sub> सँग जोडिन्छ, र तिनीहरूले **गेटहरू** मार्फत राज्य C मा के हुन्छ नियन्त्रण गर्छन्। प्रत्येक गेट एक न्यूरल नेटवर्क हो जसमा सिग्मोइड सक्रियता हुन्छ (आउटपुट [0,1] को दायरामा), जसलाई राज्य भेक्टरसँग गुणा गर्दा बिटवाइज मास्कको रूपमा सोच्न सकिन्छ। निम्न गेटहरू छन् (चित्रमा बाँयाबाट दायाँ):

* **भुल्ने गेट** लुकेको भेक्टर लिन्छ र भेक्टर C का कुन घटकहरू भुल्नुपर्छ र कुन पास गर्नुपर्छ निर्धारण गर्छ।
* **इनपुट गेट** इनपुट र लुकेको भेक्टरबाट केही जानकारी लिन्छ र राज्यमा समावेश गर्छ।
* **आउटपुट गेट** राज्यलाई *tanh* सक्रियता भएको रेखीय तहमार्फत रूपान्तरण गर्छ, त्यसपछि नयाँ राज्य C<sub>i+1</sub> उत्पादन गर्न लुकेको भेक्टर H<sub>i</sub> प्रयोग गरेर यसको केही घटकहरू चयन गर्छ।

राज्य C का घटकहरूलाई केही झण्डाहरूको रूपमा सोच्न सकिन्छ जसलाई अन र अफ गर्न सकिन्छ। उदाहरणका लागि, जब हामी अनुक्रममा *Alice* नाम भेट्छौं, हामीले यसलाई महिला पात्रलाई जनाउँछ भन्ने मान्न सक्छौं, र राज्यमा झण्डा उठाउन सक्छौं कि वाक्यमा महिला संज्ञा छ। जब हामी थप *and Tom* वाक्यांश भेट्छौं, हामीले बहुवचन संज्ञा भएको झण्डा उठाउनेछौं। यसरी राज्यलाई हेरफेर गरेर हामी वाक्यका भागहरूको व्याकरणिक गुणहरू ट्र्याक गर्न सक्छौं।

> ✅ LSTM को आन्तरिक संरचना बुझ्नको लागि उत्कृष्ट स्रोत क्रिस्टोफर ओलाहद्वारा लेखिएको [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) हो।

## द्विदिशात्मक र बहु-तह RNNहरू

हामीले एक दिशामा सञ्चालन गर्ने पुनरावर्ती नेटवर्कहरू छलफल गरेका छौं, अनुक्रमको सुरुबाट अन्त्यसम्म। यो प्राकृतिक देखिन्छ, किनकि यसले हामीले पढ्ने र बोल्ने तरिकालाई झल्काउँछ। तर, धेरै व्यावहारिक अवस्थामा हामीसँग इनपुट अनुक्रममा र्यान्डम पहुँच हुन्छ, यसले पुनरावर्ती गणना दुवै दिशामा चलाउन अर्थ राख्न सक्छ। यस्ता नेटवर्कहरूलाई **द्विदिशात्मक** RNN भनिन्छ। द्विदिशात्मक नेटवर्कसँग व्यवहार गर्दा, हामीलाई प्रत्येक दिशाको लागि दुई लुकेको राज्य भेक्टर आवश्यक पर्छ।

एक पुनरावर्ती नेटवर्क, चाहे एक-दिशात्मक होस् वा द्विदिशात्मक, अनुक्रमभित्र निश्चित ढाँचाहरू समात्छ, र तिनीहरूलाई राज्य भेक्टरमा भण्डारण गर्न वा आउटपुटमा पास गर्न सक्छ। कन्भोल्युसनल नेटवर्कहरू जस्तै, हामी पहिलो तहले निकालेका तल्लो-स्तरका ढाँचाहरूबाट उच्च-स्तरका ढाँचाहरू समात्न अर्को पुनरावर्ती तह निर्माण गर्न सक्छौं। यसले हामीलाई **बहु-तह RNN** को धारणा दिन्छ, जसमा दुई वा बढी पुनरावर्ती नेटवर्कहरू हुन्छन्, जहाँ अघिल्लो तहको आउटपुट अर्को तहमा इनपुटको रूपमा पास गरिन्छ।

![बहु-तह लामो-छोटो-अवधि-स्मृति RNN देखाउने छवि](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.ne.jpg)

*फर्नान्डो लोपेजद्वारा [यो उत्कृष्ट पोस्ट](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) बाट चित्र*

## ✍️ अभ्यास: इम्बेडिङहरू

निम्न नोटबुकहरूमा आफ्नो सिकाइ जारी राख्नुहोस्:

* [PyTorch प्रयोग गरेर RNNहरू](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow प्रयोग गरेर RNNहरू](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## निष्कर्ष

यस इकाईमा, हामीले देख्यौं कि RNNहरू अनुक्रम वर्गीकरणको लागि प्रयोग गर्न सकिन्छ, तर वास्तवमा, तिनीहरूले पाठ सिर्जना, मेसिन अनुवाद, र अन्य धेरै कार्यहरू ह्यान्डल गर्न सक्छन्। हामी ती कार्यहरूलाई अर्को इकाईमा विचार गर्नेछौं।

## 🚀 चुनौती

LSTMहरूको बारेमा केही साहित्य पढ्नुहोस् र तिनीहरूको अनुप्रयोगहरू विचार गर्नुहोस्:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [पाठ पछि क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## समीक्षा र आत्म अध्ययन

- क्रिस्टोफर ओलाहद्वारा लेखिएको [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)।

## [असाइनमेन्ट: नोटबुकहरू](assignment.md)

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव शुद्धताको प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुन सक्छ। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।