<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-26T08:43:54+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "ne"
}
-->
# पूर्व-प्रशिक्षित ठूला भाषा मोडेलहरू

हाम्रा अघिल्ला सबै कार्यहरूमा, हामीले लेबल गरिएको डाटासेट प्रयोग गरेर कुनै निश्चित कार्य गर्न न्यूरल नेटवर्कलाई प्रशिक्षण गरिरहेका थियौं। ठूला ट्रान्सफर्मर मोडेलहरू, जस्तै BERT, मा हामी भाषा मोडेलिङलाई आत्म-पर्यवेक्षित तरिकामा प्रयोग गरेर भाषा मोडेल निर्माण गर्छौं, जसलाई पछि थप डोमेन-विशिष्ट प्रशिक्षणद्वारा विशेष कार्यहरूको लागि अनुकूलित गरिन्छ। तर, यो प्रमाणित भएको छ कि ठूला भाषा मोडेलहरूले कुनै पनि डोमेन-विशिष्ट प्रशिक्षण बिना नै धेरै कार्यहरू समाधान गर्न सक्छन्। यस्तो क्षमता भएका मोडेलहरूको परिवारलाई **GPT**: जेनेरेटिभ प्रि-ट्रेनड ट्रान्सफर्मर भनिन्छ।

## [पूर्व-व्याख्यान प्रश्नोत्तरी](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## पाठ उत्पादन र पर्प्लेक्सिटी

डाउनस्ट्रीम प्रशिक्षण बिना नै सामान्य कार्यहरू गर्न सक्षम न्यूरल नेटवर्कको विचार [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) नामक कागजातमा प्रस्तुत गरिएको छ। मुख्य विचार यो हो कि धेरै अन्य कार्यहरूलाई **पाठ उत्पादन** प्रयोग गरेर मोडेल गर्न सकिन्छ, किनभने पाठ बुझ्नु भनेको त्यसलाई उत्पादन गर्न सक्षम हुनु हो। मोडेललाई मानव ज्ञान समेटिएको विशाल पाठमा प्रशिक्षण दिइएकोले, यसले विभिन्न विषयहरूको बारेमा पनि ज्ञान प्राप्त गर्छ।

> पाठ बुझ्नु र उत्पादन गर्न सक्षम हुनु भनेको वरपरको संसारको बारेमा केही जान्नु हो। मानिसहरू पनि ठूलो हदसम्म पढेर सिक्छन्, र GPT नेटवर्क यस मामिलामा यस्तै छ।

पाठ उत्पादन गर्ने नेटवर्कहरूले अर्को शब्दको सम्भावना $$P(w_N)$$ को भविष्यवाणी गरेर काम गर्छ। तर, अर्को शब्दको बिना-शर्त सम्भावना भनेको पाठ कर्पसमा उक्त शब्दको आवृत्ति हो। GPT ले हामीलाई अघिल्ला शब्दहरू दिइएको अवस्थामा अर्को शब्दको **सशर्त सम्भावना** दिन सक्षम छ: $$P(w_N | w_{n-1}, ..., w_0)$$

> सम्भावनाहरूको बारेमा थप जान्नको लागि हाम्रो [डाटा साइन्सको लागि शुरुआती पाठ्यक्रम](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) हेर्नुहोस्।

भाषा उत्पादन गर्ने मोडेलको गुणस्तरलाई **पर्प्लेक्सिटी** प्रयोग गरेर परिभाषित गर्न सकिन्छ। यो एक आन्तरिक मेट्रिक हो जसले कुनै कार्य-विशिष्ट डाटासेट बिना नै मोडेलको गुणस्तर मापन गर्न अनुमति दिन्छ। यो *वाक्यको सम्भावना* को धारणा आधारित छ - मोडेलले वास्तविक हुने सम्भावना भएको वाक्यलाई उच्च सम्भावना दिन्छ (अर्थात् मोडेल त्यसबाट **चकित** हुँदैन), र कम अर्थपूर्ण वाक्यहरूलाई कम सम्भावना दिन्छ (जस्तै, *के यसले के गर्न सक्छ?*)। जब हामी हाम्रो मोडेललाई वास्तविक पाठ कर्पसका वाक्यहरू दिन्छौं, हामी ती वाक्यहरूको उच्च सम्भावना र कम **पर्प्लेक्सिटी** हुने अपेक्षा गर्छौं। गणितीय रूपमा, यो परीक्षण सेटको सामान्यीकृत उल्टो सम्भावनाको रूपमा परिभाषित गरिन्छ:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**तपाईं [Hugging Face को GPT-समर्थित पाठ सम्पादक](https://transformer.huggingface.co/doc/gpt2-large)** प्रयोग गरेर पाठ उत्पादनमा प्रयोग गर्न सक्नुहुन्छ। यस सम्पादकमा, तपाईं आफ्नो पाठ लेख्न सुरु गर्नुहोस्, र **[TAB]** थिच्दा तपाईंलाई धेरै पूरा गर्ने विकल्पहरू दिइनेछ। यदि ती धेरै छोटा छन्, वा तपाईं तीबाट सन्तुष्ट हुनुहुन्न भने - फेरि [TAB] थिच्नुहोस्, र तपाईंलाई थप विकल्पहरू, लामो पाठहरू सहित, दिइनेछ।

## GPT एक परिवार हो

GPT कुनै एकल मोडेल होइन, बरु [OpenAI](https://openai.com) द्वारा विकसित र प्रशिक्षण गरिएका मोडेलहरूको संग्रह हो।

GPT मोडेलहरू अन्तर्गत, हामीसँग छन्:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|भाषा मोडेल जसमा १.५ अर्बसम्मका प्यारामिटरहरू छन्। | १७५ अर्बसम्मका प्यारामिटर भएको भाषा मोडेल | १०० ट्रिलियन प्यारामिटरहरू, जसले छवि र पाठ दुवै इनपुट लिन्छ र पाठ उत्पादन गर्छ। |

GPT-3 र GPT-4 मोडेलहरू [Microsoft Azure बाट एक संज्ञानात्मक सेवा](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) को रूपमा, र [OpenAI API](https://openai.com/api/) को रूपमा उपलब्ध छन्।

## प्रम्प्ट इन्जिनियरिङ

GPT लाई भाषा र कोड बुझ्नको लागि विशाल डाटामा प्रशिक्षण दिइएकोले, तिनीहरूले इनपुट (प्रम्प्ट) को जवाफमा आउटपुट प्रदान गर्छन्। प्रम्प्टहरू GPT का इनपुट वा प्रश्नहरू हुन् जसमा कसैले मोडेललाई कार्यहरू पूरा गर्न निर्देशन दिन्छ। इच्छित परिणाम प्राप्त गर्न, तपाईंलाई सबैभन्दा प्रभावकारी प्रम्प्ट चाहिन्छ, जसमा सही शब्द, ढाँचा, वाक्यांश वा प्रतीकहरू चयन गर्नुपर्छ। यस दृष्टिकोणलाई [प्रम्प्ट इन्जिनियरिङ](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) भनिन्छ।

[यस दस्तावेजले](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) तपाईंलाई प्रम्प्ट इन्जिनियरिङको बारेमा थप जानकारी प्रदान गर्दछ।

## ✍️ उदाहरण नोटबुक: [OpenAI-GPT सँग खेल्ने](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

तपाईं आफ्नो अध्ययनलाई निम्न नोटबुकहरूमा जारी राख्न सक्नुहुन्छ:

* [OpenAI-GPT र Hugging Face Transformers प्रयोग गरेर पाठ उत्पादन](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## निष्कर्ष

नयाँ सामान्य पूर्व-प्रशिक्षित भाषा मोडेलहरूले केवल भाषा संरचनालाई मोडेल गर्दैनन्, तर विशाल मात्रामा प्राकृतिक भाषा पनि समावेश गर्छन्। यसरी, तिनीहरूलाई शून्य-शट वा थोरै-शट सेटिङहरूमा केही NLP कार्यहरू समाधान गर्न प्रभावकारी रूपमा प्रयोग गर्न सकिन्छ।

## [पश्च-व्याख्यान प्रश्नोत्तरी](https://ff-quizzes.netlify.app/en/ai/quiz/40)

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव शुद्धताको प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छन्। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।