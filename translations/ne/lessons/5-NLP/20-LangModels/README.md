<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T07:23:41+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "ne"
}
-->
# प्रि-ट्रेन गरिएको ठूला भाषा मोडेलहरू

हाम्रा अघिल्ला सबै कार्यहरूमा, हामीले कुनै निश्चित कार्य गर्न तर्फ लक्षित डेटासेट प्रयोग गरेर न्यूरल नेटवर्कलाई प्रशिक्षण गरिरहेका थियौं। ठूला ट्रान्सफर्मर मोडेलहरू, जस्तै BERT, मा हामी भाषा मोडेलिङलाई आत्म-नियन्त्रित तरिकामा प्रयोग गरेर भाषा मोडेल निर्माण गर्छौं, जसलाई पछि थप डोमेन-विशिष्ट प्रशिक्षणद्वारा विशेष कार्यका लागि अनुकूलित गरिन्छ। तर, यो प्रमाणित गरिएको छ कि ठूला भाषा मोडेलहरूले कुनै पनि डोमेन-विशिष्ट प्रशिक्षण बिना नै धेरै कार्यहरू समाधान गर्न सक्छन्। यस्तो मोडेलहरूको परिवारलाई **GPT**: जेनेरेटिभ प्रि-ट्रेन गरिएको ट्रान्सफर्मर भनिन्छ।

## [प्री-लेक्चर क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## टेक्स्ट जेनेरेसन र पर्प्लेक्सिटी

न्यूरल नेटवर्कले कुनै पनि विशेष कार्य बिना सामान्य कार्यहरू गर्न सक्ने विचार [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) पेपरमा प्रस्तुत गरिएको छ। मुख्य विचार भनेको धेरै अन्य कार्यहरू **टेक्स्ट जेनेरेसन** प्रयोग गरेर मोडेल गर्न सकिन्छ, किनभने टेक्स्ट बुझ्नु भनेको त्यसलाई उत्पादन गर्न सक्षम हुनु हो। मोडेललाई मानव ज्ञान समेटिएको ठूलो मात्रामा टेक्स्टमा प्रशिक्षण दिइएकोले, यसले विभिन्न विषयहरूबारे ज्ञानसम्पन्न पनि हुन्छ।

> टेक्स्ट बुझ्नु र उत्पादन गर्न सक्षम हुनु भनेको वरपरको संसारबारे केही जान्नु पनि हो। मानिसहरू पनि ठूलो हदसम्म पढेर सिक्छन्, र GPT नेटवर्क यस मामिलामा समान छ।

टेक्स्ट जेनेरेसन नेटवर्कहरूले अर्को शब्दको सम्भावना $$P(w_N)$$ भविष्यवाणी गरेर काम गर्छ। तर, अर्को शब्दको बिना सर्त सम्भावना भनेको टेक्स्ट कर्पसमा उक्त शब्दको आवृत्ति हो। GPT ले हामीलाई अघिल्लो शब्दहरू दिइएको अवस्थामा अर्को शब्दको **सर्त सम्भावना** दिन सक्षम छ: $$P(w_N | w_{n-1}, ..., w_0)$$

> सम्भावनाहरूको बारेमा थप जान्नको लागि हाम्रो [Data Science for Beginners Curriculum](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) हेर्नुहोस्।

भाषा उत्पादन गर्ने मोडेलको गुणस्तर **पर्प्लेक्सिटी** प्रयोग गरेर परिभाषित गर्न सकिन्छ। यो एक आन्तरिक मेट्रिक हो जसले कुनै कार्य-विशिष्ट डेटासेट बिना नै मोडेलको गुणस्तर मापन गर्न अनुमति दिन्छ। यो *वाक्यको सम्भावना* को धारणा आधारित छ - मोडेलले वास्तविक हुने सम्भावना भएको वाक्यलाई उच्च सम्भावना दिन्छ (अर्थात् मोडेल त्यसबाट **पर्प्लेक्स्ड** हुँदैन), र कम सम्भावना ती वाक्यहरूलाई दिन्छ जसले कम अर्थ राख्छन् (जस्तै *Can it does what?*)। जब हामीले हाम्रो मोडेललाई वास्तविक टेक्स्ट कर्पसबाट वाक्यहरू दिन्छौं, हामी ती वाक्यहरूलाई उच्च सम्भावना र कम **पर्प्लेक्सिटी** भएको अपेक्षा गर्छौं। गणितीय रूपमा, यो परीक्षण सेटको सामान्यीकृत उल्टो सम्भावनाको रूपमा परिभाषित गरिएको छ:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**तपाईं [GPT-powered text editor from Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)** प्रयोग गरेर टेक्स्ट जेनेरेसनमा प्रयोग गर्न सक्नुहुन्छ। यस एडिटरमा, तपाईं आफ्नो टेक्स्ट लेख्न सुरु गर्नुहोस्, र **[TAB]** थिच्दा तपाईंलाई धेरै कम्प्लिसन विकल्पहरू प्राप्त हुनेछ। यदि ती छोटो छन् वा तपाईं तीबाट सन्तुष्ट हुनुहुन्न भने - फेरि [TAB] थिच्नुहोस्, र तपाईंलाई थप विकल्पहरू प्राप्त हुनेछ, जसमा लामो टेक्स्ट पनि समावेश हुन सक्छ।

## GPT एक परिवार हो

GPT एउटा एकल मोडेल होइन, तर [OpenAI](https://openai.com) द्वारा विकसित र प्रशिक्षण गरिएका मोडेलहरूको संग्रह हो।

GPT मोडेलहरू अन्तर्गत:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|भाषा मोडेल जसमा १.५ अर्बसम्मका प्यारामिटरहरू छन्। | भाषा मोडेल जसमा १७५ अर्बसम्मका प्यारामिटरहरू छन्। | १०० ट्रिलियन प्यारामिटरहरू छन् र यसले छवि र टेक्स्ट इनपुटहरू स्वीकार्छ र टेक्स्ट आउटपुट दिन्छ। |

GPT-3 र GPT-4 मोडेलहरू [Microsoft Azure बाट एक कग्निटिभ सेवा](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) को रूपमा उपलब्ध छन्, र [OpenAI API](https://openai.com/api/) को रूपमा पनि।

## प्रम्प्ट इन्जिनियरिङ

GPT लाई भाषा र कोड बुझ्नको लागि ठूलो मात्रामा डेटा प्रशिक्षण दिइएकोले, यसले इनपुट (प्रम्प्ट) को प्रतिक्रिया स्वरूप आउटपुट प्रदान गर्छ। प्रम्प्ट भनेको GPT इनपुट वा क्वेरी हो जहाँ एकले मोडेललाई कार्यहरू पूरा गर्न निर्देशन दिन्छ। इच्छित परिणाम प्राप्त गर्न, तपाईंलाई सबैभन्दा प्रभावकारी प्रम्प्ट चाहिन्छ, जसमा सही शब्दहरू, ढाँचा, वाक्यांशहरू वा प्रतीकहरू चयन गर्न समावेश हुन्छ। यस दृष्टिकोणलाई [Prompt Engineering](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) भनिन्छ।

[यस दस्तावेज](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) मा प्रम्प्ट इन्जिनियरिङको बारेमा थप जानकारी प्रदान गरिएको छ।

## ✍️ उदाहरण नोटबुक: [OpenAI-GPT सँग खेल्ने](GPT-PyTorch.ipynb)

तपाईं आफ्नो सिकाइलाई निम्न नोटबुकहरूमा जारी राख्न सक्नुहुन्छ:

* [OpenAI-GPT र Hugging Face Transformers प्रयोग गरेर टेक्स्ट उत्पादन गर्ने](GPT-PyTorch.ipynb)

## निष्कर्ष

नयाँ सामान्य प्रि-ट्रेन गरिएको भाषा मोडेलहरूले केवल भाषा संरचना मोडेल मात्र गर्दैनन्, तर ठूलो मात्रामा प्राकृतिक भाषा पनि समावेश गर्छन्। यसैले, तिनीहरूलाई शून्य-शट वा थोरै-शट सेटिङहरूमा केही NLP कार्यहरू समाधान गर्न प्रभावकारी रूपमा प्रयोग गर्न सकिन्छ।

## [पोस्ट-लेक्चर क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

