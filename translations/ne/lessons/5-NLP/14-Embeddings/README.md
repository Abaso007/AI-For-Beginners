<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-26T08:16:07+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "ne"
}
-->
# एम्बेडिङ्स

## [पूर्व-व्याख्यान क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/27)

जब हामी BoW वा TF/IDF आधारित वर्गीकरणकर्ताहरूलाई प्रशिक्षण गरिरहेका थियौं, हामी उच्च-आयामीय bag-of-words भेक्टरहरूमा काम गरिरहेका थियौं जसको लम्बाइ `vocab_size` थियो, र हामी कम-आयामीय positional representation भेक्टरहरूलाई sparse one-hot representation मा स्पष्ट रूपमा रूपान्तरण गरिरहेका थियौं। तर, यो one-hot representation स्मृति-कुशल छैन। साथै, प्रत्येक शब्दलाई एकअर्काबाट स्वतन्त्र रूपमा व्यवहार गरिन्छ, अर्थात् one-hot encoded भेक्टरहरूले शब्दहरू बीचको कुनै पनि अर्थपूर्ण समानता व्यक्त गर्दैनन्।

**एम्बेडिङ** को विचार भनेको शब्दहरूलाई कम-आयामीय घना भेक्टरहरूद्वारा प्रतिनिधित्व गर्नु हो, जसले कुनै न कुनै रूपमा शब्दको अर्थपूर्ण अर्थलाई प्रतिबिम्बित गर्छ। हामी पछि कसरी अर्थपूर्ण शब्द एम्बेडिङ्स निर्माण गर्ने भन्ने छलफल गर्नेछौं, तर अहिलेको लागि एम्बेडिङ्सलाई शब्द भेक्टरको आयाम घटाउने तरिकाको रूपमा सोचौं।

त्यसैले, एम्बेडिङ लेयरले शब्दलाई इनपुटको रूपमा लिन्छ, र निर्दिष्ट गरिएको `embedding_size` को आउटपुट भेक्टर उत्पादन गर्छ। यो केही हदसम्म `Linear` लेयरसँग धेरै मिल्दोजुल्दो छ, तर यो one-hot encoded भेक्टर लिने सट्टा, शब्द नम्बरलाई इनपुटको रूपमा लिन सक्षम हुनेछ, जसले हामीलाई ठूलो one-hot-encoded भेक्टरहरू सिर्जना गर्नबाट जोगाउँछ।

हाम्रो वर्गीकरणकर्ता नेटवर्कमा पहिलो लेयरको रूपमा एम्बेडिङ लेयर प्रयोग गरेर, हामी bag-of-words बाट **embedding bag** मोडेलमा स्विच गर्न सक्छौं, जहाँ हामी पहिलो पटक हाम्रो पाठमा प्रत्येक शब्दलाई सम्बन्धित एम्बेडिङमा रूपान्तरण गर्छौं, र त्यसपछि ती सबै एम्बेडिङ्समा कुनै समग्र कार्य जस्तै `sum`, `average` वा `max` गणना गर्छौं।  

![पाँच अनुक्रम शब्दहरूको लागि एम्बेडिङ वर्गीकरणकर्ता देखाउने छवि।](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ne.png)

> लेखकद्वारा प्रदान गरिएको छवि

## ✍️ अभ्यास: एम्बेडिङ्स

तपाईंको सिकाइलाई निम्न नोटबुकहरूमा जारी राख्नुहोस्:
* [PyTorch सँग एम्बेडिङ्स](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlow सँग एम्बेडिङ्स](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## अर्थपूर्ण एम्बेडिङ्स: Word2Vec

यद्यपि एम्बेडिङ लेयरले शब्दहरूलाई भेक्टर प्रतिनिधित्वमा म्याप गर्न सिक्यो, यो प्रतिनिधित्वले अनिवार्य रूपमा धेरै अर्थपूर्ण अर्थ राख्दैन। यस्तो भेक्टर प्रतिनिधित्व सिक्न राम्रो हुनेछ जसमा समान शब्दहरू वा पर्यायवाची शब्दहरू भेक्टर दूरी (जस्तै, Euclidean दूरी) को सन्दर्भमा एकअर्काको नजिक हुन्छन्।

यसका लागि, हामीले हाम्रो एम्बेडिङ मोडेललाई ठूलो पाठ संग्रहमा विशेष तरिकाले पूर्व-प्रशिक्षण गर्न आवश्यक छ। अर्थपूर्ण एम्बेडिङ्स प्रशिक्षण गर्ने एउटा तरिका [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) भनिन्छ। यो दुई मुख्य आर्किटेक्चरहरूमा आधारित छ, जसले शब्दहरूको वितरित प्रतिनिधित्व उत्पादन गर्न प्रयोग गरिन्छ:

 - **Continuous bag-of-words** (CBoW) — यस आर्किटेक्चरमा, हामीले मोडेललाई वरपरको सन्दर्भबाट शब्दको भविष्यवाणी गर्न प्रशिक्षण दिन्छौं। दिइएको ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, मोडेलको लक्ष्य $(W_{-2},W_{-1},W_1,W_2)$ बाट $W_0$ को भविष्यवाणी गर्नु हो।
 - **Continuous skip-gram** CBoW को विपरीत हो। मोडेलले सन्दर्भ शब्दहरूको वरपरको झ्याल प्रयोग गरेर वर्तमान शब्दको भविष्यवाणी गर्छ।

CBoW छिटो छ, जबकि skip-gram ढिलो छ, तर दुर्लभ शब्दहरूको प्रतिनिधित्व गर्न राम्रो काम गर्छ।

![शब्दहरूलाई भेक्टरमा रूपान्तरण गर्न CBoW र Skip-Gram एल्गोरिदम देखाउने छवि।](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ne.png)

> [यस पेपर](https://arxiv.org/pdf/1301.3781.pdf) बाट लिइएको छवि

Word2Vec पूर्व-प्रशिक्षित एम्बेडिङ्स (र GloVe जस्ता अन्य समान मोडेलहरू) पनि न्यूरल नेटवर्कहरूमा एम्बेडिङ लेयरको सट्टा प्रयोग गर्न सकिन्छ। तर, हामीले शब्दावलीहरूसँग व्यवहार गर्न आवश्यक छ, किनभने Word2Vec/GloVe लाई पूर्व-प्रशिक्षण गर्न प्रयोग गरिएको शब्दावली हाम्रो पाठ संग्रहको शब्दावलीसँग फरक हुन सक्छ। माथिका नोटबुकहरूमा हेर्नुहोस् कि यो समस्या कसरी समाधान गर्न सकिन्छ।

## सन्दर्भगत एम्बेडिङ्स

Word2Vec जस्ता परम्परागत पूर्व-प्रशिक्षित एम्बेडिङ प्रतिनिधित्वहरूको एउटा प्रमुख सीमा भनेको शब्द अर्थ अस्पष्टताको समस्या हो। यद्यपि पूर्व-प्रशिक्षित एम्बेडिङ्सले सन्दर्भमा शब्दहरूको केही अर्थ समात्न सक्छ, शब्दको प्रत्येक सम्भावित अर्थ एउटै एम्बेडिङमा कोड गरिएको हुन्छ। यसले डाउनस्ट्रीम मोडेलहरूमा समस्या उत्पन्न गर्न सक्छ, किनभने धेरै शब्दहरू, जस्तै 'play', सन्दर्भमा निर्भर गर्दै फरक अर्थ राख्छन्।

उदाहरणका लागि, 'play' शब्दले यी दुई वाक्यहरूमा धेरै फरक अर्थ राख्छ:

- म थिएटरमा **play** हेर्न गएँ।
- जोन आफ्ना साथीहरूसँग **play** गर्न चाहन्छ।

माथिका पूर्व-प्रशिक्षित एम्बेडिङ्सले 'play' शब्दको यी दुवै अर्थलाई एउटै एम्बेडिङमा प्रतिनिधित्व गर्छ। यस सीमालाई पार गर्न, हामीले **भाषा मोडेल** मा आधारित एम्बेडिङ्स निर्माण गर्न आवश्यक छ, जुन ठूलो पाठ संग्रहमा प्रशिक्षण गरिएको हुन्छ, र *जान्दछ* कि शब्दहरू विभिन्न सन्दर्भहरूमा कसरी राख्न सकिन्छ। सन्दर्भगत एम्बेडिङ्सको चर्चा यस ट्युटोरियलको दायराभन्दा बाहिर छ, तर हामी यसबारे पछि भाषिक मोडेलहरूको चर्चा गर्दा फर्कनेछौं।

## निष्कर्ष

यस पाठमा, तपाईंले TensorFlow र PyTorch मा एम्बेडिङ लेयरहरू निर्माण र प्रयोग गरेर शब्दहरूको अर्थपूर्ण अर्थलाई राम्रोसँग प्रतिबिम्बित गर्ने तरिका पत्ता लगाउनुभयो।

## 🚀 चुनौती

Word2Vec केही रोचक अनुप्रयोगहरूको लागि प्रयोग गरिएको छ, जस्तै गीतका शब्दहरू र कविता सिर्जना गर्नु। [यस लेख](https://www.politetype.com/blog/word2vec-color-poems) मा हेर्नुहोस्, जसले लेखकले Word2Vec प्रयोग गरेर कविता कसरी सिर्जना गरे भन्ने देखाउँछ। [Dan Shiffmann द्वारा यो भिडियो](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) पनि हेर्नुहोस्, जसले यस प्रविधिको फरक व्याख्या पत्ता लगाउन मद्दत गर्छ। त्यसपछि यी प्रविधिहरूलाई आफ्नो पाठ संग्रहमा लागू गर्न प्रयास गर्नुहोस्, सम्भवतः Kaggle बाट स्रोत गरिएको।

## [पश्च-व्याख्यान क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## समीक्षा र आत्म-अध्ययन

Word2Vec मा यो पेपर पढ्नुहोस्: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [असाइनमेन्ट: नोटबुकहरू](assignment.md)

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव शुद्धताको प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छन्। मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।