{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## एम्बेडिङहरू\n",
    "\n",
    "हाम्रो अघिल्लो उदाहरणमा, हामी उच्च-आयामीय bag-of-words भेक्टरहरूमा काम गर्यौं जसको लम्बाइ `vocab_size` थियो, र हामीले कम-आयामीय positional representation भेक्टरहरूलाई sparse one-hot representation मा स्पष्ट रूपमा परिवर्तन गर्यौं। यो one-hot representation मेमोरी-प्रभावकारी छैन। साथै, प्रत्येक शब्दलाई एक-अर्काबाट स्वतन्त्र रूपमा व्यवहार गरिन्छ, जसले गर्दा one-hot encoded भेक्टरहरूले शब्दहरू बीचको अर्थपूर्ण समानताहरू व्यक्त गर्दैनन्।\n",
    "\n",
    "यस इकाईमा, हामी **News AG** डेटासेटको अन्वेषण जारी राख्नेछौं। सुरु गर्नका लागि, डेटा लोड गरौं र अघिल्लो इकाईबाट केही परिभाषाहरू प्राप्त गरौं।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### एम्बेडिङ भनेको के हो?\n",
    "\n",
    "**एम्बेडिङ** को विचार भनेको शब्दहरूलाई तिनीहरूको अर्थपूर्ण अर्थ झल्काउने कम-आयामीय घना भेक्टरहरूद्वारा प्रतिनिधित्व गर्नु हो। हामी पछि अर्थपूर्ण शब्द एम्बेडिङ कसरी निर्माण गर्ने भन्ने छलफल गर्नेछौं, तर अहिलेको लागि एम्बेडिङलाई शब्द भेक्टरको आयाम घटाउने तरिका भनेर सोचौं। \n",
    "\n",
    "त्यसैले, एम्बेडिङ लेयरले शब्दलाई इनपुटको रूपमा लिन्छ, र निर्दिष्ट गरिएको `embedding_size` को आउटपुट भेक्टर उत्पादन गर्छ। यो केही हदसम्म `Dense` लेयरसँग मिल्दोजुल्दो छ, तर एक-हट इन्कोड गरिएको भेक्टरलाई इनपुटको रूपमा लिने सट्टा, यो शब्द नम्बर लिन सक्षम छ।\n",
    "\n",
    "हाम्रो नेटवर्कको पहिलो लेयरको रूपमा एम्बेडिङ लेयर प्रयोग गरेर, हामी ब्याग-ऑफ-वर्ड्स मोडेलबाट **एम्बेडिङ ब्याग** मोडेलमा स्विच गर्न सक्छौं, जहाँ हामी पहिलो पटक हाम्रो पाठको प्रत्येक शब्दलाई सम्बन्धित एम्बेडिङमा रूपान्तरण गर्छौं, र त्यसपछि ती सबै एम्बेडिङहरूमा केही समग्र कार्य गणना गर्छौं, जस्तै `sum`, `average` वा `max`।  \n",
    "\n",
    "![पाँच अनुक्रम शब्दहरूको लागि एम्बेडिङ वर्गीकरण देखाउने छवि।](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ne.png)\n",
    "\n",
    "हाम्रो वर्गीकरण न्युरल नेटवर्क निम्न लेयरहरू समावेश गर्दछ:\n",
    "\n",
    "* `TextVectorization` लेयर, जसले स्ट्रिङलाई इनपुटको रूपमा लिन्छ, र टोकन नम्बरहरूको टेन्सर उत्पादन गर्छ। हामी केही उपयुक्त शब्दावली आकार `vocab_size` निर्दिष्ट गर्नेछौं, र कम-प्रयोग गरिएका शब्दहरूलाई बेवास्ता गर्नेछौं। इनपुट आकार 1 हुनेछ, र आउटपुट आकार $n$ हुनेछ, किनभने हामी $n$ टोकनहरू परिणामको रूपमा प्राप्त गर्नेछौं, प्रत्येकमा 0 देखि `vocab_size` सम्मका नम्बरहरू समावेश हुनेछन्।\n",
    "* `Embedding` लेयर, जसले $n$ नम्बरहरू लिन्छ, र प्रत्येक नम्बरलाई दिइएको लम्बाइको घना भेक्टरमा घटाउँछ (हाम्रो उदाहरणमा 100)। यसरी, $n$ आकारको इनपुट टेन्सरलाई $n\\times 100$ टेन्सरमा रूपान्तरण गरिनेछ। \n",
    "* एग्रिगेसन लेयर, जसले पहिलो अक्षको साथमा यस टेन्सरको औसत लिन्छ, अर्थात् यो विभिन्न शब्दहरूका लागि सबै $n$ इनपुट टेन्सरहरूको औसत गणना गर्नेछ। यो लेयर कार्यान्वयन गर्न, हामी `Lambda` लेयर प्रयोग गर्नेछौं, र यसमा औसत गणना गर्ने कार्य पास गर्नेछौं। आउटपुटको आकार 100 हुनेछ, र यो सम्पूर्ण इनपुट अनुक्रमको संख्यात्मक प्रतिनिधित्व हुनेछ।\n",
    "* अन्तिम `Dense` रेखीय वर्गीकरणकर्ता।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary` प्रिन्टआउटमा, **output shape** स्तम्भमा पहिलो टेन्सर आयाम `None` मिनिब्याच आकारलाई जनाउँछ, र दोस्रो आयाम टोकन अनुक्रमको लम्बाइलाई जनाउँछ। मिनिब्याचमा रहेका सबै टोकन अनुक्रमहरूको लम्बाइ फरक-फरक हुन्छ। यसलाई कसरी व्यवस्थापन गर्ने भन्ने कुरा हामी अर्को खण्डमा छलफल गर्नेछौं।\n",
    "\n",
    "अब नेटवर्कलाई प्रशिक्षण गरौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **नोट** कि हामी डेटा को उपसेटको आधारमा भेक्टराइजर निर्माण गर्दैछौं। यो प्रक्रिया छिटो बनाउनको लागि गरिएको हो, र यसले यस्तो स्थिति निम्त्याउन सक्छ जहाँ हाम्रो पाठका सबै टोकनहरू शब्दकोशमा उपस्थित नहुन सक्छन्। यस्तो अवस्थामा, ती टोकनहरू बेवास्ता गरिनेछ, जसले अलिकति कम शुद्धता परिणाम दिन सक्छ। तर, वास्तविक जीवनमा पाठको उपसेटले प्रायः राम्रो शब्दकोश अनुमान प्रदान गर्दछ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### भिन्न आकारका भेरिएबल अनुक्रमहरूसँग व्यवहार गर्ने\n",
    "\n",
    "आउनुहोस्, मिनिब्याचहरूमा प्रशिक्षण कसरी हुन्छ भन्ने कुरा बुझौं। माथिको उदाहरणमा, इनपुट टेन्सरको आयाम 1 छ, र हामी 128-लामो मिनिब्याचहरू प्रयोग गर्छौं, जसले गर्दा टेन्सरको वास्तविक आकार $128 \\times 1$ हुन्छ। तर, प्रत्येक वाक्यमा टोकनहरूको संख्या फरक हुन्छ। यदि हामी `TextVectorization` लेयरलाई एकल इनपुटमा लागू गर्छौं भने, टेक्स्ट कसरी टोकनाइज गरिएको छ भन्ने आधारमा फर्काइने टोकनहरूको संख्या फरक हुन्छ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "तर, जब हामी भेक्टराइजरलाई धेरै अनुक्रमहरूमा लागू गर्छौं, यसले आयताकार आकारको टेन्सर उत्पादन गर्नुपर्छ, त्यसैले यसले प्रयोग नगरिएका तत्वहरूलाई PAD टोकन (जसको हाम्रो अवस्थामा शून्य हो) ले भरिन्छ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "यहाँ हामी एम्बेडिङहरू देख्न सक्छौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **नोट**: प्याडिङको मात्रा कम गर्नको लागि, केही अवस्थामा डेटासेटमा रहेका सबै अनुक्रमहरूलाई बढ्दो लम्बाइको क्रम (वा, अझ स्पष्ट रूपमा, टोकनहरूको संख्या) अनुसार क्रमबद्ध गर्नु उपयुक्त हुन्छ। यसले प्रत्येक मिनिब्याचमा समान लम्बाइका अनुक्रमहरू समावेश हुने सुनिश्चित गर्दछ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## सेम्यान्टिक एम्बेडिङ: Word2Vec\n",
    "\n",
    "हाम्रो अघिल्लो उदाहरणमा, एम्बेडिङ लेयरले शब्दहरूलाई भेक्टर प्रतिनिधित्वमा म्याप गर्न सिकेको थियो, तर ती प्रतिनिधित्वहरूमा सेम्यान्टिक अर्थ थिएन। यदि हामीले यस्तो भेक्टर प्रतिनिधित्व सिक्न सक्यौं जसले समान शब्दहरू वा पर्यायवाची शब्दहरूलाई कुनै भेक्टर दूरी (जस्तै युक्लिडियन दूरी) को आधारमा नजिकको भेक्टरहरूसँग मेल खान्छ भने राम्रो हुने थियो।\n",
    "\n",
    "यसका लागि, हामीले हाम्रो एम्बेडिङ मोडेललाई [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) जस्ता प्रविधि प्रयोग गरेर ठूलो पाठ संग्रहमा प्रि-ट्रेन गर्न आवश्यक छ। यो शब्दहरूको वितरणात्मक प्रतिनिधित्व उत्पादन गर्न प्रयोग गरिने दुई मुख्य आर्किटेक्चरहरूमा आधारित छ:\n",
    "\n",
    " - **कन्टिन्युअस ब्याग-अफ-वर्ड्स** (CBoW), जहाँ हामी मोडेललाई वरपरको सन्दर्भबाट शब्दको भविष्यवाणी गर्न प्रशिक्षण दिन्छौं। दिइएको ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ मा, मोडेलको लक्ष्य $(W_{-2},W_{-1},W_1,W_2)$ बाट $W_0$ को भविष्यवाणी गर्नु हो।\n",
    " - **कन्टिन्युअस स्किप-ग्राम** CBoW को विपरीत हो। मोडेलले वरपरको सन्दर्भ शब्दहरूको झ्याल प्रयोग गरेर वर्तमान शब्दको भविष्यवाणी गर्छ।\n",
    "\n",
    "CBoW छिटो हुन्छ, जबकि स्किप-ग्राम ढिलो भए पनि यो दुर्लभ शब्दहरूको प्रतिनिधित्व गर्न राम्रो काम गर्छ।\n",
    "\n",
    "![CBoW र स्किप-ग्राम एल्गोरिदमहरूलाई शब्दहरूलाई भेक्टरमा रूपान्तरण गर्न देखाउने छवि।](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ne.png)\n",
    "\n",
    "Google News डेटासेटमा प्रि-ट्रेन गरिएको Word2Vec एम्बेडिङसँग प्रयोग गर्न, हामी **gensim** लाइब्रेरी प्रयोग गर्न सक्छौं। तल हामी 'neural' शब्दसँग सबैभन्दा मिल्दोजुल्दो शब्दहरू फेला पार्छौं।\n",
    "\n",
    "> **Note:** जब तपाईं पहिलो पटक शब्द भेक्टरहरू सिर्जना गर्नुहुन्छ, तिनीहरू डाउनलोड गर्न केही समय लाग्न सक्छ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "हामी शब्दबाट भेक्टर एम्बेडिङ पनि निकाल्न सक्छौं, जसलाई वर्गीकरण मोडेल प्रशिक्षणमा प्रयोग गर्न सकिन्छ। एम्बेडिङमा ३०० घटकहरू छन्, तर यहाँ स्पष्टताको लागि हामी भेक्टरका पहिलो २० घटकहरू मात्र देखाउँछौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "महान कुरा सेमान्टिक एम्बेडिङ्सको बारेमा भनेको तपाईंले सेमान्टिक्सको आधारमा भेक्टर इनकोडिङलाई परिमार्जन गर्न सक्नुहुन्छ। उदाहरणका लागि, हामी *राजा* र *महिला* शब्दहरूको भेक्टर प्रतिनिधित्वसँग सकेसम्म नजिक र *पुरुष* शब्दबाट सकेसम्म टाढा हुने शब्द खोज्न अनुरोध गर्न सक्छौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "उपरोक्त उदाहरणले केही आन्तरिक GenSym जादू प्रयोग गर्दछ, तर आधारभूत तर्क वास्तवमै धेरै सरल छ। एम्बेडिङहरूको बारेमा रोचक कुरा यो हो कि तपाईं एम्बेडिङ भेक्टरहरूमा सामान्य भेक्टर अपरेशनहरू गर्न सक्नुहुन्छ, र त्यसले शब्दको **अर्थहरू**मा अपरेशनहरू प्रतिबिम्बित गर्दछ। उपरोक्त उदाहरणलाई भेक्टर अपरेशनहरूको रूपमा व्यक्त गर्न सकिन्छ: हामी **KING-MAN+WOMAN** सँग सम्बन्धित भेक्टर गणना गर्छौं (सम्बन्धित शब्दहरूको भेक्टर प्रतिनिधित्वमा `+` र `-` अपरेशनहरू गरिन्छ), र त्यसपछि उक्त भेक्टरसँग नजिकको शब्द शब्दकोशमा फेला पार्छौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: हामीले *man* र *woman* भेक्टरहरूमा सानो गुणांक थप्नुपरेको थियो - यसलाई हटाएर के हुन्छ हेर्न प्रयास गर्नुहोस्।\n",
    "\n",
    "निकटतम भेक्टर पत्ता लगाउन, हामीले TensorFlow को मेकानिजम प्रयोग गरेर हाम्रो भेक्टर र शब्दकोशमा रहेका सबै भेक्टरहरू बीचको दूरीको भेक्टर गणना गर्छौं, र त्यसपछि `argmin` प्रयोग गरेर न्यूनतम शब्दको सूचकांक पत्ता लगाउँछौं।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec शब्दार्थलाई व्यक्त गर्नको लागि एक उत्कृष्ट तरिका जस्तो देखिन्छ, तर यसमा निम्न जस्ता धेरै कमजोरीहरू छन्:\n",
    "\n",
    "* CBoW र skip-gram मोडेलहरू **predictive embeddings** हुन्, र यीले केवल स्थानीय सन्दर्भलाई मात्र ध्यानमा राख्छन्। Word2Vec ले विश्वव्यापी सन्दर्भको फाइदा उठाउँदैन।\n",
    "* Word2Vec ले शब्दको **morphology** लाई ध्यानमा राख्दैन, अर्थात् शब्दको अर्थ शब्दका विभिन्न भागहरू, जस्तै मूल (root), मा निर्भर हुन सक्छ भन्ने तथ्यलाई बेवास्ता गर्छ।  \n",
    "\n",
    "**FastText** ले दोस्रो सीमालाई पार गर्न प्रयास गर्छ, र Word2Vec मा आधारित भएर प्रत्येक शब्द र त्यस शब्दभित्र पाइने character n-grams का लागि भेक्टर प्रतिनिधित्वहरू सिक्छ। यी प्रतिनिधित्वहरूको मानहरूलाई प्रत्येक प्रशिक्षण चरणमा एक भेक्टरमा औसत गरिन्छ। यसले पूर्व-प्रशिक्षणमा धेरै अतिरिक्त गणना थपे पनि, यसले शब्द embeddings लाई उप-शब्द जानकारीलाई समेट्न सक्षम बनाउँछ।\n",
    "\n",
    "अर्को विधि, **GloVe**, शब्द embeddings को लागि फरक दृष्टिकोण प्रयोग गर्छ, जुन word-context matrix को factorization मा आधारित छ। सुरुमा, यो एउटा ठूलो म्याट्रिक्स बनाउँछ जसले विभिन्न सन्दर्भहरूमा शब्दहरूको उपस्थितिको संख्या गन्छ, र त्यसपछि यो म्याट्रिक्सलाई कम आयाममा प्रतिनिधित्व गर्न प्रयास गर्छ जसले पुनर्निर्माण हानिलाई न्यूनतम बनाउँछ।\n",
    "\n",
    "gensim पुस्तकालयले ती शब्द embeddings लाई समर्थन गर्छ, र तपाईंले माथिको मोडेल लोड गर्ने कोड परिवर्तन गरेर तिनीहरूसँग प्रयोग गर्न सक्नुहुन्छ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras मा pretrained embeddings प्रयोग गर्ने\n",
    "\n",
    "हामी माथिको उदाहरणलाई संशोधन गरेर हाम्रो embedding layer मा semantic embeddings, जस्तै Word2Vec, प्रयोग गर्न सक्दछौं। pretrained embedding र text corpus को शब्दावलीहरू प्रायः मेल नखाने सम्भावना हुन्छ, त्यसैले हामीलाई एउटा विकल्प छान्नुपर्छ। यहाँ हामी दुई सम्भावित विकल्पहरू अन्वेषण गर्छौं: tokenizer vocabulary प्रयोग गर्ने, र Word2Vec embeddings को vocabulary प्रयोग गर्ने।\n",
    "\n",
    "### Tokenizer vocabulary प्रयोग गर्ने\n",
    "\n",
    "Tokenizer vocabulary प्रयोग गर्दा, vocabulary का केही शब्दहरूको Word2Vec embeddings हुनेछ, र केही हराउनेछन्। हाम्रो vocabulary size `vocab_size` छ, र Word2Vec embedding vector को लम्बाइ `embed_size` छ भने, embedding layer को तौल म्याट्रिक्सको आकार `vocab_size`$\\times$`embed_size` हुनेछ। हामी यो म्याट्रिक्सलाई vocabulary मार्फत गएर भरनेछौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "शब्दहरू जुन Word2Vec शब्दकोशमा उपलब्ध छैनन्, हामी तिनीहरूलाई शून्यको रूपमा छोड्न सक्छौं, वा एउटा र्यान्डम भेक्टर सिर्जना गर्न सक्छौं।\n",
    "\n",
    "अब हामी प्रीट्रेन गरिएको वजनहरू सहितको एम्बेडिङ लेयर परिभाषित गर्न सक्छौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: हामीले `Embedding` सिर्जना गर्दा `trainable=False` सेट गरेको देखिन्छ, जसको अर्थ हामी Embedding लेयरलाई पुनः प्रशिक्षण गरिरहेका छैनौं। यसले सटीकता अलि कम हुन सक्छ, तर प्रशिक्षणको गति बढाउँछ।\n",
    "\n",
    "### Embedding शब्दकोश प्रयोग गर्दै\n",
    "\n",
    "अघिल्लो विधिको एउटा समस्या भनेको TextVectorization र Embedding मा प्रयोग गरिएका शब्दकोशहरू फरक हुनु हो। यस समस्यालाई समाधान गर्न, हामी निम्न विकल्पहरू प्रयोग गर्न सक्छौं:\n",
    "* हाम्रो शब्दकोशमा Word2Vec मोडेल पुनः प्रशिक्षण गर्नुहोस्।\n",
    "* Pretrained Word2Vec मोडेलको शब्दकोशबाट हाम्रो डेटासेट लोड गर्नुहोस्। डेटासेट लोड गर्दा प्रयोग गरिने शब्दकोश लोड गर्ने क्रममा निर्दिष्ट गर्न सकिन्छ।\n",
    "\n",
    "दोस्रो विधि सजिलो देखिन्छ, त्यसैले यसलाई कार्यान्वयन गरौं। सबैभन्दा पहिले, हामी Word2Vec embeddings बाट लिइएको निर्दिष्ट शब्दकोशसहित `TextVectorization` लेयर सिर्जना गर्नेछौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim शब्द एम्बेडिङ्स पुस्तकालयमा एउटा सुविधाजनक फङ्सन, `get_keras_embeddings`, समावेश छ, जसले तपाईंको लागि स्वचालित रूपमा सम्बन्धित Keras एम्बेडिङ्स तह सिर्जना गर्नेछ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "हामीले उच्च सटीकता देख्न नसक्नुको एक कारण भनेको हाम्रो डाटासेटका केही शब्दहरू प्रि-ट्रेन गरिएको GloVe शब्दकोशमा नभएका कारण हो, जसले गर्दा ती शब्दहरूलाई मूलतः बेवास्ता गरिन्छ। यसलाई समाधान गर्नका लागि, हामी हाम्रो डाटासेटमा आधारित आफ्नै एम्बेडिङहरू प्रशिक्षण गर्न सक्छौं।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## सन्दर्भात्मक एम्बेडिङहरू\n",
    "\n",
    "पारम्परिक प्रीट्रेन गरिएको एम्बेडिङ प्रतिनिधित्वहरू जस्तै Word2Vec को मुख्य सीमितता यो हो कि, यद्यपि तिनीहरूले शब्दको केही अर्थ समात्न सक्छन्, तिनीहरूले विभिन्न अर्थहरू बीच फरक छुट्याउन सक्दैनन्। यसले डाउनस्ट्रीम मोडेलहरूमा समस्या उत्पन्न गर्न सक्छ।\n",
    "\n",
    "उदाहरणका लागि, 'play' शब्दको यी दुई वाक्यहरूमा फरक अर्थ छ:\n",
    "- म थिएटरमा एउटा **play** हेर्न गएँ।\n",
    "- जोन आफ्ना साथीहरूसँग **play** गर्न चाहन्छ।\n",
    "\n",
    "हामीले चर्चा गरेका प्रीट्रेन गरिएको एम्बेडिङहरूले 'play' शब्दको दुवै अर्थलाई एउटै एम्बेडिङमा प्रतिनिधित्व गर्छ। यस सीमिततालाई पार गर्नका लागि, हामीलाई **भाषा मोडेल** मा आधारित एम्बेडिङहरू निर्माण गर्न आवश्यक छ, जुन ठूलो पाठको संग्रहमा प्रशिक्षित गरिएको हुन्छ, र *जान्दछ* कि शब्दहरू विभिन्न सन्दर्भहरूमा कसरी राख्न सकिन्छ। सन्दर्भात्मक एम्बेडिङहरूको चर्चा यस ट्युटोरियलको दायराभन्दा बाहिर छ, तर हामी यसबारे भाषा मोडेलहरूको कुरा गर्दा अर्को युनिटमा फर्कनेछौं।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको छ। हामी शुद्धताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T09:43:15+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}