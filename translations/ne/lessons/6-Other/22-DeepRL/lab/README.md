<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7bd8dc72040e98e35e7225e34058cd4e",
  "translation_date": "2025-08-26T10:17:01+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/lab/README.md",
  "language_code": "ne"
}
-->
## वातावरण

माउन्टेन कार वातावरणमा कार एउटा उपत्यकाभित्र फसेको छ। तपाईंको लक्ष्य उपत्यकाबाट बाहिर निस्केर झण्डासम्म पुग्नु हो। तपाईंले गर्न सक्ने क्रियाकलापहरूमा बायाँतर्फ गति बढाउनु, दायाँतर्फ गति बढाउनु, वा केही नगर्नु समावेश छ। तपाईंले कारको x-अक्षमा स्थिति र वेगलाई अवलोकन गर्न सक्नुहुन्छ।

## नोटबुक सुरु गर्दै

यो प्रयोगशाला [MountainCar.ipynb](../../../../../../lessons/6-Other/22-DeepRL/lab/MountainCar.ipynb) खोल्दै सुरु गर्नुहोस्।

## मुख्य कुरा

यस प्रयोगशालामा तपाईंले सिक्नुपर्ने कुरा के हो भने RL एल्गोरिदमलाई नयाँ वातावरणमा अपनाउनु प्रायः सरल हुन्छ, किनकि OpenAI Gym का सबै वातावरणहरूको इन्टरफेस समान छ, र एल्गोरिदमहरू वातावरणको प्रकृतिमा धेरै निर्भर गर्दैनन्। तपाईंले Python को कोडलाई पुनःसंरचना गरेर कुनै पनि वातावरणलाई RL एल्गोरिदममा एउटा प्यारामिटरको रूपमा पास गर्न सक्ने तरिकामा बनाउन सक्नुहुन्छ।

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव शुद्धताको प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुन सक्छ। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।