<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T07:15:08+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ne"
}
-->
# डीप रिइन्फोर्समेन्ट लर्निङ

रिइन्फोर्समेन्ट लर्निङ (RL) लाई सुपरभाइज्ड लर्निङ र अनसुपरभाइज्ड लर्निङसँगै आधारभूत मेसिन लर्निङको एक प्रकारको रूपमा हेरिन्छ। जहाँ सुपरभाइज्ड लर्निङमा हामीलाई निश्चित परिणामसहितको डाटासेट चाहिन्छ, RL भने **गरि सिक्ने** प्रक्रियामा आधारित छ। उदाहरणका लागि, जब हामीले पहिलो पटक कम्प्युटर गेम देख्छौं, हामी खेल्न थाल्छौं, नियमहरू नजाने पनि। खेल्दै जाँदा र आफ्नो व्यवहार समायोजन गर्दै, हामी आफ्नो सीप सुधार गर्न सक्षम हुन्छौं।

## [प्री-लेक्चर क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL गर्नका लागि हामीलाई चाहिन्छ:

* **पर्यावरण** वा **सिमुलेटर**, जसले खेलका नियमहरू सेट गर्छ। हामीले सिमुलेटरमा प्रयोगहरू चलाउन र परिणामहरू अवलोकन गर्न सक्षम हुनुपर्छ।
* केही **रिवार्ड फङ्क्सन**, जसले हाम्रो प्रयोग कत्तिको सफल भयो भन्ने संकेत दिन्छ। कम्प्युटर गेम खेल्न सिक्ने सन्दर्भमा, रिवार्ड हाम्रो अन्तिम स्कोर हुनेछ।

रिवार्ड फङ्क्सनको आधारमा, हामीले आफ्नो व्यवहार समायोजन गर्न र आफ्नो सीप सुधार गर्न सक्षम हुनुपर्छ, ताकि अर्को पटक हामी राम्रो खेल्न सकौं। अन्य प्रकारका मेसिन लर्निङ र RL बीचको मुख्य भिन्नता यो हो कि RL मा हामी सामान्यतया खेल सकिएपछि मात्र जित्ने वा हार्ने कुरा थाहा पाउँछौं। त्यसैले, कुनै निश्चित चाल मात्र राम्रो हो वा होइन भन्ने कुरा भन्न सकिँदैन - हामीलाई खेलको अन्त्यमा मात्र रिवार्ड प्राप्त हुन्छ।

RL को क्रममा, हामी सामान्यतया धेरै प्रयोगहरू गर्छौं। प्रत्येक प्रयोगको क्रममा, हामीले अहिलेसम्म सिकेको उत्तम रणनीति अनुसरण गर्ने (**एक्सप्लोइटेशन**) र नयाँ सम्भावित अवस्थाहरू अन्वेषण गर्ने (**एक्सप्लोरेशन**) बीच सन्तुलन कायम गर्नुपर्छ।

## OpenAI Gym

RL को लागि एक उत्कृष्ट उपकरण [OpenAI Gym](https://gym.openai.com/) हो - एक **सिमुलेशन पर्यावरण**, जसले विभिन्न प्रकारका पर्यावरणहरू सिमुलेट गर्न सक्छ, जस्तै Atari गेमहरूदेखि पोल ब्यालेन्सिङको भौतिकशास्त्रसम्म। यो रिइन्फोर्समेन्ट लर्निङ एल्गोरिदमहरू प्रशिक्षणका लागि सबैभन्दा लोकप्रिय सिमुलेशन पर्यावरणहरूमध्ये एक हो, र [OpenAI](https://openai.com/) द्वारा मर्मत गरिएको छ।

> **Note**: OpenAI Gym बाट उपलब्ध सबै पर्यावरणहरू [यहाँ](https://gym.openai.com/envs/#classic_control) हेर्न सकिन्छ।

## CartPole ब्यालेन्सिङ

तपाईंले आधुनिक ब्यालेन्सिङ उपकरणहरू, जस्तै *Segway* वा *Gyroscooters*, देख्नुभएको हुन सक्छ। यी उपकरणहरूले एक्सेलेरोमिटर वा जाइरोस्कोपबाट सिग्नल प्राप्त गरेर आफ्ना पाङ्ग्राहरू समायोजन गरी स्वतः ब्यालेन्स गर्न सक्षम हुन्छन्। यस खण्डमा, हामी यस्तै समस्या समाधान गर्न सिक्नेछौं - पोल ब्यालेन्सिङ। यो सर्कस कलाकारले आफ्नो हातमा पोल ब्यालेन्स गर्नुपर्ने अवस्थासँग मिल्दोजुल्दो छ - तर यो पोल ब्यालेन्सिङ केवल 1D मा हुन्छ।

ब्यालेन्सिङको सरल संस्करणलाई **CartPole** समस्या भनिन्छ। CartPole संसारमा, हामीसँग एउटा तेर्सो स्लाइडर हुन्छ, जसले बायाँ वा दायाँ सर्न सक्छ, र लक्ष्य भनेको स्लाइडरको माथि ठडिएको पोललाई ब्यालेन्स गर्नु हो।

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

यस पर्यावरण सिर्जना गर्न र प्रयोग गर्नका लागि, हामीलाई केही लाइनको Python कोड चाहिन्छ:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

प्रत्येक पर्यावरणलाई ठीक यही तरिकाले पहुँच गर्न सकिन्छ:
* `env.reset` नयाँ प्रयोग सुरु गर्छ।
* `env.step` सिमुलेशन चरण सञ्चालन गर्छ। यसले **एक्सन**लाई **एक्सन स्पेस**बाट प्राप्त गर्छ, र **अवलोकन** (अवलोकन स्पेसबाट), साथै रिवार्ड र टर्मिनेसन फ्ल्याग फर्काउँछ।

माथिको उदाहरणमा, हामी प्रत्येक चरणमा र्यान्डम एक्सन प्रदर्शन गर्छौं, जसका कारण प्रयोगको आयु धेरै छोटो हुन्छ:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL एल्गोरिदमको लक्ष्य भनेको एक मोडेल - जसलाई **पोलिसी** &pi; भनिन्छ - प्रशिक्षण गर्नु हो, जसले दिइएको अवस्थाको प्रतिक्रिया स्वरूप एक्सन फर्काउँछ। हामी पोलिसीलाई सम्भावनात्मक रूपमा पनि विचार गर्न सक्छौं, जस्तै कुनै पनि अवस्था *s* र एक्सन *a* को लागि, यसले &pi;(*a*|*s*) सम्भावना फर्काउँछ कि हामीले अवस्था *s* मा *a* लिनुपर्छ।

## पोलिसी ग्रेडियन्ट्स एल्गोरिदम

पोलिसी मोडल गर्ने सबैभन्दा स्पष्ट तरिका भनेको एउटा न्युरल नेटवर्क सिर्जना गर्नु हो, जसले अवस्थाहरूलाई इनपुटको रूपमा लिन्छ, र सम्बन्धित एक्सनहरू (वा सबै एक्सनहरूको सम्भावनाहरू) फर्काउँछ। यस अर्थमा, यो सामान्य वर्गीकरण कार्यसँग मिल्दोजुल्दो हुनेछ, तर मुख्य भिन्नता भनेको - हामीलाई प्रत्येक चरणमा कुन एक्सन लिनुपर्छ भन्ने कुरा पहिले नै थाहा हुँदैन।

यहाँको विचार भनेको ती सम्भावनाहरूको अनुमान लगाउनु हो। हामी **क्युमुलेटिभ रिवार्ड्स** को भेक्टर बनाउँछौं, जसले प्रयोगको प्रत्येक चरणमा हाम्रो कुल रिवार्ड देखाउँछ। हामी **रिवार्ड डिस्काउन्टिङ** पनि लागू गर्छौं, जहाँ पहिलेका रिवार्डहरूलाई केही गुणांक &gamma;=0.99 द्वारा गुणा गरिन्छ, ताकि पहिलेका रिवार्डहरूको भूमिका कम गरियोस्। त्यसपछि, हामी ती चरणहरूलाई बलियो बनाउँछौं, जसले ठूलो रिवार्ड दिन्छ।

> पोलिसी ग्रेडियन्ट एल्गोरिदमको बारेमा थप जान्न र यसलाई कार्यमा हेर्न [उदाहरण नोटबुक](CartPole-RL-TF.ipynb) हेर्नुहोस्।

## एक्टर-क्रिटिक एल्गोरिदम

पोलिसी ग्रेडियन्ट्सको सुधारिएको संस्करणलाई **एक्टर-क्रिटिक** भनिन्छ। यसको मुख्य विचार भनेको न्युरल नेटवर्कलाई दुई कुरा फर्काउन प्रशिक्षण दिनु हो:

* पोलिसी, जसले कुन एक्सन लिनुपर्छ भन्ने निर्धारण गर्छ। यस भागलाई **एक्टर** भनिन्छ।
* हामीले यस अवस्थामा प्राप्त गर्न सक्ने कुल रिवार्डको अनुमान - यस भागलाई **क्रिटिक** भनिन्छ।

यस अर्थमा, यो [GAN](../../4-ComputerVision/10-GANs/README.md) जस्तै देखिन्छ, जहाँ दुई नेटवर्कहरू एकअर्काविरुद्ध प्रशिक्षण गरिन्छ। एक्टर-क्रिटिक मोडेलमा, एक्टरले हामीले लिनुपर्ने एक्सन प्रस्ताव गर्छ, र क्रिटिकले आलोचनात्मक भएर परिणामको अनुमान गर्छ। तर, हाम्रो लक्ष्य भनेको यी नेटवर्कहरूलाई एकसाथ प्रशिक्षण दिनु हो।

किनभने हामीलाई प्रयोगको क्रममा वास्तविक क्युमुलेटिभ रिवार्ड्स र क्रिटिकले फर्काएको परिणाम थाहा हुन्छ, यो फरकलाई न्यूनतम बनाउने लस फङ्क्सन निर्माण गर्न तुलनात्मक रूपमा सजिलो हुन्छ। यसले हामीलाई **क्रिटिक लस** दिन्छ। हामी **एक्टर लस** पोलिसी ग्रेडियन्ट एल्गोरिदमको जस्तै दृष्टिकोण प्रयोग गरेर गणना गर्न सक्छौं।

यी एल्गोरिदमहरू चलाएपछि, हामी हाम्रो CartPole लाई यसरी व्यवहार गर्ने अपेक्षा गर्न सक्छौं:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ अभ्यास: पोलिसी ग्रेडियन्ट्स र एक्टर-क्रिटिक RL

निम्न नोटबुकहरूमा आफ्नो सिकाइ जारी राख्नुहोस्:

* [RL in TensorFlow](CartPole-RL-TF.ipynb)
* [RL in PyTorch](CartPole-RL-PyTorch.ipynb)

## अन्य RL कार्यहरू

आजकल रिइन्फोर्समेन्ट लर्निङ अनुसन्धानको तीव्र रूपमा बढ्दो क्षेत्र हो। रिइन्फोर्समेन्ट लर्निङका केही रोचक उदाहरणहरू:

* कम्प्युटरलाई **Atari गेमहरू** खेल्न सिकाउने। यस समस्यामा चुनौतीपूर्ण भाग भनेको हामीसँग साधारण अवस्था भेक्टरको रूपमा नभई स्क्रिनसट हुन्छ, र हामीले CNN प्रयोग गरेर यो स्क्रिन इमेजलाई फिचर भेक्टरमा रूपान्तरण गर्नुपर्छ, वा रिवार्ड जानकारी निकाल्नुपर्छ। Atari गेमहरू Gym मा उपलब्ध छन्।
* कम्प्युटरलाई बोर्ड गेमहरू, जस्तै Chess र Go, खेल्न सिकाउने। हालसालै **Alpha Zero** जस्ता अत्याधुनिक प्रोग्रामहरू दुई एजेन्टहरू एकअर्काविरुद्ध खेल्दै, प्रत्येक चरणमा सुधार गर्दै, स्क्र्याचबाट प्रशिक्षण गरिएका थिए।
* उद्योगमा, RL सिमुलेशनबाट कन्ट्रोल सिस्टमहरू सिर्जना गर्न प्रयोग गरिन्छ। [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) नामक सेवा विशेष रूपमा यसका लागि डिजाइन गरिएको छ।

## निष्कर्ष

हामीले अब एजेन्टहरूलाई खेलको इच्छित अवस्थालाई परिभाषित गर्ने रिवार्ड फङ्क्सन प्रदान गरेर, र उनीहरूलाई बौद्धिक रूपमा खोज स्थान अन्वेषण गर्ने अवसर दिएर राम्रो परिणाम प्राप्त गर्न प्रशिक्षण दिन सिकेका छौं। हामीले दुई एल्गोरिदमहरू सफलतापूर्वक प्रयास गरेका छौं, र तुलनात्मक रूपमा छोटो समय अवधिमा राम्रो परिणाम प्राप्त गरेका छौं। तर, यो RL मा तपाईंको यात्रा मात्र सुरु हो, र यदि तपाईं गहिराइमा जान चाहनुहुन्छ भने छुट्टै पाठ्यक्रम लिन विचार गर्नुपर्छ।

## 🚀 चुनौती

'अन्य RL कार्यहरू' खण्डमा सूचीबद्ध अनुप्रयोगहरू अन्वेषण गर्नुहोस् र एउटा कार्यान्वयन प्रयास गर्नुहोस्!

## [पोस्ट-लेक्चर क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## समीक्षा र आत्म-अध्ययन

हाम्रो [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) मा क्लासिकल रिइन्फोर्समेन्ट लर्निङको बारेमा थप जान्नुहोस्।

कम्प्युटरले सुपर मारियो खेल्न कसरी सिक्न सक्छ भन्ने बारेमा कुरा गर्ने [यो उत्कृष्ट भिडियो](https://www.youtube.com/watch?v=qv6UVOQ0F44) हेर्नुहोस्।

## असाइनमेन्ट: [Train a Mountain Car](lab/README.md)

यस असाइनमेन्टको क्रममा तपाईंको लक्ष्य भनेको Gym को अर्को पर्यावरण - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) प्रशिक्षण गर्नु हुनेछ।

---

