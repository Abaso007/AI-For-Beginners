<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-26T10:14:30+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ne"
}
-->
# गहिरो सुदृढीकरण शिक्षण

सुदृढीकरण शिक्षण (RL) लाई पर्यवेक्षित शिक्षण र अप्रेक्षित शिक्षणसँगै आधारभूत मेसिन शिक्षणको एक प्रकारको रूपमा हेरिन्छ। जहाँ पर्यवेक्षित शिक्षणमा हामी ज्ञात परिणामसहितको डेटासेटमा निर्भर गर्छौं, RL भने **गरि सिक्ने** प्रक्रियामा आधारित छ। उदाहरणका लागि, जब हामी पहिलो पटक कम्प्युटर गेम देख्छौं, हामी खेल्न थाल्छौं, नियमहरू नजाने पनि, र चाँडै नै खेल्ने र आफ्नो व्यवहार समायोजन गर्ने प्रक्रियाबाट आफ्नो सीप सुधार गर्न सक्षम हुन्छौं।

## [पूर्व-व्याख्यान क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL गर्नका लागि हामीलाई चाहिन्छ:

* **पर्यावरण** वा **सिमुलेटर**, जसले खेलको नियमहरू सेट गर्छ। हामीले सिमुलेटरमा प्रयोगहरू चलाउन र परिणामहरू अवलोकन गर्न सक्षम हुनुपर्छ।
* केही **पुरस्कार कार्य**, जसले हाम्रो प्रयोग कत्तिको सफल भयो भनेर संकेत गर्छ। कम्प्युटर गेम खेल्न सिक्ने सन्दर्भमा, पुरस्कार भनेको हाम्रो अन्तिम स्कोर हुनेछ।

पुरस्कार कार्यको आधारमा, हामीले आफ्नो व्यवहार समायोजन गर्न र आफ्नो सीप सुधार गर्न सक्षम हुनुपर्छ, ताकि अर्को पटक हामी राम्रो खेल्न सकौं। अन्य प्रकारका मेसिन शिक्षण र RL बीचको मुख्य भिन्नता भनेको RL मा हामी सामान्यतया खेल समाप्त नभएसम्म जित्ने वा हार्ने कुरा थाहा पाउँदैनौं। त्यसैले, कुनै निश्चित चाल मात्र राम्रो हो वा होइन भनेर भन्न सकिँदैन - हामीलाई खेलको अन्त्यमा मात्र पुरस्कार प्राप्त हुन्छ।

RL को क्रममा, हामी सामान्यतया धेरै प्रयोगहरू गर्छौं। प्रत्येक प्रयोगको क्रममा, हामीले अहिलेसम्म सिकेको उत्तम रणनीति अनुसरण गर्ने (**शोषण**) र नयाँ सम्भावित अवस्थाहरू अन्वेषण गर्ने (**अन्वेषण**) बीच सन्तुलन कायम गर्नुपर्छ।

## OpenAI Gym

RL को लागि एक उत्कृष्ट उपकरण भनेको [OpenAI Gym](https://gym.openai.com/) हो - एक **सिमुलेशन वातावरण**, जसले विभिन्न वातावरणहरू सिमुलेट गर्न सक्छ, जस्तै Atari गेमहरूदेखि पोल ब्यालेन्सिङको भौतिक विज्ञानसम्म। यो सुदृढीकरण शिक्षण एल्गोरिदमहरू प्रशिक्षणका लागि सबैभन्दा लोकप्रिय सिमुलेशन वातावरणहरू मध्ये एक हो, र [OpenAI](https://openai.com/) द्वारा मर्मत गरिएको छ।

> **Note**: OpenAI Gym बाट उपलब्ध सबै वातावरणहरू [यहाँ](https://gym.openai.com/envs/#classic_control) हेर्न सकिन्छ।

## CartPole ब्यालेन्सिङ

तपाईंले आधुनिक ब्यालेन्सिङ उपकरणहरू, जस्तै *Segway* वा *Gyroscooters*, देख्नुभएको हुन सक्छ। तिनीहरूले एक्सेलेरोमीटर वा गाइरोस्कोपबाट सिग्नलको प्रतिक्रिया स्वरूप आफ्ना पाङ्ग्राहरू समायोजन गरेर स्वतः ब्यालेन्स गर्न सक्षम छन्। यस खण्डमा, हामी समान समस्या समाधान गर्न सिक्नेछौं - पोल ब्यालेन्सिङ। यो सर्कस कलाकारले आफ्नो हातमा पोल ब्यालेन्स गर्नुपर्ने अवस्थासँग मिल्दोजुल्दो छ - तर यो पोल ब्यालेन्सिङ केवल 1D मा हुन्छ।

ब्यालेन्सिङको सरल संस्करणलाई **CartPole** समस्या भनेर चिनिन्छ। CartPole संसारमा, हामीसँग एउटा तेर्सो स्लाइडर हुन्छ, जसले बायाँ वा दायाँ सर्न सक्छ, र लक्ष्य भनेको स्लाइडरको माथि ठाडो पोल ब्यालेन्स गर्नु हो।

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

यो वातावरण सिर्जना गर्न र प्रयोग गर्नका लागि, हामीलाई केही लाइनको Python कोड चाहिन्छ:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

प्रत्येक वातावरणलाई ठीक त्यही तरिकाले पहुँच गर्न सकिन्छ:
* `env.reset` नयाँ प्रयोग सुरु गर्छ।
* `env.step` सिमुलेशन चरण प्रदर्शन गर्छ। यसले **action space** बाट **action** प्राप्त गर्छ, र **observation space** बाट **observation**, साथै पुरस्कार र टर्मिनेसन फ्ल्याग फर्काउँछ।

माथिको उदाहरणमा, हामी प्रत्येक चरणमा र्यान्डम कार्य प्रदर्शन गर्छौं, जसका कारण प्रयोगको जीवन धेरै छोटो हुन्छ:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL एल्गोरिदमको लक्ष्य भनेको एक मोडेल - जसलाई **policy** π भनिन्छ - प्रशिक्षण गर्नु हो, जसले दिइएको अवस्थाको प्रतिक्रिया स्वरूप कार्य फर्काउँछ। हामीले पॉलिसीलाई सम्भावनात्मक रूपमा पनि विचार गर्न सक्छौं, जस्तै कुनै पनि अवस्था *s* र कार्य *a* को लागि यो π(*a*|*s*) सम्भावना फर्काउँछ कि हामीले *s* अवस्थामा *a* लिनुपर्छ।

## पॉलिसी ग्रेडियन्ट्स एल्गोरिदम

पॉलिसी मोडल गर्ने सबैभन्दा स्पष्ट तरिका भनेको एउटा न्यूरल नेटवर्क सिर्जना गर्नु हो, जसले अवस्थाहरूलाई इनपुटको रूपमा लिन्छ, र सम्बन्धित कार्यहरू (वा सबै कार्यहरूको सम्भावनाहरू) फर्काउँछ। यस अर्थमा, यो सामान्य वर्गीकरण कार्यसँग मिल्दोजुल्दो हुनेछ, तर मुख्य भिन्नता भनेको - हामीलाई प्रत्येक चरणमा कुन कार्य लिनुपर्छ भनेर पहिले नै थाहा हुँदैन।

यहाँको विचार भनेको ती सम्भावनाहरूको अनुमान लगाउनु हो। हामी **कुल पुरस्कारहरू** को भेक्टर निर्माण गर्छौं, जसले प्रयोगको प्रत्येक चरणमा हाम्रो कुल पुरस्कार देखाउँछ। हामी **पुरस्कार छुट** पनि लागू गर्छौं, प्रारम्भिक पुरस्कारहरूलाई केही गुणांक γ=0.99 द्वारा गुणा गरेर, प्रारम्भिक पुरस्कारहरूको भूमिका घटाउन। त्यसपछि, हामी प्रयोगको मार्गमा ठूला पुरस्कार दिने चरणहरूलाई बलियो बनाउँछौं।

> पॉलिसी ग्रेडियन्ट एल्गोरिदमको बारेमा थप जान्न र यसलाई कार्यमा देख्न [उदाहरण नोटबुक](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) हेर्नुहोस्।

## Actor-Critic एल्गोरिदम

पॉलिसी ग्रेडियन्ट्स दृष्टिकोणको सुधारिएको संस्करणलाई **Actor-Critic** भनिन्छ। यसको मुख्य विचार भनेको न्यूरल नेटवर्कलाई दुई कुरा फर्काउन प्रशिक्षण गर्नु हो:

* पॉलिसी, जसले कुन कार्य लिनुपर्छ भनेर निर्धारण गर्छ। यस भागलाई **actor** भनिन्छ।
* कुल पुरस्कारको अनुमान, जुन हामीले यस अवस्थामा प्राप्त गर्न सक्छौं - यस भागलाई **critic** भनिन्छ।

यस अर्थमा, यो [GAN](../../4-ComputerVision/10-GANs/README.md) जस्तै देखिन्छ, जहाँ दुई नेटवर्कहरू एकअर्काविरुद्ध प्रशिक्षण गरिन्छ। Actor-Critic मोडेलमा, actor ले हामीले लिनुपर्ने कार्य प्रस्ताव गर्छ, र critic आलोचनात्मक भएर परिणामको अनुमान गर्न प्रयास गर्छ। तर, हाम्रो लक्ष्य भनेको ती नेटवर्कहरूलाई एकसाथ प्रशिक्षण गर्नु हो।

किनभने हामीलाई प्रयोगको क्रममा वास्तविक कुल पुरस्कारहरू र critic द्वारा फर्काइएको परिणामहरू दुवै थाहा हुन्छ, यो **critic loss** न्यूनतम बनाउनको लागि एक हानि कार्य निर्माण गर्न तुलनात्मक रूपमा सजिलो हुन्छ। हामी **actor loss** लाई पॉलिसी ग्रेडियन्ट एल्गोरिदममा जस्तै दृष्टिकोण प्रयोग गरेर गणना गर्न सक्छौं।

यी एल्गोरिदमहरू चलाएपछि, हामी हाम्रो CartPole लाई यसरी व्यवहार गर्ने अपेक्षा गर्न सक्छौं:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ अभ्यास: पॉलिसी ग्रेडियन्ट्स र Actor-Critic RL

निम्न नोटबुकहरूमा आफ्नो सिकाइ जारी राख्नुहोस्:

* [TensorFlow मा RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch मा RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## अन्य RL कार्यहरू

आजकल सुदृढीकरण शिक्षण अनुसन्धानको तीव्र रूपमा बढ्दो क्षेत्र हो। सुदृढीकरण शिक्षणका केही रोचक उदाहरणहरू हुन्:

* कम्प्युटरलाई **Atari गेमहरू** खेल्न सिकाउने। यस समस्यामा चुनौतीपूर्ण भाग भनेको हामीसँग भेक्टरको रूपमा सरल अवस्था छैन, तर स्क्रिनशट हुन्छ - र हामीले CNN प्रयोग गरेर यो स्क्रिन छविलाई फिचर भेक्टरमा रूपान्तरण गर्नुपर्छ, वा पुरस्कार जानकारी निकाल्नुपर्छ। Atari गेमहरू Gym मा उपलब्ध छन्।
* कम्प्युटरलाई बोर्ड गेमहरू, जस्तै Chess र Go, खेल्न सिकाउने। हालसालै **Alpha Zero** जस्ता अत्याधुनिक कार्यक्रमहरू दुई एजेन्टहरू एकअर्काविरुद्ध खेल्दै, प्रत्येक चरणमा सुधार गर्दै, सुरुबाटै प्रशिक्षण गरिएका थिए।
* उद्योगमा, RL सिमुलेशनबाट नियन्त्रण प्रणालीहरू सिर्जना गर्न प्रयोग गरिन्छ। [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) नामक सेवा विशेष रूपमा यसको लागि डिजाइन गरिएको छ।

## निष्कर्ष

हामीले अब एजेन्टहरूलाई राम्रो परिणाम प्राप्त गर्न प्रशिक्षण गर्ने तरिका सिकेका छौं, केवल पुरस्कार कार्य प्रदान गरेर, जसले खेलको इच्छित अवस्था परिभाषित गर्छ, र तिनीहरूलाई बुद्धिमानीपूर्वक खोज स्थान अन्वेषण गर्ने अवसर दिएर। हामीले दुई एल्गोरिदमहरू सफलतापूर्वक प्रयास गरेका छौं, र तुलनात्मक रूपमा छोटो समयमा राम्रो परिणाम प्राप्त गरेका छौं। तर, यो RL मा तपाईंको यात्रा मात्र सुरु हो, र यदि तपाईं गहिरो जान चाहनुहुन्छ भने छुट्टै पाठ्यक्रम लिन विचार गर्नुपर्छ।

## 🚀 चुनौती

'अन्य RL कार्यहरू' खण्डमा सूचीबद्ध अनुप्रयोगहरू अन्वेषण गर्नुहोस् र एउटा कार्यान्वयन प्रयास गर्नुहोस्!

## [पश्च-व्याख्यान क्विज](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## समीक्षा र आत्म अध्ययन

हाम्रो [मेसिन शिक्षणको लागि शुरुआती पाठ्यक्रम](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) मा शास्त्रीय सुदृढीकरण शिक्षणको बारेमा थप जान्नुहोस्।

कम्प्युटरले Super Mario खेल्न सिक्ने बारेमा कुरा गर्ने [यो उत्कृष्ट भिडियो](https://www.youtube.com/watch?v=qv6UVOQ0F44) हेर्नुहोस्।

## असाइनमेन्ट: [Mountain Car प्रशिक्षण गर्नुहोस्](lab/README.md)

यस असाइनमेन्टको क्रममा तपाईंको लक्ष्य भनेको फरक Gym वातावरण - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) प्रशिक्षण गर्नु हो।

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको छ। हामी यथार्थताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।