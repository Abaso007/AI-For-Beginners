{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# कार्टपोल सन्तुलन गर्न RL प्रशिक्षण\n",
    "\n",
    "यो नोटबुक [AI for Beginners Curriculum](http://aka.ms/ai-beginners) को हिस्सा हो। यसले [PyTorch को आधिकारिक ट्युटोरियल](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) र [यो Cartpole PyTorch कार्यान्वयन](https://github.com/yc930401/Actor-Critic-pytorch) बाट प्रेरणा लिएको छ।\n",
    "\n",
    "यस उदाहरणमा, हामी RL प्रयोग गरेर एउटा मोडेललाई प्रशिक्षण दिनेछौं जसले एउटा गाडीमा रहेको पोललाई सन्तुलनमा राख्न सक्छ। गाडीले क्षैतिज स्केलमा बायाँ र दायाँ सर्न सक्छ। हामी [OpenAI Gym](https://www.gymlibrary.ml/) वातावरण प्रयोग गरेर पोलको सिमुलेशन गर्नेछौं।\n",
    "\n",
    "> **Note**: तपाईं यो पाठको कोड स्थानीय रूपमा (जस्तै Visual Studio Code बाट) चलाउन सक्नुहुन्छ, जसमा सिमुलेशन नयाँ विन्डोमा खुल्छ। अनलाइन कोड चलाउँदा, तपाईंले कोडमा केही परिवर्तन गर्नुपर्ने हुन सक्छ, जस्तो [यहाँ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) वर्णन गरिएको छ।\n",
    "\n",
    "हामी Gym स्थापना भएको सुनिश्चित गरेर सुरु गर्नेछौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब हामी CartPole वातावरण सिर्जना गरौं र यसमा कसरी काम गर्ने भनेर हेर्नुहोस्। एउटा वातावरणमा निम्न गुणहरू हुन्छन्:\n",
    "\n",
    "* **Action space** भनेको प्रत्येक चरणको सिमुलेशनमा हामीले गर्न सक्ने सम्भावित क्रियाहरूको सेट हो।  \n",
    "* **Observation space** भनेको हामीले गर्न सक्ने अवलोकनहरूको क्षेत्र हो।  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आउनुहोस्, यो सिमुलेशन कसरी काम गर्छ हेर्नुहोस्। तलको लूपले सिमुलेशन चलाउँछ, जबसम्म `env.step` ले समाप्ति संकेत `done` फर्काउँदैन। हामीले `env.action_space.sample()` प्रयोग गरेर क्रियाहरू अनियमित रूपमा चयन गर्नेछौं, जसको अर्थ प्रयोग सम्भवतः धेरै छिटो असफल हुनेछ (CartPole वातावरण समाप्त हुन्छ जब CartPole को गति, यसको स्थिति वा कोण निश्चित सीमाहरू बाहिर हुन्छन्)।\n",
    "\n",
    "> सिमुलेशन नयाँ झ्यालमा खुल्नेछ। तपाईंले कोडलाई धेरै पटक चलाउन सक्नुहुन्छ र यसको व्यवहार हेर्न सक्नुहुन्छ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "तपाईंले देख्न सक्नुहुन्छ कि अवलोकनहरूमा ४ वटा सङ्ख्या छन्। ती हुन्:\n",
    "- कार्टको स्थिति\n",
    "- कार्टको वेग\n",
    "- पोलको कोण\n",
    "- पोलको घुम्ने दर\n",
    "\n",
    "`rew` भनेको हरेक चरणमा प्राप्त हुने पुरस्कार हो। CartPole वातावरणमा, तपाईंलाई प्रत्येक सिमुलेशन चरणका लागि १ अंकको पुरस्कार दिइन्छ, र लक्ष्य भनेको कुल पुरस्कारलाई अधिकतम बनाउनु हो, अर्थात् CartPole लाई लड्न नदिई सकेसम्म लामो समयसम्म सन्तुलनमा राख्नु।\n",
    "\n",
    "रिइन्फोर्समेन्ट लर्निङको क्रममा, हाम्रो लक्ष्य भनेको एउटा **नीति** $\\pi$ प्रशिक्षण गर्नु हो, जसले प्रत्येक अवस्था $s$ का लागि कुन क्रिया $a$ लिनुपर्ने हो भन्ने बताउँछ, अर्थात् $a = \\pi(s)$।\n",
    "\n",
    "यदि तपाईंलाई सम्भावनात्मक समाधान चाहिन्छ भने, तपाईं नीतिलाई प्रत्येक क्रियाका लागि सम्भावनाहरूको सेट फिर्ता गर्ने रूपमा सोच्न सक्नुहुन्छ, अर्थात् $\\pi(a|s)$ ले $s$ अवस्थामा $a$ क्रिया लिनुपर्ने सम्भावना जनाउँछ।\n",
    "\n",
    "## नीति ग्रेडियन्ट विधि\n",
    "\n",
    "सबैभन्दा साधारण RL एल्गोरिदम, जसलाई **नीति ग्रेडियन्ट** भनिन्छ, मा हामी एउटा न्यूरल नेटवर्कलाई अर्को क्रिया भविष्यवाणी गर्न प्रशिक्षण गर्नेछौं।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "हामी धेरै प्रयोगहरू चलाएर र प्रत्येक रन पछि हाम्रो नेटवर्क अद्यावधिक गरेर नेटवर्कलाई प्रशिक्षण गर्नेछौं। आउनुहोस् एउटा कार्य परिभाषित गरौं जसले प्रयोग चलाउनेछ र परिणामहरू (तथाकथित **ट्रेस**) - सबै अवस्थाहरू, कार्यहरू (र तिनीहरूको सिफारिस गरिएका सम्भावनाहरू), र पुरस्कारहरू फर्काउनेछ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "तपाईंले अप्रशिक्षित नेटवर्कसँग एक एपिसोड चलाउन सक्नुहुन्छ र कुल पुरस्कार (एपिसोडको लम्बाइ) धेरै कम भएको अवलोकन गर्न सक्नुहुन्छ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "पोलिसी ग्रेडियन्ट एल्गोरिदमको जटिल पक्षहरू मध्ये एक **छूट गरिएको पुरस्कारहरू** प्रयोग गर्नु हो। विचार यो हो कि हामी खेलको प्रत्येक चरणमा कुल पुरस्कारहरूको भेक्टर गणना गर्छौं, र यस प्रक्रियाको क्रममा हामी केही गुणांक $gamma$ प्रयोग गरेर प्रारम्भिक पुरस्कारहरू छूट गर्छौं। हामी परिणामी भेक्टरलाई पनि सामान्यीकरण गर्छौं, किनकि हामी यसलाई हाम्रो प्रशिक्षणलाई प्रभाव पार्न तौलको रूपमा प्रयोग गर्नेछौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब हामी वास्तविक प्रशिक्षण सुरु गर्नेछौं! हामी ३०० एपिसोड चलाउनेछौं, र प्रत्येक एपिसोडमा निम्न कार्यहरू गर्नेछौं:\n",
    "\n",
    "1. प्रयोग चलाउनुहोस् र ट्रेस सङ्कलन गर्नुहोस्।\n",
    "1. लिएको कार्यहरू र अनुमानित सम्भावनाहरू बीचको फरक (`gradients`) गणना गर्नुहोस्। फरक कम भएमा, हामीले सही कार्य लिएकोमा बढी निश्चित हुन्छौं।\n",
    "1. छुट्याइएको पुरस्कारहरू गणना गर्नुहोस् र छुट्याइएको पुरस्कारहरूद्वारा gradients लाई गुणा गर्नुहोस् - यसले सुनिश्चित गर्दछ कि उच्च पुरस्कार भएका चरणहरूले अन्तिम नतिजामा कम पुरस्कार भएका चरणहरूको तुलनामा बढी प्रभाव पार्नेछन्।\n",
    "1. हाम्रो न्यूरल नेटवर्कका लागि अपेक्षित लक्ष्य कार्यहरू आंशिक रूपमा प्रयोगको क्रममा अनुमानित सम्भावनाहरूबाट लिइनेछन्, र आंशिक रूपमा गणना गरिएका gradients बाट। हामी `alpha` प्यारामिटर प्रयोग गरेर gradients र पुरस्कारहरूलाई कति हदसम्म ध्यानमा राख्ने निर्धारण गर्नेछौं - यसलाई reinforcement algorithm को *learning rate* भनिन्छ।\n",
    "1. अन्ततः, हामी हाम्रो नेटवर्कलाई states र अपेक्षित कार्यहरूमा प्रशिक्षण दिनेछौं, र प्रक्रिया दोहोर्याउनेछौं।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब परिणाम हेर्नको लागि रेन्डरिङसहित एपिसोड चलाउँ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आशा छ, तपाईंले देख्न सक्नुहुन्छ कि पोल अब राम्रोसँग सन्तुलनमा रहन सक्छ!\n",
    "\n",
    "## अभिनेता-आलोचक मोडेल\n",
    "\n",
    "अभिनेता-आलोचक मोडेल नीति ग्रेडियन्टको थप विकास हो, जसमा हामी नीतिलाई सिक्न र अनुमानित पुरस्कारहरू सिक्नको लागि न्युरल नेटवर्क निर्माण गर्छौं। यस नेटवर्कमा दुई आउटपुटहरू हुनेछन् (वा तपाईं यसलाई दुई अलग नेटवर्कको रूपमा हेर्न सक्नुहुन्छ):\n",
    "* **अभिनेता**ले राज्यको सम्भाव्यता वितरण प्रदान गरेर कुन कार्य लिनु पर्ने सिफारिस गर्नेछ, जस्तै नीति ग्रेडियन्ट मोडेलमा।\n",
    "* **आलोचक**ले ती कार्यहरूबाट के पुरस्कार प्राप्त हुन सक्छ भनेर अनुमान गर्नेछ। यो दिइएको अवस्थामा भविष्यमा कुल अनुमानित पुरस्कारहरू फर्काउँछ।\n",
    "\n",
    "अब यस्तो मोडेल परिभाषित गरौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "हामीले हाम्रो `discounted_rewards` र `run_episode` कार्यहरूलाई अलि परिमार्जन गर्न आवश्यक हुनेछ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब हामी मुख्य प्रशिक्षण लूप चलाउनेछौं। हामी उचित हानिको कार्यहरू गणना गरेर र नेटवर्क प्यारामिटरहरू अद्यावधिक गरेर म्यानुअल नेटवर्क प्रशिक्षण प्रक्रिया प्रयोग गर्नेछौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## मुख्य कुरा\n",
    "\n",
    "यस डेमोमा हामीले दुईवटा RL एल्गोरिदमहरू हेरेका छौं: साधारण पॉलिसी ग्रेडियन्ट र अझ परिष्कृत एक्टर-क्रिटिक। तपाईंले देख्न सक्नुहुन्छ कि ती एल्गोरिदमहरूले अवस्था, क्रिया, र पुरस्कारको अमूर्त अवधारणाहरूसँग काम गर्छन् - त्यसैले तिनीहरू धेरै फरक वातावरणहरूमा लागू गर्न सकिन्छ।\n",
    "\n",
    "रिइन्फोर्समेन्ट लर्निङले हामीलाई अन्तिम पुरस्कार हेरेर मात्र समस्या समाधान गर्नको लागि सबैभन्दा राम्रो रणनीति सिक्न अनुमति दिन्छ। लेबल गरिएको डेटासेटको आवश्यकता नपर्ने तथ्यले हामीलाई हाम्रो मोडेलहरूलाई अनुकूलन गर्न धेरै पटक सिमुलेसनहरू दोहोर्याउन अनुमति दिन्छ। यद्यपि, RL मा अझै धेरै चुनौतीहरू छन्, जुन तपाईंले यो रोचक AI क्षेत्रमा थप ध्यान केन्द्रित गर्ने निर्णय गर्नुभयो भने सिक्न सक्नुहुन्छ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको हो। हामी यथार्थताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा असमानताहरू हुन सक्छ। यसको मूल भाषा मा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T07:33:05+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}