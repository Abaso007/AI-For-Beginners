{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力機制與Transformer模型\n",
    "\n",
    "循環神經網絡的一個主要缺點是序列中的所有詞語對結果的影響力相同。這導致標準的LSTM編碼器-解碼器模型在處理序列到序列任務（例如命名實體識別和機器翻譯）時表現不佳。實際上，輸入序列中的某些特定詞語通常比其他詞語對輸出序列的影響更大。\n",
    "\n",
    "以序列到序列模型（例如機器翻譯）為例。這種模型由兩個循環神經網絡實現，其中一個網絡（**編碼器**）將輸入序列壓縮成隱藏狀態，另一個網絡（**解碼器**）將隱藏狀態展開成翻譯結果。這種方法的問題在於，網絡的最終狀態很難記住句子的開頭部分，因此在處理長句子時模型的質量會下降。\n",
    "\n",
    "**注意力機制**提供了一種方法，能夠對每個輸入向量在RNN輸出預測中的上下文影響進行加權。其實現方式是通過在輸入RNN的中間狀態和輸出RNN之間創建捷徑。在生成輸出符號$y_t$時，我們會考慮所有輸入隱藏狀態$h_i$，並賦予不同的權重係數$\\alpha_{t,i}$。\n",
    "\n",
    "![顯示帶有加性注意力層的編碼器/解碼器模型的圖片](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)\n",
    "*帶有加性注意力機制的編碼器-解碼器模型，來自[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)，引用自[這篇博客文章](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "注意力矩陣$\\{\\alpha_{i,j}\\}$表示某些輸入詞語在生成輸出序列中的某個詞語時所起的作用程度。以下是這樣一個矩陣的示例：\n",
    "\n",
    "![顯示RNNsearch-50找到的樣本對齊的圖片，取自Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)\n",
    "\n",
    "*圖片取自[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)（圖3）*\n",
    "\n",
    "注意力機制是目前或接近目前自然語言處理領域的最先進技術的核心。儘管添加注意力機制會大幅增加模型參數的數量，但這也導致了RNN的擴展問題。RNN擴展的一個關鍵限制是模型的循環特性使得批量處理和訓練並行化變得困難。在RNN中，序列的每個元素都需要按順序處理，這意味著它無法輕易並行化。\n",
    "\n",
    "注意力機制的採用結合了這一限制，促成了如今最先進的Transformer模型的誕生，這些模型包括BERT和OpenGPT3等。\n",
    "\n",
    "## Transformer模型\n",
    "\n",
    "與將每次預測的上下文傳遞到下一個評估步驟不同，**Transformer模型**使用**位置編碼**和**注意力**來捕捉給定輸入在指定文本窗口內的上下文。下圖展示了位置編碼與注意力如何在給定窗口內捕捉上下文。\n",
    "\n",
    "![顯示Transformer模型中如何進行評估的動畫GIF](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "由於每個輸入位置可以獨立映射到每個輸出位置，Transformer模型比RNN更容易並行化，這使得能夠構建更大、更具表達力的語言模型。每個注意力頭可以用來學習詞語之間的不同關係，從而改善下游的自然語言處理任務。\n",
    "\n",
    "## 構建簡單的Transformer模型\n",
    "\n",
    "Keras並不包含內建的Transformer層，但我們可以自己構建。與之前一樣，我們將專注於AG News數據集的文本分類，但值得一提的是，Transformer模型在更困難的自然語言處理任務中表現最佳。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新的 Keras 層應該繼承 `Layer` 類別，並實現 `call` 方法。我們先從 **位置嵌入** 層開始。我們將使用[官方 Keras 文件中的一些代碼](https://keras.io/examples/nlp/text_classification_with_transformer/)。我們假設我們將所有輸入序列填充到長度 `maxlen`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這一層包含兩個 `Embedding` 層：一個用於嵌入標記（以我們之前討論過的方式），另一個用於嵌入標記的位置。標記位置是通過使用 `tf.range` 從 0 到 `maxlen` 創建的一個自然數序列，然後傳遞到嵌入層。兩個生成的嵌入向量隨後相加，產生輸入的帶位置嵌入的表示，其形狀為 `maxlen`$\\times$`embed_dim`。\n",
    "\n",
    "現在，我們來實現 transformer 區塊。它將接收之前定義的嵌入層的輸出：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在，我們準備好定義完整的 Transformer 模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer 模型\n",
    "\n",
    "**BERT**（Bidirectional Encoder Representations from Transformers，雙向編碼器表示）是一個非常大型的多層 Transformer 網絡，*BERT-base* 有 12 層，而 *BERT-large* 則有 24 層。該模型首先在大規模文本數據（維基百科 + 書籍）上進行無監督訓練（預測句子中被遮蔽的詞語）。在預訓練過程中，模型吸收了大量的語言理解能力，然後可以通過微調其他數據集來加以利用。這個過程被稱為 **遷移學習**。\n",
    "\n",
    "![圖片來源：http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)\n",
    "\n",
    "Transformer 架構有許多變體，包括 BERT、DistilBERT、BigBird、OpenGPT3 等，這些模型都可以進行微調。\n",
    "\n",
    "現在讓我們看看如何使用預訓練的 BERT 模型來解決我們傳統的序列分類問題。我們將借用[官方文檔](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)中的一些想法和代碼。\n",
    "\n",
    "為了加載預訓練模型，我們將使用 **Tensorflow hub**。首先，讓我們加載 BERT 專用的向量化工具：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用與原始網絡訓練時相同的向量化工具是非常重要的。此外，BERT 向量化工具會返回三個組件：\n",
    "* `input_word_ids`，這是一個輸入句子的標記編號序列\n",
    "* `input_mask`，顯示序列中哪些部分包含實際輸入，哪些部分是填充。這與 `Masking` 層生成的遮罩類似\n",
    "* `input_type_ids` 用於語言建模任務，允許在一個序列中指定兩個輸入句子。\n",
    "\n",
    "然後，我們可以實例化 BERT 特徵提取器：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，BERT 層會返回一些有用的結果：\n",
    "* `pooled_output` 是通過平均序列中所有 token 的結果。你可以將其視為整個網絡的智能語義嵌入。它等同於我們之前模型中的 `GlobalAveragePooling1D` 層的輸出。\n",
    "* `sequence_output` 是最後一層 transformer 的輸出（對應於我們上面模型中的 `TransformerBlock` 的輸出）。\n",
    "* `encoder_outputs` 是所有 transformer 層的輸出。由於我們載入了 4 層的 BERT 模型（從名稱中包含的 `4_H` 你可能已經猜到），它有 4 個張量。最後一個張量與 `sequence_output` 相同。\n",
    "\n",
    "現在我們將定義端到端的分類模型。我們會使用*函數式模型定義*，即定義模型輸入，然後提供一系列表達式來計算其輸出。我們還會將 BERT 模型的權重設置為不可訓練，只訓練最終的分類器：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "儘管可訓練的參數很少，但過程仍然相當緩慢，因為 BERT 特徵提取器的計算量很大。看起來我們無法達到合理的準確度，可能是因為訓練不足，或者模型參數不足。\n",
    "\n",
    "讓我們嘗試解凍 BERT 的權重並一起訓練。這需要非常小的學習率，還需要更謹慎的訓練策略，包括使用 **warmup** 和 **AdamW** 優化器。我們將使用 `tf-models-official` 套件來創建優化器：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如你所見，訓練過程相當緩慢——但你可能想嘗試進行幾個訓練週期（5-10次），看看是否能夠與我們之前使用的方法相比，獲得最佳結果。\n",
    "\n",
    "## Huggingface Transformers 庫\n",
    "\n",
    "另一種非常常見（且稍微簡單）的使用 Transformer 模型的方法是 [HuggingFace 套件](https://github.com/huggingface/)，它為不同的 NLP 任務提供了簡單的構建模塊。此套件同時支援 Tensorflow 和 PyTorch，後者是另一個非常受歡迎的神經網絡框架。\n",
    "\n",
    "> **注意**：如果你對了解 Transformers 庫的運作方式不感興趣——你可以跳到筆記本的最後部分，因為你不會看到任何與我們之前所做的有實質性不同的內容。我們將重複使用不同的庫和更大的模型來訓練 BERT 模型的相同步驟。因此，這個過程涉及一些相當長的訓練時間，所以你可能只想瀏覽一下程式碼。\n",
    "\n",
    "讓我們看看如何使用 [Huggingface Transformers](http://huggingface.co) 解決我們的問題。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們需要選擇要使用的模型。除了內建的模型之外，Huggingface 還有一個[線上模型庫](https://huggingface.co/models)，社群提供了許多預訓練模型。只需提供模型名稱，就可以載入並使用這些模型。所有模型所需的二進制檔案會自動下載。\n",
    "\n",
    "有時候你可能需要載入自己的模型，這時可以指定包含所有相關檔案的目錄，包括 tokenizer 的參數、`config.json` 文件（包含模型參數）、二進制權重等。\n",
    "\n",
    "透過模型名稱，我們可以初始化模型和 tokenizer。讓我們先從 tokenizer 開始：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` 對象包含可直接用於編碼文本的 `encode` 函數：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們還可以使用分詞器以適合傳遞給模型的方式對序列進行編碼，即包括 `token_ids`、`input_mask` 等字段。我們還可以通過提供 `return_tensors='tf'` 參數來指定我們想要 Tensorflow 張量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我們的案例中，我們將使用預訓練的 BERT 模型，名為 `bert-base-uncased`。*Uncased* 表示該模型對大小寫不敏感。\n",
    "\n",
    "在訓練模型時，我們需要提供已分詞的序列作為輸入，因此我們將設計數據處理管道。由於 `tokenizer.encode` 是一個 Python 函數，我們將採用與上一單元相同的方法，使用 `py_function` 來調用它：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們可以使用 `BertForSequenceClassfication` 套件加載實際模型。這確保了我們的模型已經具備分類所需的架構，包括最終的分類器。你會看到一條警告訊息，指出最終分類器的權重尚未初始化，並且模型需要進行預訓練——這完全沒問題，因為這正是我們即將進行的操作！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從 `summary()` 可以看到，該模型包含了接近 1.1 億個參數！假設我們想在相對較小的數據集上進行簡單的分類任務，我們可能不希望訓練 BERT 的基礎層：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們準備開始訓練！\n",
    "\n",
    "> **注意**：訓練完整規模的 BERT 模型可能會非常耗時！因此，我們只會訓練前 32 個批次。這只是為了展示模型訓練的設置方式。如果你有興趣嘗試完整規模的訓練，只需移除 `steps_per_epoch` 和 `validation_steps` 參數，然後準備耐心等待！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你增加迭代次數並耐心等待，並且訓練多個epoch，你可以預期BERT分類會給我們最好的準確率！這是因為BERT已經相當理解語言的結構，我們只需要微調最終的分類器。然而，由於BERT是一個大型模型，整個訓練過程需要很長時間，並且需要強大的計算能力！（GPU，最好是多於一個）。\n",
    "\n",
    "> **Note:** 在我們的例子中，我們使用的是其中一個最小的預訓練BERT模型。還有更大的模型可能會帶來更好的結果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重點\n",
    "\n",
    "在本單元中，我們探討了基於 **transformers** 的最新模型架構。我們已將其應用於文本分類任務，但同樣地，BERT 模型也可以用於實體抽取、問題回答以及其他自然語言處理任務。\n",
    "\n",
    "Transformer 模型代表了自然語言處理領域的最新技術，並且在大多數情況下，應該是您在實現自定義 NLP 解決方案時首先嘗試的選擇。然而，如果您希望構建更高級的神經網絡模型，理解本模組中討論的循環神經網絡的基本原理是非常重要的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責聲明**：  \n本文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。應以原文文件作為權威來源。對於關鍵資訊，建議尋求專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-31T10:43:28+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "hk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}