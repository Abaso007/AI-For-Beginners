<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-28T15:36:39+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "sv"
}
-->
# Introduktion till neurala n√§tverk. Multi-Layered Perceptron

I f√∂reg√•ende avsnitt l√§rde du dig om den enklaste modellen f√∂r neurala n√§tverk - enlagers perceptron, en linj√§r tv√•klassklassificeringsmodell.

I detta avsnitt kommer vi att ut√∂ka denna modell till en mer flexibel ram, vilket g√∂r det m√∂jligt f√∂r oss att:

* utf√∂ra **multiklassklassificering** ut√∂ver tv√•klassklassificering
* l√∂sa **regressionsproblem** ut√∂ver klassificering
* separera klasser som inte √§r linj√§rt separerbara

Vi kommer ocks√• att utveckla v√•r egen modul√§ra ram i Python som g√∂r det m√∂jligt f√∂r oss att konstruera olika arkitekturer f√∂r neurala n√§tverk.

## [F√∂rtest inf√∂r f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalisering av maskininl√§rning

L√•t oss b√∂rja med att formalisera problemet med maskininl√§rning. Anta att vi har ett tr√§ningsdataset **X** med etiketter **Y**, och vi beh√∂ver bygga en modell *f* som g√∂r de mest exakta f√∂ruts√§gelserna. Kvaliteten p√• f√∂ruts√§gelserna m√§ts med hj√§lp av en **f√∂rlustfunktion** ‚Ñí. F√∂ljande f√∂rlustfunktioner anv√§nds ofta:

* F√∂r regressionsproblem, n√§r vi beh√∂ver f√∂ruts√§ga ett tal, kan vi anv√§nda **absolut fel** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadratiskt fel** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* F√∂r klassificering anv√§nder vi **0-1-f√∂rlust** (som i princip √§r detsamma som modellens **noggrannhet**) eller **logistisk f√∂rlust**.

F√∂r enlagers perceptron definierades funktionen *f* som en linj√§r funktion *f(x)=wx+b* (h√§r √§r *w* viktmatrisen, *x* √§r vektorn av indatafunktioner, och *b* √§r biasvektorn). F√∂r olika arkitekturer av neurala n√§tverk kan denna funktion anta en mer komplex form.

> Vid klassificering √§r det ofta √∂nskv√§rt att f√• sannolikheter f√∂r motsvarande klasser som n√§tverkets output. F√∂r att konvertera godtyckliga tal till sannolikheter (t.ex. f√∂r att normalisera outputen) anv√§nder vi ofta **softmax**-funktionen œÉ, och funktionen *f* blir *f(x)=œÉ(wx+b)*

I definitionen av *f* ovan kallas *w* och *b* f√∂r **parametrar** Œ∏=‚ü®*w,b*‚ü©. Givet datasetet ‚ü®**X**,**Y**‚ü© kan vi ber√§kna ett √∂vergripande fel f√∂r hela datasetet som en funktion av parametrarna Œ∏.

> ‚úÖ **M√•let med tr√§ning av neurala n√§tverk √§r att minimera felet genom att variera parametrarna Œ∏**

## Optimering med gradientnedstigning

Det finns en v√§lk√§nd metod f√∂r funktionsoptimering som kallas **gradientnedstigning**. Id√©n √§r att vi kan ber√§kna en derivata (i flerdimensionella fall kallad **gradient**) av f√∂rlustfunktionen med avseende p√• parametrarna och variera parametrarna p√• ett s√§tt som minskar felet. Detta kan formaliseras enligt f√∂ljande:

* Initiera parametrarna med n√•gra slumpm√§ssiga v√§rden w<sup>(0)</sup>, b<sup>(0)</sup>
* Upprepa f√∂ljande steg m√•nga g√•nger:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

Under tr√§ningen ska optimeringsstegen ber√§knas med h√§nsyn till hela datasetet (kom ih√•g att f√∂rlusten ber√§knas som en summa genom alla tr√§ningsprover). Men i verkligheten tar vi sm√• delar av datasetet som kallas **minibatcher**, och ber√§knar gradienter baserat p√• en delm√§ngd av data. Eftersom delm√§ngden tas slumpm√§ssigt varje g√•ng kallas en s√•dan metod f√∂r **stokastisk gradientnedstigning** (SGD).

## Multi-Layered Perceptrons och backpropagation

Ett enlagers n√§tverk, som vi har sett ovan, kan klassificera linj√§rt separerbara klasser. F√∂r att bygga en rikare modell kan vi kombinera flera lager i n√§tverket. Matematiskt skulle det inneb√§ra att funktionen *f* f√•r en mer komplex form och ber√§knas i flera steg:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

H√§r √§r Œ± en **icke-linj√§r aktiveringsfunktion**, œÉ √§r en softmax-funktion, och parametrarna √§r Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientnedstigningsalgoritmen skulle f√∂rbli densamma, men det skulle vara sv√•rare att ber√§kna gradienterna. Med hj√§lp av kedjeregeln f√∂r derivator kan vi ber√§kna derivator som:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ Kedjeregeln anv√§nds f√∂r att ber√§kna derivator av f√∂rlustfunktionen med avseende p√• parametrarna.

Observera att den v√§nstra delen av alla dessa uttryck √§r densamma, och d√§rf√∂r kan vi effektivt ber√§kna derivator genom att b√∂rja fr√•n f√∂rlustfunktionen och g√• "bak√•t" genom ber√§kningsgrafen. D√§rf√∂r kallas metoden f√∂r att tr√§na ett flerskiktat perceptron f√∂r **backpropagation**, eller 'backprop'.

<img alt="ber√§kningsgraf" src="images/ComputeGraphGrad.png"/>

> TODO: bildcitering

> ‚úÖ Vi kommer att g√• igenom backpropagation mycket mer detaljerat i v√•rt exempel i notebooken.  

## Slutsats

I denna lektion har vi byggt v√•rt eget bibliotek f√∂r neurala n√§tverk och anv√§nt det f√∂r en enkel tv√•dimensionell klassificeringsuppgift.

## üöÄ Utmaning

I den medf√∂ljande notebooken kommer du att implementera din egen ram f√∂r att bygga och tr√§na flerskiktade perceptrons. Du kommer att kunna se i detalj hur moderna neurala n√§tverk fungerar.

G√• vidare till [OwnFramework](OwnFramework.ipynb) notebooken och arbeta igenom den.

## [Eftertest efter f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## Granskning och sj√§lvstudier

Backpropagation √§r en vanlig algoritm som anv√§nds inom AI och ML, v√§rd att studera [mer i detalj](https://wikipedia.org/wiki/Backpropagation)

## [Uppgift](lab/README.md)

I detta labb ombeds du anv√§nda den ram du konstruerade i denna lektion f√∂r att l√∂sa klassificeringen av handskrivna siffror i MNIST.

* [Instruktioner](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som kan uppst√• vid anv√§ndning av denna √∂vers√§ttning.