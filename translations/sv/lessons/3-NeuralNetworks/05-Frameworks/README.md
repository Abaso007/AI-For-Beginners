<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddd216f558a255260a9374008002c971",
  "translation_date": "2025-09-23T09:22:45+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "sv"
}
-->
# Ramverk f√∂r neurala n√§tverk

Som vi redan har l√§rt oss, f√∂r att kunna tr√§na neurala n√§tverk effektivt beh√∂ver vi g√∂ra tv√• saker:

* Arbeta med tensorer, t.ex. multiplicera, addera och ber√§kna funktioner som sigmoid eller softmax.
* Ber√§kna gradienter f√∂r alla uttryck f√∂r att kunna utf√∂ra gradientnedstigningsoptimering.

## [F√∂rtest](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Medan biblioteket `numpy` kan hantera den f√∂rsta delen, beh√∂ver vi en mekanism f√∂r att ber√§kna gradienter. I [v√•rt ramverk](../04-OwnFramework/OwnFramework.ipynb) som vi utvecklade i f√∂reg√•ende avsnitt var vi tvungna att manuellt programmera alla derivatafunktioner i metoden `backward`, som utf√∂r backpropagation. Idealt sett b√∂r ett ramverk ge oss m√∂jlighet att ber√§kna gradienter f√∂r *vilket uttryck som helst* som vi kan definiera.

En annan viktig aspekt √§r att kunna utf√∂ra ber√§kningar p√• GPU eller andra specialiserade ber√§kningsenheter, som [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Tr√§ning av djupa neurala n√§tverk kr√§ver *v√§ldigt mycket* ber√§kningar, och att kunna parallellisera dessa ber√§kningar p√• GPU:er √§r mycket viktigt.

> ‚úÖ Begreppet 'parallellisera' betyder att f√∂rdela ber√§kningarna √∂ver flera enheter.

De tv√• mest popul√§ra ramverken f√∂r neurala n√§tverk idag √§r: [TensorFlow](http://TensorFlow.org) och [PyTorch](https://pytorch.org/). B√•da erbjuder ett l√•g-niv√• API f√∂r att arbeta med tensorer p√• b√•de CPU och GPU. Ut√∂ver l√•g-niv√• API finns det ocks√• h√∂g-niv√• API, kallade [Keras](https://keras.io/) respektive [PyTorch Lightning](https://pytorchlightning.ai/).

L√•g-niv√• API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
-------------|-------------------------------------|--------------------------------
H√∂g-niv√• API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**L√•g-niv√• API** i b√•da ramverken l√•ter dig bygga s√• kallade **ber√§kningsgrafer**. Denna graf definierar hur man ber√§knar utdata (vanligtvis f√∂rlustfunktionen) med givna indata och kan skickas f√∂r ber√§kning p√• GPU, om s√•dan finns tillg√§nglig. Det finns funktioner f√∂r att differentiera denna ber√§kningsgraf och ber√§kna gradienter, som sedan kan anv√§ndas f√∂r att optimera modellparametrar.

**H√∂g-niv√• API** betraktar i stort sett neurala n√§tverk som en **sekvens av lager**, vilket g√∂r konstruktionen av de flesta neurala n√§tverk mycket enklare. Att tr√§na modellen kr√§ver vanligtvis att man f√∂rbereder data och sedan anropar en `fit`-funktion f√∂r att utf√∂ra jobbet.

H√∂g-niv√• API g√∂r det m√∂jligt att snabbt konstruera typiska neurala n√§tverk utan att beh√∂va oroa sig f√∂r m√•nga detaljer. Samtidigt erbjuder l√•g-niv√• API mycket mer kontroll √∂ver tr√§ningsprocessen och anv√§nds d√§rf√∂r ofta inom forskning, n√§r man arbetar med nya arkitekturer f√∂r neurala n√§tverk.

Det √§r ocks√• viktigt att f√∂rst√• att du kan anv√§nda b√•da API:erna tillsammans, t.ex. kan du utveckla din egen n√§tverkslagerarkitektur med l√•g-niv√• API och sedan anv√§nda den i ett st√∂rre n√§tverk som konstrueras och tr√§nas med h√∂g-niv√• API. Eller s√• kan du definiera ett n√§tverk med h√∂g-niv√• API som en sekvens av lager och sedan anv√§nda din egen l√•g-niv√• tr√§ningsloop f√∂r att utf√∂ra optimering. B√•da API:erna bygger p√• samma grundl√§ggande koncept och √§r designade f√∂r att fungera bra tillsammans.

## L√§rande

I den h√§r kursen erbjuder vi det mesta av inneh√•llet b√•de f√∂r PyTorch och TensorFlow. Du kan v√§lja ditt f√∂redragna ramverk och endast g√• igenom motsvarande anteckningsb√∂cker. Om du inte √§r s√§ker p√• vilket ramverk du ska v√§lja, l√§s n√•gra diskussioner p√• internet om **PyTorch vs. TensorFlow**. Du kan ocks√• titta p√• b√•da ramverken f√∂r att f√• en b√§ttre f√∂rst√•else.

D√§r det √§r m√∂jligt kommer vi att anv√§nda h√∂g-niv√• API f√∂r enkelhetens skull. Men vi anser att det √§r viktigt att f√∂rst√• hur neurala n√§tverk fungerar fr√•n grunden, s√• i b√∂rjan b√∂rjar vi med att arbeta med l√•g-niv√• API och tensorer. Om du d√§remot vill komma ig√•ng snabbt och inte vill spendera mycket tid p√• att l√§ra dig dessa detaljer, kan du hoppa √∂ver dem och g√• direkt till anteckningsb√∂ckerna f√∂r h√∂g-niv√• API.

## ‚úçÔ∏è √ñvningar: Ramverk

Forts√§tt ditt l√§rande i f√∂ljande anteckningsb√∂cker:

L√•g-niv√• API | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
-------------|-------------------------------------|--------------------------------
H√∂g-niv√• API | [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Efter att ha bem√§strat ramverken, l√•t oss repetera begreppet √∂veranpassning.

# √ñveranpassning

√ñveranpassning √§r ett extremt viktigt koncept inom maskininl√§rning, och det √§r mycket viktigt att f√∂rst√• det r√§tt!

T√§nk p√• f√∂ljande problem med att approximera 5 punkter (representerade av `x` p√• graferna nedan):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.sv.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.sv.jpg)
-------------------------|--------------------------
**Linj√§r modell, 2 parametrar** | **Icke-linj√§r modell, 7 parametrar**
Tr√§ningsfel = 5.3 | Tr√§ningsfel = 0
Valideringsfel = 5.1 | Valideringsfel = 20

* Till v√§nster ser vi en bra rak linje-approximation. Eftersom antalet parametrar √§r tillr√§ckligt, f√•ngar modellen korrekt punkternas f√∂rdelning.
* Till h√∂ger √§r modellen f√∂r kraftfull. Eftersom vi bara har 5 punkter och modellen har 7 parametrar, kan den justeras s√• att den passerar genom alla punkter, vilket g√∂r tr√§ningsfelet till 0. Detta hindrar dock modellen fr√•n att f√∂rst√• det korrekta m√∂nstret i data, vilket resulterar i ett mycket h√∂gt valideringsfel.

Det √§r mycket viktigt att hitta en korrekt balans mellan modellens komplexitet (antal parametrar) och antalet tr√§ningsprover.

## Varf√∂r √∂veranpassning uppst√•r

  * F√∂r lite tr√§ningsdata
  * F√∂r kraftfull modell
  * F√∂r mycket brus i indata

## Hur man uppt√§cker √∂veranpassning

Som du kan se fr√•n grafen ovan kan √∂veranpassning uppt√§ckas genom ett mycket l√•gt tr√§ningsfel och ett h√∂gt valideringsfel. Normalt under tr√§ning ser vi b√•de tr√§nings- och valideringsfel minska, och sedan vid n√•gon punkt kan valideringsfelet sluta minska och b√∂rja √∂ka. Detta √§r ett tecken p√• √∂veranpassning och en indikation p√• att vi f√∂rmodligen b√∂r sluta tr√§na vid denna punkt (eller √•tminstone spara en √∂gonblicksbild av modellen).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.sv.png)

## Hur man f√∂rhindrar √∂veranpassning

Om du m√§rker att √∂veranpassning uppst√•r kan du g√∂ra n√•got av f√∂ljande:

 * √ñka m√§ngden tr√§ningsdata
 * Minska modellens komplexitet
 * Anv√§nd n√•gon [regulariseringsteknik](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), som [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), vilket vi kommer att g√• igenom senare.

## √ñveranpassning och Bias-Variance-avv√§gning

√ñveranpassning √§r faktiskt ett fall av ett mer generellt problem inom statistik som kallas [Bias-Variance-avv√§gning](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Om vi betraktar m√∂jliga felk√§llor i v√•r modell kan vi se tv√• typer av fel:

* **Bias-fel** orsakas av att v√•r algoritm inte kan f√•nga relationen mellan tr√§ningsdata korrekt. Det kan bero p√• att v√•r modell inte √§r tillr√§ckligt kraftfull (**underanpassning**).
* **Variansfel**, som orsakas av att modellen approximera brus i indata ist√§llet f√∂r meningsfulla relationer (**√∂veranpassning**).

Under tr√§ning minskar bias-fel (eftersom v√•r modell l√§r sig att approximera data), och variansfel √∂kar. Det √§r viktigt att sluta tr√§na - antingen manuellt (n√§r vi uppt√§cker √∂veranpassning) eller automatiskt (genom att inf√∂ra regularisering) - f√∂r att f√∂rhindra √∂veranpassning.

## Slutsats

I denna lektion l√§rde du dig om skillnaderna mellan de olika API:erna f√∂r de tv√• mest popul√§ra AI-ramverken, TensorFlow och PyTorch. Dessutom l√§rde du dig om ett mycket viktigt √§mne, √∂veranpassning.

## üöÄ Utmaning

I de medf√∂ljande anteckningsb√∂ckerna hittar du 'uppgifter' l√§ngst ner; arbeta igenom anteckningsb√∂ckerna och slutf√∂r uppgifterna.

## [Eftertest](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Granskning & Sj√§lvstudier

G√∂r lite forskning om f√∂ljande √§mnen:

- TensorFlow
- PyTorch
- √ñveranpassning

St√§ll dig sj√§lv f√∂ljande fr√•gor:

- Vad √§r skillnaden mellan TensorFlow och PyTorch?
- Vad √§r skillnaden mellan √∂veranpassning och underanpassning?

## [Uppgift](lab/README.md)

I denna labb ska du l√∂sa tv√• klassificeringsproblem med hj√§lp av enkla och flerskiktade fullt anslutna n√§tverk med PyTorch eller TensorFlow.

* [Instruktioner](lab/README.md)
* [Anteckningsbok](lab/LabFrameworks.ipynb)

---

