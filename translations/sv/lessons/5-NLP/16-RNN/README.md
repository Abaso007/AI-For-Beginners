<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2273cc150380a5e191903cea858f021",
  "translation_date": "2025-09-23T09:25:23+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "sv"
}
-->
# Rekurrenta neurala n√§tverk

## [Quiz f√∂re f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/31)

I tidigare avsnitt har vi anv√§nt rika semantiska representationer av text och en enkel linj√§r klassificerare ovanp√• embeddingarna. Vad denna arkitektur g√∂r √§r att f√•nga den aggregerade betydelsen av ord i en mening, men den tar inte h√§nsyn till **ordningen** av orden, eftersom aggregeringsoperationen ovanp√• embeddingarna tar bort denna information fr√•n den ursprungliga texten. Eftersom dessa modeller inte kan modellera ordningen av ord, kan de inte l√∂sa mer komplexa eller tvetydiga uppgifter som textgenerering eller fr√•gehantering.

F√∂r att f√•nga betydelsen av en textsekvens beh√∂ver vi anv√§nda en annan neural n√§tverksarkitektur, som kallas f√∂r ett **rekurrent neuralt n√§tverk**, eller RNN. I RNN skickar vi v√•r mening genom n√§tverket en symbol i taget, och n√§tverket producerar ett **tillst√•nd**, som vi sedan skickar tillbaka till n√§tverket tillsammans med n√§sta symbol.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.sv.png)

> Bild av f√∂rfattaren

Givet en inmatningssekvens av token X<sub>0</sub>,...,X<sub>n</sub>, skapar RNN en sekvens av neurala n√§tverksblock och tr√§nar denna sekvens fr√•n b√∂rjan till slut med hj√§lp av backpropagation. Varje n√§tverksblock tar ett par (X<sub>i</sub>,S<sub>i</sub>) som inmatning och producerar S<sub>i+1</sub> som resultat. Det slutliga tillst√•ndet S<sub>n</sub> eller (utg√•ng Y<sub>n</sub>) g√•r in i en linj√§r klassificerare f√∂r att producera resultatet. Alla n√§tverksblock delar samma vikter och tr√§nas fr√•n b√∂rjan till slut med en enda backpropagation-pass.

Eftersom tillst√•ndsvektorerna S<sub>0</sub>,...,S<sub>n</sub> skickas genom n√§tverket, kan det l√§ra sig de sekventiella beroendena mellan ord. Till exempel, n√§r ordet *inte* dyker upp n√•gonstans i sekvensen, kan det l√§ra sig att f√∂rneka vissa element inom tillst√•ndsvektorn, vilket resulterar i negation.

> ‚úÖ Eftersom vikterna f√∂r alla RNN-block i bilden ovan √§r delade, kan samma bild representeras som ett enda block (till h√∂ger) med en rekurrent √•terkopplingsslinga, som skickar n√§tverkets utg√•ngstillst√•nd tillbaka till inmatningen.

## Anatomi av en RNN-cell

L√•t oss se hur en enkel RNN-cell √§r organiserad. Den tar emot det tidigare tillst√•ndet S<sub>i-1</sub> och den aktuella symbolen X<sub>i</sub> som inmatningar och m√•ste producera utg√•ngstillst√•ndet S<sub>i</sub> (och ibland √§r vi ocks√• intresserade av n√•gon annan utg√•ng Y<sub>i</sub>, som i fallet med generativa n√§tverk).

En enkel RNN-cell har tv√• viktmatriser inuti: en som transformerar en inmatningssymbol (vi kallar den W), och en annan som transformerar ett inmatningstillst√•nd (H). I detta fall ber√§knas n√§tverkets utg√•ng som &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), d√§r &sigma; √§r aktiveringsfunktionen och b √§r en ytterligare bias.

<img alt="RNN Cell Anatomy" src="images/rnn-anatomy.png" width="50%"/>

> Bild av f√∂rfattaren

I m√•nga fall skickas inmatningstoken genom ett embedding-lager innan de g√•r in i RNN f√∂r att minska dimensionen. I detta fall, om dimensionen f√∂r inmatningsvektorerna √§r *emb_size*, och tillst√•ndsvektorn √§r *hid_size* - √§r storleken p√• W *emb_size*&times;*hid_size*, och storleken p√• H √§r *hid_size*&times;*hid_size*.

## Long Short Term Memory (LSTM)

Ett av de st√∂rsta problemen med klassiska RNN √§r det s√• kallade **f√∂rsvinnande gradienter**-problemet. Eftersom RNN tr√§nas fr√•n b√∂rjan till slut i en enda backpropagation-pass, har det sv√•rt att sprida fel till de f√∂rsta lagren i n√§tverket, och d√§rmed kan n√§tverket inte l√§ra sig relationer mellan avl√§gsna token. Ett s√§tt att undvika detta problem √§r att inf√∂ra **explicit tillst√•ndshantering** genom att anv√§nda s√• kallade **grindar**. Det finns tv√• v√§lk√§nda arkitekturer av detta slag: **Long Short Term Memory** (LSTM) och **Gated Relay Unit** (GRU).

![Bild som visar ett exempel p√• en Long Short Term Memory-cell](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Bildk√§lla TBD

LSTM-n√§tverket √§r organiserat p√• ett s√§tt som liknar RNN, men det finns tv√• tillst√•nd som skickas fr√•n lager till lager: det faktiska tillst√•ndet C och den dolda vektorn H. Vid varje enhet kombineras den dolda vektorn H<sub>i</sub> med inmatningen X<sub>i</sub>, och de styr vad som h√§nder med tillst√•ndet C via **grindar**. Varje grind √§r ett neuralt n√§tverk med sigmoid-aktivering (utg√•ng i intervallet [0,1]), som kan betraktas som en bitvis mask n√§r den multipliceras med tillst√•ndsvektorn. F√∂ljande grindar finns (fr√•n v√§nster till h√∂ger p√• bilden ovan):

* **Gl√∂mskegrinden** tar en dold vektor och avg√∂r vilka komponenter i vektorn C vi beh√∂ver gl√∂mma och vilka vi ska skicka vidare.
* **Inmatningsgrinden** tar viss information fr√•n inmatnings- och dolda vektorer och inf√∂r den i tillst√•ndet.
* **Utg√•ngsgrinden** transformerar tillst√•ndet via ett linj√§rt lager med *tanh*-aktivering och v√§ljer sedan n√•gra av dess komponenter med hj√§lp av en dold vektor H<sub>i</sub> f√∂r att producera ett nytt tillst√•nd C<sub>i+1</sub>.

Komponenter i tillst√•ndet C kan betraktas som flaggor som kan sl√•s p√• och av. Till exempel, n√§r vi st√∂ter p√• namnet *Alice* i sekvensen, kanske vi vill anta att det h√§nvisar till en kvinnlig karakt√§r och h√∂ja flaggan i tillst√•ndet att vi har ett kvinnligt substantiv i meningen. N√§r vi senare st√∂ter p√• frasen *och Tom*, kommer vi att h√∂ja flaggan att vi har ett plural substantiv. Genom att manipulera tillst√•ndet kan vi allts√• h√•lla reda p√• de grammatiska egenskaperna hos meningens delar.

> ‚úÖ En utm√§rkt resurs f√∂r att f√∂rst√• LSTM:s interna funktioner √§r denna fantastiska artikel [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) av Christopher Olah.

## Bidirektionella och flerskiktade RNN

Vi har diskuterat rekurrenta n√§tverk som arbetar i en riktning, fr√•n b√∂rjan av en sekvens till slutet. Det verkar naturligt, eftersom det liknar s√§ttet vi l√§ser och lyssnar p√• tal. Men eftersom vi i m√•nga praktiska fall har slumpm√§ssig √•tkomst till inmatningssekvensen, kan det vara vettigt att k√∂ra rekurrent ber√§kning i b√•da riktningarna. S√•dana n√§tverk kallas **bidirektionella** RNN. N√§r vi arbetar med ett bidirektionellt n√§tverk beh√∂ver vi tv√• dolda tillst√•ndsvektorer, en f√∂r varje riktning.

Ett rekurrent n√§tverk, antingen enkelriktat eller bidirektionellt, f√•ngar vissa m√∂nster inom en sekvens och kan lagra dem i en tillst√•ndsvektor eller skicka dem till utg√•ngen. Precis som med konvolutionella n√§tverk kan vi bygga ett annat rekurrent lager ovanp√• det f√∂rsta f√∂r att f√•nga h√∂gre niv√•m√∂nster och bygga fr√•n l√•gniv√•m√∂nster som extraherats av det f√∂rsta lagret. Detta leder oss till begreppet **flerskiktat RNN**, som best√•r av tv√• eller fler rekurrenta n√§tverk, d√§r utg√•ngen fr√•n det f√∂reg√•ende lagret skickas till n√§sta lager som inmatning.

![Bild som visar ett flerskiktat Long Short Term Memory-RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.sv.jpg)

*Bild fr√•n [detta fantastiska inl√§gg](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) av Fernando L√≥pez*

## ‚úçÔ∏è √ñvningar: Embeddingar

Forts√§tt ditt l√§rande i f√∂ljande notebooks:

* [RNN med PyTorch](RNNPyTorch.ipynb)
* [RNN med TensorFlow](RNNTF.ipynb)

## Slutsats

I denna enhet har vi sett att RNN kan anv√§ndas f√∂r sekvensklassificering, men de kan faktiskt hantera m√•nga fler uppgifter, s√•som textgenerering, maskin√∂vers√§ttning och mer. Vi kommer att behandla dessa uppgifter i n√§sta enhet.

## üöÄ Utmaning

L√§s igenom lite litteratur om LSTM och fundera √∂ver deras till√§mpningar:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz efter f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Granskning & Sj√§lvstudier

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) av Christopher Olah.

## [Uppgift: Notebooks](assignment.md)

---

