<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-28T15:58:41+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "sv"
}
-->
# Rekurrenta neurala n√§tverk

## [Quiz f√∂re f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/31)

I tidigare avsnitt har vi anv√§nt rika semantiska representationer av text och en enkel linj√§r klassificerare ovanp√• inb√§ddningarna. Vad denna arkitektur g√∂r √§r att f√•nga den aggregerade betydelsen av ord i en mening, men den tar inte h√§nsyn till **ordningen** av orden, eftersom aggregeringsoperationen ovanp√• inb√§ddningarna tog bort denna information fr√•n den ursprungliga texten. Eftersom dessa modeller inte kan modellera ordordning, kan de inte l√∂sa mer komplexa eller tvetydiga uppgifter som textgenerering eller fr√•gehantering.

F√∂r att f√•nga betydelsen av textsekvenser beh√∂ver vi anv√§nda en annan neural n√§tverksarkitektur, som kallas ett **rekurrent neuralt n√§tverk**, eller RNN. I ett RNN skickar vi v√•r mening genom n√§tverket en symbol i taget, och n√§tverket producerar ett **tillst√•nd**, som vi sedan skickar tillbaka till n√§tverket tillsammans med n√§sta symbol.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.sv.png)

> Bild av f√∂rfattaren

Givet en inmatningssekvens av token X<sub>0</sub>,...,X<sub>n</sub>, skapar RNN en sekvens av neurala n√§tverksblock och tr√§nar denna sekvens fr√•n b√∂rjan till slut med hj√§lp av backpropagation. Varje n√§tverksblock tar ett par (X<sub>i</sub>,S<sub>i</sub>) som indata och producerar S<sub>i+1</sub> som resultat. Det slutliga tillst√•ndet S<sub>n</sub> eller (utdata Y<sub>n</sub>) skickas till en linj√§r klassificerare f√∂r att producera resultatet. Alla n√§tverksblock delar samma vikter och tr√§nas fr√•n b√∂rjan till slut med en enda backpropagation-pass.

Eftersom tillst√•ndsvektorerna S<sub>0</sub>,...,S<sub>n</sub> skickas genom n√§tverket, kan det l√§ra sig sekventiella beroenden mellan ord. Till exempel, n√§r ordet *inte* dyker upp n√•gonstans i sekvensen, kan det l√§ra sig att negera vissa element inom tillst√•ndsvektorn, vilket resulterar i negation.

> ‚úÖ Eftersom vikterna f√∂r alla RNN-block i bilden ovan √§r delade, kan samma bild representeras som ett block (till h√∂ger) med en rekurrent √•terkopplingsslinga, som skickar n√§tverkets utg√•ngstillst√•nd tillbaka till indata.

## Anatomi av en RNN-cell

L√•t oss se hur en enkel RNN-cell √§r organiserad. Den tar emot det f√∂reg√•ende tillst√•ndet S<sub>i-1</sub> och den aktuella symbolen X<sub>i</sub> som indata och m√•ste producera utg√•ngstillst√•ndet S<sub>i</sub> (och ibland √§r vi ocks√• intresserade av n√•gon annan utdata Y<sub>i</sub>, som i fallet med generativa n√§tverk).

En enkel RNN-cell har tv√• viktmatriser inuti: en som transformerar en indatasymbol (vi kallar den W) och en annan som transformerar ett indatatillst√•nd (H). I detta fall ber√§knas n√§tverkets utdata som œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b), d√§r œÉ √§r aktiveringsfunktionen och b √§r en ytterligare bias.

<img alt="RNN Cell Anatomy" src="images/rnn-anatomy.png" width="50%"/>

> Bild av f√∂rfattaren

I m√•nga fall skickas indatatoken genom inb√§ddningslagret innan de g√•r in i RNN f√∂r att minska dimensionen. I detta fall, om dimensionen av indatavektorerna √§r *emb_size* och tillst√•ndsvektorn √§r *hid_size* - √§r storleken p√• W *emb_size*√ó*hid_size*, och storleken p√• H √§r *hid_size*√ó*hid_size*.

## Long Short Term Memory (LSTM)

Ett av de st√∂rsta problemen med klassiska RNN √§r det s√• kallade **f√∂rsvinnande gradienter**-problemet. Eftersom RNN tr√§nas fr√•n b√∂rjan till slut i en enda backpropagation-pass, har det sv√•rt att sprida fel till de f√∂rsta lagren i n√§tverket, och d√§rmed kan n√§tverket inte l√§ra sig relationer mellan avl√§gsna token. Ett s√§tt att undvika detta problem √§r att inf√∂ra **explicit tillst√•ndshantering** genom att anv√§nda s√• kallade **grindar**. Det finns tv√• v√§lk√§nda arkitekturer av detta slag: **Long Short Term Memory** (LSTM) och **Gated Relay Unit** (GRU).

![Bild som visar ett exempel p√• en long short term memory-cell](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Bildk√§lla TBD

LSTM-n√§tverket √§r organiserat p√• ett s√§tt som liknar RNN, men det finns tv√• tillst√•nd som skickas fr√•n lager till lager: det faktiska tillst√•ndet C och den dolda vektorn H. Vid varje enhet sammanfogas den dolda vektorn H<sub>i</sub> med indata X<sub>i</sub>, och de styr vad som h√§nder med tillst√•ndet C via **grindar**. Varje grind √§r ett neuralt n√§tverk med sigmoidaktivering (utdata i intervallet [0,1]), som kan ses som en bitmask n√§r den multipliceras med tillst√•ndsvektorn. F√∂ljande grindar finns (fr√•n v√§nster till h√∂ger i bilden ovan):

* **Gl√∂mskegrinden** tar en dold vektor och avg√∂r vilka komponenter i vektorn C vi beh√∂ver gl√∂mma och vilka vi ska sl√§ppa igenom.
* **Indatagrinden** tar viss information fr√•n indata- och dolda vektorer och inf√∂r den i tillst√•ndet.
* **Utg√•ngsgrinden** transformerar tillst√•ndet via ett linj√§rt lager med *tanh*-aktivering och v√§ljer sedan vissa av dess komponenter med hj√§lp av den dolda vektorn H<sub>i</sub> f√∂r att producera ett nytt tillst√•nd C<sub>i+1</sub>.

Komponenter i tillst√•ndet C kan ses som flaggor som kan sl√•s p√• och av. Till exempel, n√§r vi st√∂ter p√• namnet *Alice* i sekvensen, kanske vi vill anta att det h√§nvisar till en kvinnlig karakt√§r och h√∂ja flaggan i tillst√•ndet att vi har ett kvinnligt substantiv i meningen. N√§r vi senare st√∂ter p√• frasen *och Tom*, h√∂jer vi flaggan att vi har ett plural substantiv. Genom att manipulera tillst√•ndet kan vi allts√• h√•lla reda p√• de grammatiska egenskaperna hos meningsdelar.

> ‚úÖ En utm√§rkt resurs f√∂r att f√∂rst√• LSTM:s interna funktioner √§r denna fantastiska artikel [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) av Christopher Olah.

## Bidirektionella och flerskikts-RNN

Vi har diskuterat rekurrenta n√§tverk som arbetar i en riktning, fr√•n b√∂rjan av en sekvens till slutet. Det verkar naturligt, eftersom det liknar hur vi l√§ser och lyssnar p√• tal. Men eftersom vi i m√•nga praktiska fall har slumpm√§ssig √•tkomst till inmatningssekvensen, kan det vara vettigt att k√∂ra rekurrent ber√§kning i b√•da riktningarna. S√•dana n√§tverk kallas **bidirektionella** RNN. N√§r vi arbetar med ett bidirektionellt n√§tverk beh√∂ver vi tv√• dolda tillst√•ndsvektorer, en f√∂r varje riktning.

Ett rekurrent n√§tverk, antingen enkelriktat eller bidirektionellt, f√•ngar vissa m√∂nster inom en sekvens och kan lagra dem i en tillst√•ndsvektor eller skicka dem till utdata. Precis som med konvolutionella n√§tverk kan vi bygga ett annat rekurrent lager ovanp√• det f√∂rsta f√∂r att f√•nga h√∂gre niv√•m√∂nster och bygga fr√•n l√•g-niv√•m√∂nster som extraherats av det f√∂rsta lagret. Detta leder oss till begreppet ett **flerskikts-RNN**, som best√•r av tv√• eller fler rekurrenta n√§tverk, d√§r utdata fr√•n det f√∂reg√•ende lagret skickas till n√§sta lager som indata.

![Bild som visar ett flerskikts long-short-term-memory-RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.sv.jpg)

*Bild fr√•n [detta fantastiska inl√§gg](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) av Fernando L√≥pez*

## ‚úçÔ∏è √ñvningar: Inb√§ddningar

Forts√§tt ditt l√§rande i f√∂ljande anteckningsb√∂cker:

* [RNN med PyTorch](RNNPyTorch.ipynb)
* [RNN med TensorFlow](RNNTF.ipynb)

## Slutsats

I denna enhet har vi sett att RNN kan anv√§ndas f√∂r sekvensklassificering, men de kan faktiskt hantera m√•nga fler uppgifter, s√•som textgenerering, maskin√∂vers√§ttning och mer. Vi kommer att titta p√• dessa uppgifter i n√§sta enhet.

## üöÄ Utmaning

L√§s igenom lite litteratur om LSTM och fundera √∂ver deras till√§mpningar:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz efter f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Granskning & Sj√§lvstudier

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) av Christopher Olah.

## [Uppgift: Anteckningsb√∂cker](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r du vara medveten om att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.