<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbd3f73e4139f030ecb2e20387d70fee",
  "translation_date": "2025-09-23T09:26:11+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "sv"
}
-->
# Representera text som tensorer

## [F√∂rhandsquiz](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Textklassificering

I den f√∂rsta delen av detta avsnitt kommer vi att fokusera p√• uppgiften **textklassificering**. Vi kommer att anv√§nda [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)-datasetet, som inneh√•ller nyhetsartiklar som f√∂ljande:

* Kategori: Vetenskap/Teknik  
* Titel: Ky. F√∂retag vinner bidrag f√∂r att studera peptider (AP)  
* Br√∂dtext: AP - Ett f√∂retag grundat av en kemiforskare vid University of Louisville vann ett bidrag f√∂r att utveckla...

V√•rt m√•l kommer att vara att klassificera nyhetsartikeln i en av kategorierna baserat p√• texten.

## Att representera text

Om vi vill l√∂sa uppgifter inom Natural Language Processing (NLP) med neurala n√§tverk beh√∂ver vi ett s√§tt att representera text som tensorer. Datorer representerar redan texttecken som siffror som mappas till typsnitt p√• din sk√§rm med hj√§lp av kodningar som ASCII eller UTF-8.

<img alt="Bild som visar ett diagram som mappar ett tecken till en ASCII- och bin√§r representation" src="images/ascii-character-map.png" width="50%"/>

> [Bildk√§lla](https://www.seobility.net/en/wiki/ASCII)

Som m√§nniskor f√∂rst√•r vi vad varje bokstav **representerar** och hur alla tecken tillsammans bildar orden i en mening. Datorer har dock inte en s√•dan f√∂rst√•else p√• egen hand, och det neurala n√§tverket m√•ste l√§ra sig betydelsen under tr√§ningen.

D√§rf√∂r kan vi anv√§nda olika metoder f√∂r att representera text:

* **Teckenniv√•representation**, d√§r vi representerar text genom att behandla varje tecken som ett nummer. Givet att vi har *C* olika tecken i v√•r textkorpus, skulle ordet *Hello* representeras av en 5x*C*-tensor. Varje bokstav skulle motsvara en tensor-kolumn i en one-hot-kodning.  
* **Ordniv√•representation**, d√§r vi skapar ett **ordf√∂rr√•d** av alla ord i v√•r text och sedan representerar ord med hj√§lp av one-hot-kodning. Denna metod √§r n√•got b√§ttre eftersom varje bokstav i sig inte har mycket betydelse, och genom att anv√§nda h√∂gre semantiska koncept - ord - f√∂renklar vi uppgiften f√∂r det neurala n√§tverket. Dock, med tanke p√• den stora ordbokens storlek, m√•ste vi hantera h√∂gdimensionella glesa tensorer.

Oavsett representation m√•ste vi f√∂rst konvertera texten till en sekvens av **token**, d√§r en token kan vara ett tecken, ett ord eller ibland till och med en del av ett ord. Sedan konverterar vi token till ett nummer, vanligtvis med hj√§lp av ett **ordf√∂rr√•d**, och detta nummer kan matas in i ett neuralt n√§tverk med hj√§lp av one-hot-kodning.

## N-Gram

I naturligt spr√•k kan den exakta betydelsen av ord endast best√§mmas i sitt sammanhang. Till exempel har *neuralt n√§tverk* och *fiskn√§tverk* helt olika betydelser. Ett s√§tt att ta h√§nsyn till detta √§r att bygga v√•r modell p√• par av ord och betrakta ordpar som separata ordf√∂rr√•dstoken. P√• detta s√§tt kommer meningen *I like to go fishing* att representeras av f√∂ljande sekvens av token: *I like*, *like to*, *to go*, *go fishing*. Problemet med denna metod √§r att ordf√∂rr√•dets storlek √∂kar avsev√§rt, och kombinationer som *go fishing* och *go shopping* representeras av olika token, som inte delar n√•gon semantisk likhet trots samma verb.

I vissa fall kan vi √∂verv√§ga att anv√§nda tri-gram ‚Äì kombinationer av tre ord ‚Äì ocks√•. D√§rf√∂r kallas denna metod ofta f√∂r **n-gram**. Det kan ocks√• vara vettigt att anv√§nda n-gram med teckenniv√•representation, d√§r n-gram ungef√§r motsvarar olika stavelser.

## Bag-of-Words och TF/IDF

N√§r vi l√∂ser uppgifter som textklassificering beh√∂ver vi kunna representera text med en fast storlek p√• vektorn, som vi kommer att anv√§nda som indata till den slutliga t√§ta klassificeraren. Ett av de enklaste s√§tten att g√∂ra detta √§r att kombinera alla individuella ordrepresentationer, t.ex. genom att addera dem. Om vi adderar one-hot-kodningar av varje ord kommer vi att f√• en frekvensvektor som visar hur m√•nga g√•nger varje ord f√∂rekommer i texten. En s√•dan representation av text kallas **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Bild av f√∂rfattaren

En BoW representerar i huvudsak vilka ord som f√∂rekommer i texten och i vilka m√§ngder, vilket verkligen kan vara en bra indikation p√• vad texten handlar om. Till exempel √§r det troligt att en nyhetsartikel om politik inneh√•ller ord som *president* och *land*, medan en vetenskaplig publikation skulle ha n√•got som *kolliderare*, *uppt√§ckt*, etc. S√•ledes kan ordfrekvenser i m√•nga fall vara en bra indikator p√• textens inneh√•ll.

Problemet med BoW √§r att vissa vanliga ord, som *och*, *√§r*, etc., f√∂rekommer i de flesta texter och har de h√∂gsta frekvenserna, vilket d√∂ljer de ord som verkligen √§r viktiga. Vi kan minska vikten av dessa ord genom att ta h√§nsyn till frekvensen med vilken ord f√∂rekommer i hela dokumentkollektionen. Detta √§r huvudid√©n bakom TF/IDF-metoden, som behandlas mer i detalj i de bifogade anteckningsb√∂ckerna till denna lektion.

Dock kan ingen av dessa metoder fullt ut ta h√§nsyn till textens **semantik**. Vi beh√∂ver kraftfullare neurala n√§tverksmodeller f√∂r att g√∂ra detta, vilket vi kommer att diskutera senare i detta avsnitt.

## ‚úçÔ∏è √ñvningar: Textrepresentation

Forts√§tt ditt l√§rande i f√∂ljande anteckningsb√∂cker:

* [Textrepresentation med PyTorch](TextRepresentationPyTorch.ipynb)  
* [Textrepresentation med TensorFlow](TextRepresentationTF.ipynb)  

## Slutsats

Hittills har vi studerat tekniker som kan l√§gga till frekvensvikt till olika ord. De kan dock inte representera betydelse eller ordning. Som den ber√∂mda lingvisten J. R. Firth sa 1935: "Den fullst√§ndiga betydelsen av ett ord √§r alltid kontextuell, och ingen studie av betydelse utanf√∂r kontext kan tas p√• allvar." Vi kommer senare i kursen att l√§ra oss hur man f√•ngar kontextuell information fr√•n text med hj√§lp av spr√•kmodellering.

## üöÄ Utmaning

Prova n√•gra andra √∂vningar med bag-of-words och olika datamodeller. Du kanske blir inspirerad av denna [t√§vling p√• Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Efterf√∂rel√§sningsquiz](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## Granskning och sj√§lvstudier

√ñva dina f√§rdigheter med textinb√§ddningar och bag-of-words-tekniker p√• [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Uppgift: Anteckningsb√∂cker](assignment.md)

---

