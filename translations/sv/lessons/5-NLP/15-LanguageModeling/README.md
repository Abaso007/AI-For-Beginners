<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T09:23:36+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "sv"
}
-->
# Spr√•kmodellering

Semantiska inb√§ddningar, s√•som Word2Vec och GloVe, √§r faktiskt ett f√∂rsta steg mot **spr√•kmodellering** ‚Äì att skapa modeller som p√• n√•got s√§tt *f√∂rst√•r* (eller *representerar*) spr√•kets natur.

## [Quiz f√∂re f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Huvudid√©n bakom spr√•kmodellering √§r att tr√§na dem p√• oetiketterade dataset p√• ett osuperviserat s√§tt. Detta √§r viktigt eftersom vi har enorma m√§ngder oetiketterad text tillg√§nglig, medan m√§ngden etiketterad text alltid skulle vara begr√§nsad av den tid och anstr√§ngning vi kan l√§gga p√• att etikettera. Oftast kan vi bygga spr√•kmodeller som kan **f√∂ruts√§ga saknade ord** i texten, eftersom det √§r enkelt att maskera ett slumpm√§ssigt ord i texten och anv√§nda det som ett tr√§ningsprov.

## Tr√§ning av inb√§ddningar

I v√•ra tidigare exempel anv√§nde vi f√∂rtr√§nade semantiska inb√§ddningar, men det √§r intressant att se hur dessa inb√§ddningar kan tr√§nas. Det finns flera m√∂jliga id√©er som kan anv√§ndas:

* **N-Gram** spr√•kmodellering, d√§r vi f√∂rutsp√•r en token genom att titta p√• N f√∂reg√•ende tokens (N-gram).
* **Continuous Bag-of-Words** (CBoW), d√§r vi f√∂rutsp√•r den mittersta token $W_0$ i en sekvens av tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, d√§r vi f√∂rutsp√•r en upps√§ttning n√§rliggande tokens {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} fr√•n den mittersta token $W_0$.

![bild fr√•n artikel om att konvertera ord till vektorer](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sv.png)

> Bild fr√•n [denna artikel](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Exempel p√• Notebooks: Tr√§ning av CBoW-modell

Forts√§tt ditt l√§rande i f√∂ljande notebooks:

* [Tr√§ning av CBoW Word2Vec med TensorFlow](CBoW-TF.ipynb)
* [Tr√§ning av CBoW Word2Vec med PyTorch](CBoW-PyTorch.ipynb)

## Slutsats

I den f√∂reg√•ende lektionen s√•g vi att ordinb√§ddningar fungerar som magi! Nu vet vi att tr√§ning av ordinb√§ddningar inte √§r en s√§rskilt komplex uppgift, och vi borde kunna tr√§na v√•ra egna ordinb√§ddningar f√∂r dom√§nspecifik text om det beh√∂vs.

## [Quiz efter f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Granskning & Sj√§lvstudier

* [Officiell PyTorch-handledning om spr√•kmodellering](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Officiell TensorFlow-handledning om att tr√§na Word2Vec-modell](https://www.TensorFlow.org/tutorials/text/word2vec).
* Anv√§ndning av **gensim**-ramverket f√∂r att tr√§na de mest anv√§nda inb√§ddningarna med n√•gra f√• kodrader beskrivs [i denna dokumentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Uppgift: Tr√§na Skip-Gram-modell](lab/README.md)

I labbet utmanar vi dig att modifiera koden fr√•n denna lektion f√∂r att tr√§na en skip-gram-modell ist√§llet f√∂r CBoW. [L√§s detaljerna](lab/README.md)

---

