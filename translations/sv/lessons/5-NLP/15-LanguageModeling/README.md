<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-28T15:53:15+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "sv"
}
-->
# Spr√•kmodellering

Semantiska inb√§ddningar, s√•som Word2Vec och GloVe, √§r faktiskt ett f√∂rsta steg mot **spr√•kmodellering** ‚Äì att skapa modeller som p√• n√•got s√§tt *f√∂rst√•r* (eller *representerar*) spr√•kets natur.

## [F√∂rtest-quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

Huvudid√©n bakom spr√•kmodellering √§r att tr√§na modeller p√• oetiketterade dataset p√• ett o√∂vervakat s√§tt. Detta √§r viktigt eftersom vi har enorma m√§ngder oetiketterad text tillg√§nglig, medan m√§ngden etiketterad text alltid skulle vara begr√§nsad av den tid och anstr√§ngning vi kan l√§gga p√• att skapa etiketter. Oftast kan vi bygga spr√•kmodeller som kan **f√∂ruts√§ga saknade ord** i texten, eftersom det √§r enkelt att maskera ett slumpm√§ssigt ord i texten och anv√§nda det som ett tr√§ningsprov.

## Tr√§ning av inb√§ddningar

I v√•ra tidigare exempel anv√§nde vi f√∂rtr√§nade semantiska inb√§ddningar, men det √§r intressant att se hur dessa inb√§ddningar kan tr√§nas. Det finns flera m√∂jliga id√©er som kan anv√§ndas:

* **N-Gram** spr√•kmodellering, d√§r vi f√∂ruts√§ger en token genom att titta p√• N f√∂reg√•ende tokens (N-gram).
* **Continuous Bag-of-Words** (CBoW), d√§r vi f√∂ruts√§ger den mittersta token $W_0$ i en sekvens av tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, d√§r vi f√∂ruts√§ger en upps√§ttning n√§rliggande tokens {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} fr√•n den mittersta token $W_0$.

![bild fr√•n artikel om att konvertera ord till vektorer](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sv.png)

> Bild fr√•n [denna artikel](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Exempelnotebooks: Tr√§ning av CBoW-modell

Forts√§tt ditt l√§rande i f√∂ljande notebooks:

* [Tr√§ning av CBoW Word2Vec med TensorFlow](CBoW-TF.ipynb)
* [Tr√§ning av CBoW Word2Vec med PyTorch](CBoW-PyTorch.ipynb)

## Slutsats

I den f√∂reg√•ende lektionen s√•g vi att ordb√§ddningar fungerar som magi! Nu vet vi att tr√§ning av ordb√§ddningar inte √§r en s√§rskilt komplex uppgift, och vi b√∂r kunna tr√§na v√•ra egna ordb√§ddningar f√∂r textsamlingar inom specifika dom√§ner om det beh√∂vs.

## [Eftertest-quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Granskning & Sj√§lvstudier

* [Officiell PyTorch-handledning om spr√•kmodellering](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Officiell TensorFlow-handledning om tr√§ning av Word2Vec-modell](https://www.TensorFlow.org/tutorials/text/word2vec).
* Anv√§ndning av **gensim**-ramverket f√∂r att tr√§na de mest anv√§nda inb√§ddningarna med n√•gra f√• kodrader beskrivs [i denna dokumentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Uppgift: Tr√§na Skip-Gram-modell](lab/README.md)

I labbet utmanar vi dig att modifiera koden fr√•n denna lektion f√∂r att tr√§na en skip-gram-modell ist√§llet f√∂r CBoW. [L√§s detaljerna](lab/README.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som kan uppst√• vid anv√§ndning av denna √∂vers√§ttning.