<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "51be6057374d01d70e07dd5ec88ebc0d",
  "translation_date": "2025-09-23T09:23:09+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "sv"
}
-->
# Generativa n√§tverk

## [Quiz f√∂re f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Recurrent Neural Networks (RNNs) och deras varianter med grindade celler, s√•som Long Short Term Memory Cells (LSTMs) och Gated Recurrent Units (GRUs), erbjuder en mekanism f√∂r spr√•kmodellering genom att de kan l√§ra sig ordordning och ge f√∂ruts√§gelser f√∂r n√§sta ord i en sekvens. Detta g√∂r att vi kan anv√§nda RNNs f√∂r **generativa uppgifter**, s√•som vanlig textgenerering, maskin√∂vers√§ttning och till och med bildbeskrivning.

> ‚úÖ T√§nk p√• alla g√•nger du har haft nytta av generativa uppgifter, som textkomplettering n√§r du skriver. G√∂r lite research om dina favoritapplikationer f√∂r att se om de anv√§nde RNNs.

I RNN-arkitekturen som vi diskuterade i f√∂reg√•ende enhet, producerade varje RNN-enhet n√§sta dolda tillst√•nd som en output. Men vi kan ocks√• l√§gga till en annan output till varje √•terkommande enhet, vilket skulle g√∂ra det m√∂jligt att generera en **sekvens** (som √§r lika l√•ng som den ursprungliga sekvensen). Dessutom kan vi anv√§nda RNN-enheter som inte tar emot en input vid varje steg, utan bara tar en initial tillst√•ndsvektor och sedan producerar en sekvens av outputs.

Detta m√∂jligg√∂r olika neurala arkitekturer som visas i bilden nedan:

![Bild som visar vanliga m√∂nster f√∂r √•terkommande neurala n√§tverk.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.sv.jpg)

> Bild fr√•n blogginl√§gget [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) av [Andrej Karpaty](http://karpathy.github.io/)

* **One-to-one** √§r ett traditionellt neuralt n√§tverk med en input och en output
* **One-to-many** √§r en generativ arkitektur som tar emot ett inputv√§rde och genererar en sekvens av outputv√§rden. Till exempel, om vi vill tr√§na ett n√§tverk f√∂r **bildbeskrivning** som skulle producera en textuell beskrivning av en bild, kan vi ta en bild som input, passera den genom en CNN f√∂r att f√• dess dolda tillst√•nd och sedan l√•ta en √•terkommande kedja generera beskrivningen ord f√∂r ord.
* **Many-to-one** motsvarar RNN-arkitekturerna vi beskrev i f√∂reg√•ende enhet, s√•som textklassificering.
* **Many-to-many**, eller **sequence-to-sequence**, motsvarar uppgifter som **maskin√∂vers√§ttning**, d√§r vi f√∂rst har en RNN som samlar all information fr√•n inputsekvensen till det dolda tillst√•ndet, och en annan RNN-kedja som rullar ut detta tillst√•nd till outputsekvensen.

I denna enhet kommer vi att fokusera p√• enkla generativa modeller som hj√§lper oss att generera text. F√∂r enkelhetens skull kommer vi att anv√§nda tokenisering p√• teckenniv√•.

Vi kommer att tr√§na denna RNN att generera text steg f√∂r steg. Vid varje steg tar vi en sekvens av tecken med l√§ngden `nchars` och ber n√§tverket att generera n√§sta outputtecken f√∂r varje inputtecken:

![Bild som visar ett exempel p√• RNN-generering av ordet 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.sv.png)

N√§r vi genererar text (under inferens), b√∂rjar vi med en **prompt**, som passeras genom RNN-celler f√∂r att generera dess mellanliggande tillst√•nd, och sedan b√∂rjar genereringen fr√•n detta tillst√•nd. Vi genererar ett tecken i taget och skickar tillst√•ndet och det genererade tecknet till en annan RNN-cell f√∂r att generera n√§sta, tills vi har genererat tillr√§ckligt m√•nga tecken.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Bild av f√∂rfattaren

## ‚úçÔ∏è √ñvningar: Generativa n√§tverk

Forts√§tt ditt l√§rande i f√∂ljande notebooks:

* [Generativa n√§tverk med PyTorch](GenerativePyTorch.ipynb)
* [Generativa n√§tverk med TensorFlow](GenerativeTF.ipynb)

## Mjuk textgenerering och temperatur

Outputen fr√•n varje RNN-cell √§r en sannolikhetsf√∂rdelning av tecken. Om vi alltid tar tecknet med h√∂gst sannolikhet som n√§sta tecken i den genererade texten, kan texten ofta bli "cyklisk" mellan samma teckensekvenser om och om igen, som i detta exempel:

```
today of the second the company and a second the company ...
```

Men om vi tittar p√• sannolikhetsf√∂rdelningen f√∂r n√§sta tecken, kan det vara s√• att skillnaden mellan n√•gra av de h√∂gsta sannolikheterna inte √§r stor, t.ex. ett tecken kan ha sannolikheten 0.2, ett annat 0.19, etc. Till exempel, n√§r vi letar efter n√§sta tecken i sekvensen '*play*', kan n√§sta tecken lika g√§rna vara ett mellanslag eller **e** (som i ordet *player*).

Detta leder oss till slutsatsen att det inte alltid √§r "r√§ttvist" att v√§lja tecknet med h√∂gst sannolikhet, eftersom att v√§lja det n√§st h√∂gsta fortfarande kan leda till meningsfull text. Det √§r klokare att **sampla** tecken fr√•n sannolikhetsf√∂rdelningen som n√§tverket ger som output. Vi kan ocks√• anv√§nda en parameter, **temperatur**, som kommer att j√§mna ut sannolikhetsf√∂rdelningen om vi vill l√§gga till mer slumpm√§ssighet, eller g√∂ra den brantare om vi vill h√•lla oss mer till tecken med h√∂gst sannolikhet.

Utforska hur denna mjuka textgenerering implementeras i de notebooks som √§r l√§nkade ovan.

## Slutsats

√Ñven om textgenerering kan vara anv√§ndbar i sig, kommer de st√∂rsta f√∂rdelarna fr√•n m√∂jligheten att generera text med RNNs fr√•n en initial funktionsvektor. Till exempel anv√§nds textgenerering som en del av maskin√∂vers√§ttning (sequence-to-sequence, i detta fall anv√§nds tillst√•ndsvektorn fr√•n *encoder* f√∂r att generera eller *dekoda* det √∂versatta meddelandet), eller f√∂r att generera textuell beskrivning av en bild (i vilket fall funktionsvektorn skulle komma fr√•n CNN-extraktorn).

## üöÄ Utmaning

Ta n√•gra lektioner p√• Microsoft Learn om detta √§mne

* Textgenerering med [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz efter f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Granskning & Sj√§lvstudier

H√§r √§r n√•gra artiklar f√∂r att ut√∂ka din kunskap

* Olika metoder f√∂r textgenerering med Markov Chain, LSTM och GPT-2: [blogginl√§gg](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exempel p√• textgenerering i [Keras dokumentation](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Uppgift](lab/README.md)

Vi har sett hur man genererar text tecken f√∂r tecken. I labben kommer du att utforska textgenerering p√• ordniv√•.

---

