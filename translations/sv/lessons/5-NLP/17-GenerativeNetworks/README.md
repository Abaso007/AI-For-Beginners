<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-28T15:51:51+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "sv"
}
-->
# Generativa n√§tverk

## [Quiz f√∂re f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Recurrent Neural Networks (RNNs) och deras varianter med grindade celler, s√•som Long Short Term Memory Cells (LSTMs) och Gated Recurrent Units (GRUs), erbjuder en mekanism f√∂r spr√•kmodellering genom att de kan l√§ra sig ordordning och ge f√∂ruts√§gelser f√∂r n√§sta ord i en sekvens. Detta g√∂r det m√∂jligt att anv√§nda RNNs f√∂r **generativa uppgifter**, s√•som vanlig textgenerering, maskin√∂vers√§ttning och till och med bildbeskrivning.

> ‚úÖ T√§nk p√• alla g√•nger du har haft nytta av generativa uppgifter, som textkomplettering n√§r du skriver. G√∂r lite research om dina favoritapplikationer f√∂r att se om de anv√§nder RNNs.

I RNN-arkitekturen som vi diskuterade i f√∂reg√•ende enhet, producerade varje RNN-enhet n√§sta dolda tillst√•nd som en utg√•ng. Men vi kan ocks√• l√§gga till en annan utg√•ng till varje √•terkommande enhet, vilket g√∂r det m√∂jligt att generera en **sekvens** (som √§r lika l√•ng som den ursprungliga sekvensen). Dessutom kan vi anv√§nda RNN-enheter som inte tar emot en inmatning vid varje steg, utan bara tar en initial tillst√•ndsvektor och sedan producerar en sekvens av utg√•ngar.

Detta m√∂jligg√∂r olika neurala arkitekturer som visas i bilden nedan:

![Bild som visar vanliga m√∂nster f√∂r √•terkommande neurala n√§tverk.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.sv.jpg)

> Bild fr√•n blogginl√§gget [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) av [Andrej Karpaty](http://karpathy.github.io/)

* **Ett-till-ett** √§r ett traditionellt neuralt n√§tverk med en inmatning och en utg√•ng.
* **Ett-till-m√•nga** √§r en generativ arkitektur som tar emot ett inmatningsv√§rde och genererar en sekvens av utg√•ngsv√§rden. Till exempel, om vi vill tr√§na ett n√§tverk f√∂r **bildbeskrivning** som genererar en textuell beskrivning av en bild, kan vi ta en bild som inmatning, passera den genom en CNN f√∂r att f√• dess dolda tillst√•nd och sedan l√•ta en √•terkommande kedja generera beskrivningen ord f√∂r ord.
* **M√•nga-till-ett** motsvarar RNN-arkitekturerna vi beskrev i f√∂reg√•ende enhet, s√•som textklassificering.
* **M√•nga-till-m√•nga**, eller **sekvens-till-sekvens**, motsvarar uppgifter som **maskin√∂vers√§ttning**, d√§r vi f√∂rst l√•ter en RNN samla all information fr√•n inmatningssekvensen till det dolda tillst√•ndet, och en annan RNN-kedja vecklar ut detta tillst√•nd till utg√•ngssekvensen.

I denna enhet kommer vi att fokusera p√• enkla generativa modeller som hj√§lper oss att generera text. F√∂r enkelhetens skull kommer vi att anv√§nda tokenisering p√• teckenniv√•.

Vi kommer att tr√§na denna RNN att generera text steg f√∂r steg. Vid varje steg tar vi en sekvens av tecken med l√§ngden `nchars` och ber n√§tverket att generera n√§sta utg√•ngstecken f√∂r varje inmatningstecken:

![Bild som visar ett exempel p√• RNN-generering av ordet 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.sv.png)

N√§r vi genererar text (under inferens) b√∂rjar vi med en **prompt**, som passeras genom RNN-celler f√∂r att generera dess mellanliggande tillst√•nd, och sedan b√∂rjar genereringen fr√•n detta tillst√•nd. Vi genererar ett tecken i taget och skickar tillst√•ndet och det genererade tecknet till en annan RNN-cell f√∂r att generera n√§sta, tills vi har genererat tillr√§ckligt m√•nga tecken.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Bild av f√∂rfattaren

## ‚úçÔ∏è √ñvningar: Generativa n√§tverk

Forts√§tt ditt l√§rande i f√∂ljande notebooks:

* [Generativa n√§tverk med PyTorch](GenerativePyTorch.ipynb)
* [Generativa n√§tverk med TensorFlow](GenerativeTF.ipynb)

## Mjuk textgenerering och temperatur

Utg√•ngen fr√•n varje RNN-cell √§r en sannolikhetsf√∂rdelning av tecken. Om vi alltid v√§ljer tecknet med h√∂gst sannolikhet som n√§sta tecken i den genererade texten, kan texten ofta bli "cyklisk" och upprepa samma teckensekvenser om och om igen, som i detta exempel:

```
today of the second the company and a second the company ...
```

Men om vi tittar p√• sannolikhetsf√∂rdelningen f√∂r n√§sta tecken, kan det vara s√• att skillnaden mellan n√•gra av de h√∂gsta sannolikheterna inte √§r stor, t.ex. ett tecken kan ha sannolikheten 0.2, ett annat 0.19, etc. Till exempel, n√§r vi letar efter n√§sta tecken i sekvensen '*play*', kan n√§sta tecken lika g√§rna vara ett mellanslag eller **e** (som i ordet *player*).

Detta leder oss till slutsatsen att det inte alltid √§r "r√§ttvist" att v√§lja tecknet med h√∂gst sannolikhet, eftersom att v√§lja det n√§st h√∂gsta fortfarande kan leda till meningsfull text. Det √§r klokare att **sampla** tecken fr√•n sannolikhetsf√∂rdelningen som n√§tverket ger. Vi kan ocks√• anv√§nda en parameter, **temperatur**, som j√§mnar ut sannolikhetsf√∂rdelningen om vi vill l√§gga till mer slumpm√§ssighet, eller g√∂ra den brantare om vi vill h√•lla oss mer till tecken med h√∂gst sannolikhet.

Utforska hur denna mjuka textgenerering implementeras i notebooks som √§r l√§nkade ovan.

## Slutsats

√Ñven om textgenerering kan vara anv√§ndbar i sig, kommer de st√∂rsta f√∂rdelarna fr√•n f√∂rm√•gan att generera text med RNNs fr√•n en initial funktionsvektor. Till exempel anv√§nds textgenerering som en del av maskin√∂vers√§ttning (sekvens-till-sekvens, i detta fall anv√§nds tillst√•ndsvektorn fr√•n *encoder* f√∂r att generera eller *dekoda* det √∂versatta meddelandet), eller f√∂r att generera textuell beskrivning av en bild (i vilket fall funktionsvektorn skulle komma fr√•n CNN-extraktorn).

## üöÄ Utmaning

Ta n√•gra lektioner p√• Microsoft Learn om detta √§mne:

* Textgenerering med [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz efter f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Granskning & Sj√§lvstudier

H√§r √§r n√•gra artiklar f√∂r att ut√∂ka din kunskap:

* Olika metoder f√∂r textgenerering med Markov Chain, LSTM och GPT-2: [blogginl√§gg](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exempel p√• textgenerering i [Keras-dokumentation](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Uppgift](lab/README.md)

Vi har sett hur man genererar text tecken f√∂r tecken. I labben kommer du att utforska textgenerering p√• ordniv√•.

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiska √∂vers√§ttningar kan inneh√•lla fel eller inexaktheter. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.