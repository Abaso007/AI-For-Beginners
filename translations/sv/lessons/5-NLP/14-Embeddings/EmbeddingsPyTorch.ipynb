{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inbäddningar\n",
    "\n",
    "I vårt tidigare exempel arbetade vi med högdimensionella bag-of-words-vektorer med längden `vocab_size`, och vi konverterade uttryckligen från lågdimensionella positionsrepresentationsvektorer till glesa one-hot-representationer. Denna one-hot-representation är inte minneseffektiv, dessutom behandlas varje ord oberoende av varandra, dvs. one-hot-kodade vektorer uttrycker ingen semantisk likhet mellan ord.\n",
    "\n",
    "I denna enhet kommer vi att fortsätta utforska **News AG**-datasetet. För att börja, låt oss ladda data och hämta några definitioner från den tidigare notebooken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vad är inbäddning?\n",
    "\n",
    "Idén med **inbäddning** är att representera ord med lägre dimensionella täta vektorer, som på något sätt reflekterar ett ords semantiska betydelse. Vi kommer senare att diskutera hur man bygger meningsfulla ordinbäddningar, men för tillfället kan vi tänka på inbäddningar som ett sätt att minska dimensionen av en ordvektor.\n",
    "\n",
    "Så, en inbäddningslager skulle ta ett ord som input och producera en utgångsvektor med en specificerad `embedding_size`. På ett sätt är det väldigt likt `Linear`-lagret, men istället för att ta en one-hot-kodad vektor, kan det ta ett ordnummer som input.\n",
    "\n",
    "Genom att använda inbäddningslagret som det första lagret i vårt nätverk kan vi byta från bag-of-words till **embedding bag**-modellen, där vi först konverterar varje ord i vår text till motsvarande inbäddning och sedan beräknar någon aggregeringsfunktion över alla dessa inbäddningar, såsom `sum`, `average` eller `max`.\n",
    "\n",
    "![Bild som visar en inbäddningsklassificerare för fem sekvensord.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sv.png)\n",
    "\n",
    "Vårt klassificerande neurala nätverk kommer att börja med ett inbäddningslager, sedan ett aggregeringslager och en linjär klassificerare ovanpå det:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hantering av variabel sekvensstorlek\n",
    "\n",
    "Som ett resultat av denna arkitektur behöver minibatcher till vårt nätverk skapas på ett visst sätt. I den föregående enheten, när vi använde bag-of-words, hade alla BoW-tenstorer i en minibatch samma storlek `vocab_size`, oavsett den faktiska längden på vår textsekvens. När vi övergår till ordinbäddningar kommer vi att ha ett varierande antal ord i varje textprov, och när vi kombinerar dessa prover till minibatcher måste vi använda någon form av utfyllnad.\n",
    "\n",
    "Detta kan göras genom att använda samma teknik som att tillhandahålla en `collate_fn`-funktion till datakällan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Träna inbäddningsklassificerare\n",
    "\n",
    "Nu när vi har definierat en korrekt dataladdare kan vi träna modellen med hjälp av träningsfunktionen som vi definierade i föregående enhet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observera**: Vi tränar endast på 25k poster här (mindre än en full epok) för att spara tid, men du kan fortsätta träna, skriva en funktion för att träna under flera epoker och experimentera med inlärningshastighetsparametern för att uppnå högre noggrannhet. Du bör kunna nå en noggrannhet på cirka 90 %.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBag-lager och representation av sekvenser med variabel längd\n",
    "\n",
    "I den tidigare arkitekturen behövde vi fylla ut alla sekvenser till samma längd för att passa in dem i en minibatch. Detta är inte det mest effektiva sättet att representera sekvenser med variabel längd - ett annat tillvägagångssätt skulle vara att använda en **offset**-vektor, som innehåller offset för alla sekvenser lagrade i en stor vektor.\n",
    "\n",
    "![Bild som visar en offset-sekvensrepresentation](../../../../../translated_images/offset-sequence-representation.eb73fcefb29b46eecfbe74466077cfeb7c0f93a4f254850538a2efbc63517479.sv.png)\n",
    "\n",
    "> **Note**: På bilden ovan visar vi en sekvens av tecken, men i vårt exempel arbetar vi med sekvenser av ord. Principen för att representera sekvenser med en offset-vektor förblir dock densamma.\n",
    "\n",
    "För att arbeta med offset-representation använder vi [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html)-lagret. Det liknar `Embedding`, men det tar innehållsvektorn och offset-vektorn som indata, och det inkluderar också ett aggregeringslager, som kan vara `mean`, `sum` eller `max`.\n",
    "\n",
    "Här är ett modifierat nätverk som använder `EmbeddingBag`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att förbereda datasetet för träning behöver vi tillhandahålla en konverteringsfunktion som kommer att förbereda offsetvektorn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observera att till skillnad från alla tidigare exempel accepterar vårt nätverk nu två parametrar: datavektor och offsetvektor, som har olika storlekar. På samma sätt ger vår dataladdare oss 3 värden istället för 2: både text- och offsetvektorer tillhandahålls som funktioner. Därför måste vi justera vår träningsfunktion något för att hantera detta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantiska inbäddningar: Word2Vec\n",
    "\n",
    "I vårt tidigare exempel lärde sig modellens inbäddningslager att översätta ord till vektorrepresentationer, men denna representation hade inte mycket semantisk betydelse. Det skulle vara bra att lära sig en sådan vektorrepresentation där liknande ord eller synonymer motsvarar vektorer som ligger nära varandra baserat på någon vektordistans (t.ex. euklidisk distans).\n",
    "\n",
    "För att göra detta behöver vi förträna vår inbäddningsmodell på en stor samling text på ett specifikt sätt. Ett av de första sätten att träna semantiska inbäddningar kallas [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Det bygger på två huvudsakliga arkitekturer som används för att skapa en distribuerad representation av ord:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW) — i denna arkitektur tränar vi modellen att förutsäga ett ord utifrån den omgivande kontexten. Givet ngrammet $(W_{-2},W_{-1},W_0,W_1,W_2)$ är målet för modellen att förutsäga $W_0$ utifrån $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** är motsatsen till CBoW. Modellen använder det omgivande fönstret av kontextord för att förutsäga det aktuella ordet.\n",
    "\n",
    "CBoW är snabbare, medan skip-gram är långsammare men gör ett bättre jobb med att representera sällsynta ord.\n",
    "\n",
    "![Bild som visar både CBoW- och Skip-Gram-algoritmer för att konvertera ord till vektorer.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sv.png)\n",
    "\n",
    "För att experimentera med Word2Vec-inbäddningar förtränade på Google News-datasetet kan vi använda **gensim**-biblioteket. Nedan hittar vi de ord som är mest lik 'neural'.\n",
    "\n",
    "> **Note:** När du först skapar ordvektorer kan nedladdningen ta lite tid!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan också beräkna vektorinbäddningar från ordet, för att användas vid träning av klassificeringsmodellen (vi visar endast de första 20 komponenterna av vektorn för tydlighet):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det fantastiska med semantiska inbäddningar är att du kan manipulera vektorkodningen för att ändra semantiken. Till exempel kan vi be om att hitta ett ord vars vektorrepresentation är så nära som möjligt orden *kung* och *kvinna*, och så långt bort som möjligt från ordet *man*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Både CBoW och Skip-Grams är \"prediktiva\" inbäddningar, eftersom de endast tar hänsyn till lokala kontexter. Word2Vec utnyttjar inte global kontext.\n",
    "\n",
    "**FastText** bygger vidare på Word2Vec genom att lära sig vektorrepresentationer för varje ord och de tecken-n-gram som finns inom varje ord. Värdena för representationerna genomsnittas sedan till en vektor vid varje träningssteg. Även om detta lägger till mycket extra beräkning under förträningen, gör det att ordinbäddningar kan koda information på sub-ordsnivå.\n",
    "\n",
    "En annan metod, **GloVe**, utnyttjar idén om samförekomstmatris och använder neurala metoder för att dekomponera samförekomstmatrisen till mer uttrycksfulla och icke-linjära ordvektorer.\n",
    "\n",
    "Du kan experimentera med exemplet genom att ändra inbäddningar till FastText och GloVe, eftersom gensim stödjer flera olika modeller för ordinbäddning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Använda förtränade inbäddningar i PyTorch\n",
    "\n",
    "Vi kan ändra exemplet ovan för att förfylla matrisen i vårt inbäddningslager med semantiska inbäddningar, såsom Word2Vec. Vi måste ta hänsyn till att vokabulärerna för förtränade inbäddningar och vår textkorpus sannolikt inte kommer att matcha, så vi kommer att initialisera vikterna för de saknade orden med slumpmässiga värden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ska vi träna vår modell. Observera att tiden det tar att träna modellen är avsevärt längre än i det tidigare exemplet, på grund av den större storleken på inbäddningslagret och därmed ett mycket högre antal parametrar. Dessutom kan vi behöva träna vår modell på fler exempel om vi vill undvika överanpassning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I vårt fall ser vi inte någon stor ökning i noggrannhet, vilket troligen beror på ganska olika ordförråd.  \n",
    "För att lösa problemet med olika ordförråd kan vi använda en av följande lösningar:  \n",
    "* Träna om word2vec-modellen på vårt ordförråd  \n",
    "* Ladda vår dataset med ordförrådet från den förtränade word2vec-modellen. Ordförrådet som används för att ladda datasetet kan specificeras under laddningen.  \n",
    "\n",
    "Den senare metoden verkar enklare, särskilt eftersom PyTorch `torchtext`-ramverket innehåller inbyggt stöd för inbäddningar. Vi kan till exempel skapa ett GloVe-baserat ordförråd på följande sätt:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laddat ordförråd har följande grundläggande operationer:\n",
    "* `vocab.stoi` är en ordbok som låter oss konvertera ett ord till dess index i ordboken\n",
    "* `vocab.itos` gör motsatsen - konverterar ett nummer till ett ord\n",
    "* `vocab.vectors` är matrisen av inbäddningsvektorer, så för att få inbäddningen av ett ord `s` behöver vi använda `vocab.vectors[vocab.stoi[s]]`\n",
    "\n",
    "Här är ett exempel på hur man manipulerar inbäddningar för att demonstrera ekvationen **snäll-man+kvinna = drottning** (jag var tvungen att justera koefficienten lite för att få det att fungera):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att träna klassificeraren med hjälp av dessa inbäddningar måste vi först koda vår dataset med GloVe-ordförråd:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi har sett ovan, lagras alla vektorinbäddningar i matrisen `vocab.vectors`. Det gör det superenkelt att ladda dessa vikter i vikterna för inbäddningslagret genom enkel kopiering:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu låt oss träna vår modell och se om vi får bättre resultat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En av anledningarna till att vi inte ser en betydande ökning i noggrannhet är att vissa ord från vår dataset saknas i den förtränade GloVe-ordlistan och därför i princip ignoreras. För att övervinna detta kan vi träna våra egna inbäddningar på vår dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontextuella Embeddingar\n",
    "\n",
    "En viktig begränsning med traditionella förtränade embedding-representationer som Word2Vec är problemet med ords betydelseutredning. Även om förtränade embeddingar kan fånga en del av ordens betydelse i kontext, kodas alla möjliga betydelser av ett ord in i samma embedding. Detta kan orsaka problem i nedströmsmodeller, eftersom många ord, som ordet 'play', har olika betydelser beroende på sammanhanget de används i.\n",
    "\n",
    "Till exempel har ordet 'play' i dessa två meningar ganska olika betydelser:\n",
    "- Jag gick på en **pjäs** på teatern.\n",
    "- John vill **leka** med sina vänner.\n",
    "\n",
    "De förtränade embeddingarna ovan representerar båda dessa betydelser av ordet 'play' i samma embedding. För att övervinna denna begränsning behöver vi bygga embeddingar baserade på **språkmodellen**, som tränas på en stor textkorpus och *vet* hur ord kan sättas ihop i olika kontexter. Att diskutera kontextuella embeddingar ligger utanför ramen för denna handledning, men vi kommer tillbaka till dem när vi pratar om språkmodeller i nästa enhet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfriskrivning**:  \nDetta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör det noteras att automatiserade översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell human översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som kan uppstå vid användning av denna översättning.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-28T17:49:36+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "sv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}