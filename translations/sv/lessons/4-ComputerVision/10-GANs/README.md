<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0ff65b4da07b23697235de2beb2a3c25",
  "translation_date": "2025-09-23T09:19:20+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "sv"
}
-->
# Generativa Adversariella N√§tverk

I f√∂reg√•ende avsnitt l√§rde vi oss om **generativa modeller**: modeller som kan generera nya bilder som liknar de i tr√§ningsdatam√§ngden. VAE var ett bra exempel p√• en generativ modell.

## [Quiz f√∂re f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Men om vi f√∂rs√∂ker generera n√•got riktigt meningsfullt, som en m√•lning med rimlig uppl√∂sning, med VAE, kommer vi att m√§rka att tr√§ningen inte konvergerar s√§rskilt bra. F√∂r detta anv√§ndningsomr√•de b√∂r vi l√§ra oss om en annan arkitektur som √§r specifikt inriktad p√• generativa modeller - **Generativa Adversariella N√§tverk**, eller GANs.

Huvudid√©n med en GAN √§r att ha tv√• neurala n√§tverk som tr√§nas mot varandra:

<img src="images/gan_architecture.png" width="70%"/>

> Bild av [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Lite vokabul√§r:
> * **Generator** √§r ett n√§tverk som tar en slumpm√§ssig vektor och producerar en bild som resultat.
> * **Discriminator** √§r ett n√§tverk som tar en bild och ska avg√∂ra om det √§r en riktig bild (fr√•n tr√§ningsdatam√§ngden) eller om den genererats av en generator. Det √§r i grunden en bildklassificerare.

### Discriminator

Discriminatorns arkitektur skiljer sig inte fr√•n ett vanligt bildklassificeringsn√§tverk. I det enklaste fallet kan det vara en helt ansluten klassificerare, men oftast kommer det att vara ett [konvolutionellt n√§tverk](../07-ConvNets/README.md).

> ‚úÖ En GAN baserad p√• konvolutionella n√§tverk kallas en [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

En CNN-discriminator best√•r av f√∂ljande lager: flera konvolutioner + pooling (med minskande spatial storlek) och en eller flera helt anslutna lager f√∂r att f√• en "funktionsvektor", samt en slutlig bin√§r klassificerare.

> ‚úÖ "Pooling" i detta sammanhang √§r en teknik som minskar bildens storlek. "Pooling-lager minskar datans dimensioner genom att kombinera utg√•ngarna fr√•n neuronkluster i ett lager till en enda neuron i n√§sta lager." - [k√§lla](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

En generator √§r n√•got mer komplicerad. Du kan se den som en omv√§nd discriminator. Med start fr√•n en latent vektor (ist√§llet f√∂r en funktionsvektor) har den ett helt anslutet lager f√∂r att omvandla den till √∂nskad storlek/form, f√∂ljt av dekonvolutioner + uppskalning. Detta liknar *dekoder*-delen av [autoencoder](../09-Autoencoders/README.md).

> ‚úÖ Eftersom konvolutionslagret implementeras som ett linj√§rt filter som traverserar bilden, √§r dekonvolution i princip liknande konvolution och kan implementeras med samma lagerlogik.

<img src="images/gan_arch_detail.png" width="70%"/>

> Bild av [Dmitry Soshnikov](http://soshnikov.com)

### Tr√§ning av GAN

GANs kallas **adversariella** eftersom det p√•g√•r en st√§ndig t√§vling mellan generatorn och discriminatorn. Under denna t√§vling f√∂rb√§ttras b√•de generatorn och discriminatorn, vilket g√∂r att n√§tverket l√§r sig att producera b√§ttre och b√§ttre bilder.

Tr√§ningen sker i tv√• steg:

* **Tr√§ning av discriminatorn**. Denna uppgift √§r ganska enkel: vi genererar en batch av bilder med generatorn, m√§rker dem som 0 (vilket st√•r f√∂r falska bilder), och tar en batch av bilder fr√•n inmatningsdatam√§ngden (med etikett 1, riktiga bilder). Vi f√•r en *discriminatorf√∂rlust* och utf√∂r backpropagation.
* **Tr√§ning av generatorn**. Detta √§r n√•got mer komplicerat eftersom vi inte direkt vet det f√∂rv√§ntade resultatet f√∂r generatorn. Vi tar hela GAN-n√§tverket som best√•r av en generator f√∂ljt av en discriminator, matar det med n√•gra slumpm√§ssiga vektorer och f√∂rv√§ntar oss att resultatet ska vara 1 (motsvarande riktiga bilder). Vi fryser sedan parametrarna f√∂r discriminatorn (vi vill inte att den ska tr√§nas i detta steg) och utf√∂r backpropagation.

Under denna process g√•r varken generatorns eller discriminatorns f√∂rluster ner s√§rskilt mycket. I en idealisk situation b√∂r de oscillera, vilket motsvarar att b√•da n√§tverken f√∂rb√§ttrar sin prestanda.

## ‚úçÔ∏è √ñvningar: GANs

* [GAN Notebook i TensorFlow/Keras](GANTF.ipynb)
* [GAN Notebook i PyTorch](GANPyTorch.ipynb)

### Problem med GAN-tr√§ning

GANs √§r k√§nda f√∂r att vara s√§rskilt sv√•ra att tr√§na. H√§r √§r n√•gra problem:

* **Mode Collapse**. Med detta menas att generatorn l√§r sig att producera en framg√•ngsrik bild som lurar discriminatorn, men inte en variation av olika bilder.
* **K√§nslighet f√∂r hyperparametrar**. Ofta kan man se att en GAN inte konvergerar alls, och sedan pl√∂tsligt minskar inl√§rningshastigheten vilket leder till konvergens.
* Att h√•lla en **balans** mellan generatorn och discriminatorn. I m√•nga fall kan discriminatorns f√∂rlust sjunka till noll relativt snabbt, vilket resulterar i att generatorn inte kan tr√§na vidare. F√∂r att √∂vervinna detta kan vi f√∂rs√∂ka s√§tta olika inl√§rningshastigheter f√∂r generatorn och discriminatorn, eller hoppa √∂ver tr√§ningen av discriminatorn om f√∂rlusten redan √§r f√∂r l√•g.
* Tr√§ning f√∂r **h√∂g uppl√∂sning**. Detta problem liknar det med autoencoders och uppst√•r eftersom rekonstruktion av f√∂r m√•nga lager i ett konvolutionellt n√§tverk leder till artefakter. Problemet l√∂ses vanligtvis med s√• kallad **progressiv tillv√§xt**, d√§r de f√∂rsta lagren tr√§nas p√• l√•guppl√∂sta bilder och sedan "l√•ses upp" eller l√§ggs till fler lager. En annan l√∂sning √§r att l√§gga till extra kopplingar mellan lager och tr√§na flera uppl√∂sningar samtidigt - se denna [Multi-Scale Gradient GANs-artikel](https://arxiv.org/abs/1903.06048) f√∂r detaljer.

## Stil√∂verf√∂ring

GANs √§r ett utm√§rkt s√§tt att generera konstn√§rliga bilder. En annan intressant teknik √§r s√• kallad **stil√∂verf√∂ring**, som tar en **inneh√•llsbild** och ritar om den i en annan stil genom att applicera filter fr√•n en **stilbild**.

S√• h√§r fungerar det:
* Vi b√∂rjar med en slumpm√§ssig brusbild (eller med en inneh√•llsbild, men f√∂r f√∂rst√•elsens skull √§r det enklare att b√∂rja med slumpm√§ssigt brus).
* V√•rt m√•l √§r att skapa en bild som ligger n√§ra b√•de inneh√•llsbilden och stilbilden. Detta best√§ms av tv√• f√∂rlustfunktioner:
   - **Inneh√•llsf√∂rlust** ber√§knas baserat p√• de funktioner som extraheras av CNN vid vissa lager fr√•n den aktuella bilden och inneh√•llsbilden.
   - **Stilf√∂rlust** ber√§knas mellan den aktuella bilden och stilbilden p√• ett smart s√§tt med hj√§lp av Gram-matriser (mer detaljer i [exempelnotebooken](StyleTransfer.ipynb)).
* F√∂r att g√∂ra bilden mjukare och ta bort brus introducerar vi ocks√• **Variationsf√∂rlust**, som ber√§knar det genomsnittliga avst√•ndet mellan n√§rliggande pixlar.
* Den huvudsakliga optimeringsloopen justerar den aktuella bilden med hj√§lp av gradientnedstigning (eller n√•gon annan optimeringsalgoritm) f√∂r att minimera den totala f√∂rlusten, som √§r en viktad summa av alla tre f√∂rluster.

## ‚úçÔ∏è Exempel: [Stil√∂verf√∂ring](StyleTransfer.ipynb)

## [Quiz efter f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Slutsats

I denna lektion l√§rde du dig om GANs och hur man tr√§nar dem. Du l√§rde dig ocks√• om de speciella utmaningar som denna typ av neuralt n√§tverk kan m√∂ta och n√•gra strategier f√∂r att √∂vervinna dem.

## üöÄ Utmaning

K√∂r igenom [Stil√∂verf√∂ringsnotebooken](StyleTransfer.ipynb) med dina egna bilder.

## Granskning & Sj√§lvstudier

F√∂r referens, l√§s mer om GANs i dessa resurser:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), en *de facto* GAN-arkitektur att √∂verv√§ga
* [Skapa generativ konst med GANs p√• Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Uppgift

G√• tillbaka till en av de tv√• notebookarna som √§r kopplade till denna lektion och tr√§na om GAN p√• dina egna bilder. Vad kan du skapa?

---

