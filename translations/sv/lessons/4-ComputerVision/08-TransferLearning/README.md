<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "717775c4050ccbffbe0c961ad8bf7bf7",
  "translation_date": "2025-08-28T15:14:34+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/README.md",
  "language_code": "sv"
}
-->
# F√∂rtr√§nade n√§tverk och transferinl√§rning

Att tr√§na CNNs kan ta mycket tid, och det kr√§vs en stor m√§ngd data f√∂r den uppgiften. Dock spenderas mycket av tiden p√• att l√§ra sig de b√§sta l√•g-niv√• filtren som ett n√§tverk kan anv√§nda f√∂r att extrahera m√∂nster fr√•n bilder. En naturlig fr√•ga uppst√•r - kan vi anv√§nda ett neuralt n√§tverk som tr√§nats p√• ett dataset och anpassa det f√∂r att klassificera andra bilder utan att beh√∂va genomf√∂ra en fullst√§ndig tr√§ningsprocess?

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/108)

Denna metod kallas **transferinl√§rning**, eftersom vi √∂verf√∂r viss kunskap fr√•n en neuralt n√§tverksmodell till en annan. Vid transferinl√§rning b√∂rjar vi vanligtvis med en f√∂rtr√§nad modell, som har tr√§nats p√• ett stort bilddataset, s√•som **ImageNet**. Dessa modeller kan redan g√∂ra ett bra jobb med att extrahera olika funktioner fr√•n generiska bilder, och i m√•nga fall kan man bygga en klassificerare ovanp√• dessa extraherade funktioner f√∂r att uppn√• ett bra resultat.

> ‚úÖ Transferinl√§rning √§r ett begrepp som du √§ven hittar inom andra akademiska omr√•den, s√•som utbildning. Det h√§nvisar till processen att ta kunskap fr√•n ett omr√•de och till√§mpa det p√• ett annat.

## F√∂rtr√§nade modeller som funktionsutvinnare

De konvolutionella n√§tverk som vi har pratat om i f√∂reg√•ende avsnitt inneh√•ller ett antal lager, d√§r varje lager √§r avsett att extrahera vissa funktioner fr√•n bilden, fr√•n l√•g-niv√• pixelkombinationer (s√•som horisontella/vertikala linjer eller streck) till h√∂gre niv√• kombinationer av funktioner, som motsvarar saker som ett √∂ga eller en flamma. Om vi tr√§nar CNN p√• ett tillr√§ckligt stort dataset med generiska och varierande bilder, b√∂r n√§tverket l√§ra sig att extrahera dessa vanliga funktioner.

B√•de Keras och PyTorch inneh√•ller funktioner f√∂r att enkelt ladda f√∂rtr√§nade neurala n√§tverksvikter f√∂r vissa vanliga arkitekturer, varav de flesta har tr√§nats p√• ImageNet-bilder. De mest anv√§nda beskrivs p√• sidan [CNN Architectures](../07-ConvNets/CNN_Architectures.md) fr√•n f√∂reg√•ende lektion. I synnerhet kan du √∂verv√§ga att anv√§nda en av f√∂ljande:

* **VGG-16/VGG-19** som √§r relativt enkla modeller som fortfarande ger bra noggrannhet. Att anv√§nda VGG som ett f√∂rsta f√∂rs√∂k √§r ofta ett bra val f√∂r att se hur transferinl√§rning fungerar.
* **ResNet** √§r en familj av modeller som f√∂reslogs av Microsoft Research 2015. De har fler lager och kr√§ver d√§rf√∂r mer resurser.
* **MobileNet** √§r en familj av modeller med reducerad storlek, l√§mpliga f√∂r mobila enheter. Anv√§nd dem om du har begr√§nsade resurser och kan offra lite noggrannhet.

H√§r √§r exempel p√• funktioner som extraherats fr√•n en bild av en katt med VGG-16-n√§tverket:

![Features extracted by VGG-16](../../../../../translated_images/features.6291f9c7ba3a0b951af88fc9864632b9115365410765680680d30c927dd67354.sv.png)

## Dataset f√∂r katter och hundar

I detta exempel kommer vi att anv√§nda ett dataset med [Katter och Hundar](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), vilket √§r mycket n√§ra ett verkligt scenario f√∂r bildklassificering.

## ‚úçÔ∏è √ñvning: Transferinl√§rning

L√•t oss se transferinl√§rning i praktiken i motsvarande notebooks:

* [Transfer Learning - PyTorch](TransferLearningPyTorch.ipynb)
* [Transfer Learning - TensorFlow](TransferLearningTF.ipynb)

## Visualisera en "idealkatt"

Ett f√∂rtr√§nat neuralt n√§tverk inneh√•ller olika m√∂nster i sitt *"hj√§rna"*, inklusive f√∂rest√§llningar om **idealkatt** (samt idealhund, idealzebra, etc.). Det skulle vara intressant att p√• n√•got s√§tt **visualisera denna bild**. Dock √§r det inte enkelt, eftersom m√∂nstren √§r spridda √∂ver n√§tverkets vikter och dessutom organiserade i en hierarkisk struktur.

En metod vi kan anv√§nda √§r att b√∂rja med en slumpm√§ssig bild och sedan f√∂rs√∂ka anv√§nda **gradientnedstigningsoptimering** f√∂r att justera den bilden p√• ett s√•dant s√§tt att n√§tverket b√∂rjar tro att det √§r en katt.

![Image Optimization Loop](../../../../../translated_images/ideal-cat-loop.999fbb8ff306e044f997032f4eef9152b453e6a990e449bbfb107de2493cc37e.sv.png)

Men om vi g√∂r detta kommer vi att f√• n√•got som liknar slumpm√§ssigt brus. Detta beror p√• att *det finns m√•nga s√§tt att f√• n√§tverket att tro att inmatningsbilden √§r en katt*, inklusive n√•gra som inte √§r visuellt meningsfulla. √Ñven om dessa bilder inneh√•ller m√•nga m√∂nster som √§r typiska f√∂r en katt, finns det inget som begr√§nsar dem till att vara visuellt distinkta.

F√∂r att f√∂rb√§ttra resultatet kan vi l√§gga till en annan term i f√∂rlustfunktionen, som kallas **variationsf√∂rlust**. Det √§r ett m√•tt som visar hur lika angr√§nsande pixlar i bilden √§r. Genom att minimera variationsf√∂rlusten g√∂rs bilden mjukare och bruset tas bort - vilket avsl√∂jar mer visuellt tilltalande m√∂nster. H√§r √§r ett exempel p√• s√•dana "ideala" bilder, som klassificeras som katt och zebra med h√∂g sannolikhet:

![Ideal Cat](../../../../../translated_images/ideal-cat.203dd4597643d6b0bd73038b87f9c0464322725e3a06ab145d25d4a861c70592.sv.png) | ![Ideal Zebra](../../../../../translated_images/ideal-zebra.7f70e8b54ee15a7a314000bb5df38a6cfe086ea04d60df4d3ef313d046b98a2b.sv.png)
-----|-----
 *Idealkatt* | *Idealzebra*

En liknande metod kan anv√§ndas f√∂r att utf√∂ra s√• kallade **adversariala attacker** p√• ett neuralt n√§tverk. Anta att vi vill lura ett neuralt n√§tverk och f√• en hund att se ut som en katt. Om vi tar en hundbild, som n√§tverket k√§nner igen som en hund, kan vi sedan justera den lite med hj√§lp av gradientnedstigningsoptimering tills n√§tverket b√∂rjar klassificera den som en katt:

![Picture of a Dog](../../../../../translated_images/original-dog.8f68a67d2fe0911f33041c0f7fce8aa4ea919f9d3917ec4b468298522aeb6356.sv.png) | ![Picture of a dog classified as a cat](../../../../../translated_images/adversarial-dog.d9fc7773b0142b89752539bfbf884118de845b3851c5162146ea0b8809fc820f.sv.png)
-----|-----
*Originalbild av en hund* | *Bild av en hund klassificerad som en katt*

Se koden f√∂r att reproducera resultaten ovan i f√∂ljande notebook:

* [Ideal and Adversarial Cat - TensorFlow](AdversarialCat_TF.ipynb)

## Slutsats

Med hj√§lp av transferinl√§rning kan du snabbt s√§tta ihop en klassificerare f√∂r en anpassad objektklassificeringsuppgift och uppn√• h√∂g noggrannhet. Du kan se att mer komplexa uppgifter som vi l√∂ser nu kr√§ver h√∂gre ber√§kningskraft och inte enkelt kan l√∂sas p√• en CPU. I n√§sta enhet kommer vi att f√∂rs√∂ka anv√§nda en mer l√§ttviktig implementation f√∂r att tr√§na samma modell med l√§gre ber√§kningsresurser, vilket resulterar i bara n√•got l√§gre noggrannhet.

## üöÄ Utmaning

I de medf√∂ljande notebooks finns det anteckningar l√§ngst ner om hur transferkunskap fungerar b√§st med n√•got liknande tr√§ningsdata (en ny typ av djur, kanske). G√∂r lite experiment med helt nya typer av bilder f√∂r att se hur bra eller d√•ligt dina transferkunskapsmodeller presterar.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## Granskning & Sj√§lvstudier

L√§s igenom [TrainingTricks.md](TrainingTricks.md) f√∂r att f√∂rdjupa din kunskap om andra s√§tt att tr√§na dina modeller.

## [Uppgift](lab/README.md)

I detta labb kommer vi att anv√§nda det verkliga [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) husdjursdatasetet med 35 raser av katter och hundar, och vi kommer att bygga en transferinl√§rningsklassificerare.

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.