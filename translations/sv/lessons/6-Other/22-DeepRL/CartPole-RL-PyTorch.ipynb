{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Träna RL för att balansera Cartpole\n",
    "\n",
    "Den här anteckningsboken är en del av [AI för nybörjare-kursplanen](http://aka.ms/ai-beginners). Den har inspirerats av [officiell PyTorch-handledning](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) och [denna Cartpole PyTorch-implementation](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "I det här exemplet kommer vi att använda RL för att träna en modell att balansera en stång på en vagn som kan röra sig åt vänster och höger på en horisontell skala. Vi kommer att använda [OpenAI Gym](https://www.gymlibrary.ml/) för att simulera stången.\n",
    "\n",
    "> **Note**: Du kan köra kodexemplen i denna lektion lokalt (t.ex. från Visual Studio Code), i vilket fall simuleringen öppnas i ett nytt fönster. När du kör koden online kan du behöva göra vissa justeringar i koden, som beskrivs [här](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Vi börjar med att säkerställa att Gym är installerat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ska vi skapa CartPole-miljön och se hur vi kan arbeta med den. En miljö har följande egenskaper:\n",
    "\n",
    "* **Action space** är mängden möjliga åtgärder som vi kan utföra vid varje steg i simuleringen\n",
    "* **Observation space** är utrymmet för observationer som vi kan göra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Låt oss se hur simuleringen fungerar. Följande loop kör simuleringen tills `env.step` inte returnerar termineringsflaggan `done`. Vi kommer att välja åtgärder slumpmässigt med hjälp av `env.action_space.sample()`, vilket innebär att experimentet förmodligen kommer att misslyckas väldigt snabbt (CartPole-miljön avslutas när hastigheten på CartPole, dess position eller vinkel är utanför vissa gränser).\n",
    "\n",
    "> Simuleringen kommer att öppnas i ett nytt fönster. Du kan köra koden flera gånger och se hur den beter sig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan märka att observationerna innehåller 4 siffror. Dessa är:\n",
    "- Vagnens position\n",
    "- Vagnens hastighet\n",
    "- Stolpens vinkel\n",
    "- Stolpens rotationshastighet\n",
    "\n",
    "`rew` är belöningen vi får vid varje steg. Du kan se att i CartPole-miljön får du 1 poäng för varje simuleringssteg, och målet är att maximera den totala belöningen, det vill säga tiden som CartPole kan balansera utan att falla.\n",
    "\n",
    "Under förstärkningsinlärning är vårt mål att träna en **policy** $\\pi$, som för varje tillstånd $s$ talar om för oss vilken åtgärd $a$ vi ska ta, så i princip $a = \\pi(s)$.\n",
    "\n",
    "Om du vill ha en probabilistisk lösning kan du tänka på policyn som att den returnerar en uppsättning sannolikheter för varje åtgärd, det vill säga $\\pi(a|s)$ skulle innebära sannolikheten att vi ska ta åtgärd $a$ i tillstånd $s$.\n",
    "\n",
    "## Policy Gradient-metoden\n",
    "\n",
    "I den enklaste RL-algoritmen, kallad **Policy Gradient**, kommer vi att träna ett neuralt nätverk för att förutsäga nästa åtgärd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kommer att träna nätverket genom att köra många experiment och uppdatera vårt nätverk efter varje körning. Låt oss definiera en funktion som kommer att köra experimentet och returnera resultaten (så kallad **spår**) - alla tillstånd, handlingar (och deras rekommenderade sannolikheter) och belöningar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan köra ett avsnitt med ett otränat nätverk och observera att den totala belöningen (AKA längden på avsnittet) är mycket låg:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En av de knepiga aspekterna med policy gradient-algoritmen är att använda **diskonterade belöningar**. Idén är att vi beräknar vektorn av totala belöningar vid varje steg i spelet, och under denna process diskonterar vi de tidiga belöningarna med någon koefficient $gamma$. Vi normaliserar också den resulterande vektorn, eftersom vi kommer att använda den som vikt för att påverka vår träning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kör vi träningen! Vi kommer att köra 300 episoder, och vid varje episod gör vi följande:\n",
    "\n",
    "1. Kör experimentet och samla in spårdata.\n",
    "2. Beräkna skillnaden (`gradients`) mellan de utförda handlingarna och de förutspådda sannolikheterna. Ju mindre skillnaden är, desto säkrare är vi på att vi har valt rätt handling.\n",
    "3. Beräkna diskonterade belöningar och multiplicera gradienterna med de diskonterade belöningarna - detta säkerställer att steg med högre belöningar påverkar slutresultatet mer än de med lägre belöningar.\n",
    "4. Förväntade målhandlingar för vårt neurala nätverk kommer delvis att tas från de förutspådda sannolikheterna under körningen och delvis från de beräknade gradienterna. Vi använder parametern `alpha` för att bestämma i vilken utsträckning gradienter och belöningar tas i beaktande - detta kallas *inlärningshastighet* för förstärkningsalgoritmen.\n",
    "5. Slutligen tränar vi vårt nätverk på tillstånd och förväntade handlingar och upprepar processen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu låt oss köra avsnittet med rendering för att se resultatet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Förhoppningsvis kan du nu se att stången kan balansera ganska bra!\n",
    "\n",
    "## Actor-Critic Modell\n",
    "\n",
    "Actor-Critic-modellen är en vidareutveckling av policy gradients, där vi bygger ett neuralt nätverk för att lära oss både policyn och de uppskattade belöningarna. Nätverket kommer att ha två utgångar (eller så kan du se det som två separata nätverk):\n",
    "* **Actor** kommer att rekommendera vilken åtgärd som ska vidtas genom att ge oss sannolikhetsfördelningen för tillståndet, precis som i policy gradient-modellen.\n",
    "* **Critic** skulle uppskatta vad belöningen skulle bli från dessa åtgärder. Den returnerar totalt uppskattade belöningar i framtiden vid det givna tillståndet.\n",
    "\n",
    "Låt oss definiera en sådan modell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi skulle behöva göra små ändringar i våra `discounted_rewards` och `run_episode` funktioner:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kommer vi att köra huvudträningsloopen. Vi kommer att använda en manuell nätverksträningsprocess genom att beräkna korrekta förlustfunktioner och uppdatera nätverksparametrar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slutsats\n",
    "\n",
    "Vi har sett två RL-algoritmer i denna demonstration: enkel policy gradient och den mer sofistikerade actor-critic. Du kan se att dessa algoritmer arbetar med abstrakta begrepp som tillstånd, handling och belöning - vilket gör att de kan tillämpas på mycket olika miljöer.\n",
    "\n",
    "Förstärkningsinlärning gör det möjligt för oss att lära oss den bästa strategin för att lösa problemet enbart genom att titta på den slutliga belöningen. Det faktum att vi inte behöver märkta dataset gör att vi kan upprepa simuleringar många gånger för att optimera våra modeller. Dock finns det fortfarande många utmaningar inom RL, som du kan lära dig mer om om du väljer att fokusera mer på detta intressanta område inom AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfriskrivning**:  \nDetta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör du vara medveten om att automatiska översättningar kan innehålla fel eller felaktigheter. Det ursprungliga dokumentet på dess originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T16:13:05+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "sv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}