{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Učenje RL za uravnoteženje Cartpole\n",
    "\n",
    "Ta zvezek je del [učnega načrta AI za začetnike](http://aka.ms/ai-beginners). Navdihnjen je bil z [uradnim PyTorch vodičem](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) in [to implementacijo Cartpole v PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "V tem primeru bomo uporabili RL za treniranje modela, ki bo uravnotežil drog na vozičku, ki se lahko premika levo in desno po vodoravni osi. Za simulacijo droga bomo uporabili okolje [OpenAI Gym](https://www.gymlibrary.ml/).\n",
    "\n",
    "> **Opomba**: Kodo te lekcije lahko zaženete lokalno (npr. iz Visual Studio Code), v tem primeru se bo simulacija odprla v novem oknu. Če kodo izvajate na spletu, boste morda morali narediti nekaj prilagoditev kode, kot je opisano [tukaj](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Začeli bomo s preverjanjem, ali je Gym nameščen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj ustvarimo okolje CartPole in si oglejmo, kako delovati v njem. Okolje ima naslednje lastnosti:\n",
    "\n",
    "* **Prostor akcij** je množica možnih akcij, ki jih lahko izvedemo na vsakem koraku simulacije\n",
    "* **Prostor opazovanj** je prostor opazovanj, ki jih lahko opravimo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poglejmo, kako simulacija deluje. Naslednja zanka izvaja simulacijo, dokler `env.step` ne vrne zaključne zastavice `done`. Naključno bomo izbrali akcije z uporabo `env.action_space.sample()`, kar pomeni, da bo poskus verjetno zelo hitro spodletel (okolje CartPole se zaključi, ko hitrost CartPole, njegova pozicija ali kot presežejo določene meje).\n",
    "\n",
    "> Simulacija se bo odprla v novem oknu. Kodo lahko zaženete večkrat in opazujete, kako se obnaša.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opazite lahko, da opazovanja vsebujejo 4 številke. Te so:\n",
    "- Položaj vozička\n",
    "- Hitrost vozička\n",
    "- Kot droga\n",
    "- Hitrost vrtenja droga\n",
    "\n",
    "`rew` je nagrada, ki jo prejmemo na vsakem koraku. V okolju CartPole ste nagrajeni z 1 točko za vsak simulacijski korak, cilj pa je maksimizirati skupno nagrado, torej čas, ko lahko CartPole ohranja ravnotežje, ne da bi padel.\n",
    "\n",
    "Med krepitvenim učenjem je naš cilj trenirati **politiko** $\\pi$, ki nam za vsako stanje $s$ pove, katero dejanje $a$ naj izvedemo, torej v bistvu $a = \\pi(s)$.\n",
    "\n",
    "Če želite verjetnostno rešitev, lahko politiko razumete kot vračanje nabora verjetnosti za vsako dejanje, torej $\\pi(a|s)$ bi pomenilo verjetnost, da bi morali v stanju $s$ izvesti dejanje $a$.\n",
    "\n",
    "## Metoda gradienta politike\n",
    "\n",
    "V najpreprostejšem algoritmu za krepitveno učenje, imenovanem **gradient politike**, bomo trenirali nevronsko mrežo za napovedovanje naslednjega dejanja.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirali bomo mrežo z izvajanjem številnih poskusov in posodabljanjem naše mreže po vsakem poskusu. Določimo funkcijo, ki bo izvedla poskus in vrnila rezultate (tako imenovano **sled**) - vse stanje, akcije (in njihove priporočene verjetnosti) ter nagrade:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ena epizoda se lahko izvede z neizurjenim omrežjem, pri čemer opazimo, da je skupna nagrada (tj. dolžina epizode) zelo nizka:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eden iz zahtevnih vidikov algoritma za gradient politike je uporaba **diskontiranih nagrad**. Ideja je, da izračunamo vektor skupnih nagrad na vsakem koraku igre, pri čemer med tem procesom diskontiramo zgodnje nagrade z uporabo nekega koeficienta $gamma$. Prav tako normaliziramo nastali vektor, saj ga bomo uporabili kot utež za vplivanje na naše učenje:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj pa začnimo z dejanskim treningom! Izvedli bomo 300 epizod, pri vsaki epizodi pa bomo naredili naslednje:\n",
    "\n",
    "1. Izvedemo eksperiment in zberemo sled.\n",
    "2. Izračunamo razliko (`gradients`) med izvedenimi akcijami in napovedanimi verjetnostmi. Manjša kot je razlika, bolj smo prepričani, da smo izbrali pravo akcijo.\n",
    "3. Izračunamo diskontirane nagrade in pomnožimo gradient z diskontiranimi nagradami - to zagotavlja, da koraki z višjimi nagradami bolj vplivajo na končni rezultat kot tisti z nižjimi nagradami.\n",
    "4. Pričakovane ciljne akcije za naš nevronski model bodo delno izhajale iz napovedanih verjetnosti med izvajanjem in delno iz izračunanih gradientov. Parameter `alpha` bomo uporabili za določitev, v kolikšni meri se gradienti in nagrade upoštevajo - to se imenuje *stopnja učenja* algoritma za okrepitev.\n",
    "5. Na koncu treniramo naš model na stanjih in pričakovanih akcijah ter ponovimo proces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj zaženimo epizodo z upodabljanjem, da vidimo rezultat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upajmo, da lahko zdaj vidite, da se droga kar dobro uravnoteži!\n",
    "\n",
    "## Model Actor-Critic\n",
    "\n",
    "Model Actor-Critic je nadaljnji razvoj politik gradientov, pri katerem zgradimo nevronsko mrežo za učenje tako politike kot ocenjenih nagrad. Mreža bo imela dva izhoda (ali pa si jo lahko predstavljate kot dve ločeni mreži):\n",
    "* **Actor** bo priporočil dejanje, ki ga je treba izvesti, tako da nam poda porazdelitev verjetnosti stanj, kot v modelu politik gradientov.\n",
    "* **Critic** bo ocenil, kakšna bi bila nagrada za ta dejanja. Vrne skupne ocenjene nagrade v prihodnosti za dano stanje.\n",
    "\n",
    "Definirajmo tak model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morali bi nekoliko spremeniti naši funkciji `discounted_rewards` in `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj bomo zagnali glavno zanko za učenje. Uporabili bomo ročni postopek učenja mreže z izračunavanjem ustreznih funkcij izgube in posodabljanjem parametrov mreže:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ključne točke\n",
    "\n",
    "V tem prikazu smo spoznali dva algoritma za okrepitevno učenje: preprost gradient politike in bolj izpopolnjen igralec-kritik. Vidite lahko, da ti algoritmi delujejo z abstraktnimi pojmi stanja, dejanja in nagrade – zato jih je mogoče uporabiti v zelo različnih okoljih.\n",
    "\n",
    "Okrepitevno učenje nam omogoča, da se naučimo najboljše strategije za reševanje problema zgolj z opazovanjem končne nagrade. Dejstvo, da ne potrebujemo označenih podatkovnih zbirk, nam omogoča, da simulacije večkrat ponovimo in tako optimiziramo naše modele. Kljub temu pa okrepitevno učenje še vedno prinaša številne izzive, ki jih lahko spoznate, če se odločite poglobiti v to zanimivo področje umetne inteligence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Omejitev odgovornosti**:  \nTa dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo strokovno človeško prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napačne razlage, ki izhajajo iz uporabe tega prevoda.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-30T07:12:35+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "sl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}