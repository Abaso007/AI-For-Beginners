{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vdelave\n",
    "\n",
    "V prejšnjem primeru smo delali z visoko-dimenzionalnimi vektorji vreče besed dolžine `vocab_size` in izrecno pretvorili nizko-dimenzionalne vektorske predstavitve položajev v redko enotočno predstavitev. Ta enotočna predstavitev ni pomnilniško učinkovita. Poleg tega se vsaka beseda obravnava neodvisno od drugih, zato enotočno kodirani vektorji ne izražajo semantičnih podobnosti med besedami.\n",
    "\n",
    "V tej enoti bomo nadaljevali raziskovanje podatkovne zbirke **News AG**. Za začetek naložimo podatke in pridobimo nekaj definicij iz prejšnje enote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaj je vdelava?\n",
    "\n",
    "Ideja **vdelave** (embedding) je predstaviti besede z nižjedimenzionalnimi gostimi vektorji, ki odražajo semantični pomen besede. Kasneje bomo razpravljali o tem, kako zgraditi smiselne vdelave besed, za zdaj pa si vdelave predstavljajmo kot način za zmanjšanje dimenzionalnosti vektorja besede.\n",
    "\n",
    "Torej, vdelavni sloj prejme besedo kot vhod in ustvari izhodni vektor določene velikosti `embedding_size`. Na nek način je zelo podoben sloju `Dense`, vendar namesto da bi kot vhod prejel vektor z eno vročo kodiranjem (one-hot encoding), lahko sprejme številko besede.\n",
    "\n",
    "Z uporabo vdelavnega sloja kot prvega sloja v naši mreži lahko preklopimo iz modela vreče besed (bag-of-words) na model **vreče vdelav** (embedding bag), kjer najprej vsako besedo v našem besedilu pretvorimo v ustrezno vdelavo, nato pa izračunamo neko agregatno funkcijo nad vsemi temi vdelavami, na primer `sum`, `average` ali `max`.\n",
    "\n",
    "![Slika, ki prikazuje klasifikator z vdelavami za pet zaporednih besed.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sl.png)\n",
    "\n",
    "Naša nevronska mreža za klasifikacijo je sestavljena iz naslednjih slojev:\n",
    "\n",
    "* Sloj `TextVectorization`, ki prejme niz kot vhod in ustvari tenzor številk tokenov. Določili bomo neko smiselno velikost besedišča `vocab_size` in ignorirali manj pogosto uporabljene besede. Vhodna oblika bo 1, izhodna oblika pa $n$, saj bomo kot rezultat dobili $n$ tokenov, pri čemer vsak vsebuje številke od 0 do `vocab_size`.\n",
    "* Sloj `Embedding`, ki prejme $n$ številk in vsako številko zmanjša na gosti vektor določene dolžine (v našem primeru 100). Tako se vhodni tenzor oblike $n$ pretvori v tenzor oblike $n\\times 100$.\n",
    "* Agregacijski sloj, ki izračuna povprečje tega tenzorja vzdolž prve osi, tj. izračuna povprečje vseh $n$ vhodnih tenzorjev, ki ustrezajo različnim besedam. Za implementacijo tega sloja bomo uporabili sloj `Lambda` in vanj posredovali funkcijo za izračun povprečja. Izhod bo imel obliko 100 in bo številska predstavitev celotnega vhodnega zaporedja.\n",
    "* Končni linearni klasifikator `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V povzetku, v stolpcu **oblika izhoda**, prva dimenzija tenzorja `None` ustreza velikosti minibatcha, druga pa dolžini zaporedja tokenov. Vsa zaporedja tokenov v minibatchu imajo različne dolžine. O tem, kako se spopasti s tem, bomo razpravljali v naslednjem razdelku.\n",
    "\n",
    "Zdaj pa trenirajmo mrežo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Opomba**: gradimo vektorizator na podlagi podmnožice podatkov. To je storjeno za pospešitev procesa, kar lahko privede do situacije, ko vsi tokeni iz našega besedila niso prisotni v besedišču. V tem primeru bodo ti tokeni prezrti, kar lahko povzroči nekoliko nižjo natančnost. Vendar pa v resničnem življenju podmnožica besedila pogosto omogoča dobro oceno besedišča.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delo z različnimi dolžinami zaporedij spremenljivk\n",
    "\n",
    "Poglejmo, kako poteka učenje v mini serijah. V zgornjem primeru ima vhodni tenzor dimenzijo 1, uporabljamo pa mini serije dolžine 128, tako da je dejanska velikost tenzorja $128 \\times 1$. Vendar je število tokenov v vsakem stavku različno. Če uporabimo plast `TextVectorization` na en sam vhod, je število vrnjenih tokenov različno, odvisno od tega, kako je besedilo tokenizirano:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vendar, ko uporabimo vektorizator na več zaporedij, mora ustvariti tenzor pravokotne oblike, zato neuporabljene elemente zapolni z oznako PAD (ki je v našem primeru nič):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tukaj lahko vidimo vdelave:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Opomba**: Da zmanjšamo količino dopolnjevanja, je v nekaterih primerih smiselno razvrstiti vse zaporedje v podatkovnem naboru po naraščajoči dolžini (ali, natančneje, po številu tokenov). To bo zagotovilo, da vsak minibatch vsebuje zaporedja podobne dolžine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantične vektorske predstavitve: Word2Vec\n",
    "\n",
    "V našem prejšnjem primeru se je vektorska plast naučila preslikati besede v vektorske predstavitve, vendar te predstavitve niso imele semantičnega pomena. Bilo bi koristno, če bi se naučili vektorske predstavitve, kjer bi bile podobne besede ali sinonimi predstavljeni z vektorji, ki so si blizu glede na neko vektorsko razdaljo (na primer evklidsko razdaljo).\n",
    "\n",
    "Da to dosežemo, moramo naš model za vektorske predstavitve predhodno naučiti na veliki zbirki besedil z uporabo tehnike, kot je [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ta tehnika temelji na dveh glavnih arhitekturah, ki se uporabljata za ustvarjanje porazdeljenih predstavitev besed:\n",
    "\n",
    " - **Neprekinjena vreča besed** (CBoW), kjer model treniramo, da napove besedo na podlagi okoliškega konteksta. Glede na n-gram $(W_{-2},W_{-1},W_0,W_1,W_2)$ je cilj modela napovedati $W_0$ na podlagi $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Neprekinjeni preskok-gram** (Continuous skip-gram) je nasproten CBoW. Model uporablja okoliško okno kontekstnih besed za napoved trenutne besede.\n",
    "\n",
    "CBoW je hitrejši, medtem ko je skip-gram počasnejši, vendar bolje predstavlja redke besede.\n",
    "\n",
    "![Slika, ki prikazuje algoritma CBoW in Skip-Gram za pretvorbo besed v vektorje.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sl.png)\n",
    "\n",
    "Za eksperimentiranje z Word2Vec vektorskimi predstavitvami, predhodno naučenimi na zbirki podatkov Google News, lahko uporabimo knjižnico **gensim**. Spodaj poiščemo besede, ki so najbolj podobne 'neural'.\n",
    "\n",
    "> **Opomba:** Ko prvič ustvarjate vektorske predstavitve besed, lahko prenos podatkov traja nekaj časa!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prav tako lahko iz besede pridobimo vektorsko vdelavo, ki jo uporabimo pri usposabljanju modela za klasifikacijo. Vdelava ima 300 komponent, vendar tukaj za jasnost prikažemo le prvih 20 komponent vektorja:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odlična stvar pri semantičnih vdelavah je, da lahko manipulirate z vektorskim kodiranjem na podlagi semantike. Na primer, lahko zahtevamo, da najdemo besedo, katere vektorska predstavitev je čim bližje besedama *kralj* in *ženska*, ter čim bolj oddaljena od besede *moški*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Zgornji primer uporablja nekaj notranje magije GenSym, vendar je osnovna logika pravzaprav precej preprosta. Zanimiva stvar pri vdelavah je, da lahko na vektorskih vdelavah izvajate običajne vektorske operacije, kar odraža operacije na pomenskih ravneh besed. Zgornji primer lahko izrazimo z vektorskimi operacijami: izračunamo vektor, ki ustreza **KRALJ-MOŠKI+ŽENSKA** (operaciji `+` in `-` se izvajata na vektorskih predstavitvah ustreznih besed), nato pa poiščemo najbližjo besedo v slovarju temu vektorju:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **OPOMBA**: Dodali smo majhne koeficiente k vektorjema *man* in *woman* – poskusite jih odstraniti, da vidite, kaj se zgodi.\n",
    "\n",
    "Za iskanje najbližjega vektorja uporabimo TensorFlow orodja za izračun vektorja razdalj med našim vektorjem in vsemi vektorji v besedišču, nato pa z `argmin` poiščemo indeks najmanjše besede.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medtem ko se Word2Vec zdi odličen način za izražanje semantike besed, ima številne pomanjkljivosti, med drugim naslednje:\n",
    "\n",
    "* Tako modeli CBoW kot skip-gram so **prediktivni vektorji**, ki upoštevajo le lokalni kontekst. Word2Vec ne izkorišča globalnega konteksta.\n",
    "* Word2Vec ne upošteva **morfologije** besed, tj. dejstva, da lahko pomen besede temelji na različnih delih besede, kot je koren.\n",
    "\n",
    "**FastText** poskuša premagati drugo omejitev in gradi na Word2Vec tako, da se uči vektorske predstavitve za vsako besedo in za n-grame znakov, ki jih najde znotraj vsake besede. Vrednosti teh predstavitev se nato povprečijo v en vektor pri vsakem koraku učenja. Čeprav to doda veliko dodatnega računanja pri predtreningu, omogoča vektorskim predstavitvam besed, da kodirajo informacije o podbesedah.\n",
    "\n",
    "Druga metoda, **GloVe**, uporablja drugačen pristop k vektorskim predstavitvam besed, ki temelji na faktorizaciji matrike beseda-kontekst. Najprej ustvari veliko matriko, ki šteje število pojavitev besed v različnih kontekstih, nato pa poskuša to matriko predstaviti v nižjih dimenzijah na način, ki minimizira izgubo rekonstrukcije.\n",
    "\n",
    "Knjižnica gensim podpira te vektorske predstavitve besed, in z njimi lahko eksperimentirate tako, da spremenite zgornjo kodo za nalaganje modela.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uporaba vnaprej naučenih vektorskih predstavitev v Kerasu\n",
    "\n",
    "Primer zgoraj lahko prilagodimo tako, da matriko v naši vektorski plasti vnaprej napolnimo s semantičnimi vektorskimi predstavitvami, kot je Word2Vec. Besedišči vnaprej naučenih vektorskih predstavitev in besedilnega korpusa se verjetno ne bosta ujemali, zato moramo izbrati eno. Tukaj raziskujemo dve možni možnosti: uporabo besedišča iz tokenizerja in uporabo besedišča iz Word2Vec predstavitev.\n",
    "\n",
    "### Uporaba besedišča iz tokenizerja\n",
    "\n",
    "Pri uporabi besedišča iz tokenizerja bodo nekatere besede iz besedišča imele ustrezne Word2Vec predstavitve, nekatere pa bodo manjkale. Glede na to, da je velikost našega besedišča `vocab_size`, dolžina vektorskih predstavitev Word2Vec pa `embed_size`, bo vektorska plast predstavljena z matriko uteži oblike `vocab_size`$\\times$`embed_size`. To matriko bomo napolnili tako, da bomo prešli skozi besedišče:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za besede, ki niso prisotne v Word2Vec besedišču, lahko pustimo vrednosti kot ničle ali pa ustvarimo naključni vektor.\n",
    "\n",
    "Zdaj lahko definiramo plast za vdelavo s predhodno naučenimi utežmi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Opomba**: Upoštevajte, da smo nastavili `trainable=False` pri ustvarjanju `Embedding`, kar pomeni, da sloja Embedding ne bomo ponovno trenirali. To lahko povzroči nekoliko nižjo natančnost, vendar pospeši proces učenja.\n",
    "\n",
    "### Uporaba besedišča za vdelave\n",
    "\n",
    "Ena težava pri prejšnjem pristopu je, da se besedišča, uporabljena v TextVectorization in Embedding, razlikujejo. Da bi rešili ta problem, lahko uporabimo eno od naslednjih rešitev:\n",
    "* Ponovno treniramo model Word2Vec na našem besedišču.\n",
    "* Naložimo naš nabor podatkov z besediščem iz vnaprej naučenega modela Word2Vec. Besedišča, uporabljena za nalaganje nabora podatkov, je mogoče določiti med nalaganjem.\n",
    "\n",
    "Drugi pristop se zdi enostavnejši, zato ga bomo implementirali. Najprej bomo ustvarili sloj `TextVectorization` z določenim besediščem, vzetim iz vdelav Word2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knjižnica gensim za vektorske predstavitve besed vsebuje priročno funkcijo `get_keras_embeddings`, ki bo samodejno ustvarila ustrezno Kerasovo plast za vektorske predstavitve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eden od razlogov, zakaj ne dosegamo večje natančnosti, je ta, da nekaterih besed iz našega nabora podatkov ni v predhodno naučenem GloVe besedišču, zato so v bistvu prezrte. Da bi to presegli, lahko naučimo lastne vdelave na podlagi našega nabora podatkov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontekstualne vektorske predstavitve\n",
    "\n",
    "Ena ključnih omejitev tradicionalnih vnaprej naučenih vektorskih predstavitev, kot je Word2Vec, je dejstvo, da kljub temu, da lahko zajamejo določen pomen besede, ne morejo razlikovati med različnimi pomeni. To lahko povzroči težave pri nadaljnjih modelih.\n",
    "\n",
    "Na primer, beseda 'play' ima različne pomene v teh dveh stavkih:\n",
    "- Šel sem na **igro** v gledališče.\n",
    "- John želi **igrati** s svojimi prijatelji.\n",
    "\n",
    "Vnaprej naučene vektorske predstavitve, o katerih smo govorili, predstavljajo oba pomena besede 'play' z isto vektorsko predstavitvijo. Da bi premagali to omejitev, moramo zgraditi vektorske predstavitve na podlagi **jezikovnega modela**, ki je naučen na velikem korpusu besedil in *ve*, kako se besede lahko povezujejo v različnih kontekstih. Razprava o kontekstualnih vektorskih predstavitvah presega okvir tega vodiča, vendar se bomo k njim vrnili, ko bomo govorili o jezikovnih modelih v naslednji enoti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Omejitev odgovornosti**:  \nTa dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo strokovno človeško prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napačne razlage, ki izhajajo iz uporabe tega prevoda.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-30T08:14:01+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "sl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}