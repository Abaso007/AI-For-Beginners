{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naloga razvrščanja besedila\n",
    "\n",
    "Kot smo že omenili, se bomo osredotočili na preprosto nalogo razvrščanja besedila na podlagi podatkovnega nabora **AG_NEWS**, kjer razvrščamo naslove novic v eno od 4 kategorij: Svet, Šport, Posel in Znanost/Tehnologija.\n",
    "\n",
    "## Podatkovni nabor\n",
    "\n",
    "Ta podatkovni nabor je vgrajen v modul [`torchtext`](https://github.com/pytorch/text), zato imamo do njega enostaven dostop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tukaj `train_dataset` in `test_dataset` vsebujeta zbirke, ki vračajo pare oznake (številka razreda) in besedila, na primer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torej, natisnimo prvih 10 novih naslovov iz našega nabora podatkov:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ker so podatkovne zbirke iteratorji, jih moramo, če želimo podatke uporabiti večkrat, pretvoriti v seznam:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacija\n",
    "\n",
    "Zdaj moramo besedilo pretvoriti v **številke**, ki jih lahko predstavimo kot tenzorje. Če želimo predstavitev na ravni besed, moramo narediti dve stvari:\n",
    "* uporabiti **tokenizator**, da razdelimo besedilo na **tokene**\n",
    "* zgraditi **besedišče** teh tokenov.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z uporabo besedišča lahko zlahka kodiramo našo tokenizirano niz v niz številk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predstavitev besedila z metodo Bag of Words\n",
    "\n",
    "Ker besede predstavljajo pomen, lahko včasih ugotovimo pomen besedila zgolj z opazovanjem posameznih besed, ne glede na njihov vrstni red v stavku. Na primer, pri razvrščanju novic besede, kot so *vreme* in *sneg*, verjetno nakazujejo na *vremensko napoved*, medtem ko besede, kot so *delnice* in *dolar*, kažejo na *finančne novice*.\n",
    "\n",
    "**Bag of Words** (BoW) je najpogosteje uporabljena tradicionalna predstavitev vektorskih podatkov. Vsaka beseda je povezana z indeksom vektorja, element vektorja pa vsebuje število pojavitev besede v določenem dokumentu.\n",
    "\n",
    "![Slika prikazuje, kako je predstavitev vektorja Bag of Words shranjena v pomnilniku.](../../../../../translated_images/bag-of-words-example.606fc1738f1d7ba98a9d693e3bcd706c6e83fa7bf8221e6e90d1a206d82f2ea4.sl.png) \n",
    "\n",
    "> **Note**: Na Bag of Words lahko gledate tudi kot na vsoto vseh vektorjev, kodiranih z metodo one-hot, za posamezne besede v besedilu.\n",
    "\n",
    "Spodaj je primer, kako ustvariti predstavitev Bag of Words z uporabo knjižnice Scikit Learn v Pythonu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za izračun vektorja vreče besed iz vektorske predstavitve našega nabora podatkov AG_NEWS lahko uporabimo naslednjo funkcijo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Opomba:** Tukaj uporabljamo globalno spremenljivko `vocab_size` za določitev privzete velikosti besedišča. Ker je velikost besedišča pogosto precej velika, lahko omejimo velikost besedišča na najpogostejše besede. Poskusite znižati vrednost `vocab_size` in zagnati spodnjo kodo ter opazujte, kako to vpliva na natančnost. Pričakujete lahko nekaj zmanjšanja natančnosti, vendar ne dramatičnega, v zameno za boljšo zmogljivost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Učenje BoW klasifikatorja\n",
    "\n",
    "Zdaj, ko smo se naučili, kako zgraditi predstavitev Bag-of-Words za naš besedilni korpus, lahko na tej osnovi naučimo klasifikator. Najprej moramo naš nabor podatkov za učenje pretvoriti tako, da se vse pozicijske vektorske predstavitve spremenijo v predstavitev Bag-of-Words. To lahko dosežemo tako, da funkcijo `bowify` podamo kot parameter `collate_fn` standardnemu torch `DataLoader`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj definirajmo preprosto nevronsko mrežo za klasifikacijo, ki vsebuje eno linearno plast. Velikost vhodnega vektorja je enaka `vocab_size`, velikost izhoda pa ustreza številu razredov (4). Ker rešujemo nalogo klasifikacije, je končna aktivacijska funkcija `LogSoftmax()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj bomo definirali standardno učno zanko v PyTorchu. Ker je naš nabor podatkov precej velik, bomo za namen učenja trenirali le eno epoho, včasih pa celo manj kot eno epoho (določitev parametra `epoch_size` nam omogoča omejitev učenja). Prav tako bomo med učenjem poročali o akumulirani natančnosti učenja; pogostost poročanja je določena s parametrom `report_freq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiGrami, TriGrami in N-Grami\n",
    "\n",
    "Ena od omejitev pristopa vreče besed je, da so nekatere besede del večbesednih izrazov. Na primer, beseda 'hot dog' ima popolnoma drugačen pomen kot besedi 'hot' in 'dog' v drugih kontekstih. Če besedi 'hot' in 'dog' vedno predstavljamo z istimi vektorji, lahko to zmede naš model.\n",
    "\n",
    "Da bi to rešili, se pogosto uporabljajo **N-gramske reprezentacije** pri metodah za klasifikacijo dokumentov, kjer je frekvenca vsake besede, dvobesedne ali tribesedne kombinacije koristna značilnost za treniranje klasifikatorjev. Pri bigramski reprezentaciji, na primer, dodamo v besedišče vse pare besed poleg izvirnih besed.\n",
    "\n",
    "Spodaj je primer, kako ustvariti bigramsko vrečo besed z uporabo Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glavna pomanjkljivost pristopa N-gram je, da velikost besedišča začne izjemno hitro naraščati. V praksi moramo združiti predstavitev N-gram z nekaterimi tehnikami za zmanjšanje dimenzionalnosti, kot so *vgnezditve* (embeddings), o katerih bomo govorili v naslednji enoti.\n",
    "\n",
    "Za uporabo predstavitve N-gram v našem naboru podatkov **AG News** moramo zgraditi posebno ngram besedišče:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nato bi lahko uporabili isto kodo kot zgoraj za treniranje klasifikatorja, vendar bi bila to zelo neučinkovita uporaba pomnilnika. V naslednji enoti bomo trenirali bigram klasifikator z uporabo vdelav.\n",
    "\n",
    "> **Opomba:** Pustite lahko samo tiste ngrame, ki se v besedilu pojavijo več kot določeno število krat. To bo zagotovilo, da bodo redki bigrami izpuščeni, in bo znatno zmanjšalo dimenzionalnost. Za to nastavite parameter `min_freq` na višjo vrednost in opazujte spremembo dolžine besedišča.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frekvenca izraza in obratna frekvenca dokumenta (TF-IDF)\n",
    "\n",
    "V predstavitvi BoW so pojavitve besed enakovredno ovrednotene, ne glede na samo besedo. Vendar je jasno, da so pogoste besede, kot so *a*, *in* itd., veliko manj pomembne za klasifikacijo kot specializirani izrazi. Pravzaprav so pri večini nalog NLP nekatere besede bolj relevantne kot druge.\n",
    "\n",
    "**TF-IDF** pomeni **frekvenca izraza–obratna frekvenca dokumenta**. Gre za različico vreče besed, kjer se namesto binarne vrednosti 0/1, ki označuje pojav besede v dokumentu, uporablja vrednost s plavajočo vejico, ki je povezana s frekvenco pojavljanja besede v korpusu.\n",
    "\n",
    "Bolj formalno je teža $w_{ij}$ besede $i$ v dokumentu $j$ definirana kot:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "kjer\n",
    "* $tf_{ij}$ predstavlja število pojavitev $i$ v $j$, torej vrednost BoW, ki smo jo videli prej\n",
    "* $N$ je število dokumentov v zbirki\n",
    "* $df_i$ je število dokumentov, ki vsebujejo besedo $i$ v celotni zbirki\n",
    "\n",
    "Vrednost TF-IDF $w_{ij}$ se povečuje sorazmerno s številom pojavitev besede v dokumentu in se zmanjša glede na število dokumentov v korpusu, ki vsebujejo to besedo, kar pomaga prilagoditi dejstvo, da se nekatere besede pojavljajo pogosteje kot druge. Na primer, če se beseda pojavi v *vsakem* dokumentu v zbirki, potem je $df_i=N$, in $w_{ij}=0$, takšni izrazi pa bi bili popolnoma prezrti.\n",
    "\n",
    "TF-IDF vektorizacijo besedila lahko enostavno ustvarite s pomočjo Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaključek\n",
    "\n",
    "Čeprav TF-IDF predstavitve besedam dodelijo uteži glede na njihovo pogostost, ne morejo predstaviti pomena ali vrstnega reda. Kot je slavni jezikoslovec J. R. Firth leta 1935 dejal: \"Popoln pomen besede je vedno kontekstualen in nobena študija pomena brez konteksta ne more biti resna.\" Kasneje v tečaju se bomo naučili, kako zajeti kontekstualne informacije iz besedila z uporabo jezikovnega modeliranja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Omejitev odgovornosti**:  \nTa dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da se zavedate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo strokovno človeško prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napačne razlage, ki izhajajo iz uporabe tega prevoda.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-30T08:17:59+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "sl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}