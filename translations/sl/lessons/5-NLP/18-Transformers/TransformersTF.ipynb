{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mehanizmi pozornosti in transformatorji\n",
    "\n",
    "Ena glavnih pomanjkljivosti rekurzivnih omrežij je, da imajo vse besede v zaporedju enak vpliv na rezultat. To povzroča suboptimalno delovanje pri standardnih modelih LSTM kodirnik-dekodirnik za naloge zaporedja v zaporedje, kot so prepoznavanje imenovanih entitet in strojno prevajanje. V resnici imajo določene besede v vhodnem zaporedju pogosto večji vpliv na izhodne rezultate kot druge.\n",
    "\n",
    "Razmislimo o modelu zaporedja v zaporedje, kot je strojno prevajanje. Ta model je implementiran z dvema rekurzivnima omrežjema, kjer eno omrežje (**kodirnik**) stisne vhodno zaporedje v skrito stanje, drugo omrežje (**dekodirnik**) pa razširi to skrito stanje v preveden rezultat. Težava pri tem pristopu je, da ima končno stanje omrežja težave pri pomnjenju začetka stavka, kar povzroča slabšo kakovost modela pri dolgih stavkih.\n",
    "\n",
    "**Mehanizmi pozornosti** omogočajo tehtanje kontekstualnega vpliva vsakega vhodnega vektorja na vsako izhodno napoved RNN. To se implementira z ustvarjanjem bližnjic med vmesnimi stanji vhodnega RNN in izhodnega RNN. Na ta način, ko generiramo izhodni simbol $y_t$, upoštevamo vsa skrita stanja vhodov $h_i$ z različnimi utežnimi koeficienti $\\alpha_{t,i}$.\n",
    "\n",
    "![Slika prikazuje model kodirnik/dekodirnik z dodatno plastjo pozornosti](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.sl.png)\n",
    "*Model kodirnik-dekodirnik z mehanizmom dodatne pozornosti v [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citirano iz [tega bloga](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matrika pozornosti $\\{\\alpha_{i,j}\\}$ predstavlja stopnjo, do katere določene vhodne besede vplivajo na generacijo določene besede v izhodnem zaporedju. Spodaj je primer takšne matrike:\n",
    "\n",
    "![Slika prikazuje vzorčno poravnavo, ki jo je našel RNNsearch-50, vzeto iz Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.sl.png)\n",
    "\n",
    "*Slika vzeta iz [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Mehanizmi pozornosti so odgovorni za velik del trenutnega ali skoraj trenutnega stanja umetnosti na področju obdelave naravnega jezika. Dodajanje pozornosti pa močno poveča število parametrov modela, kar je povzročilo težave pri skaliranju RNN-jev. Ključna omejitev skaliranja RNN-jev je, da rekurzivna narava modelov otežuje združevanje in paralelizacijo učenja. Pri RNN mora biti vsak element zaporedja obdelan v zaporednem vrstnem redu, kar pomeni, da ga ni mogoče enostavno paralelizirati.\n",
    "\n",
    "Uporaba mehanizmov pozornosti v kombinaciji s to omejitvijo je privedla do nastanka transformatorjev, ki so trenutno stanje umetnosti in jih danes poznamo ter uporabljamo, od BERT do OpenGPT3.\n",
    "\n",
    "## Transformatorji\n",
    "\n",
    "Namesto da bi prenašali kontekst vsake prejšnje napovedi v naslednji korak ocenjevanja, **transformatorji** uporabljajo **pozicijske kodiranja** in **pozornost**, da zajamejo kontekst danega vhoda znotraj določenega okna besedila. Spodnja slika prikazuje, kako pozicijska kodiranja s pozornostjo zajamejo kontekst znotraj določenega okna.\n",
    "\n",
    "![Animiran GIF prikazuje, kako se ocene izvajajo v transformatorjih.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Ker je vsaka vhodna pozicija neodvisno preslikana v vsako izhodno pozicijo, transformatorji omogočajo boljšo paralelizacijo kot RNN-ji, kar omogoča veliko večje in bolj izrazne jezikovne modele. Vsaka glava pozornosti se lahko uporablja za učenje različnih odnosov med besedami, kar izboljša naloge obdelave naravnega jezika.\n",
    "\n",
    "## Gradnja preprostega modela transformatorja\n",
    "\n",
    "Keras ne vsebuje vgrajene plasti transformatorja, vendar jo lahko zgradimo sami. Kot prej se bomo osredotočili na klasifikacijo besedila iz nabora podatkov AG News, vendar je vredno omeniti, da transformatorji kažejo najboljše rezultate pri težjih nalogah obdelave naravnega jezika.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nove plasti v Kerasu morajo podrazrediti razred `Layer` in implementirati metodo `call`. Začnimo s plastjo **Positional Embedding**. Uporabili bomo [nekaj kode iz uradne dokumentacije Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Predpostavljali bomo, da zapolnimo vse vhodne zaporedja na dolžino `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta plast je sestavljena iz dveh `Embedding` plasti: za vdelavo tokenov (na način, ki smo ga že obravnavali) in položajev tokenov. Položaji tokenov so ustvarjeni kot zaporedje naravnih števil od 0 do `maxlen` z uporabo `tf.range`, nato pa so posredovani skozi vdelano plast. Dva nastala vdelana vektorja se nato seštejeta, kar ustvari položajsko vdelano predstavitev vhodnih podatkov oblike `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Zdaj pa implementirajmo transformacijski blok. Ta bo sprejel izhod prej definirane vdelane plasti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj smo pripravljeni definirati celoten model transformatorja:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer modeli\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) je zelo velik večplastni transformatorni model z 12 plastmi za *BERT-base* in 24 za *BERT-large*. Model je najprej predhodno usposobljen na velikem korpusu besedilnih podatkov (WikiPedia + knjige) z uporabo nenadzorovanega učenja (napovedovanje zakritih besed v stavku). Med predhodnim usposabljanjem model pridobi pomembno raven razumevanja jezika, ki jo je nato mogoče uporabiti z drugimi nabori podatkov prek finega prilagajanja. Ta proces se imenuje **prenosno učenje**.\n",
    "\n",
    "![slika s http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.sl.png)\n",
    "\n",
    "Obstaja veliko različic transformatornih arhitektur, vključno z BERT, DistilBERT, BigBird, OpenGPT3 in drugimi, ki jih je mogoče fino prilagoditi.\n",
    "\n",
    "Poglejmo, kako lahko uporabimo predhodno usposobljen model BERT za reševanje našega tradicionalnega problema klasifikacije zaporedij. Idejo in nekaj kode bomo vzeli iz [uradne dokumentacije](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Za nalaganje predhodno usposobljenih modelov bomo uporabili **Tensorflow hub**. Najprej naložimo BERT-specifični vektorizator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomembno je, da uporabite isti vektorizator, kot je bil uporabljen pri treniranju originalnega omrežja. Poleg tega BERT vektorizator vrne tri komponente:\n",
    "* `input_word_ids`, ki je zaporedje številk tokenov za vhodni stavek\n",
    "* `input_mask`, ki prikazuje, kateri del zaporedja vsebuje dejanski vhod in kateri del je polnilo. To je podobno maski, ki jo ustvari plast `Masking`\n",
    "* `input_type_ids` se uporablja za naloge jezikovnega modeliranja in omogoča določitev dveh vhodnih stavkov v enem zaporedju.\n",
    "\n",
    "Nato lahko ustvarimo BERT ekstraktor značilnosti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torej, BERT plast vrne več uporabnih rezultatov:\n",
    "* `pooled_output` je rezultat povprečenja vseh tokenov v zaporedju. Lahko ga razumete kot inteligentno semantično vdelavo celotnega omrežja. To je enakovredno izhodu sloja `GlobalAveragePooling1D` v našem prejšnjem modelu.\n",
    "* `sequence_output` je izhod zadnje transformer plasti (ustreza izhodu `TransformerBlock` v našem zgornjem modelu).\n",
    "* `encoder_outputs` so izhodi vseh transformer plasti. Ker smo naložili 4-slojni BERT model (kot verjetno lahko sklepate iz imena, ki vsebuje `4_H`), ima 4 tenzorje. Zadnji je enak kot `sequence_output`.\n",
    "\n",
    "Zdaj bomo definirali končni model za klasifikacijo. Uporabili bomo *funkcionalno definicijo modela*, kjer definiramo vhod modela in nato podamo niz izrazov za izračun njegovega izhoda. Prav tako bomo uteži BERT modela nastavili kot netrenirljive in trenirali samo končni klasifikator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kljub temu, da je malo parametrov za učenje, je proces precej počasen, saj je BERT-ov ekstraktor značilnosti računsko zahteven. Zdi se, da nismo uspeli doseči zadovoljive natančnosti, bodisi zaradi pomanjkanja učenja ali pomanjkanja parametrov modela.\n",
    "\n",
    "Poskusimo odmrzniti uteži BERT-a in ga tudi trenirati. To zahteva zelo majhno hitrost učenja ter bolj previdno strategijo učenja z **ogrevanjem** in uporabo optimizatorja **AdamW**. Za ustvarjanje optimizatorja bomo uporabili paket `tf-models-official`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kot lahko vidiš, gre učenje precej počasi – vendar bi morda želel eksperimentirati in model trenirati za nekaj epochov (5–10) ter preveriti, ali lahko dosežeš boljši rezultat v primerjavi s pristopi, ki smo jih uporabili prej.\n",
    "\n",
    "## Knjižnica Huggingface Transformers\n",
    "\n",
    "Drug zelo pogost (in nekoliko enostavnejši) način uporabe Transformer modelov je [HuggingFace paket](https://github.com/huggingface/), ki ponuja preproste gradnike za različne naloge obdelave naravnega jezika (NLP). Na voljo je tako za Tensorflow kot za PyTorch, še en zelo priljubljen okvir za nevronske mreže.\n",
    "\n",
    "> **Opomba**: Če te ne zanima, kako deluje knjižnica Transformers, lahko preskočiš na konec tega zvezka, saj ne boš videl ničesar bistveno drugačnega od tega, kar smo naredili zgoraj. Ponovili bomo iste korake treniranja BERT modela z uporabo druge knjižnice in bistveno večjega modela. Zato proces vključuje precej dolgotrajno učenje, tako da morda želiš le preleteti kodo.\n",
    "\n",
    "Poglejmo, kako lahko naš problem rešimo z uporabo [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najprej moramo izbrati model, ki ga bomo uporabljali. Poleg nekaterih vgrajenih modelov Huggingface vsebuje [spletni repozitorij modelov](https://huggingface.co/models), kjer lahko najdete veliko več predhodno naučenih modelov, ki jih je prispevala skupnost. Vse te modele je mogoče naložiti in uporabljati zgolj z navedbo imena modela. Vsi potrebni binarni datoteki za model se bodo samodejno prenesli.\n",
    "\n",
    "V določenih primerih boste morali naložiti svoje modele, v tem primeru lahko določite imenik, ki vsebuje vse ustrezne datoteke, vključno s parametri za tokenizer, datoteko `config.json` s parametri modela, binarne uteži itd.\n",
    "\n",
    "Iz imena modela lahko ustvarimo tako model kot tokenizer. Začnimo s tokenizerjem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` objekt vsebuje funkcijo `encode`, ki jo je mogoče neposredno uporabiti za kodiranje besedila:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prav tako lahko uporabimo tokenizer za kodiranje zaporedja na način, ki je primeren za posredovanje modelu, tj. vključno s polji `token_ids`, `input_mask` itd. Prav tako lahko določimo, da želimo Tensorflow tenzorje, tako da podamo argument `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V našem primeru bomo uporabili vnaprej naučen BERT model z imenom `bert-base-uncased`. *Uncased* pomeni, da je model neobčutljiv na velike in male črke.\n",
    "\n",
    "Pri treniranju modela moramo kot vhod zagotoviti tokenizirano zaporedje, zato bomo zasnovali podatkovno procesno cevovod. Ker je `tokenizer.encode` Python funkcija, bomo uporabili enak pristop kot v prejšnji enoti, kjer jo pokličemo z uporabo `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj lahko naložimo dejanski model z uporabo paketa `BertForSequenceClassification`. To zagotavlja, da ima naš model že zahtevano arhitekturo za klasifikacijo, vključno s končnim klasifikatorjem. Videli boste opozorilno sporočilo, ki navaja, da uteži končnega klasifikatorja niso inicializirane in da model potrebuje predhodno učenje - to je povsem v redu, saj je točno to tisto, kar bomo storili!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kot lahko vidite iz `summary()`, model vsebuje skoraj 110 milijonov parametrov! Predvidoma, če želimo preprosto nalogo klasifikacije na razmeroma majhnem naboru podatkov, ne želimo trenirati osnovnega sloja BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdaj smo pripravljeni na začetek usposabljanja!\n",
    "\n",
    "> **Opomba**: Usposabljanje celotnega modela BERT lahko vzame veliko časa! Zato ga bomo usposabljali le za prvih 32 serij. To je zgolj za prikaz, kako je usposabljanje modela nastavljeno. Če vas zanima polno usposabljanje - preprosto odstranite parametra `steps_per_epoch` in `validation_steps` ter se pripravite na čakanje!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Če povečate število iteracij in počakate dovolj dolgo ter trenirate skozi več epochov, lahko pričakujete, da bo klasifikacija z BERT dosegla najboljšo natančnost! To je zato, ker BERT že zelo dobro razume strukturo jezika, zato je potrebno le prilagoditi končni klasifikator. Vendar pa, ker je BERT velik model, celoten proces učenja traja dolgo in zahteva veliko računske moči! (GPU, in po možnosti več kot enega).\n",
    "\n",
    "> **Opomba:** V našem primeru uporabljamo enega najmanjših vnaprej naučenih modelov BERT. Obstajajo večji modeli, ki bodo verjetno dali boljše rezultate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ključne točke\n",
    "\n",
    "V tej enoti smo si ogledali zelo nedavne arhitekture modelov, ki temeljijo na **transformerjih**. Uporabili smo jih za našo nalogo klasifikacije besedila, vendar lahko modele BERT podobno uporabimo tudi za ekstrakcijo entitet, odgovarjanje na vprašanja in druge naloge obdelave naravnega jezika (NLP).\n",
    "\n",
    "Transformer modeli predstavljajo trenutno najsodobnejši pristop v NLP-ju in v večini primerov bi morali biti prva rešitev, s katero začnete eksperimentirati pri implementaciji prilagojenih NLP rešitev. Kljub temu je razumevanje osnovnih načel rekurentnih nevronskih mrež, o katerih smo govorili v tem modulu, izjemno pomembno, če želite graditi napredne nevronske modele.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Omejitev odgovornosti**:  \nTa dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne prevzemamo odgovornosti za morebitna nesporazume ali napačne razlage, ki bi nastale zaradi uporabe tega prevoda.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-30T08:06:40+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "sl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}