<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T06:40:40+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ur"
}
-->
# ڈیپ ریئنفورسمنٹ لرننگ

ریئنفورسمنٹ لرننگ (RL) کو مشین لرننگ کے بنیادی پیراڈائمز میں سے ایک سمجھا جاتا ہے، جیسے سپروائزڈ لرننگ اور انسپروائزڈ لرننگ۔ جہاں سپروائزڈ لرننگ میں ہم معلوم نتائج والے ڈیٹا سیٹ پر انحصار کرتے ہیں، RL **عمل کے ذریعے سیکھنے** پر مبنی ہے۔ مثال کے طور پر، جب ہم پہلی بار کسی کمپیوٹر گیم کو دیکھتے ہیں، تو ہم کھیلنا شروع کرتے ہیں، چاہے ہمیں قواعد معلوم نہ ہوں، اور جلد ہی ہم صرف کھیلنے اور اپنے رویے کو ایڈجسٹ کرنے کے عمل کے ذریعے اپنی مہارت کو بہتر بنا لیتے ہیں۔

## [لیکچر سے پہلے کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL انجام دینے کے لیے ہمیں ضرورت ہوتی ہے:

* ایک **ماحول** یا **سیمولیٹر** جو گیم کے قواعد طے کرتا ہے۔ ہمیں سیمولیٹر میں تجربات چلانے اور نتائج کا مشاہدہ کرنے کے قابل ہونا چاہیے۔
* کچھ **ریوارڈ فنکشن**، جو یہ ظاہر کرتا ہے کہ ہمارا تجربہ کتنا کامیاب رہا۔ کمپیوٹر گیم کھیلنا سیکھنے کے معاملے میں، انعام ہمارا آخری اسکور ہوگا۔

ریوارڈ فنکشن کی بنیاد پر، ہمیں اپنے رویے کو ایڈجسٹ کرنے اور اپنی مہارت کو بہتر بنانے کے قابل ہونا چاہیے، تاکہ اگلی بار ہم بہتر کھیل سکیں۔ مشین لرننگ کی دیگر اقسام اور RL کے درمیان بنیادی فرق یہ ہے کہ RL میں ہمیں عام طور پر یہ معلوم نہیں ہوتا کہ ہم جیتے ہیں یا ہارے ہیں جب تک کہ ہم گیم ختم نہ کریں۔ لہذا، ہم یہ نہیں کہہ سکتے کہ کوئی خاص حرکت اکیلے اچھی ہے یا نہیں - ہمیں صرف گیم کے اختتام پر انعام ملتا ہے۔

RL کے دوران، ہم عام طور پر بہت سے تجربات کرتے ہیں۔ ہر تجربے کے دوران، ہمیں اب تک سیکھے گئے بہترین حکمت عملی پر عمل کرنے (**استفادہ**) اور نئے ممکنہ حالات کو دریافت کرنے (**تلاش**) کے درمیان توازن قائم کرنا ہوتا ہے۔

## اوپن اے آئی جم

ریئنفورسمنٹ لرننگ کے لیے ایک بہترین ٹول [اوپن اے آئی جم](https://gym.openai.com/) ہے - ایک **سیمولیشن ماحول**، جو مختلف ماحول کو سیمولیٹ کر سکتا ہے، جیسے اٹاری گیمز سے لے کر پول بیلنسنگ کے فزکس تک۔ یہ ریئنفورسمنٹ لرننگ الگورتھمز کی تربیت کے لیے سب سے زیادہ مقبول سیمولیشن ماحول میں سے ایک ہے، اور اسے [اوپن اے آئی](https://openai.com/) برقرار رکھتا ہے۔

> **نوٹ**: آپ اوپن اے آئی جم کے تمام دستیاب ماحول [یہاں](https://gym.openai.com/envs/#classic_control) دیکھ سکتے ہیں۔

## کارٹ پول بیلنسنگ

آپ نے شاید جدید بیلنسنگ ڈیوائسز جیسے *سیگ وے* یا *جائرو سکوٹرز* دیکھے ہوں گے۔ یہ خود بخود بیلنس کرنے کے قابل ہوتے ہیں، اپنے پہیوں کو ایکسیلرومیٹر یا جائرو سکوپ کے سگنل کے جواب میں ایڈجسٹ کر کے۔ اس سیکشن میں، ہم ایک مشابہ مسئلہ حل کرنا سیکھیں گے - پول کو بیلنس کرنا۔ یہ اس صورتحال سے مشابہ ہے جب ایک سرکس کا فنکار اپنے ہاتھ پر پول کو بیلنس کرتا ہے - لیکن یہ پول بیلنسنگ صرف 1D میں ہوتی ہے۔

بیلنسنگ کا ایک سادہ ورژن **کارٹ پول** مسئلے کے نام سے جانا جاتا ہے۔ کارٹ پول کی دنیا میں، ہمارے پاس ایک افقی سلائیڈر ہوتا ہے جو بائیں یا دائیں حرکت کر سکتا ہے، اور مقصد یہ ہے کہ سلائیڈر کے اوپر ایک عمودی پول کو بیلنس کیا جائے۔

<img alt="ایک کارٹ پول" src="images/cartpole.png" width="200"/>

اس ماحول کو بنانے اور استعمال کرنے کے لیے، ہمیں چند لائنز پائتھون کوڈ کی ضرورت ہوتی ہے:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

ہر ماحول کو بالکل اسی طرح رسائی حاصل کی جا سکتی ہے:
* `env.reset` ایک نیا تجربہ شروع کرتا ہے
* `env.step` ایک سیمولیشن قدم انجام دیتا ہے۔ یہ **ایکشن اسپیس** سے ایک **ایکشن** وصول کرتا ہے، اور **آبزرویشن اسپیس** سے ایک **آبزرویشن** واپس کرتا ہے، ساتھ ہی ایک انعام اور ایک ختم ہونے کا پرچم۔

اوپر دیے گئے مثال میں ہم ہر قدم پر ایک بے ترتیب عمل انجام دیتے ہیں، یہی وجہ ہے کہ تجربے کی زندگی بہت مختصر ہے:

![غیر بیلنسنگ کارٹ پول](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL الگورتھم کا مقصد ایک ماڈل - جسے **پالیسی** &pi; کہا جاتا ہے - کو تربیت دینا ہے، جو دیے گئے حالت کے جواب میں عمل واپس کرے گا۔ ہم پالیسی کو احتمالی بھی سمجھ سکتے ہیں، یعنی کسی بھی حالت *s* اور عمل *a* کے لیے یہ احتمال &pi;(*a*|*s*) واپس کرے گا کہ ہمیں حالت *s* میں *a* لینا چاہیے۔

## پالیسی گریڈینٹس الگورتھم

پالیسی کو ماڈل کرنے کا سب سے واضح طریقہ یہ ہے کہ ایک نیورل نیٹ ورک بنایا جائے جو حالتوں کو ان پٹ کے طور پر لے، اور متعلقہ اعمال (یا تمام اعمال کے احتمالات) واپس کرے۔ ایک لحاظ سے، یہ ایک عام درجہ بندی کے کام کی طرح ہوگا، ایک بڑے فرق کے ساتھ - ہمیں پہلے سے معلوم نہیں ہوتا کہ ہر قدم پر کون سے اعمال لینے چاہئیں۔

یہاں خیال یہ ہے کہ ان احتمالات کا اندازہ لگایا جائے۔ ہم **کمیولیٹو ریوارڈز** کا ایک ویکٹر بناتے ہیں جو تجربے کے ہر قدم پر ہمارا کل انعام ظاہر کرتا ہے۔ ہم **ریوارڈ ڈسکاؤنٹنگ** بھی لاگو کرتے ہیں، پہلے کے انعامات کو کچھ کوفیشینٹ &gamma;=0.99 سے ضرب دے کر، تاکہ پہلے کے انعامات کے کردار کو کم کیا جا سکے۔ پھر، ہم ان قدموں کو تقویت دیتے ہیں جو زیادہ انعامات دیتے ہیں۔

> پالیسی گریڈینٹ الگورتھم کے بارے میں مزید جانیں اور اسے عمل میں دیکھیں [مثال نوٹ بک](CartPole-RL-TF.ipynb) میں۔

## ایکٹر-کریٹک الگورتھم

پالیسی گریڈینٹس کے طریقہ کار کا ایک بہتر ورژن **ایکٹر-کریٹک** کہلاتا ہے۔ اس کے پیچھے بنیادی خیال یہ ہے کہ نیورل نیٹ ورک کو دو چیزیں واپس کرنے کے لیے تربیت دی جائے:

* پالیسی، جو طے کرتی ہے کہ کون سا عمل لینا ہے۔ اس حصے کو **ایکٹر** کہا جاتا ہے۔
* کل انعام کا اندازہ جو ہم اس حالت میں حاصل کر سکتے ہیں - اس حصے کو **کریٹک** کہا جاتا ہے۔

ایک لحاظ سے، یہ [GAN](../../4-ComputerVision/10-GANs/README.md) کی طرح ہے، جہاں ہمارے پاس دو نیٹ ورکس ہیں جو ایک دوسرے کے خلاف تربیت یافتہ ہیں۔ ایکٹر-کریٹک ماڈل میں، ایکٹر وہ عمل تجویز کرتا ہے جو ہمیں لینا چاہیے، اور کریٹک تنقیدی ہونے کی کوشش کرتا ہے اور نتیجہ کا اندازہ لگاتا ہے۔ تاہم، ہمارا مقصد ان نیٹ ورکس کو ہم آہنگی سے تربیت دینا ہے۔

چونکہ ہمیں تجربے کے دوران حقیقی کمیولیٹو ریوارڈز اور کریٹک کے ذریعے واپس کیے گئے نتائج دونوں معلوم ہوتے ہیں، اس لیے ان کے درمیان فرق کو کم کرنے کے لیے ایک لاس فنکشن بنانا نسبتاً آسان ہے۔ یہ ہمیں **کریٹک لاس** دے گا۔ ہم **ایکٹر لاس** کو پالیسی گریڈینٹ الگورتھم کے طریقہ کار کا استعمال کرتے ہوئے حساب کر سکتے ہیں۔

ان الگورتھمز میں سے کسی ایک کو چلانے کے بعد، ہم توقع کر سکتے ہیں کہ ہمارا کارٹ پول اس طرح برتاؤ کرے گا:

![ایک بیلنسنگ کارٹ پول](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ مشقیں: پالیسی گریڈینٹس اور ایکٹر-کریٹک RL

اپنی تعلیم کو درج ذیل نوٹ بکس میں جاری رکھیں:

* [TensorFlow میں RL](CartPole-RL-TF.ipynb)
* [PyTorch میں RL](CartPole-RL-PyTorch.ipynb)

## دیگر RL کام

ریئنفورسمنٹ لرننگ آج کل تحقیق کا ایک تیزی سے بڑھتا ہوا میدان ہے۔ ریئنفورسمنٹ لرننگ کی کچھ دلچسپ مثالیں ہیں:

* کمپیوٹر کو **اٹاری گیمز** کھیلنا سکھانا۔ اس مسئلے میں چیلنجنگ حصہ یہ ہے کہ ہمارے پاس ایک سادہ حالت نہیں ہوتی جو ایک ویکٹر کی طرح ہو، بلکہ ایک اسکرین شاٹ ہوتا ہے - اور ہمیں CNN کا استعمال کرتے ہوئے اس اسکرین امیج کو فیچر ویکٹر میں تبدیل کرنا ہوتا ہے، یا انعام کی معلومات نکالنی ہوتی ہے۔ اٹاری گیمز جم میں دستیاب ہیں۔
* کمپیوٹر کو بورڈ گیمز کھیلنا سکھانا، جیسے شطرنج اور گو۔ حال ہی میں، جدید ترین پروگرام جیسے **الفا زیرو** کو دو ایجنٹس کے ایک دوسرے کے خلاف کھیلنے اور ہر قدم پر بہتر ہونے کے ذریعے شروع سے تربیت دی گئی۔
* صنعت میں، RL کو سیمولیشن سے کنٹرول سسٹمز بنانے کے لیے استعمال کیا جاتا ہے۔ ایک سروس جسے [بونسائی](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) کہا جاتا ہے، خاص طور پر اس کے لیے ڈیزائن کی گئی ہے۔

## نتیجہ

ہم نے اب سیکھ لیا ہے کہ ایجنٹس کو صرف ایک ریوارڈ فنکشن فراہم کر کے، جو گیم کی مطلوبہ حالت کی وضاحت کرتا ہے، اور انہیں تلاش کی جگہ کو ذہانت سے دریافت کرنے کا موقع دے کر اچھے نتائج حاصل کرنے کے لیے تربیت دینا کیسے ہے۔ ہم نے کامیابی سے دو الگورتھمز آزمائے، اور نسبتاً مختصر وقت میں اچھا نتیجہ حاصل کیا۔ تاہم، یہ RL میں آپ کے سفر کا صرف آغاز ہے، اور اگر آپ مزید گہرائی میں جانا چاہتے ہیں تو آپ کو ایک الگ کورس لینے پر غور کرنا چاہیے۔

## 🚀 چیلنج

'دیگر RL کام' سیکشن میں درج ایپلیکیشنز کو دریافت کریں اور ایک کو نافذ کرنے کی کوشش کریں!

## [لیکچر کے بعد کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## جائزہ اور خود مطالعہ

کلاسیکل ریئنفورسمنٹ لرننگ کے بارے میں مزید جانیں ہمارے [مشین لرننگ فار بیگنرز نصاب](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) میں۔

[یہ زبردست ویڈیو](https://www.youtube.com/watch?v=qv6UVOQ0F44) دیکھیں جو بتاتی ہے کہ کمپیوٹر سپر ماریو کھیلنا کیسے سیکھ سکتا ہے۔

## اسائنمنٹ: [ایک ماؤنٹین کار کو تربیت دیں](lab/README.md)

اس اسائنمنٹ کے دوران آپ کا مقصد ایک مختلف جم ماحول - [ماؤنٹین کار](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) کو تربیت دینا ہوگا۔

---

