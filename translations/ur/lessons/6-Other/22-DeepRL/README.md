<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-26T10:12:18+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ur"
}
-->
# ڈیپ ری انفورسمنٹ لرننگ

ری انفورسمنٹ لرننگ (RL) کو مشین لرننگ کے بنیادی پیراڈائمز میں سے ایک سمجھا جاتا ہے، جیسے سپروائزڈ لرننگ اور ان سپروائزڈ لرننگ۔ جہاں سپروائزڈ لرننگ میں ہم معلوم نتائج والے ڈیٹا سیٹ پر انحصار کرتے ہیں، RL **عمل کے ذریعے سیکھنے** پر مبنی ہے۔ مثال کے طور پر، جب ہم پہلی بار کسی کمپیوٹر گیم کو دیکھتے ہیں، تو ہم کھیلنا شروع کرتے ہیں، چاہے ہمیں اصول معلوم نہ ہوں، اور جلد ہی ہم صرف کھیلنے اور اپنے رویے کو ایڈجسٹ کرنے کے عمل سے اپنی مہارت کو بہتر بنا لیتے ہیں۔

## [لیکچر سے پہلے کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL انجام دینے کے لیے ہمیں ضرورت ہوتی ہے:

* ایک **ماحول** یا **سیمیولیٹر** جو گیم کے اصول طے کرتا ہے۔ ہمیں سیمیولیٹر میں تجربات چلانے اور نتائج کا مشاہدہ کرنے کے قابل ہونا چاہیے۔
* کچھ **ریوارڈ فنکشن**، جو یہ ظاہر کرتا ہے کہ ہمارا تجربہ کتنا کامیاب رہا۔ کمپیوٹر گیم کھیلنا سیکھنے کے معاملے میں، انعام ہمارا آخری اسکور ہوگا۔

ریوارڈ فنکشن کی بنیاد پر، ہمیں اپنے رویے کو ایڈجسٹ کرنے اور اپنی مہارت کو بہتر بنانے کے قابل ہونا چاہیے، تاکہ اگلی بار ہم بہتر کھیل سکیں۔ دیگر قسم کی مشین لرننگ اور RL کے درمیان بنیادی فرق یہ ہے کہ RL میں ہمیں عام طور پر یہ معلوم نہیں ہوتا کہ ہم جیتے یا ہارے جب تک کہ ہم گیم ختم نہ کریں۔ لہذا، ہم یہ نہیں کہہ سکتے کہ کوئی خاص حرکت اکیلے اچھی ہے یا نہیں - ہمیں صرف گیم کے اختتام پر انعام ملتا ہے۔

RL کے دوران، ہم عام طور پر کئی تجربات انجام دیتے ہیں۔ ہر تجربے کے دوران، ہمیں اس حکمت عملی پر عمل کرنے کے درمیان توازن قائم کرنا ہوتا ہے جو ہم نے اب تک سیکھی ہے (**استفادہ**) اور نئے ممکنہ حالات کو دریافت کرنا (**تلاش**).

## اوپن اے آئی جم

RL کے لیے ایک بہترین ٹول [اوپن اے آئی جم](https://gym.openai.com/) ہے - ایک **سیمیولیشن ماحول**، جو مختلف ماحول کو سیمیولیٹ کر سکتا ہے، جیسے اٹاری گیمز سے لے کر پول بیلنسنگ کے پیچھے فزکس تک۔ یہ ری انفورسمنٹ لرننگ الگورتھمز کی تربیت کے لیے سب سے زیادہ مقبول سیمیولیشن ماحول میں سے ایک ہے، اور اسے [اوپن اے آئی](https://openai.com/) برقرار رکھتا ہے۔

> **نوٹ**: آپ اوپن اے آئی جم کے تمام دستیاب ماحول [یہاں](https://gym.openai.com/envs/#classic_control) دیکھ سکتے ہیں۔

## کارٹ پول بیلنسنگ

آپ نے شاید جدید بیلنسنگ ڈیوائسز جیسے *سیگ وے* یا *جائرو سکوٹرز* دیکھے ہوں گے۔ یہ خود بخود بیلنس کرنے کے قابل ہوتے ہیں، اپنے پہیوں کو ایکسلرومیٹر یا جائروسکوپ کے سگنل کے جواب میں ایڈجسٹ کر کے۔ اس سیکشن میں، ہم ایک مشابہ مسئلہ حل کرنا سیکھیں گے - پول کو بیلنس کرنا۔ یہ اس صورتحال سے مشابہ ہے جب ایک سرکس کا فنکار اپنے ہاتھ پر پول کو بیلنس کرنے کی کوشش کرتا ہے - لیکن یہ پول بیلنسنگ صرف 1D میں ہوتی ہے۔

بیلنسنگ کا ایک سادہ ورژن **کارٹ پول** مسئلے کے نام سے جانا جاتا ہے۔ کارٹ پول کی دنیا میں، ہمارے پاس ایک افقی سلائیڈر ہوتا ہے جو بائیں یا دائیں حرکت کر سکتا ہے، اور مقصد یہ ہے کہ سلائیڈر کے اوپر ایک عمودی پول کو بیلنس کیا جائے۔

<img alt="ایک کارٹ پول" src="images/cartpole.png" width="200"/>

اس ماحول کو بنانے اور استعمال کرنے کے لیے، ہمیں چند لائنز پائتھون کوڈ کی ضرورت ہوتی ہے:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

ہر ماحول کو بالکل اسی طرح رسائی حاصل کی جا سکتی ہے:
* `env.reset` ایک نیا تجربہ شروع کرتا ہے
* `env.step` ایک سیمیولیشن قدم انجام دیتا ہے۔ یہ **ایکشن** کو **ایکشن اسپیس** سے وصول کرتا ہے، اور **مشاہدہ** (مشاہدہ اسپیس سے)، انعام اور ختم ہونے کا پرچم واپس کرتا ہے۔

اوپر دیے گئے مثال میں، ہم ہر قدم پر ایک بے ترتیب ایکشن انجام دیتے ہیں، یہی وجہ ہے کہ تجربے کی زندگی بہت مختصر ہے:

![غیر بیلنسنگ کارٹ پول](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL الگورتھم کا مقصد ایک ماڈل - جسے **پالیسی** π کہا جاتا ہے - کو تربیت دینا ہے، جو دیے گئے حالت کے جواب میں ایکشن واپس کرے گا۔ ہم پالیسی کو احتمالی بھی سمجھ سکتے ہیں، یعنی کسی بھی حالت *s* اور ایکشن *a* کے لیے یہ احتمال π(*a*|*s*) واپس کرے گا کہ ہمیں حالت *s* میں *a* لینا چاہیے۔

## پالیسی گریڈینٹس الگورتھم

پالیسی کو ماڈل کرنے کا سب سے واضح طریقہ ایک نیورل نیٹ ورک بنانا ہے جو حالتوں کو ان پٹ کے طور پر لے، اور متعلقہ ایکشنز (یا تمام ایکشنز کے امکانات) واپس کرے۔ ایک لحاظ سے، یہ ایک عام درجہ بندی کے کام کی طرح ہوگا، ایک بڑے فرق کے ساتھ - ہمیں پہلے سے معلوم نہیں ہوتا کہ ہر قدم پر کون سے ایکشنز لینے چاہئیں۔

یہاں خیال یہ ہے کہ ان امکانات کا اندازہ لگایا جائے۔ ہم **مجموعی انعامات** کا ایک ویکٹر بناتے ہیں جو تجربے کے ہر قدم پر ہمارا کل انعام ظاہر کرتا ہے۔ ہم **انعام کی رعایت** بھی لاگو کرتے ہیں، پہلے کے انعامات کو کچھ گتانک γ=0.99 سے ضرب دے کر، تاکہ پہلے کے انعامات کے کردار کو کم کیا جا سکے۔ پھر، ہم ان قدموں کو تقویت دیتے ہیں جو زیادہ انعامات دیتے ہیں۔

> پالیسی گریڈینٹ الگورتھم کے بارے میں مزید جانیں اور اسے عمل میں دیکھیں [مثال نوٹ بک](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) میں۔

## ایکٹر-کریٹک الگورتھم

پالیسی گریڈینٹس کے طریقہ کار کا ایک بہتر ورژن **ایکٹر-کریٹک** کہلاتا ہے۔ اس کے پیچھے بنیادی خیال یہ ہے کہ نیورل نیٹ ورک کو دو چیزیں واپس کرنے کے لیے تربیت دی جائے:

* پالیسی، جو طے کرتی ہے کہ کون سا ایکشن لینا ہے۔ اس حصے کو **ایکٹر** کہا جاتا ہے۔
* کل انعام کا تخمینہ جو ہم اس حالت میں حاصل کرنے کی توقع کر سکتے ہیں - اس حصے کو **کریٹک** کہا جاتا ہے۔

ایک لحاظ سے، یہ [GAN](../../4-ComputerVision/10-GANs/README.md) کی طرح ہے، جہاں ہمارے پاس دو نیٹ ورکس ہیں جو ایک دوسرے کے خلاف تربیت یافتہ ہیں۔ ایکٹر-کریٹک ماڈل میں، ایکٹر وہ ایکشن تجویز کرتا ہے جو ہمیں لینا چاہیے، اور کریٹک تنقیدی ہونے کی کوشش کرتا ہے اور نتیجہ کا اندازہ لگاتا ہے۔ تاہم، ہمارا مقصد ان نیٹ ورکس کو ہم آہنگی میں تربیت دینا ہے۔

چونکہ ہمیں تجربے کے دوران حقیقی مجموعی انعامات اور کریٹک کے ذریعے واپس کیے گئے نتائج دونوں معلوم ہیں، اس لیے ان کے درمیان فرق کو کم کرنے کے لیے ایک نقصان فنکشن بنانا نسبتاً آسان ہے۔ یہ ہمیں **کریٹک نقصان** دے گا۔ ہم **ایکٹر نقصان** کو پالیسی گریڈینٹ الگورتھم کے طریقہ کار کا استعمال کرتے ہوئے حساب کر سکتے ہیں۔

ان الگورتھمز میں سے کسی ایک کو چلانے کے بعد، ہم توقع کر سکتے ہیں کہ ہمارا کارٹ پول اس طرح برتاؤ کرے گا:

![ایک بیلنسنگ کارٹ پول](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ مشقیں: پالیسی گریڈینٹس اور ایکٹر-کریٹک RL

اپنی تعلیم کو درج ذیل نوٹ بکس میں جاری رکھیں:

* [TensorFlow میں RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch میں RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## دیگر RL کام

ری انفورسمنٹ لرننگ آج کل تحقیق کا ایک تیزی سے بڑھتا ہوا میدان ہے۔ ری انفورسمنٹ لرننگ کی کچھ دلچسپ مثالیں ہیں:

* کمپیوٹر کو **اٹاری گیمز** کھیلنا سکھانا۔ اس مسئلے میں چیلنجنگ حصہ یہ ہے کہ ہمارے پاس ایک سادہ حالت نہیں ہے جو ایک ویکٹر کے طور پر ظاہر ہو، بلکہ ایک اسکرین شاٹ ہے - اور ہمیں اس اسکرین امیج کو فیچر ویکٹر میں تبدیل کرنے یا انعام کی معلومات نکالنے کے لیے CNN کا استعمال کرنا ہوگا۔ اٹاری گیمز جم میں دستیاب ہیں۔
* کمپیوٹر کو بورڈ گیمز کھیلنا سکھانا، جیسے شطرنج اور گو۔ حال ہی میں، جدید ترین پروگرام جیسے **الفا زیرو** کو دو ایجنٹس کے ذریعے ایک دوسرے کے خلاف کھیل کر اور ہر قدم پر بہتر ہو کر شروع سے تربیت دی گئی۔
* صنعت میں، RL کو سیمیولیشن سے کنٹرول سسٹمز بنانے کے لیے استعمال کیا جاتا ہے۔ ایک سروس جسے [بونسائی](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) کہا جاتا ہے خاص طور پر اس کے لیے ڈیزائن کی گئی ہے۔

## نتیجہ

ہم نے اب سیکھ لیا ہے کہ ایجنٹس کو اچھے نتائج حاصل کرنے کے لیے تربیت کیسے دی جائے، صرف ایک انعام فنکشن فراہم کر کے جو گیم کی مطلوبہ حالت کی وضاحت کرتا ہے، اور انہیں تلاش کی جگہ کو ذہانت سے دریافت کرنے کا موقع دے کر۔ ہم نے کامیابی کے ساتھ دو الگورتھمز آزمائے، اور نسبتاً مختصر وقت میں اچھا نتیجہ حاصل کیا۔ تاہم، یہ RL میں آپ کے سفر کا صرف آغاز ہے، اور اگر آپ مزید گہرائی میں جانا چاہتے ہیں تو آپ کو ایک الگ کورس لینے پر غور کرنا چاہیے۔

## 🚀 چیلنج

'دیگر RL کام' سیکشن میں درج ایپلیکیشنز کو دریافت کریں اور ایک کو نافذ کرنے کی کوشش کریں!

## [لیکچر کے بعد کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## جائزہ اور خود مطالعہ

کلاسیکل ری انفورسمنٹ لرننگ کے بارے میں مزید جانیں ہمارے [مشین لرننگ فار بیگنرز نصاب](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) میں۔

[یہ زبردست ویڈیو](https://www.youtube.com/watch?v=qv6UVOQ0F44) دیکھیں جو بتاتی ہے کہ کمپیوٹر سپر ماریو کھیلنا کیسے سیکھ سکتا ہے۔

## اسائنمنٹ: [ایک ماؤنٹین کار کو تربیت دیں](lab/README.md)

اس اسائنمنٹ کے دوران آپ کا مقصد ایک مختلف جم ماحول - [ماؤنٹین کار](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) کو تربیت دینا ہوگا۔

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔