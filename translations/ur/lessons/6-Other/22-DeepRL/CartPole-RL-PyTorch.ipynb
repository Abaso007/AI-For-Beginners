{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# کارٹ پول بیلنسنگ کے لیے RL کی تربیت\n",
    "\n",
    "یہ نوٹ بک [AI for Beginners Curriculum](http://aka.ms/ai-beginners) کا حصہ ہے۔ یہ [PyTorch کے آفیشل ٹیوٹوریل](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) اور [کارٹ پول کے اس PyTorch امپلیمنٹیشن](https://github.com/yc930401/Actor-Critic-pytorch) سے متاثر ہو کر لکھی گئی ہے۔\n",
    "\n",
    "اس مثال میں، ہم RL کا استعمال کرتے ہوئے ایک ماڈل کو تربیت دیں گے تاکہ وہ ایک کارٹ پر موجود پول کو بیلنس کر سکے، جو افقی سطح پر دائیں اور بائیں حرکت کر سکتا ہے۔ ہم [OpenAI Gym](https://www.gymlibrary.ml/) کے ماحول کا استعمال کریں گے تاکہ پول کی نقل و حرکت کو سیمولیٹ کیا جا سکے۔\n",
    "\n",
    "> **نوٹ**: آپ اس سبق کے کوڈ کو مقامی طور پر (مثلاً Visual Studio Code سے) چلا سکتے ہیں، اس صورت میں سیمولیشن ایک نئی ونڈو میں کھلے گی۔ اگر آپ کوڈ کو آن لائن چلا رہے ہیں، تو آپ کو کوڈ میں کچھ تبدیلیاں کرنی پڑ سکتی ہیں، جیسا کہ [یہاں](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) بیان کیا گیا ہے۔\n",
    "\n",
    "ہم اس بات کو یقینی بنانے سے شروع کریں گے کہ Gym انسٹال ہے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اب آئیے CartPole ماحول بناتے ہیں اور دیکھتے ہیں کہ اس پر کیسے کام کیا جا سکتا ہے۔ ایک ماحول کی درج ذیل خصوصیات ہوتی ہیں:\n",
    "\n",
    "* **ایکشن اسپیس** وہ ممکنہ اقدامات کا مجموعہ ہے جو ہم ہر قدم پر انجام دے سکتے ہیں۔\n",
    "* **آبزرویشن اسپیس** وہ مشاہدات کا دائرہ ہے جو ہم کر سکتے ہیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "آئیے دیکھتے ہیں کہ سیمولیشن کیسے کام کرتی ہے۔ درج ذیل لوپ سیمولیشن کو چلاتا ہے، جب تک کہ `env.step` اختتامی پرچم `done` واپس نہ کرے۔ ہم `env.action_space.sample()` کا استعمال کرتے ہوئے بے ترتیب طور پر اعمال منتخب کریں گے، جس کا مطلب ہے کہ تجربہ شاید بہت جلد ناکام ہو جائے گا (CartPole ماحول اس وقت ختم ہو جاتا ہے جب CartPole کی رفتار، اس کی پوزیشن یا زاویہ مخصوص حدود سے باہر ہو)۔\n",
    "\n",
    "> سیمولیشن ایک نئی ونڈو میں کھلے گی۔ آپ کوڈ کو کئی بار چلا سکتے ہیں اور دیکھ سکتے ہیں کہ یہ کیسے برتاؤ کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "آپ دیکھ سکتے ہیں کہ مشاہدات میں چار نمبر شامل ہیں۔ یہ ہیں:  \n",
    "- گاڑی کی پوزیشن  \n",
    "- گاڑی کی رفتار  \n",
    "- پول کا زاویہ  \n",
    "- پول کی گردش کی شرح  \n",
    "\n",
    "`rew` وہ انعام ہے جو ہمیں ہر قدم پر ملتا ہے۔ CartPole ماحول میں آپ کو ہر سیمولیشن قدم کے لیے 1 پوائنٹ کا انعام ملتا ہے، اور مقصد یہ ہے کہ کل انعام کو زیادہ سے زیادہ کیا جائے، یعنی CartPole کو گرنے کے بغیر زیادہ سے زیادہ وقت تک توازن برقرار رکھنے کے قابل بنایا جائے۔\n",
    "\n",
    "ری انفورسمنٹ لرننگ کے دوران، ہمارا مقصد ایک **پالیسی** $\\pi$ کو تربیت دینا ہے، جو ہر حالت $s$ کے لیے ہمیں بتائے گی کہ کون سا عمل $a$ لینا ہے، یعنی بنیادی طور پر $a = \\pi(s)$۔\n",
    "\n",
    "اگر آپ ایک احتمالی حل چاہتے ہیں، تو آپ پالیسی کو اس طرح سوچ سکتے ہیں کہ یہ ہر عمل کے لیے امکانات کا ایک سیٹ واپس کرے، یعنی $\\pi(a|s)$ کا مطلب ہوگا کہ ہمیں حالت $s$ پر عمل $a$ لینا چاہیے اس کا امکان۔\n",
    "\n",
    "## پالیسی گریڈینٹ طریقہ\n",
    "\n",
    "سب سے سادہ RL الگورتھم، جسے **پالیسی گریڈینٹ** کہا جاتا ہے، میں ہم ایک نیورل نیٹ ورک کو تربیت دیں گے تاکہ اگلا عمل پیش گوئی کر سکے۔  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ہم نیٹ ورک کو تربیت دینے کے لیے کئی تجربات چلائیں گے، اور ہر تجربے کے بعد اپنے نیٹ ورک کو اپ ڈیٹ کریں گے۔ آئیے ایک فنکشن کی تعریف کرتے ہیں جو تجربہ چلائے گا اور نتائج واپس کرے گا (جسے **ٹریس** کہا جاتا ہے) - تمام حالتیں، اعمال (اور ان کی تجویز کردہ امکانات)، اور انعامات:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "آپ ایک غیر تربیت یافتہ نیٹ ورک کے ساتھ ایک قسط چلا سکتے ہیں اور مشاہدہ کر سکتے ہیں کہ کل انعام (یعنی قسط کی لمبائی) بہت کم ہے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "پالیسی گریڈینٹ الگورتھم کے پیچیدہ پہلوؤں میں سے ایک **ڈسکاؤنٹڈ انعامات** کا استعمال ہے۔ خیال یہ ہے کہ ہم کھیل کے ہر مرحلے پر کل انعامات کے ویکٹر کا حساب لگاتے ہیں، اور اس عمل کے دوران ہم ابتدائی انعامات کو کسی عددی $gamma$ کے ذریعے ڈسکاؤنٹ کرتے ہیں۔ ہم نتیجے میں حاصل ہونے والے ویکٹر کو بھی معمول پر لاتے ہیں، کیونکہ ہم اسے اپنے تربیتی عمل پر اثر ڈالنے کے لیے وزن کے طور پر استعمال کریں گے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اب ہم اصل تربیت شروع کرتے ہیں! ہم 300 اقساط چلائیں گے، اور ہر قسط میں ہم درج ذیل کریں گے:\n",
    "\n",
    "1. تجربہ چلائیں اور ٹریس جمع کریں۔\n",
    "2. ان اعمال کے درمیان فرق (`gradients`) کا حساب لگائیں جو کیے گئے ہیں، اور پیش گوئی شدہ امکانات کے ذریعے۔ جتنا کم فرق ہوگا، اتنا ہی زیادہ یقین ہوگا کہ ہم نے صحیح عمل کیا ہے۔\n",
    "3. ڈسکاؤنٹڈ انعامات کا حساب لگائیں اور `gradients` کو ڈسکاؤنٹڈ انعامات سے ضرب دیں - یہ یقینی بنائے گا کہ زیادہ انعام والے اقدامات کا حتمی نتیجے پر کم انعام والے اقدامات کے مقابلے میں زیادہ اثر ہوگا۔\n",
    "4. ہمارے نیورل نیٹ ورک کے لیے متوقع ہدفی اعمال جزوی طور پر دوڑ کے دوران پیش گوئی شدہ امکانات سے لیے جائیں گے، اور جزوی طور پر حساب شدہ `gradients` سے۔ ہم `alpha` پیرامیٹر استعمال کریں گے تاکہ یہ طے کیا جا سکے کہ `gradients` اور انعامات کو کس حد تک مدنظر رکھا جائے - اسے تقویتی الگورتھم کی *سیکھنے کی شرح* کہا جاتا ہے۔\n",
    "5. آخر میں، ہم اپنے نیٹ ورک کو حالات اور متوقع اعمال پر تربیت دیں گے، اور اس عمل کو دہرائیں گے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اب آئیے قسط کو رینڈرنگ کے ساتھ چلائیں تاکہ نتیجہ دیکھ سکیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "امید ہے کہ آپ دیکھ سکتے ہیں کہ اب پول کافی اچھی طرح توازن برقرار رکھ سکتا ہے!\n",
    "\n",
    "## ایکٹر-کریٹک ماڈل\n",
    "\n",
    "ایکٹر-کریٹک ماڈل پالیسی گریڈینٹس کی مزید ترقی ہے، جس میں ہم ایک نیورل نیٹ ورک بناتے ہیں جو پالیسی اور متوقع انعامات دونوں سیکھتا ہے۔ نیٹ ورک کے دو آؤٹ پٹس ہوں گے (یا آپ اسے دو الگ الگ نیٹ ورکس کے طور پر دیکھ سکتے ہیں):\n",
    "* **ایکٹر** ہمیں اسٹیٹ پروبیبلٹی ڈسٹریبیوشن دے کر وہ عمل تجویز کرے گا جو لینا چاہیے، جیسا کہ پالیسی گریڈینٹ ماڈل میں ہوتا ہے۔\n",
    "* **کریٹک** ان اعمال سے ممکنہ انعامات کا اندازہ لگائے گا۔ یہ دیے گئے اسٹیٹ پر مستقبل میں کل متوقع انعامات واپس کرے گا۔\n",
    "\n",
    "آئیے ایسا ماڈل ڈیفائن کرتے ہیں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ہمیں اپنی `discounted_rewards` اور `run_episode` فنکشنز کو تھوڑا سا تبدیل کرنے کی ضرورت ہوگی:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اب ہم مرکزی تربیتی لوپ چلائیں گے۔ ہم دستی نیٹ ورک تربیتی عمل استعمال کریں گے جس میں مناسب نقصان کے افعال کا حساب لگانا اور نیٹ ورک کے پیرامیٹرز کو اپ ڈیٹ کرنا شامل ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## خلاصہ\n",
    "\n",
    "ہم نے اس ڈیمو میں دو RL الگورتھمز دیکھے: سادہ پالیسی گریڈینٹ اور زیادہ پیچیدہ ایکٹر-کریٹک۔ آپ دیکھ سکتے ہیں کہ یہ الگورتھمز حالت، عمل اور انعام کے تجریدی تصورات کے ساتھ کام کرتے ہیں - اس لیے انہیں بہت مختلف ماحول میں لاگو کیا جا سکتا ہے۔\n",
    "\n",
    "ری انفورسمنٹ لرننگ ہمیں صرف آخری انعام کو دیکھ کر مسئلہ حل کرنے کی بہترین حکمت عملی سیکھنے کی اجازت دیتا ہے۔ یہ حقیقت کہ ہمیں لیبل شدہ ڈیٹاسیٹس کی ضرورت نہیں ہوتی، ہمیں اپنے ماڈلز کو بہتر بنانے کے لیے کئی بار سیمولیشنز دہرانے کی اجازت دیتی ہے۔ تاہم، RL میں اب بھی بہت سے چیلنجز موجود ہیں، جنہیں آپ سیکھ سکتے ہیں اگر آپ اس دلچسپ AI کے شعبے پر زیادہ توجہ دینے کا فیصلہ کریں۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ڈسکلیمر**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T02:53:13+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}