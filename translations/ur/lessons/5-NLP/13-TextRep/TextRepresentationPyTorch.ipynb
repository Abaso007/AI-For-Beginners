{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# متن کی درجہ بندی کا کام\n",
    "\n",
    "جیسا کہ ہم نے ذکر کیا ہے، ہم ایک سادہ متن کی درجہ بندی کے کام پر توجہ مرکوز کریں گے جو **AG_NEWS** ڈیٹاسیٹ پر مبنی ہے۔ اس کا مقصد خبروں کی سرخیوں کو چار زمروں میں تقسیم کرنا ہے: عالمی، کھیل، کاروبار، اور سائنس/ٹیکنالوجی۔\n",
    "\n",
    "## ڈیٹاسیٹ\n",
    "\n",
    "یہ ڈیٹاسیٹ [`torchtext`](https://github.com/pytorch/text) ماڈیول میں شامل ہے، اس لیے ہم اسے آسانی سے حاصل کر سکتے ہیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "یہاں، `train_dataset` اور `test_dataset` میں مجموعے شامل ہیں جو بالترتیب لیبل (کلاس کا نمبر) اور متن کے جوڑے واپس کرتے ہیں، مثال کے طور پر:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تو، آئیے اپنے ڈیٹاسیٹ سے پہلی 10 نئی سرخیاں پرنٹ کریں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "چونکہ ڈیٹاسیٹس اٹریٹرز ہیں، اگر ہم ڈیٹا کو متعدد بار استعمال کرنا چاہتے ہیں تو ہمیں اسے فہرست میں تبدیل کرنا ہوگا:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ٹوکنائزیشن\n",
    "\n",
    "اب ہمیں متن کو **نمبرز** میں تبدیل کرنے کی ضرورت ہے تاکہ انہیں ٹینسرز کے طور پر ظاہر کیا جا سکے۔ اگر ہم لفظی سطح کی نمائندگی چاہتے ہیں، تو ہمیں دو کام کرنے ہوں گے:\n",
    "* **ٹوکنائزر** کا استعمال کرتے ہوئے متن کو **ٹوکنز** میں تقسیم کریں\n",
    "* ان ٹوکنز کا ایک **لغت** بنائیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لفظیات کا استعمال کرتے ہوئے، ہم آسانی سے اپنے ٹوکنائزڈ سٹرنگ کو نمبروں کے ایک سیٹ میں انکوڈ کر سکتے ہیں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## الفاظ کے تھیلے کی متن کی نمائندگی\n",
    "\n",
    "چونکہ الفاظ معنی کی نمائندگی کرتے ہیں، بعض اوقات ہم صرف انفرادی الفاظ کو دیکھ کر متن کا مطلب سمجھ سکتے ہیں، چاہے جملے میں ان کی ترتیب کچھ بھی ہو۔ مثال کے طور پر، خبروں کی درجہ بندی کرتے وقت، الفاظ جیسے *موسم*، *برف* ممکنہ طور پر *موسمی پیش گوئی* کی طرف اشارہ کریں گے، جبکہ الفاظ جیسے *حصص*، *ڈالر* *مالی خبروں* کی طرف شمار ہوں گے۔\n",
    "\n",
    "**الفاظ کے تھیلے** (BoW) ویکٹر کی نمائندگی سب سے زیادہ استعمال ہونے والی روایتی ویکٹر نمائندگی ہے۔ ہر لفظ کو ایک ویکٹر انڈیکس سے جوڑا جاتا ہے، اور ویکٹر عنصر کسی دیے گئے دستاویز میں کسی لفظ کے وقوعات کی تعداد کو ظاہر کرتا ہے۔\n",
    "\n",
    "![تصویر جو دکھاتی ہے کہ الفاظ کے تھیلے کی ویکٹر نمائندگی میموری میں کیسے ظاہر کی جاتی ہے۔](../../../../../translated_images/bag-of-words-example.606fc1738f1d7ba98a9d693e3bcd706c6e83fa7bf8221e6e90d1a206d82f2ea4.ur.png)\n",
    "\n",
    "> **نوٹ**: آپ BoW کو متن میں انفرادی الفاظ کے لیے تمام ایک-ہاٹ-انکوڈڈ ویکٹرز کے مجموعے کے طور پر بھی سوچ سکتے ہیں۔\n",
    "\n",
    "نیچے ایک مثال دی گئی ہے کہ Scikit Learn پائتھون لائبریری کا استعمال کرتے ہوئے الفاظ کے تھیلے کی نمائندگی کیسے بنائی جائے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اے جی_نیوز ڈیٹاسیٹ کی ویکٹر نمائندگی سے بیگ آف ورڈز ویکٹر کا حساب لگانے کے لیے، ہم درج ذیل فنکشن استعمال کر سکتے ہیں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **نوٹ:** یہاں ہم عالمی `vocab_size` متغیر استعمال کر رہے ہیں تاکہ لغت کے ڈیفالٹ سائز کو متعین کیا جا سکے۔ چونکہ اکثر لغت کا سائز کافی بڑا ہوتا ہے، ہم لغت کے سائز کو سب سے زیادہ عام الفاظ تک محدود کر سکتے ہیں۔ `vocab_size` کی قدر کو کم کرنے کی کوشش کریں اور نیچے دیے گئے کوڈ کو چلائیں، اور دیکھیں کہ یہ درستگی پر کیسے اثر ڈالتا ہے۔ آپ کو کچھ درستگی میں کمی کی توقع کرنی چاہیے، لیکن کارکردگی میں اضافے کے بدلے یہ کمی زیادہ نہیں ہوگی۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## بو ڈبلیو کلاسیفائر کی تربیت\n",
    "\n",
    "اب جب کہ ہم نے یہ سیکھ لیا ہے کہ اپنے متن کا بیگ آف ورڈز نمائندگی کیسے بنائیں، آئیے اس پر ایک کلاسیفائر تربیت کریں۔ سب سے پہلے، ہمیں اپنے ڈیٹاسیٹ کو تربیت کے لیے اس طرح تبدیل کرنا ہوگا کہ تمام پوزیشنل ویکٹر نمائندگیاں بیگ آف ورڈز نمائندگی میں تبدیل ہو جائیں۔ یہ اس طرح ممکن ہے کہ `bowify` فنکشن کو معیاری ٹارچ `DataLoader` میں `collate_fn` پیرامیٹر کے طور پر پاس کیا جائے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "آئیے اب ایک سادہ کلاسفائر نیورل نیٹ ورک کی تعریف کرتے ہیں جو ایک لکیری پرت پر مشتمل ہے۔ ان پٹ ویکٹر کا سائز `vocab_size` کے برابر ہے، اور آؤٹ پٹ سائز کلاسز کی تعداد (4) کے مطابق ہے۔ چونکہ ہم درجہ بندی کا کام حل کر رہے ہیں، آخری ایکٹیویشن فنکشن `LogSoftmax()` ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اب ہم معیاری PyTorch تربیتی لوپ کی وضاحت کریں گے۔ چونکہ ہمارا ڈیٹا سیٹ کافی بڑا ہے، تدریسی مقصد کے لیے ہم صرف ایک ایپوک کے لیے تربیت کریں گے، اور کبھی کبھار ایک ایپوک سے بھی کم ( `epoch_size` پیرامیٹر کی وضاحت کرنے سے ہمیں تربیت کو محدود کرنے کی اجازت ملتی ہے)۔ ہم تربیت کے دوران جمع شدہ تربیتی درستگی کی بھی رپورٹ کریں گے؛ رپورٹنگ کی تعدد `report_freq` پیرامیٹر کا استعمال کرتے ہوئے وضاحت کی جاتی ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## بائی گرامز، ٹرائی گرامز اور این-گرامز\n",
    "\n",
    "الفاظ کے تھیلے کے طریقے کی ایک محدودیت یہ ہے کہ کچھ الفاظ کئی الفاظ پر مشتمل اظہار کا حصہ ہوتے ہیں، مثال کے طور پر، لفظ 'ہاٹ ڈاگ' کا مطلب بالکل مختلف ہوتا ہے 'ہاٹ' اور 'ڈاگ' کے دوسرے سیاق و سباق میں۔ اگر ہم ہمیشہ الفاظ 'ہاٹ' اور 'ڈاگ' کو ایک ہی ویکٹرز کے ذریعے ظاہر کریں، تو یہ ہمارے ماڈل کو الجھا سکتا ہے۔\n",
    "\n",
    "اس مسئلے کو حل کرنے کے لیے، **این-گرام کی نمائندگی** اکثر دستاویز کی درجہ بندی کے طریقوں میں استعمال کی جاتی ہے، جہاں ہر لفظ، دو الفاظ یا تین الفاظ کی فریکوئنسی کو کلاسیفائرز کی تربیت کے لیے ایک مفید خصوصیت سمجھا جاتا ہے۔ بائی گرام کی نمائندگی میں، مثال کے طور پر، ہم اصل الفاظ کے علاوہ تمام لفظی جوڑوں کو بھی ذخیرہ الفاظ میں شامل کریں گے۔\n",
    "\n",
    "نیچے ایک مثال دی گئی ہے کہ کس طرح Scikit Learn کا استعمال کرتے ہوئے بائی گرام الفاظ کے تھیلے کی نمائندگی تیار کی جا سکتی ہے:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-گرام طریقہ کار کا سب سے بڑا نقصان یہ ہے کہ الفاظ کی تعداد بہت تیزی سے بڑھنے لگتی ہے۔ عملی طور پر، ہمیں N-گرام کی نمائندگی کو کچھ جہتی کمی کی تکنیکوں کے ساتھ جوڑنے کی ضرورت ہوتی ہے، جیسے *ایمبیڈنگز*، جن پر ہم اگلے یونٹ میں بات کریں گے۔\n",
    "\n",
    "**AG News** ڈیٹا سیٹ میں N-گرام کی نمائندگی استعمال کرنے کے لیے، ہمیں خاص ngram الفاظ کی فہرست بنانی ہوگی:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ہم اوپر دیے گئے کوڈ کو استعمال کرتے ہوئے classifier کو ٹرین کر سکتے ہیں، لیکن یہ طریقہ میموری کے لحاظ سے بہت غیر مؤثر ہوگا۔ اگلے یونٹ میں، ہم bigram classifier کو embeddings کے ذریعے ٹرین کریں گے۔\n",
    "\n",
    "> **نوٹ:** آپ صرف وہی ngrams چھوڑ سکتے ہیں جو متن میں مخصوص تعداد سے زیادہ بار ظاہر ہوں۔ اس سے یہ یقینی ہوگا کہ کم ظاہر ہونے والے bigrams کو نظرانداز کیا جائے گا، اور dimensionality میں نمایاں کمی آئے گی۔ اس کے لیے، `min_freq` پیرامیٹر کو ایک زیادہ قدر پر سیٹ کریں، اور vocabulary کی لمبائی میں تبدیلی کا مشاہدہ کریں۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## اصطلاح فریکوئنسی انورس ڈاکیومنٹ فریکوئنسی TF-IDF\n",
    "\n",
    "BoW کی نمائندگی میں، الفاظ کی موجودگی کو یکساں وزن دیا جاتا ہے، چاہے وہ لفظ خود کتنا ہی اہم کیوں نہ ہو۔ تاہم، یہ واضح ہے کہ عام الفاظ جیسے *a*، *in* وغیرہ، خاص اصطلاحات کے مقابلے میں درجہ بندی کے لیے کم اہم ہیں۔ حقیقت میں، زیادہ تر NLP کاموں میں کچھ الفاظ دوسروں کے مقابلے میں زیادہ اہمیت رکھتے ہیں۔\n",
    "\n",
    "**TF-IDF** کا مطلب ہے **اصطلاح فریکوئنسی–انورس ڈاکیومنٹ فریکوئنسی**۔ یہ الفاظ کے بیگ کی ایک قسم ہے، جہاں کسی دستاویز میں کسی لفظ کی موجودگی کو ظاہر کرنے کے لیے 0/1 کی بائنری ویلیو کے بجائے ایک فلوٹنگ پوائنٹ ویلیو استعمال کی جاتی ہے، جو کارپس میں لفظ کی موجودگی کی فریکوئنسی سے متعلق ہوتی ہے۔\n",
    "\n",
    "زیادہ رسمی طور پر، کسی لفظ $i$ کا وزن $w_{ij}$ کسی دستاویز $j$ میں درج ذیل طور پر بیان کیا جاتا ہے:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "جہاں\n",
    "* $tf_{ij}$ کسی دستاویز $j$ میں لفظ $i$ کی موجودگی کی تعداد ہے، یعنی وہ BoW ویلیو جو ہم پہلے دیکھ چکے ہیں\n",
    "* $N$ مجموعی دستاویزات کی تعداد ہے\n",
    "* $df_i$ وہ دستاویزات کی تعداد ہے جن میں لفظ $i$ پورے مجموعے میں موجود ہے\n",
    "\n",
    "TF-IDF ویلیو $w_{ij}$ کسی دستاویز میں کسی لفظ کے ظاہر ہونے کی تعداد کے تناسب سے بڑھتی ہے اور کارپس میں موجود دستاویزات کی تعداد کے ذریعے ایڈجسٹ کی جاتی ہے جن میں وہ لفظ موجود ہے۔ یہ اس حقیقت کو ایڈجسٹ کرنے میں مدد کرتا ہے کہ کچھ الفاظ دوسروں کے مقابلے میں زیادہ بار ظاہر ہوتے ہیں۔ مثال کے طور پر، اگر کوئی لفظ *ہر* دستاویز میں موجود ہو، تو $df_i=N$ ہوگا، اور $w_{ij}=0$ ہوگا، اور وہ اصطلاحات مکمل طور پر نظرانداز کر دی جائیں گی۔\n",
    "\n",
    "آپ Scikit Learn کا استعمال کرتے ہوئے آسانی سے متن کی TF-IDF ویکٹرائزیشن بنا سکتے ہیں:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## نتیجہ\n",
    "\n",
    "اگرچہ TF-IDF نمائندگی مختلف الفاظ کو فریکوئنسی وزن فراہم کرتی ہے، یہ معنی یا ترتیب کو ظاہر کرنے سے قاصر ہے۔ جیسا کہ مشہور لسانیات دان جے آر فرث نے 1935 میں کہا تھا، \"کسی لفظ کا مکمل مطلب ہمیشہ سیاق و سباق پر مبنی ہوتا ہے، اور سیاق و سباق کے بغیر معنی کا کوئی مطالعہ سنجیدگی سے نہیں لیا جا سکتا۔\"۔ ہم اس کورس میں آگے چل کر سیکھیں گے کہ زبان ماڈلنگ کا استعمال کرتے ہوئے متن سے سیاق و سباق کی معلومات کو کیسے حاصل کیا جا سکتا ہے۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ڈسکلیمر**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا عدم درستگی ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-28T04:36:35+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}