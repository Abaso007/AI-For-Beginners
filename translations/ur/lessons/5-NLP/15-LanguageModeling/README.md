<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-26T08:29:29+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "ur"
}
-->
# زبان ماڈلنگ

سمینٹک ایمبیڈنگز، جیسے Word2Vec اور GloVe، دراصل **زبان ماڈلنگ** کی طرف پہلا قدم ہیں - ایسے ماڈلز بنانا جو کسی طرح زبان کی *فطرت کو سمجھیں* (یا *پیش کریں*)۔

## [لیکچر سے پہلے کا کوئز](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

زبان ماڈلنگ کے پیچھے بنیادی خیال یہ ہے کہ انہیں بغیر لیبل والے ڈیٹا سیٹس پر غیر نگرانی شدہ طریقے سے تربیت دی جائے۔ یہ اس لیے اہم ہے کیونکہ ہمارے پاس بغیر لیبل والے متن کی بڑی مقدار دستیاب ہے، جبکہ لیبل والے متن کی مقدار ہمیشہ محدود ہوگی کیونکہ لیبلنگ پر خرچ ہونے والی محنت محدود ہوتی ہے۔ اکثر اوقات، ہم ایسے زبان ماڈلز بنا سکتے ہیں جو **غائب الفاظ کی پیش گوئی** کر سکیں، کیونکہ متن میں کسی بے ترتیب لفظ کو ماسک کرنا اور اسے تربیتی نمونے کے طور پر استعمال کرنا آسان ہے۔

## ایمبیڈنگز کی تربیت

پچھلی مثالوں میں، ہم نے پہلے سے تربیت یافتہ سمینٹک ایمبیڈنگز استعمال کیں، لیکن یہ دیکھنا دلچسپ ہے کہ ان ایمبیڈنگز کو کیسے تربیت دی جا سکتی ہے۔ کئی ممکنہ خیالات ہیں جو استعمال کیے جا سکتے ہیں:

* **N-Gram** زبان ماڈلنگ، جہاں ہم N پچھلے ٹوکنز کو دیکھ کر ایک ٹوکن کی پیش گوئی کرتے ہیں (N-gram)
* **Continuous Bag-of-Words** (CBoW)، جہاں ہم ٹوکن سیکوئنس $W_{-N}$, ..., $W_N$ میں درمیانی ٹوکن $W_0$ کی پیش گوئی کرتے ہیں۔
* **Skip-gram**، جہاں ہم درمیانی ٹوکن $W_0$ سے پڑوسی ٹوکنز {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} کی پیش گوئی کرتے ہیں۔

![الفاظ کو ویکٹرز میں تبدیل کرنے کے لیے الگورتھمز پر مبنی تصویر](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ur.png)

> تصویر [اس مقالے](https://arxiv.org/pdf/1301.3781.pdf) سے لی گئی ہے

## ✍️ مثال نوٹ بکس: CBoW ماڈل کی تربیت

اپنی تعلیم کو درج ذیل نوٹ بکس میں جاری رکھیں:

* [TensorFlow کے ساتھ CBoW Word2Vec کی تربیت](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [PyTorch کے ساتھ CBoW Word2Vec کی تربیت](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## نتیجہ

پچھلے سبق میں ہم نے دیکھا کہ الفاظ کی ایمبیڈنگز جادو کی طرح کام کرتی ہیں! اب ہم جانتے ہیں کہ الفاظ کی ایمبیڈنگز کی تربیت کوئی بہت پیچیدہ کام نہیں ہے، اور اگر ضرورت ہو تو ہم اپنے مخصوص شعبے کے متن کے لیے اپنی ایمبیڈنگز تربیت دے سکتے ہیں۔

## [لیکچر کے بعد کا کوئز](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## جائزہ اور خود مطالعہ

* [زبان ماڈلنگ پر PyTorch کا آفیشل ٹیوٹوریل](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)۔
* [Word2Vec ماڈل کی تربیت پر TensorFlow کا آفیشل ٹیوٹوریل](https://www.TensorFlow.org/tutorials/text/word2vec)۔
* **gensim** فریم ورک کا استعمال کرتے ہوئے چند لائنز کوڈ میں سب سے زیادہ استعمال ہونے والی ایمبیڈنگز کی تربیت [اس دستاویز میں بیان کی گئی ہے](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)۔

## 🚀 [اسائنمنٹ: Skip-Gram ماڈل کی تربیت کریں](lab/README.md)

لیب میں، ہم آپ کو اس سبق کے کوڈ میں ترمیم کرنے کا چیلنج دیتے ہیں تاکہ CBoW کی بجائے Skip-Gram ماڈل تربیت دی جا سکے۔ [تفصیلات پڑھیں](lab/README.md)

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔