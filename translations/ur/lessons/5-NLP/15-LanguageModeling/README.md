<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T06:48:22+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "ur"
}
-->
# زبان ماڈلنگ

سمینٹک ایمبیڈنگز، جیسے Word2Vec اور GloVe، دراصل **زبان ماڈلنگ** کی طرف پہلا قدم ہیں - ایسے ماڈلز بنانا جو کسی طرح زبان کی *فطرت کو سمجھتے* (یا *نمائندگی کرتے*) ہیں۔

## [لیکچر سے پہلے کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/29)

زبان ماڈلنگ کے پیچھے بنیادی خیال یہ ہے کہ انہیں بغیر لیبل والے ڈیٹا سیٹس پر غیر نگرانی شدہ طریقے سے تربیت دی جائے۔ یہ اس لیے اہم ہے کیونکہ ہمارے پاس بغیر لیبل والے متن کی بڑی مقدار دستیاب ہے، جبکہ لیبل والے متن کی مقدار ہمیشہ محدود ہوگی کیونکہ لیبلنگ پر خرچ ہونے والی محنت محدود ہوتی ہے۔ اکثر اوقات، ہم ایسے زبان ماڈلز بنا سکتے ہیں جو **غائب الفاظ کی پیش گوئی** کر سکیں، کیونکہ متن میں کسی بھی بے ترتیب لفظ کو ماسک کرنا اور اسے تربیتی نمونے کے طور پر استعمال کرنا آسان ہے۔

## ایمبیڈنگز کی تربیت

پچھلی مثالوں میں، ہم نے پہلے سے تربیت یافتہ سمینٹک ایمبیڈنگز استعمال کیں، لیکن یہ دیکھنا دلچسپ ہے کہ یہ ایمبیڈنگز کیسے تربیت دی جا سکتی ہیں۔ کئی ممکنہ خیالات ہیں جو استعمال کیے جا سکتے ہیں:

* **N-Gram** زبان ماڈلنگ، جہاں ہم N پچھلے ٹوکنز کو دیکھ کر ایک ٹوکن کی پیش گوئی کرتے ہیں (N-gram)۔
* **Continuous Bag-of-Words** (CBoW)، جہاں ہم ٹوکن سیکوئنس $W_{-N}$, ..., $W_N$ میں درمیانی ٹوکن $W_0$ کی پیش گوئی کرتے ہیں۔
* **Skip-gram**، جہاں ہم درمیانی ٹوکن $W_0$ سے پڑوسی ٹوکنز {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} کی پیش گوئی کرتے ہیں۔

![الفاظ کو ویکٹرز میں تبدیل کرنے کے لیے پیپر سے تصویر](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ur.png)

> تصویر [اس پیپر](https://arxiv.org/pdf/1301.3781.pdf) سے لی گئی ہے۔

## ✍️ مثال نوٹ بکس: CBoW ماڈل کی تربیت

اپنی تعلیم کو درج ذیل نوٹ بکس میں جاری رکھیں:

* [TensorFlow کے ساتھ CBoW Word2Vec کی تربیت](CBoW-TF.ipynb)
* [PyTorch کے ساتھ CBoW Word2Vec کی تربیت](CBoW-PyTorch.ipynb)

## نتیجہ

پچھلے سبق میں ہم نے دیکھا کہ الفاظ کی ایمبیڈنگز جادو کی طرح کام کرتی ہیں! اب ہم جانتے ہیں کہ الفاظ کی ایمبیڈنگز کی تربیت کوئی بہت پیچیدہ کام نہیں ہے، اور اگر ضرورت ہو تو ہم اپنے مخصوص ڈومین کے متن کے لیے اپنی ایمبیڈنگز تربیت دے سکتے ہیں۔

## [لیکچر کے بعد کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## جائزہ اور خود مطالعہ

* [PyTorch کی آفیشل زبان ماڈلنگ ٹیوٹوریل](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)۔
* [TensorFlow کی آفیشل Word2Vec ماڈل تربیت کی ٹیوٹوریل](https://www.TensorFlow.org/tutorials/text/word2vec)۔
* **gensim** فریم ورک کا استعمال کرتے ہوئے چند لائنز کوڈ میں سب سے زیادہ استعمال ہونے والی ایمبیڈنگز کی تربیت [اس دستاویز میں بیان کی گئی ہے](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)۔

## 🚀 [اسائنمنٹ: Skip-Gram ماڈل کی تربیت کریں](lab/README.md)

لیب میں، ہم آپ کو چیلنج دیتے ہیں کہ اس سبق کے کوڈ میں ترمیم کریں تاکہ CBoW کی بجائے Skip-Gram ماڈل تربیت دی جا سکے۔ [تفصیلات پڑھیں](lab/README.md)۔

---

