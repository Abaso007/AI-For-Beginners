<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-26T08:14:11+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "ur"
}
-->
# ایمبیڈنگز

## [لیکچر سے پہلے کا کوئز](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

جب ہم BoW یا TF/IDF پر مبنی کلاسیفائرز کو ٹرین کر رہے تھے، تو ہم `vocab_size` کی لمبائی والے ہائی ڈائمینشنل بیگ آف ورڈز ویکٹرز پر کام کر رہے تھے، اور ہم کم ڈائمینشنل پوزیشنل ریپریزنٹیشن ویکٹرز کو واضح طور پر اسپارس ون-ہاٹ ریپریزنٹیشن میں تبدیل کر رہے تھے۔ تاہم، یہ ون-ہاٹ ریپریزنٹیشن میموری کے لحاظ سے مؤثر نہیں ہے۔ اس کے علاوہ، ہر لفظ کو دوسرے الفاظ سے آزاد سمجھا جاتا ہے، یعنی ون-ہاٹ انکوڈڈ ویکٹرز الفاظ کے درمیان کسی بھی معنوی مشابہت کو ظاہر نہیں کرتے۔

**ایمبیڈنگ** کا خیال یہ ہے کہ الفاظ کو کم ڈائمینشنل ڈینس ویکٹرز کے ذریعے ظاہر کیا جائے، جو کسی نہ کسی طرح ایک لفظ کے معنوی مطلب کو ظاہر کرتے ہیں۔ ہم بعد میں یہ سیکھیں گے کہ بامعنی ورڈ ایمبیڈنگز کیسے بنائی جائیں، لیکن فی الحال ایمبیڈنگز کو ایک طریقہ سمجھیں جو لفظ کے ویکٹر کی ڈائمینشن کو کم کرتا ہے۔

ایمبیڈنگ لیئر ایک لفظ کو ان پٹ کے طور پر لے گی، اور ایک مخصوص `embedding_size` کے آؤٹ پٹ ویکٹر کو پیدا کرے گی۔ ایک لحاظ سے، یہ `Linear` لیئر سے بہت مشابہ ہے، لیکن ون-ہاٹ انکوڈڈ ویکٹر لینے کے بجائے، یہ ایک لفظ کے نمبر کو ان پٹ کے طور پر لے سکے گی، جس سے بڑے ون-ہاٹ انکوڈڈ ویکٹرز بنانے سے بچا جا سکے گا۔

ایمبیڈنگ لیئر کو ہمارے کلاسیفائر نیٹ ورک کی پہلی لیئر کے طور پر استعمال کرکے، ہم بیگ آف ورڈز ماڈل سے **ایمبیڈنگ بیگ** ماڈل میں تبدیل ہو سکتے ہیں، جہاں ہم پہلے اپنے متن کے ہر لفظ کو متعلقہ ایمبیڈنگ میں تبدیل کرتے ہیں، اور پھر ان تمام ایمبیڈنگز پر کوئی مجموعی فنکشن جیسے `sum`، `average` یا `max` کا حساب لگاتے ہیں۔

![پانچ سیکوئنس الفاظ کے لیے ایک ایمبیڈنگ کلاسیفائر کی تصویر۔](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ur.png)

> تصویر مصنف کی جانب سے

## ✍️ مشقیں: ایمبیڈنگز

مندرجہ ذیل نوٹ بکس میں اپنی تعلیم جاری رکھیں:
* [PyTorch کے ساتھ ایمبیڈنگز](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlow کے ساتھ ایمبیڈنگز](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## معنوی ایمبیڈنگز: Word2Vec

اگرچہ ایمبیڈنگ لیئر نے الفاظ کو ویکٹر ریپریزنٹیشن میں میپ کرنا سیکھا، لیکن یہ ریپریزنٹیشن ضروری نہیں کہ زیادہ معنوی ہو۔ یہ بہتر ہوگا کہ ہم ایک ایسا ویکٹر ریپریزنٹیشن سیکھیں جس میں مشابہ الفاظ یا مترادفات ایسے ویکٹرز کے مطابق ہوں جو کسی ویکٹر فاصلے (مثلاً یُوکلِیڈین فاصلے) کے لحاظ سے ایک دوسرے کے قریب ہوں۔

اس کے لیے، ہمیں اپنے ایمبیڈنگ ماڈل کو ایک بڑے ٹیکسٹ کلیکشن پر ایک خاص طریقے سے پہلے سے ٹرین کرنے کی ضرورت ہے۔ معنوی ایمبیڈنگز کو ٹرین کرنے کا ایک طریقہ [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) کہلاتا ہے۔ یہ دو اہم آرکیٹیکچرز پر مبنی ہے جو الفاظ کی تقسیم شدہ ریپریزنٹیشن پیدا کرنے کے لیے استعمال ہوتے ہیں:

- **کنٹینیوس بیگ آف ورڈز** (CBoW) — اس آرکیٹیکچر میں، ہم ماڈل کو ارد گرد کے سیاق و سباق سے ایک لفظ کی پیش گوئی کرنے کے لیے ٹرین کرتے ہیں۔ دیے گئے ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$، ماڈل کا مقصد $(W_{-2},W_{-1},W_1,W_2)$ سے $W_0$ کی پیش گوئی کرنا ہے۔
- **کنٹینیوس اسکیپ-گرام** CBoW کے برعکس ہے۔ ماڈل سیاق و سباق کے ارد گرد کے الفاظ کی ونڈو کا استعمال موجودہ لفظ کی پیش گوئی کے لیے کرتا ہے۔

CBoW تیز ہے، جبکہ اسکیپ-گرام سست ہے، لیکن کم استعمال ہونے والے الفاظ کی بہتر نمائندگی کرتا ہے۔

![CBoW اور اسکیپ-گرام الگورتھمز کی تصویر جو الفاظ کو ویکٹرز میں تبدیل کرتے ہیں۔](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ur.png)

> تصویر [اس مقالے](https://arxiv.org/pdf/1301.3781.pdf) سے

Word2Vec کے پہلے سے ٹرین کیے گئے ایمبیڈنگز (اور دیگر مشابہ ماڈلز جیسے GloVe) کو نیورل نیٹ ورکس میں ایمبیڈنگ لیئر کی جگہ بھی استعمال کیا جا سکتا ہے۔ تاہم، ہمیں وکیبلری کے مسائل سے نمٹنا ہوگا، کیونکہ Word2Vec/GloVe کو پہلے سے ٹرین کرنے کے لیے استعمال ہونے والی وکیبلری ممکنہ طور پر ہمارے ٹیکسٹ کارپس کی وکیبلری سے مختلف ہوگی۔ اس مسئلے کو حل کرنے کے لیے اوپر دیے گئے نوٹ بکس کو دیکھیں۔

## سیاقی ایمبیڈنگز

روایتی پہلے سے ٹرین کیے گئے ایمبیڈنگ ریپریزنٹیشنز جیسے Word2Vec کی ایک اہم حد الفاظ کے معنی کی وضاحت کا مسئلہ ہے۔ اگرچہ پہلے سے ٹرین کیے گئے ایمبیڈنگز سیاق و سباق میں الفاظ کے کچھ معنی کو پکڑ سکتے ہیں، لیکن ایک لفظ کے ہر ممکنہ معنی کو ایک ہی ایمبیڈنگ میں انکوڈ کیا جاتا ہے۔ یہ ڈاؤن اسٹریم ماڈلز میں مسائل پیدا کر سکتا ہے، کیونکہ بہت سے الفاظ جیسے 'play' کے مختلف معنی ہوتے ہیں جو ان کے استعمال کے سیاق و سباق پر منحصر ہوتے ہیں۔

مثال کے طور پر، لفظ 'play' ان دو مختلف جملوں میں کافی مختلف معنی رکھتا ہے:

- میں تھیٹر میں ایک **play** دیکھنے گیا۔
- جان اپنے دوستوں کے ساتھ **play** کرنا چاہتا ہے۔

اوپر دیے گئے پہلے سے ٹرین کیے گئے ایمبیڈنگز لفظ 'play' کے ان دونوں معنوں کو ایک ہی ایمبیڈنگ میں ظاہر کرتے ہیں۔ اس حد پر قابو پانے کے لیے، ہمیں **لینگویج ماڈل** پر مبنی ایمبیڈنگز بنانی ہوں گی، جو ایک بڑے ٹیکسٹ کارپس پر ٹرین کیا گیا ہو، اور *جانتا ہو* کہ الفاظ کو مختلف سیاق و سباق میں کیسے ترتیب دیا جا سکتا ہے۔ سیاقی ایمبیڈنگز پر بات کرنا اس ٹیوٹوریل کے دائرہ کار سے باہر ہے، لیکن ہم ان پر بعد میں کورس میں بات کریں گے جب لینگویج ماڈلز پر بات ہوگی۔

## نتیجہ

اس سبق میں، آپ نے سیکھا کہ TensorFlow اور PyTorch میں ایمبیڈنگ لیئرز کو کیسے بنایا اور استعمال کیا جائے تاکہ الفاظ کے معنوی مطلب کو بہتر طور پر ظاہر کیا جا سکے۔

## 🚀 چیلنج

Word2Vec کو کچھ دلچسپ ایپلیکیشنز کے لیے استعمال کیا گیا ہے، بشمول گانے کے بول اور شاعری بنانا۔ [اس مضمون](https://www.politetype.com/blog/word2vec-color-poems) کو دیکھیں جو یہ بتاتا ہے کہ مصنف نے Word2Vec کو شاعری بنانے کے لیے کیسے استعمال کیا۔ [ڈین شِفمین کی یہ ویڈیو](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) بھی دیکھیں تاکہ اس تکنیک کی ایک مختلف وضاحت دریافت کی جا سکے۔ پھر ان تکنیکوں کو اپنے ٹیکسٹ کارپس پر لاگو کرنے کی کوشش کریں، شاید Kaggle سے حاصل کردہ۔

## [لیکچر کے بعد کا کوئز](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## جائزہ اور خود مطالعہ

Word2Vec پر اس مقالے کو پڑھیں: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [اسائنمنٹ: نوٹ بکس](assignment.md)

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے پوری کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا خامیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔