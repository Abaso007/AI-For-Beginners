<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-26T08:18:55+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "ur"
}
-->
# جنریٹیو نیٹ ورکس

## [لیکچر سے پہلے کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/33)

ریکرنٹ نیورل نیٹ ورکس (RNNs) اور ان کے گیٹڈ سیل ویریئنٹس جیسے کہ لانگ شارٹ ٹرم میموری سیلز (LSTMs) اور گیٹڈ ریکرنٹ یونٹس (GRUs) زبان کی ماڈلنگ کے لیے ایک طریقہ فراہم کرتے ہیں، کیونکہ یہ الفاظ کی ترتیب سیکھ سکتے ہیں اور کسی سیکوئنس میں اگلے لفظ کی پیش گوئی کر سکتے ہیں۔ اس سے ہمیں RNNs کو **جنریٹیو ٹاسکس** کے لیے استعمال کرنے کی اجازت ملتی ہے، جیسے کہ عام ٹیکسٹ جنریشن، مشین ٹرانسلیشن، اور یہاں تک کہ امیج کیپشننگ۔

> ✅ ان تمام مواقع کے بارے میں سوچیں جب آپ نے جنریٹیو ٹاسکس جیسے کہ ٹیکسٹ کمپلیشن سے فائدہ اٹھایا ہو۔ اپنی پسندیدہ ایپلیکیشنز پر تحقیق کریں تاکہ معلوم ہو سکے کہ آیا انہوں نے RNNs کا استعمال کیا۔

پچھلے یونٹ میں زیر بحث RNN آرکیٹیکچر میں، ہر RNN یونٹ نے اگلی چھپی ہوئی حالت کو آؤٹ پٹ کے طور پر پیدا کیا۔ تاہم، ہم ہر ریکرنٹ یونٹ میں ایک اور آؤٹ پٹ بھی شامل کر سکتے ہیں، جو ہمیں ایک **سیکوئنس** آؤٹ پٹ کرنے کی اجازت دے گا (جو اصل سیکوئنس کے برابر لمبائی کا ہوگا)۔ مزید برآں، ہم ایسے RNN یونٹس استعمال کر سکتے ہیں جو ہر مرحلے پر ان پٹ قبول نہیں کرتے، بلکہ صرف ایک ابتدائی اسٹیٹ ویکٹر لیتے ہیں، اور پھر آؤٹ پٹس کا ایک سیکوئنس پیدا کرتے ہیں۔

یہ مختلف نیورل آرکیٹیکچرز کی اجازت دیتا ہے، جیسا کہ نیچے دی گئی تصویر میں دکھایا گیا ہے:

![ریکرنٹ نیورل نیٹ ورکس کے عام پیٹرنز کی تصویر۔](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.ur.jpg)

> تصویر بلاگ پوسٹ [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) از [Andrej Karpaty](http://karpathy.github.io/) سے لی گئی ہے۔

* **ون ٹو ون** ایک روایتی نیورل نیٹ ورک ہے جس میں ایک ان پٹ اور ایک آؤٹ پٹ ہوتا ہے۔
* **ون ٹو مینی** ایک جنریٹیو آرکیٹیکچر ہے جو ایک ان پٹ ویلیو قبول کرتا ہے اور آؤٹ پٹ ویلیوز کا ایک سیکوئنس پیدا کرتا ہے۔ مثال کے طور پر، اگر ہم ایک **امیج کیپشننگ** نیٹ ورک کو ٹرین کرنا چاہتے ہیں جو کسی تصویر کی ٹیکسچوئل وضاحت پیدا کرے، تو ہم تصویر کو ان پٹ کے طور پر لے سکتے ہیں، اسے CNN کے ذریعے چھپی ہوئی حالت حاصل کرنے کے لیے پاس کر سکتے ہیں، اور پھر ایک ریکرنٹ چین کیپشن کو لفظ بہ لفظ جنریٹ کرے۔
* **مینی ٹو ون** ان RNN آرکیٹیکچرز سے مطابقت رکھتا ہے جنہیں ہم نے پچھلے یونٹ میں بیان کیا، جیسے کہ ٹیکسٹ کلاسیفکیشن۔
* **مینی ٹو مینی** یا **سیکوئنس ٹو سیکوئنس** ان ٹاسکس سے مطابقت رکھتا ہے جیسے کہ **مشین ٹرانسلیشن**، جہاں پہلا RNN ان پٹ سیکوئنس سے تمام معلومات کو چھپی ہوئی حالت میں جمع کرتا ہے، اور دوسرا RNN چین اس حالت کو آؤٹ پٹ سیکوئنس میں ان رول کرتا ہے۔

اس یونٹ میں، ہم سادہ جنریٹیو ماڈلز پر توجہ مرکوز کریں گے جو ہمیں ٹیکسٹ جنریٹ کرنے میں مدد دیتے ہیں۔ سادگی کے لیے، ہم کریکٹر لیول ٹوکنائزیشن استعمال کریں گے۔

ہم اس RNN کو ٹیکسٹ کو قدم بہ قدم جنریٹ کرنے کے لیے ٹرین کریں گے۔ ہر قدم پر، ہم `nchars` کی لمبائی کے کریکٹرز کا ایک سیکوئنس لیں گے، اور نیٹ ورک سے کہیں گے کہ وہ ہر ان پٹ کریکٹر کے لیے اگلا آؤٹ پٹ کریکٹر جنریٹ کرے:

![تصویر جس میں 'HELLO' لفظ کے RNN جنریشن کی مثال دکھائی گئی ہے۔](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.ur.png)

جب ٹیکسٹ جنریٹ کیا جا رہا ہو (انفرنس کے دوران)، ہم کسی **پرومپٹ** سے شروع کرتے ہیں، جسے RNN سیلز کے ذریعے اس کی انٹرمیڈیٹ اسٹیٹ جنریٹ کرنے کے لیے پاس کیا جاتا ہے، اور پھر اس اسٹیٹ سے جنریشن شروع ہوتی ہے۔ ہم ایک وقت میں ایک کریکٹر جنریٹ کرتے ہیں، اور اسٹیٹ اور جنریٹ کیے گئے کریکٹر کو اگلے کریکٹر کو جنریٹ کرنے کے لیے دوسرے RNN سیل میں پاس کرتے ہیں، جب تک کہ ہم کافی کریکٹرز جنریٹ نہ کر لیں۔

<img src="images/rnn-generate-inf.png" width="60%"/>

> تصویر مصنف کی جانب سے

## ✍️ مشقیں: جنریٹیو نیٹ ورکس

اپنی تعلیم کو درج ذیل نوٹ بکس میں جاری رکھیں:

* [PyTorch کے ساتھ جنریٹیو نیٹ ورکس](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [TensorFlow کے ساتھ جنریٹیو نیٹ ورکس](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## نرم ٹیکسٹ جنریشن اور ٹمپریچر

ہر RNN سیل کا آؤٹ پٹ کریکٹرز کی ایک پروبیبلٹی ڈسٹریبیوشن ہوتا ہے۔ اگر ہم ہمیشہ سب سے زیادہ پروبیبلٹی والے کریکٹر کو اگلے کریکٹر کے طور پر منتخب کریں، تو جنریٹ کیا گیا ٹیکسٹ اکثر ایک ہی کریکٹر سیکوئنسز کے درمیان "چکر" میں پھنس سکتا ہے، جیسا کہ اس مثال میں:

```
today of the second the company and a second the company ...
```

تاہم، اگر ہم اگلے کریکٹر کے لیے پروبیبلٹی ڈسٹریبیوشن کو دیکھیں، تو یہ ہو سکتا ہے کہ چند سب سے زیادہ پروبیبلٹیز کے درمیان فرق زیادہ نہ ہو، مثلاً ایک کریکٹر کی پروبیبلٹی 0.2 ہو، اور دوسرے کی 0.19۔ مثال کے طور پر، جب '*play*' سیکوئنس میں اگلے کریکٹر کی تلاش ہو، تو اگلا کریکٹر اسپیس یا **e** (جیسا کہ لفظ *player* میں) دونوں ہو سکتے ہیں۔

یہ ہمیں اس نتیجے پر لے جاتا ہے کہ ہمیشہ سب سے زیادہ پروبیبلٹی والے کریکٹر کو منتخب کرنا "منصفانہ" نہیں ہوتا، کیونکہ دوسرے سب سے زیادہ پروبیبلٹی والے کریکٹر کو منتخب کرنا بھی معنی خیز ٹیکسٹ کی طرف لے جا سکتا ہے۔ زیادہ دانشمندی یہ ہے کہ نیٹ ورک آؤٹ پٹ کی دی گئی پروبیبلٹی ڈسٹریبیوشن سے کریکٹرز کو **سیمپل** کیا جائے۔ ہم ایک پیرامیٹر، **ٹمپریچر**، بھی استعمال کر سکتے ہیں، جو پروبیبلٹی ڈسٹریبیوشن کو ہموار کر دے گا، اگر ہم زیادہ رینڈمنیس شامل کرنا چاہیں، یا اسے زیادہ کھڑا کر دے گا، اگر ہم سب سے زیادہ پروبیبلٹی والے کریکٹرز پر زیادہ زور دینا چاہیں۔

اوپر دی گئی نوٹ بکس میں دیکھیں کہ یہ نرم ٹیکسٹ جنریشن کیسے نافذ کی گئی ہے۔

## نتیجہ

اگرچہ ٹیکسٹ جنریشن بذات خود مفید ہو سکتی ہے، لیکن بڑے فوائد اس وقت حاصل ہوتے ہیں جب ہم RNNs کا استعمال کرتے ہوئے کسی ابتدائی فیچر ویکٹر سے ٹیکسٹ جنریٹ کرتے ہیں۔ مثال کے طور پر، ٹیکسٹ جنریشن مشین ٹرانسلیشن (سیکوئنس ٹو سیکوئنس، اس صورت میں اسٹیٹ ویکٹر *اینکوڈر* سے لیا جاتا ہے تاکہ ترجمہ شدہ پیغام کو *ڈیکوڈ* کیا جا سکے) یا کسی تصویر کی ٹیکسچوئل وضاحت جنریٹ کرنے کے لیے استعمال ہوتی ہے (جس صورت میں فیچر ویکٹر CNN ایکسٹریکٹر سے آتا ہے)۔

## 🚀 چیلنج

Microsoft Learn پر اس موضوع پر کچھ اسباق لیں:

* [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste) کے ساتھ ٹیکسٹ جنریشن

## [لیکچر کے بعد کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## جائزہ اور خود مطالعہ

اپنے علم کو بڑھانے کے لیے یہاں کچھ مضامین ہیں:

* مارکوف چین، LSTM اور GPT-2 کے ساتھ ٹیکسٹ جنریشن کے مختلف طریقے: [بلاگ پوسٹ](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Keras دستاویزات](https://keras.io/examples/generative/lstm_character_level_text_generation/) میں ٹیکسٹ جنریشن کی مثال

## [اسائنمنٹ](lab/README.md)

ہم نے دیکھا کہ کریکٹر بہ کریکٹر ٹیکسٹ کیسے جنریٹ کیا جاتا ہے۔ لیب میں، آپ ورڈ لیول ٹیکسٹ جنریشن کو دریافت کریں گے۔

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔