<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T06:49:06+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "ur"
}
-->
# پہلے سے تربیت یافتہ بڑے زبان کے ماڈلز

ہمارے پچھلے تمام کاموں میں، ہم ایک نیورل نیٹ ورک کو لیبل شدہ ڈیٹا سیٹ کے ذریعے ایک خاص کام انجام دینے کے لیے تربیت دے رہے تھے۔ بڑے ٹرانسفارمر ماڈلز، جیسے BERT، میں ہم زبان کی ماڈلنگ کو خود نگرانی کے انداز میں استعمال کرتے ہیں تاکہ ایک زبان کا ماڈل بنایا جا سکے، جسے پھر مزید ڈومین مخصوص تربیت کے ذریعے مخصوص نیچے والے کام کے لیے خاص بنایا جاتا ہے۔ تاہم، یہ ثابت ہوا ہے کہ بڑے زبان کے ماڈلز بہت سے کام بغیر کسی ڈومین مخصوص تربیت کے بھی حل کر سکتے ہیں۔ ایسے ماڈلز کے خاندان کو **GPT** کہا جاتا ہے: جنریٹو پری ٹرینڈ ٹرانسفارمر۔

## [لیکچر سے پہلے کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## متن کی تخلیق اور Perplexity

ایک نیورل نیٹ ورک کے بغیر نیچے والے تربیت کے عمومی کام انجام دینے کے خیال کو [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) مقالے میں پیش کیا گیا ہے۔ بنیادی خیال یہ ہے کہ بہت سے دوسرے کاموں کو **متن کی تخلیق** کے ذریعے ماڈل کیا جا سکتا ہے، کیونکہ متن کو سمجھنا بنیادی طور پر اسے پیدا کرنے کے قابل ہونا ہے۔ چونکہ ماڈل کو انسانی علم پر مشتمل بہت بڑی مقدار میں متن پر تربیت دی گئی ہے، یہ مختلف موضوعات کے بارے میں بھی علم حاصل کر لیتا ہے۔

> متن کو سمجھنا اور پیدا کرنے کے قابل ہونا دنیا کے ارد گرد کچھ جاننے کا مطلب بھی ہے۔ لوگ بھی بڑی حد تک پڑھ کر سیکھتے ہیں، اور GPT نیٹ ورک اس لحاظ سے اسی طرح ہے۔

متن تخلیق کرنے والے نیٹ ورکس اگلے لفظ کی احتمال $$P(w_N)$$ کی پیش گوئی کے ذریعے کام کرتے ہیں۔ تاہم، اگلے لفظ کی غیر مشروط احتمال متن کے کارپس میں اس لفظ کی تعدد کے برابر ہوتی ہے۔ GPT ہمیں اگلے لفظ کی **مشروط احتمال** دے سکتا ہے، پچھلے الفاظ کے دیے گئے: $$P(w_N | w_{n-1}, ..., w_0)$$

> آپ ہمارے [Data Science for Beginners Curriculum](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) میں احتمالات کے بارے میں مزید پڑھ سکتے ہیں۔

زبان تخلیق کرنے والے ماڈل کے معیار کو **perplexity** کے ذریعے بیان کیا جا سکتا ہے۔ یہ ایک اندرونی میٹرک ہے جو ہمیں کسی کام کے مخصوص ڈیٹا سیٹ کے بغیر ماڈل کے معیار کو ماپنے کی اجازت دیتا ہے۔ یہ *جملے کی احتمال* کے تصور پر مبنی ہے - ماڈل ان جملوں کو جو حقیقی ہونے کا امکان رکھتے ہیں (یعنی ماڈل ان سے **حیران** نہیں ہوتا) زیادہ احتمال دیتا ہے، اور ان جملوں کو جو کم معنی رکھتے ہیں (جیسے *Can it does what?*) کم احتمال دیتا ہے۔ جب ہم اپنے ماڈل کو حقیقی متن کے کارپس سے جملے دیتے ہیں، تو ہم توقع کرتے ہیں کہ ان کی احتمال زیادہ ہو، اور **perplexity** کم ہو۔ ریاضیاتی طور پر، یہ ٹیسٹ سیٹ کی نارملائزڈ الٹی احتمال کے طور پر بیان کیا جاتا ہے:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**آپ [Hugging Face کے GPT-powered text editor](https://transformer.huggingface.co/doc/gpt2-large)** کے ذریعے متن تخلیق کرنے کے ساتھ تجربہ کر سکتے ہیں۔ اس ایڈیٹر میں، آپ اپنا متن لکھنا شروع کرتے ہیں، اور **[TAB]** دبانے سے آپ کو کئی تکمیل کے اختیارات ملتے ہیں۔ اگر وہ بہت مختصر ہیں، یا آپ ان سے مطمئن نہیں ہیں - دوبارہ [TAB] دبائیں، اور آپ کو مزید اختیارات ملیں گے، جن میں طویل متن کے ٹکڑے بھی شامل ہیں۔

## GPT ایک خاندان ہے

GPT ایک واحد ماڈل نہیں ہے، بلکہ [OpenAI](https://openai.com) کے ذریعے تیار اور تربیت یافتہ ماڈلز کا مجموعہ ہے۔

GPT ماڈلز کے تحت، ہمارے پاس ہیں:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| زبان کا ماڈل جس میں 1.5 بلین پیرامیٹرز تک ہیں۔ | زبان کا ماڈل جس میں 175 بلین پیرامیٹرز تک ہیں۔ | 100T پیرامیٹرز اور متن کے ساتھ ساتھ تصاویر کو ان پٹ کے طور پر قبول کرتا ہے اور متن کو آؤٹ پٹ کرتا ہے۔ |

GPT-3 اور GPT-4 ماڈلز [Microsoft Azure کے ذریعے ایک cognitive service](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) کے طور پر دستیاب ہیں، اور [OpenAI API](https://openai.com/api/) کے طور پر بھی۔

## پرامپٹ انجینئرنگ

چونکہ GPT کو زبان اور کوڈ کو سمجھنے کے لیے وسیع مقدار میں ڈیٹا پر تربیت دی گئی ہے، یہ ان پٹ (پرامپٹس) کے جواب میں آؤٹ پٹ فراہم کرتا ہے۔ پرامپٹس GPT کے ان پٹ یا سوالات ہیں جن کے ذریعے کوئی ماڈلز کو کاموں پر ہدایات فراہم کرتا ہے جو وہ مکمل کرتے ہیں۔ مطلوبہ نتیجہ حاصل کرنے کے لیے، آپ کو سب سے مؤثر پرامپٹ کی ضرورت ہوتی ہے، جس میں صحیح الفاظ، فارمیٹس، جملے یا یہاں تک کہ علامتوں کا انتخاب شامل ہوتا ہے۔ اس طریقہ کار کو [Prompt Engineering](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) کہا جاتا ہے۔

[یہ دستاویزات](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) پرامپٹ انجینئرنگ کے بارے میں مزید معلومات فراہم کرتی ہیں۔

## ✍️ مثال نوٹ بک: [OpenAI-GPT کے ساتھ کھیلنا](GPT-PyTorch.ipynb)

اپنی تعلیم کو درج ذیل نوٹ بکس میں جاری رکھیں:

* [OpenAI-GPT اور Hugging Face Transformers کے ساتھ متن تخلیق کرنا](GPT-PyTorch.ipynb)

## نتیجہ

نئے عمومی پری ٹرینڈ زبان کے ماڈلز نہ صرف زبان کی ساخت کو ماڈل کرتے ہیں، بلکہ قدرتی زبان کی بڑی مقدار بھی رکھتے ہیں۔ اس طرح، انہیں زیرو شاپ یا فیو شاپ سیٹنگز میں کچھ NLP کاموں کو مؤثر طریقے سے حل کرنے کے لیے استعمال کیا جا سکتا ہے۔

## [لیکچر کے بعد کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

