<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-26T08:42:12+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "ur"
}
-->
# پہلے سے تربیت یافتہ بڑے زبان کے ماڈلز

ہم نے اپنے پچھلے تمام کاموں میں ایک نیورل نیٹ ورک کو ایک خاص کام انجام دینے کے لیے لیبل شدہ ڈیٹاسیٹ کے ذریعے تربیت دی۔ بڑے ٹرانسفارمر ماڈلز، جیسے کہ BERT، میں ہم زبان کی ماڈلنگ کو خودکار طریقے سے استعمال کرتے ہیں تاکہ ایک زبان کا ماڈل بنایا جا سکے، جسے پھر مزید ڈومین مخصوص تربیت کے ذریعے کسی خاص نیچے والے کام کے لیے ماہر بنایا جاتا ہے۔ تاہم، یہ ثابت ہوا ہے کہ بڑے زبان کے ماڈلز بغیر کسی ڈومین مخصوص تربیت کے بھی کئی کام انجام دے سکتے ہیں۔ ایسے ماڈلز کے خاندان کو **GPT** کہا جاتا ہے: جنریٹو پری ٹرینڈ ٹرانسفارمر۔

## [لیکچر سے پہلے کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## متن کی تخلیق اور پرپلیکسیٹی

ایک نیورل نیٹ ورک کے بغیر نیچے والے تربیت کے عمومی کام انجام دینے کے خیال کو [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) کے مقالے میں پیش کیا گیا ہے۔ بنیادی خیال یہ ہے کہ کئی دوسرے کاموں کو **متن کی تخلیق** کے ذریعے ماڈل کیا جا سکتا ہے، کیونکہ متن کو سمجھنا بنیادی طور پر اسے تخلیق کرنے کی صلاحیت رکھنا ہے۔ چونکہ ماڈل کو ایک بہت بڑی مقدار میں متن پر تربیت دی گئی ہے جو انسانی علم کو شامل کرتی ہے، یہ مختلف موضوعات کے بارے میں بھی جانکاری حاصل کر لیتا ہے۔

> متن کو سمجھنا اور تخلیق کرنے کی صلاحیت رکھنا دنیا کے بارے میں کچھ جاننے کو بھی شامل کرتا ہے۔ لوگ بھی بڑی حد تک پڑھ کر سیکھتے ہیں، اور GPT نیٹ ورک اس لحاظ سے مشابہ ہے۔

متن تخلیق کرنے والے نیٹ ورکس اگلے لفظ کی احتمال $$P(w_N)$$ کی پیش گوئی کے ذریعے کام کرتے ہیں۔ تاہم، اگلے لفظ کی غیر مشروط احتمال متن کے مجموعے میں اس لفظ کی فریکوئنسی کے برابر ہوتی ہے۔ GPT ہمیں اگلے لفظ کی **مشروط احتمال** فراہم کرنے کے قابل ہے، پچھلے الفاظ کے پیش نظر: $$P(w_N | w_{n-1}, ..., w_0)$$

> آپ ہمارے [Data Science for Beginners Curriculum](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) میں احتمال کے بارے میں مزید پڑھ سکتے ہیں۔

زبان تخلیق کرنے والے ماڈل کے معیار کو **پرپلیکسیٹی** کے ذریعے بیان کیا جا سکتا ہے۔ یہ ایک اندرونی میٹرک ہے جو ہمیں کسی بھی کام کے مخصوص ڈیٹاسیٹ کے بغیر ماڈل کے معیار کو ماپنے کی اجازت دیتا ہے۔ یہ *جملے کی احتمال* کے تصور پر مبنی ہے - ماڈل ایک جملے کو جو حقیقی ہونے کا امکان رکھتا ہے، زیادہ احتمال دیتا ہے (یعنی ماڈل اس سے **حیران** نہیں ہوتا)، اور ان جملوں کو کم احتمال دیتا ہے جو کم معنی خیز ہوتے ہیں (مثلاً *کیا یہ کر سکتا ہے؟*)۔ جب ہم اپنے ماڈل کو حقیقی متن کے مجموعے سے جملے دیتے ہیں، تو ہم توقع کرتے ہیں کہ ان کی احتمال زیادہ ہو، اور **پرپلیکسیٹی** کم ہو۔ ریاضیاتی طور پر، یہ ٹیسٹ سیٹ کی نارملائزڈ انورس احتمال کے طور پر بیان کیا جاتا ہے:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**آپ [Hugging Face کے GPT سے چلنے والے متن ایڈیٹر](https://transformer.huggingface.co/doc/gpt2-large)** کے ذریعے متن تخلیق کے ساتھ تجربہ کر سکتے ہیں۔ اس ایڈیٹر میں، آپ اپنا متن لکھنا شروع کرتے ہیں، اور **[TAB]** دبانے سے آپ کو کئی تکمیل کے اختیارات ملتے ہیں۔ اگر وہ بہت مختصر ہوں، یا آپ ان سے مطمئن نہ ہوں - [TAB] دوبارہ دبائیں، اور آپ کو مزید اختیارات ملیں گے، جن میں طویل متن کے ٹکڑے بھی شامل ہیں۔

## GPT ایک خاندان ہے

GPT ایک واحد ماڈل نہیں ہے، بلکہ [OpenAI](https://openai.com) کے ذریعے تیار اور تربیت یافتہ ماڈلز کا ایک مجموعہ ہے۔

GPT ماڈلز کے تحت:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| زبان کا ماڈل جس میں 1.5 بلین تک کے پیرامیٹرز ہیں۔ | زبان کا ماڈل جس میں 175 بلین تک کے پیرامیٹرز ہیں۔ | 100 ٹریلین پیرامیٹرز اور یہ متن اور تصاویر دونوں کو ان پٹ کے طور پر قبول کرتا ہے اور متن آؤٹ پٹ کرتا ہے۔ |

GPT-3 اور GPT-4 ماڈلز [Microsoft Azure کے ذریعے ایک کگنیٹیو سروس](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) اور [OpenAI API](https://openai.com/api/) کے طور پر دستیاب ہیں۔

## پرامپٹ انجینئرنگ

چونکہ GPT کو زبان اور کوڈ کو سمجھنے کے لیے وسیع مقدار میں ڈیٹا پر تربیت دی گئی ہے، یہ ان پٹ (پرامپٹس) کے جواب میں آؤٹ پٹ فراہم کرتا ہے۔ پرامپٹس GPT کے ان پٹ یا سوالات ہیں جن کے ذریعے کسی کو ماڈلز کو ہدایات فراہم کرنی ہوتی ہیں کہ اگلا کام کیسے مکمل کیا جائے۔ مطلوبہ نتیجہ حاصل کرنے کے لیے، آپ کو سب سے مؤثر پرامپٹ کی ضرورت ہوتی ہے، جس میں صحیح الفاظ، فارمیٹس، جملے یا یہاں تک کہ علامات کا انتخاب شامل ہوتا ہے۔ اس طریقہ کار کو [پرامپٹ انجینئرنگ](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) کہا جاتا ہے۔

[یہ دستاویز](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) آپ کو پرامپٹ انجینئرنگ کے بارے میں مزید معلومات فراہم کرتی ہے۔

## ✍️ مثال نوٹ بک: [OpenAI-GPT کے ساتھ کھیلنا](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

اپنی تعلیم کو درج ذیل نوٹ بکس میں جاری رکھیں:

* [OpenAI-GPT اور Hugging Face Transformers کے ساتھ متن تخلیق کرنا](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## نتیجہ

نئے عمومی پری ٹرینڈ زبان کے ماڈلز نہ صرف زبان کی ساخت کو ماڈل کرتے ہیں، بلکہ قدرتی زبان کی ایک بڑی مقدار بھی شامل کرتے ہیں۔ اس طرح، انہیں زیرو شاٹ یا فیو شاٹ سیٹنگز میں کچھ NLP کاموں کو مؤثر طریقے سے حل کرنے کے لیے استعمال کیا جا سکتا ہے۔

## [لیکچر کے بعد کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/40)

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے پوری کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا خامیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔