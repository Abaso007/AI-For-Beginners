<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-26T10:24:51+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "ur"
}
-->
# نیورل نیٹ ورکس کا تعارف: ملٹی لیئرڈ پرسیپٹرون

پچھلے حصے میں، آپ نے سب سے سادہ نیورل نیٹ ورک ماڈل - ایک لیئرڈ پرسیپٹرون، جو کہ ایک لکیری دو کلاسز کی درجہ بندی کا ماڈل ہے، کے بارے میں سیکھا۔

اس حصے میں ہم اس ماڈل کو ایک زیادہ لچکدار فریم ورک میں توسیع دیں گے، جو ہمیں یہ کرنے کی اجازت دے گا:

* **ملٹی کلاس درجہ بندی** انجام دینا، دو کلاسز کے علاوہ
* **ریگریشن مسائل** حل کرنا، درجہ بندی کے علاوہ
* ایسی کلاسز کو الگ کرنا جو لکیری طور پر الگ نہیں کی جا سکتیں

ہم Python میں اپنا ایک ماڈیولر فریم ورک بھی تیار کریں گے جو ہمیں مختلف نیورل نیٹ ورک آرکیٹیکچرز بنانے کی اجازت دے گا۔

## [لیکچر سے پہلے کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## مشین لرننگ کی رسمی وضاحت

آئیے مشین لرننگ کے مسئلے کو رسمی طور پر بیان کرنے سے شروع کرتے ہیں۔ فرض کریں کہ ہمارے پاس ایک تربیتی ڈیٹاسیٹ **X** ہے جس کے لیبلز **Y** ہیں، اور ہمیں ایک ماڈل *f* بنانا ہے جو سب سے زیادہ درست پیش گوئیاں کرے۔ پیش گوئیوں کے معیار کو **لاس فنکشن** ℒ کے ذریعے ناپا جاتا ہے۔ درج ذیل لاس فنکشنز اکثر استعمال کیے جاتے ہیں:

* ریگریشن مسئلے کے لیے، جب ہمیں ایک عدد کی پیش گوئی کرنی ہو، ہم **ایبسولیوٹ ایرر** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| یا **اسکوئرڈ ایرر** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> استعمال کر سکتے ہیں۔
* درجہ بندی کے لیے، ہم **0-1 لاس** (جو بنیادی طور پر ماڈل کی **درستگی** کے برابر ہے) یا **لاجسٹک لاس** استعمال کرتے ہیں۔

ایک لیول پرسیپٹرون کے لیے، فنکشن *f* کو ایک لکیری فنکشن *f(x)=wx+b* کے طور پر بیان کیا گیا تھا (یہاں *w* وزن میٹرکس ہے، *x* ان پٹ فیچرز کا ویکٹر ہے، اور *b* بایس ویکٹر ہے)۔ مختلف نیورل نیٹ ورک آرکیٹیکچرز کے لیے، یہ فنکشن زیادہ پیچیدہ شکل اختیار کر سکتا ہے۔

> درجہ بندی کے معاملے میں، اکثر یہ مطلوب ہوتا ہے کہ نیٹ ورک آؤٹ پٹ کے طور پر متعلقہ کلاسز کے امکانات فراہم کرے۔ کسی بھی عدد کو امکانات میں تبدیل کرنے کے لیے (مثلاً آؤٹ پٹ کو نارملائز کرنے کے لیے)، ہم اکثر **سافٹ میکس** فنکشن σ استعمال کرتے ہیں، اور فنکشن *f* بن جاتا ہے *f(x)=σ(wx+b)*۔

اوپر دی گئی *f* کی تعریف میں، *w* اور *b* کو **پیرامیٹرز** θ=⟨*w,b*⟩ کہا جاتا ہے۔ دیے گئے ڈیٹاسیٹ ⟨**X**,**Y**⟩ کے ساتھ، ہم پورے ڈیٹاسیٹ پر مجموعی غلطی کو پیرامیٹرز θ کے فنکشن کے طور پر شمار کر سکتے ہیں۔

> ✅ **نیورل نیٹ ورک کی تربیت کا مقصد پیرامیٹرز θ کو تبدیل کر کے غلطی کو کم سے کم کرنا ہے**

## گریڈینٹ ڈیسنٹ آپٹیمائزیشن

فنکشن آپٹیمائزیشن کا ایک معروف طریقہ **گریڈینٹ ڈیسنٹ** کہلاتا ہے۔ اس کا خیال یہ ہے کہ ہم لاس فنکشن کا مشتق (کثیر جہتی صورت میں **گریڈینٹ** کہلاتا ہے) پیرامیٹرز کے لحاظ سے شمار کر سکتے ہیں، اور پیرامیٹرز کو اس طرح تبدیل کر سکتے ہیں کہ غلطی کم ہو۔ اسے درج ذیل طور پر رسمی بنایا جا سکتا ہے:

* پیرامیٹرز کو کچھ بے ترتیب اقدار سے شروع کریں w<sup>(0)</sup>, b<sup>(0)</sup>
* درج ذیل قدم کو کئی بار دہرائیں:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

تربیت کے دوران، آپٹیمائزیشن کے قدم پورے ڈیٹاسیٹ کو مدنظر رکھتے ہوئے شمار کیے جانے چاہئیں (یاد رکھیں کہ لاس تمام تربیتی نمونوں کے ذریعے مجموعے کے طور پر شمار کیا جاتا ہے)۔ تاہم، حقیقی زندگی میں ہم ڈیٹاسیٹ کے چھوٹے حصے لیتے ہیں جنہیں **منی بیچز** کہا جاتا ہے، اور ڈیٹا کے ایک ذیلی سیٹ کی بنیاد پر گریڈینٹس شمار کرتے ہیں۔ چونکہ ہر بار ذیلی سیٹ کو بے ترتیب طور پر لیا جاتا ہے، اس طریقے کو **اسٹوکاسٹک گریڈینٹ ڈیسنٹ** (SGD) کہا جاتا ہے۔

## ملٹی لیئرڈ پرسیپٹرونز اور بیک پروپیگیشن

جیسا کہ ہم نے اوپر دیکھا، ایک لیئرڈ نیٹ ورک لکیری طور پر الگ کی جانے والی کلاسز کو درجہ بندی کرنے کے قابل ہے۔ ایک زیادہ بھرپور ماڈل بنانے کے لیے، ہم نیٹ ورک کی کئی لیئرز کو یکجا کر سکتے ہیں۔ ریاضیاتی طور پر اس کا مطلب یہ ہوگا کہ فنکشن *f* ایک زیادہ پیچیدہ شکل اختیار کرے گا، اور کئی مراحل میں شمار کیا جائے گا:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

یہاں، α ایک **نان لکیری ایکٹیویشن فنکشن** ہے، σ ایک سافٹ میکس فنکشن ہے، اور پیرامیٹرز θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> ہیں۔

گریڈینٹ ڈیسنٹ الگورتھم وہی رہے گا، لیکن گریڈینٹس کو شمار کرنا زیادہ مشکل ہوگا۔ چین ڈفرینشی ایشن رول کے مطابق، ہم مشتقات کو درج ذیل طور پر شمار کر سکتے ہیں:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ چین ڈفرینشی ایشن رول کو لاس فنکشن کے پیرامیٹرز کے لحاظ سے مشتقات شمار کرنے کے لیے استعمال کیا جاتا ہے۔

نوٹ کریں کہ ان تمام تاثرات کے بائیں جانب کا حصہ ایک جیسا ہے، اور اس طرح ہم مؤثر طریقے سے مشتقات کو لاس فنکشن سے شروع کر کے "پیچھے کی طرف" کمپیوٹیشنل گراف کے ذریعے شمار کر سکتے ہیں۔ اس طرح ملٹی لیئرڈ پرسیپٹرون کی تربیت کے طریقے کو **بیک پروپیگیشن** یا 'بیک پروپ' کہا جاتا ہے۔

<img alt="کمپیوٹ گراف" src="images/ComputeGraphGrad.png"/>

> TODO: تصویر کا حوالہ

> ✅ ہم اپنے نوٹ بک کی مثال میں بیک پروپ کو بہت زیادہ تفصیل سے کور کریں گے۔

## نتیجہ

اس سبق میں، ہم نے اپنی نیورل نیٹ ورک لائبریری بنائی، اور اسے ایک سادہ دو جہتی درجہ بندی کے کام کے لیے استعمال کیا۔

## 🚀 چیلنج

ساتھ دی گئی نوٹ بک میں، آپ ملٹی لیئرڈ پرسیپٹرونز بنانے اور تربیت دینے کے لیے اپنا فریم ورک نافذ کریں گے۔ آپ تفصیل سے دیکھ سکیں گے کہ جدید نیورل نیٹ ورکس کیسے کام کرتے ہیں۔

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) نوٹ بک پر جائیں اور اس پر کام کریں۔

## [لیکچر کے بعد کا کوئز](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## جائزہ اور خود مطالعہ

بیک پروپیگیشن ایک عام الگورتھم ہے جو AI اور ML میں استعمال ہوتا ہے، [مزید تفصیل سے مطالعہ کرنے کے قابل](https://wikipedia.org/wiki/Backpropagation) ہے۔

## [اسائنمنٹ](lab/README.md)

اس لیب میں، آپ سے کہا گیا ہے کہ آپ اس سبق میں بنائے گئے فریم ورک کو MNIST ہاتھ سے لکھے گئے ہندسوں کی درجہ بندی کے مسئلے کو حل کرنے کے لیے استعمال کریں۔

* [ہدایات](lab/README.md)
* [نوٹ بک](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**ڈس کلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے پوری کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا عدم درستگی ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔