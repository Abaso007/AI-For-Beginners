<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-10-11T11:47:17+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ta"
}
-->
# ஆழமான பலனளிக்கும் கற்றல்

பலனளிக்கும் கற்றல் (Reinforcement Learning - RL) என்பது மேற்பார்வை கற்றல் மற்றும் மேற்பார்வையற்ற கற்றலுக்கு அடுத்ததாக உள்ள அடிப்படை இயந்திர கற்றல் முறைகளில் ஒன்றாகக் கருதப்படுகிறது. மேற்பார்வை கற்றலில் நாம் அறியப்பட்ட முடிவுகளுடன் கூடிய தரவுத்தொகுப்பை நம்புகிறோம், ஆனால் RL என்பது **செய்து கற்றல்** அடிப்படையில் செயல்படுகிறது. உதாரணமாக, முதலில் ஒரு கணினி விளையாட்டைப் பார்க்கும்போது, விதிகளை அறியாமல் விளையாடத் தொடங்குகிறோம், பின்னர் விளையாடி, நமது நடத்தை மாற்றுவதன் மூலம் திறன்களை மேம்படுத்த முடிகிறது.

## [முன்-வகுப்பு வினாடி வினா](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL செய்ய, நமக்கு தேவையானவை:

* **சுற்றுப்புறம்** அல்லது **சிமுலேட்டர்**, இது விளையாட்டின் விதிகளை அமைக்கிறது. சிமுலேட்டரில் பரிசோதனைகளை இயக்கி அதன் முடிவுகளை கவனிக்க வேண்டும்.
* **பரிசு செயல்பாடு**, இது நமது பரிசோதனை எவ்வளவு வெற்றிகரமாக இருந்தது என்பதை காட்டுகிறது. கணினி விளையாட்டை விளையாட கற்றுக்கொள்வதற்கான சூழலில், பரிசு என்பது இறுதிக் கணக்காக இருக்கும்.

பரிசு செயல்பாட்டின் அடிப்படையில், நமது நடத்தை மாற்றி திறன்களை மேம்படுத்த வேண்டும், அடுத்த முறை நன்றாக விளையாடுவதற்காக. மற்ற இயந்திர கற்றல் வகைகளுக்கும் RL-க்கும் உள்ள முக்கிய வித்தியாசம் என்னவென்றால், RL-ல் நாம் விளையாட்டை முடிக்கும்வரை வெற்றி அல்லது தோல்வி என்பதை அறிய முடியாது. எனவே, ஒரு குறிப்பிட்ட நகர்வு தனியாக நல்லதா இல்லையா என்பதை கூற முடியாது - விளையாட்டின் முடிவில் மட்டுமே பரிசு கிடைக்கும்.

RL செய்யும்போது, பல பரிசோதனைகளை நடத்துகிறோம். ஒவ்வொரு பரிசோதனையிலும், இதுவரை கற்றுக்கொண்ட சிறந்த உத்தியை பின்பற்றுவதற்கும் (**பயன்பாடு**) புதிய நிலைகளை ஆராய்வதற்கும் (**ஆராய்ச்சி**) இடையே சமநிலையை ஏற்படுத்த வேண்டும்.

## OpenAI Gym

RL க்கான ஒரு சிறந்த கருவி [OpenAI Gym](https://gym.openai.com/) ஆகும் - இது பல்வேறு சூழல்களை சிமுலேட் செய்யும் **சிமுலேஷன் சூழல்**. இது Atari விளையாட்டுகள் முதல் தண்டை சமநிலையைப் பின்பற்றும் இயற்பியல் வரை பல சூழல்களை சிமுலேட் செய்ய முடியும். இது பலனளிக்கும் கற்றல் அல்காரிதங்களைப் பயிற்சி செய்ய மிகவும் பிரபலமான சிமுலேஷன் சூழல்களில் ஒன்றாகும், மேலும் [OpenAI](https://openai.com/) மூலம் பராமரிக்கப்படுகிறது.

> **குறிப்பு**: OpenAI Gym-ல் கிடைக்கும் அனைத்து சூழல்களையும் [இங்கே](https://gym.openai.com/envs/#classic_control) பார்க்கலாம்.

## CartPole சமநிலை

நீங்கள் அனைவரும் *Segway* அல்லது *Gyroscooters* போன்ற சமநிலை சாதனங்களை கண்டிருப்பீர்கள். அவை தானாகவே சமநிலையைப் பேண முடியும், அதாவது accelerometer அல்லது gyroscope-ல் இருந்து வரும் சிக்னலுக்கு பதிலளித்து அதன் சக்கரங்களை சரிசெய்து. இந்த பிரிவில், ஒரு தண்டை சமநிலையைப் பேணுவது போன்ற ஒரு பிரச்சினையை எவ்வாறு தீர்க்கலாம் என்பதை கற்றுக்கொள்வோம். இது ஒரு சிர்க்கஸ் கலைஞர் தனது கையில் தண்டை சமநிலையைப் பேணுவது போன்றது - ஆனால் இந்த தண்டை சமநிலை 1D-ல் மட்டுமே நிகழ்கிறது.

சமநிலையின் எளிமையான பதிப்பு **CartPole** பிரச்சினையாக அறியப்படுகிறது. CartPole உலகில், இடது அல்லது வலது நோக்கி நகரும் ஒரு கிடைமட்ட ஸ்லைடர் உள்ளது, மேலும் இலக்கு என்பது ஸ்லைடரின் மேல் ஒரு செங்குத்து தண்டை சமநிலையைப் பேணுவது.

<img alt="ஒரு CartPole" src="../../../../../translated_images/cartpole.f52a67f27e058170c25efc1bca8375b60906570ea757fe8d7ef04ae8e53df29d.ta.png" width="200"/>

இந்த சூழலை உருவாக்கி பயன்படுத்த, Python கோடின் சில வரிகள் தேவை:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

ஒவ்வொரு சூழலையும் ஒரே வழியில் அணுகலாம்:
* `env.reset` ஒரு புதிய பரிசோதனையைத் தொடங்குகிறது
* `env.step` ஒரு சிமுலேஷன் படியைச் செய்கிறது. இது **action space**-ல் இருந்து ஒரு **action**-ஐ பெறுகிறது, மேலும் **observation space**-ல் இருந்து ஒரு **observation**, பரிசு மற்றும் முடிவு கொடி ஆகியவற்றை திருப்பி அனுப்புகிறது.

மேலே உள்ள உதாரணத்தில், ஒவ்வொரு படியிலும் ஒரு சீரற்ற செயலைச் செய்கிறோம், அதனால் பரிசோதனையின் ஆயுள் மிகவும் குறுகியது:

![சமநிலை இல்லாத CartPole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL அல்காரிதத்தின் இலக்கு ஒரு மாதிரியைப் பயிற்சி செய்வது - **policy** &pi; என அழைக்கப்படும் - இது கொடுக்கப்பட்ட நிலைக்கு பதிலளிக்கும் செயலை திருப்பி அனுப்பும். நாம் policy-ஐ probabilistic ஆகவும் கருதலாம், உதாரணமாக எந்த state *s* மற்றும் action *a*-க்கும் &pi;(*a*|*s*) என்ற probability-ஐ திருப்பி அனுப்பும், அதில் state *s*-ல் *a* செயல் எடுக்க வேண்டும்.

## Policy Gradients அல்காரிதம்

ஒரு policy-ஐ மாதிரி அமைப்பதற்கான மிக தெளிவான வழி, states-ஐ input ஆக எடுத்து, அதற்கான actions (அல்லது அனைத்து actions-களின் probabilities) திருப்பி அனுப்பும் ஒரு நரம்பு வலைப்பின்னல் (neural network) உருவாக்குவது. இது ஒரு சாதாரண வகைப்பாடு (classification) பணிக்குச் சமமாக இருக்கும், ஆனால் ஒரு முக்கிய வித்தியாசம் - ஒவ்வொரு படியிலும் எவ்வாறு செயல்பட வேண்டும் என்பதை முன்கூட்டியே அறிய முடியாது.

இங்கு எண்ணம் என்னவென்றால், அந்த probabilities-ஐ மதிப்பீடு செய்வது. பரிசோதனையின் ஒவ்வொரு படியிலும் நமது மொத்த பரிசை காட்டும் **cumulative rewards** என்ற ஒரு வெக்டரை உருவாக்குகிறோம். மேலும், **reward discounting**-ஐ பயன்படுத்தி, &gamma;=0.99 என்ற ஒரு குணகத்தைப் பயன்படுத்தி முந்தைய பரிசுகளை குறைக்கிறோம், இதனால் முந்தைய பரிசுகளின் பங்கு குறைகிறது. பின்னர், அதிக பரிசுகளை வழங்கும் பரிசோதனை பாதையின் படிகளை வலுப்படுத்துகிறோம்.

> Policy Gradient அல்காரிதம் பற்றி மேலும் அறியவும் மற்றும் அதை செயல்படுத்தும் உதாரணத்தை [example notebook](CartPole-RL-TF.ipynb) இல் பார்க்கவும்.

## Actor-Critic அல்காரிதம்

Policy Gradients அணுகுமுறையின் மேம்பட்ட பதிப்பு **Actor-Critic** என அழைக்கப்படுகிறது. இதன் முக்கிய எண்ணம் என்னவென்றால், நரம்பு வலைப்பின்னல் இரண்டு விஷயங்களை திருப்பி அனுப்ப பயிற்சி செய்யப்படும்:

* policy, இது எந்த action எடுக்க வேண்டும் என்பதை தீர்மானிக்கிறது. இந்த பகுதி **actor** என அழைக்கப்படுகிறது.
* இந்த state-ல் நாம் பெறக்கூடிய மொத்த பரிசின் மதிப்பீடு - இந்த பகுதி **critic** என அழைக்கப்படுகிறது.

ஒரு [GAN](../../4-ComputerVision/10-GANs/README.md) போன்ற ஒரு கட்டமைப்பை இது ஒத்திருக்கிறது, இதில் இரண்டு வலைப்பின்னல்கள் ஒருவருக்கொருவர் எதிராக பயிற்சி செய்யப்படுகின்றன. Actor-Critic மாதிரியில், actor நாம் எடுக்க வேண்டிய action-ஐ முன்மொழிகிறது, மற்றும் critic விமர்சனமாக இருந்து முடிவை மதிப்பீடு செய்ய முயற்சிக்கிறது. ஆனால், நமது இலக்கு இந்த வலைப்பின்னல்களை ஒருமித்தமாக பயிற்சி செய்வது.

பரிசோதனையின் போது நமது மொத்த cumulative rewards மற்றும் critic மூலம் திருப்பி அனுப்பப்படும் முடிவுகளை இரண்டையும் அறிந்திருப்பதால், அவற்றின் இடையேயான வேறுபாட்டை குறைக்கும் **critic loss**-ஐ உருவாக்குவது எளிதாக இருக்கும். Policy Gradient அல்காரிதத்தில் பயன்படுத்திய அணுகுமுறையைப் பயன்படுத்தி **actor loss**-ஐ கணக்கிடலாம்.

இந்த அல்காரிதங்களில் ஒன்றை இயக்கிய பிறகு, நமது CartPole இவ்வாறு செயல்படும்:

![சமநிலை CartPole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ பயிற்சிகள்: Policy Gradients மற்றும் Actor-Critic RL

பின்வரும் நோட்புக்குகளில் உங்கள் கற்றலை தொடருங்கள்:

* [RL in TensorFlow](CartPole-RL-TF.ipynb)
* [RL in PyTorch](CartPole-RL-PyTorch.ipynb)

## மற்ற RL பணிகள்

இன்றைய காலத்தில் பலனளிக்கும் கற்றல் ஒரு வேகமாக வளர்ந்து வரும் ஆராய்ச்சி துறையாக உள்ளது. பலனளிக்கும் கற்றலின் சில சுவாரஸ்யமான உதாரணங்கள்:

* **Atari விளையாட்டுகளை** விளையாட ஒரு கணினியை கற்றல். இந்த பிரச்சினையின் சவாலான பகுதி என்னவென்றால், எளிய state ஒரு வெக்டராக பிரதிநிதித்துவம் செய்யப்படுவதில்லை, ஆனால் ஒரு screenshot - மேலும் இந்த screen image-ஐ ஒரு feature vector-ஆக மாற்ற அல்லது reward தகவலை எடுக்க CNN-ஐ பயன்படுத்த வேண்டும். Atari விளையாட்டுகள் Gym-ல் கிடைக்கின்றன.
* Chess மற்றும் Go போன்ற பலகை விளையாட்டுகளை விளையாட ஒரு கணினியை கற்றல். சமீபத்தில் **Alpha Zero** போன்ற state-of-the-art திட்டங்கள் இரண்டு முகவர்கள் ஒருவருக்கொருவர் எதிராக விளையாடி, ஒவ்வொரு படியிலும் மேம்படுத்துவதன் மூலம் அடிப்படையில் பயிற்சி செய்யப்பட்டது.
* தொழில்துறையில், சிமுலேஷனில் இருந்து கட்டுப்பாட்டு அமைப்புகளை உருவாக்க RL பயன்படுத்தப்படுகிறது. [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) எனும் சேவை இதற்காகவே வடிவமைக்கப்பட்டுள்ளது.

## முடிவு

விளையாட்டின் விரும்பத்தகுந்த நிலையை வரையறுக்கும் பரிசு செயல்பாட்டை வழங்குவதன் மூலம், மற்றும் தேடல் இடத்தை புத்திசாலியாக ஆராய்வதற்கான வாய்ப்பை வழங்குவதன் மூலம், நல்ல முடிவுகளை அடைய முகவர்களை பயிற்சி செய்ய எவ்வாறு கற்றுக்கொண்டோம் என்பதை இப்போது அறிந்துள்ளோம். இரண்டு அல்காரிதங்களை வெற்றிகரமாக முயற்சி செய்து, குறுகிய காலத்தில் நல்ல முடிவை அடைந்தோம். இருப்பினும், இது RL-ல் உங்கள் பயணத்தின் தொடக்கமே, மேலும் நீங்கள் ஆழமாக கற்றுக்கொள்ள விரும்பினால் தனி பாடநெறியை எடுத்துக் கொள்ள வேண்டும்.

## 🚀 சவால்

'மற்ற RL பணிகள்' பிரிவில் பட்டியலிடப்பட்ட பயன்பாடுகளை ஆராய்ந்து, ஒன்றை செயல்படுத்த முயற்சிக்கவும்!

## [பின்-வகுப்பு வினாடி வினா](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## மதிப்பீடு மற்றும் சுய கற்றல்

நமது [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) இல் பாரம்பரிய பலனளிக்கும் கற்றல் பற்றி மேலும் அறிக.

Super Mario விளையாட ஒரு கணினி எப்படி கற்றுக்கொள்ள முடியும் என்பதைப் பற்றி பேசும் [இந்த சிறந்த வீடியோ](https://www.youtube.com/watch?v=qv6UVOQ0F44) பாருங்கள்.

## பணிக்கட்டளை: [Train a Mountain Car](lab/README.md)

இந்த பணிக்கட்டளையின் போது உங்கள் இலக்கு [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) எனும் Gym சூழலை பயிற்சி செய்வதாக இருக்கும்.

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையை பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. எங்கள் நோக்கம் துல்லியமாக இருக்க வேண்டும் என்பதுதான், ஆனால் தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது துல்லியமின்மைகள் இருக்கக்கூடும் என்பதை தயவுசெய்து கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.