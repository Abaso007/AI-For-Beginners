{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уграђивања\n",
    "\n",
    "У претходном примеру радили смо са векторима високе димензије заснованим на моделу \"вреће речи\" дужине `vocab_size`, и експлицитно смо конвертовали векторе нискодимензионалне позиционе репрезентације у ретке једнодимензионалне репрезентације. Ова једнодимензионална репрезентација није ефикасна у погледу меморије. Поред тога, свака реч се третира независно од других, тако да једнодимензионални кодирани вектори не изражавају семантичке сличности између речи.\n",
    "\n",
    "У овој јединици наставићемо да истражујемо скуп података **News AG**. За почетак, учитаћемо податке и преузети неке дефиниције из претходне јединице.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шта је уграђивање?\n",
    "\n",
    "Идеја **уграђивања** је да се речи представе помоћу густих вектора нижих димензија који одражавају семантичко значење речи. Касније ћемо разговарати о томе како изградити смислена уграђивања речи, али за сада замислимо уграђивања као начин да се смањи димензионалност векторa речи.\n",
    "\n",
    "Дакле, слој за уграђивање узима реч као улаз и производи излазни вектор одређене величине `embedding_size`. У неком смислу, веома је сличан слоју `Dense`, али уместо да узима једновекторски кодирани улаз, он може да прихвати број речи.\n",
    "\n",
    "Коришћењем слоја за уграђивање као првог слоја у нашој мрежи, можемо прећи са модела „вреће речи“ на модел **вреће уграђивања**, где прво претварамо сваку реч у нашем тексту у одговарајуће уграђивање, а затим израчунавамо неку агрегатну функцију над свим тим уграђивањима, као што су `sum`, `average` или `max`.\n",
    "\n",
    "![Слика која приказује класификатор са уграђивањем за пет речи у низу.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sr.png)\n",
    "\n",
    "Наша неуронска мрежа класификатора састоји се од следећих слојева:\n",
    "\n",
    "* Слој `TextVectorization`, који узима стринг као улаз и производи тензор бројева токена. Одредићемо разумну величину речника `vocab_size` и игнорисати речи које се ређе користе. Улазни облик ће бити 1, а излазни облик ће бити $n$, јер ћемо добити $n$ токена као резултат, при чему сваки од њих садржи бројеве од 0 до `vocab_size`.\n",
    "* Слој `Embedding`, који узима $n$ бројева и смањује сваки број на густ вектор одређене дужине (100 у нашем примеру). Тако ће улазни тензор облика $n$ бити трансформисан у тензор облика $n\\times 100$.\n",
    "* Агрегациони слој, који узима просек овог тензора дуж прве осе, тј. израчунава просек свих $n$ улазних тензора који одговарају различитим речима. За имплементацију овог слоја користићемо слој `Lambda` и у њега проследити функцију за израчунавање просека. Излаз ће имати облик 100 и представљаће нумеричку репрезентацију целокупног улазног низа.\n",
    "* Завршни `Dense` линеарни класификатор.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У `summary` испису, у колони **output shape**, прва димензија тензора `None` одговара величини мини-бача, а друга дужини секвенце токена. Све секвенце токена у мини-бачу имају различите дужине. Разговараћемо о томе како се носити с тим у наредном одељку.\n",
    "\n",
    "Сада хајде да обучимо мрежу:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Напомена** да градимо векторизатор на основу подскупа података. Ово се ради како би се убрзао процес, и то може довести до ситуације у којој нису сви токени из нашег текста присутни у речнику. У том случају, ти токени би били игнорисани, што може резултирати нешто нижом тачношћу. Међутим, у стварном животу подскуп текста често даје добру процену речника.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рад са променљивим величинама секвенци\n",
    "\n",
    "Хајде да разумемо како се обука одвија у мини-бачовима. У горњем примеру, улазни тензор има димензију 1, и користимо мини-бачове дужине 128, тако да је стварна величина тензора $128 \\times 1$. Међутим, број токена у свакој реченици је различит. Ако применимо слој `TextVectorization` на један улаз, број враћених токена је различит, у зависности од тога како је текст токенизован:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Међутим, када применимо векторизатор на неколико секвенци, он мора да произведе тензор правоугаоног облика, па попуњава неискоришћене елементе са PAD токеном (који је у нашем случају нула):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Овде можемо видети уградње:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Напомена**: Да би се минимизирала количина попуњавања, у неким случајевима има смисла сортирати све секвенце у датасету по редоследу растуће дужине (или, прецизније, броја токена). Ово ће осигурати да сваки минибатч садржи секвенце сличне дужине.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантичке уградње: Word2Vec\n",
    "\n",
    "У нашем претходном примеру, слој за уградњу је научио да мапира речи у векторске репрезентације, али те репрезентације нису имале семантичко значење. Било би корисно научити векторску репрезентацију тако да сличне речи или синоними одговарају векторима који су блиски једни другима у смислу неке векторске удаљености (на пример, еуклидске удаљености).\n",
    "\n",
    "Да бисмо то постигли, потребно је да претходно обучимо наш модел за уградњу на великој колекцији текста користећи технику као што је [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ова техника се заснива на две главне архитектуре које се користе за производњу расподељене репрезентације речи:\n",
    "\n",
    " - **Континуална торба речи** (CBoW), где тренирамо модел да предвиди реч из околног контекста. Дат је нграм $(W_{-2},W_{-1},W_0,W_1,W_2)$, циљ модела је да предвиди $W_0$ из $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Континуални скип-грам** је супротан од CBoW. Модел користи околни прозор контекстуалних речи да би предвидео тренутну реч.\n",
    "\n",
    "CBoW је бржи, док је скип-грам спорији, али боље представља ретке речи.\n",
    "\n",
    "![Слика која приказује алгоритме CBoW и Skip-Gram за конвертовање речи у векторе.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sr.png)\n",
    "\n",
    "Да бисмо експериментисали са Word2Vec уградњом претходно обученом на Google News скупу података, можемо користити библиотеку **gensim**. Испод налазимо речи које су најсличније 'neural'.\n",
    "\n",
    "> **Напомена:** Када први пут креирате векторе речи, њихово преузимање може потрајати!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можемо такође извући векторско уграђивање из речи, које ће се користити у обуци модела за класификацију. Уграђивање има 300 компоненти, али овде приказујемо само првих 20 компоненти вектора ради прегледности:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сјајна ствар код семантичких угњежђивања је то што можете манипулисати кодирањем вектора на основу семантике. На пример, можемо тражити реч чија је векторска репрезентација што је могуће ближа речима *краљ* и *жена*, а што је могуће даља од речи *човек*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Пример изнад користи неку унутрашњу GenSym магију, али основна логика је заправо прилично једноставна. Занимљива ствар код уграђивања је да можете изводити нормалне операције са векторима на векторима уграђивања, и то би одражавало операције на значењима речи. Пример изнад може се изразити у терминима векторских операција: израчунавамо вектор који одговара **КРАЉ-ЧОВЕК+ЖЕНА** (операције `+` и `-` се изводе на векторским представама одговарајућих речи), а затим проналазимо најближу реч у речнику том вектору:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **НАПОМЕНА**: Морали смо да додамо мале коефицијенте на *man* и *woman* векторе - пробајте да их уклоните да видите шта ће се десити.\n",
    "\n",
    "Да бисмо пронашли најближи вектор, користимо TensorFlow механизам за израчунавање вектора растојања између нашег вектора и свих вектора у речнику, а затим проналазимо индекс минималне речи користећи `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иако Word2Vec изгледа као одличан начин за изражавање семантике речи, има много недостатака, укључујући следеће:\n",
    "\n",
    "* И CBoW и skip-gram модели су **предиктивни уградни вектори**, и узимају у обзир само локални контекст. Word2Vec не користи предности глобалног контекста.\n",
    "* Word2Vec не узима у обзир **морфологију речи**, односно чињеницу да значење речи може зависити од различитих делова речи, као што је корен.\n",
    "\n",
    "**FastText** покушава да превазиђе друго ограничење и надограђује Word2Vec тако што учи векторске репрезентације за сваку реч и n-граме карактера који се налазе унутар сваке речи. Вредности репрезентација се затим просечавају у један вектор на сваком кораку тренинга. Иако ово додаје много додатних рачунања током претренинга, омогућава уградним векторима речи да кодирају информације о подречима.\n",
    "\n",
    "Други метод, **GloVe**, користи другачији приступ уградним векторима речи, заснован на факторизацији матрице контекста речи. Прво, гради велику матрицу која броји број појављивања речи у различитим контекстима, а затим покушава да представи ову матрицу у нижим димензијама на начин који минимизира губитак реконструкције.\n",
    "\n",
    "Библиотека gensim подржава те уградне векторе речи, и можете експериментисати са њима тако што ћете променити код за учитавање модела изнад.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коришћење претходно обучених уграђивања у Keras-у\n",
    "\n",
    "Можемо изменити горњи пример како бисмо унапред попунили матрицу у нашем слоју за уграђивање семантичким уграђивањима, као што је Word2Vec. Речници претходно обученог уграђивања и текстуалног корпуса вероватно се неће поклапати, па морамо изабрати један. Овде истражујемо две могуће опције: коришћење речника токенизатора и коришћење речника из Word2Vec уграђивања.\n",
    "\n",
    "### Коришћење речника токенизатора\n",
    "\n",
    "Када користимо речник токенизатора, неке речи из речника ће имати одговарајућа Word2Vec уграђивања, а неке ће недостајати. С обзиром на то да је величина нашег речника `vocab_size`, а дужина вектора Word2Vec уграђивања `embed_size`, слој за уграђивање ће бити представљен тежинском матрицом облика `vocab_size`$\\times$`embed_size`. Ову матрицу ћемо попунити пролазећи кроз речник:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За речи које нису присутне у Word2Vec речнику, можемо их оставити као нуле или генерисати случајни вектор.\n",
    "\n",
    "Сада можемо дефинисати слој за уграђивање са унапред обученим тежинама:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Напомена**: Обратите пажњу да смо поставили `trainable=False` приликом креирања `Embedding`, што значи да нећемо поново тренирати слој Embedding. Ово може довести до благо смањене тачности, али убрзава процес тренирања.\n",
    "\n",
    "### Коришћење вокабулара за уграђивање\n",
    "\n",
    "Један од проблема са претходним приступом је то што су вокабулари који се користе у TextVectorization и Embedding различити. Да бисмо превазишли овај проблем, можемо користити једно од следећих решења:\n",
    "* Поново тренирати Word2Vec модел на нашем вокабулару.\n",
    "* Учитати наш скуп података са вокабуларом из претходно тренираног Word2Vec модела. Вокабулари који се користе за учитавање скупа података могу се специфицирати током учитавања.\n",
    "\n",
    "Други приступ изгледа једноставније, па хајде да га имплементирамо. Пре свега, креираћемо слој `TextVectorization` са специфицираним вокабуларом, преузетим из Word2Vec уграђивања:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генсим библиотека за угњежђивање речи садржи погодну функцију, `get_keras_embeddings`, која ће вам аутоматски креирати одговарајући слој за угњежђивање у Керасу.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Један од разлога зашто не видимо већу тачност је то што неке речи из нашег скупа података недостају у претходно обученом GloVe речнику, и стога се у суштини игноришу. Да бисмо то превазишли, можемо обучити сопствене уградње засноване на нашем скупу података.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контекстуални уграђени прикази\n",
    "\n",
    "Једно од главних ограничења традиционалних унапред обучених уграђених приказа, као што је Word2Vec, јесте чињеница да, иако могу да ухвате одређено значење речи, не могу да разликују различита значења. Ово може изазвати проблеме у моделима који се користе након тога.\n",
    "\n",
    "На пример, реч 'play' има различита значења у следећим реченицама:\n",
    "- Ишао сам на **представу** у позоришту.\n",
    "- Џон жели да се **игра** са својим пријатељима.\n",
    "\n",
    "Унапред обучени уграђени прикази о којима смо говорили представљају оба значења речи 'play' у истом уграђеном приказу. Да бисмо превазишли ово ограничење, потребно је да изградимо уграђене приказе засноване на **језичком моделу**, који је обучен на великом корпусу текста и *зна* како се речи могу комбиновати у различитим контекстима. Разматрање контекстуалних уграђених приказа је ван домета овог туторијала, али ћемо се вратити на њих када будемо говорили о језичким моделима у наредној јединици.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Одрицање од одговорности**:  \nОвај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитативним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-30T01:05:51+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "sr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}