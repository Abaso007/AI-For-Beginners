<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-25T22:08:06+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "sr"
}
-->
# Унапред обучени велики језички модели

У свим нашим претходним задацима, тренирали смо неуронску мрежу да изврши одређени задатак користећи означени скуп података. Са великим трансформер моделима, као што је BERT, користимо моделирање језика у самонадзирајућем режиму како бисмо изградили језички модел, који се затим специјализује за одређени задатак уз додатну обуку специфичну за домен. Међутим, показано је да велики језички модели могу решавати многе задатке и без икакве обуке специфичне за домен. Породица модела која је способна за то назива се **GPT**: Генеративни Унапред Обучени Трансформер.

## [Квиз пре предавања](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## Генерисање текста и перплексност

Идеја да неуронска мрежа може извршавати опште задатке без додатне обуке представљена је у раду [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Главна идеја је да се многи други задаци могу моделирати коришћењем **генерисања текста**, јер разумевање текста у суштини значи способност његовог стварања. Пошто је модел обучен на огромној количини текста који обухвата људско знање, он такође постаје упућен у широк спектар тема.

> Разумевање и способност стварања текста такође подразумева познавање нечега о свету око нас. Људи такође у великој мери уче читањем, а GPT мрежа је у том погледу слична.

Мреже за генерисање текста раде тако што предвиђају вероватноћу следеће речи $$P(w_N)$$. Међутим, безусловна вероватноћа следеће речи једнака је учесталости те речи у корпусу текста. GPT нам може дати **условну вероватноћу** следеће речи, с обзиром на претходне: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Више о вероватноћама можете прочитати у нашем [курикулуму за почетнике у науци о подацима](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Квалитет модела за генерисање текста може се дефинисати коришћењем **перплексности**. То је унутрашња метрика која нам омогућава да измеримо квалитет модела без икаквог скупа података специфичног за задатак. Она се заснива на концепту *вероватноће реченице* - модел додељује високу вероватноћу реченици која је вероватно реална (тј. модел није **збуњен** њоме), и ниску вероватноћу реченицама које мање имају смисла (нпр. *Може ли то шта урадити?*). Када моделу дамо реченице из стварног корпуса текста, очекујемо да имају високу вероватноћу и ниску **перплексност**. Математички, она се дефинише као нормализована инверзна вероватноћа тестног скупа:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Можете експериментисати са генерисањем текста користећи [GPT-омогућени текст едитор од Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. У овом едитору, започните писање текста, а притиском на **[TAB]** добићете неколико опција за наставак. Ако су превише кратке или нисте задовољни њима - поново притисните [TAB], и добићете више опција, укључујући дуже делове текста.

## GPT је породица

GPT није један модел, већ колекција модела које је развио и обучио [OpenAI](https://openai.com). 

У оквиру GPT модела, имамо:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|Језички модел са до 1,5 милијарди параметара. | Језички модел са до 175 милијарди параметара | 100Т параметара и прихвата и слике и текст као улаз, а излази текст. |

Модели GPT-3 и GPT-4 доступни су [као когнитивна услуга преко Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste), као и преко [OpenAI API](https://openai.com/api/).

## Инжењеринг упита (Prompt Engineering)

Пошто је GPT обучен на огромним количинама података да разуме језик и код, он пружа одговоре на уносе (упите). Упити су уноси или захтеви за GPT где се моделу дају инструкције о задацима које треба да изврши. Да бисте добили жељени резултат, потребан вам је најефикаснији упит, што подразумева одабир правих речи, формата, фраза или чак симбола. Овај приступ се назива [Инжењеринг упита](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Ова документација](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) пружа вам више информација о инжењерингу упита.

## ✍️ Пример бележнице: [Играње са OpenAI-GPT](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

Наставите своје учење у следећим бележницама:

* [Генерисање текста са OpenAI-GPT и Hugging Face Transformers](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## Закључак

Нови општи унапред обучени језички модели не само да моделирају структуру језика, већ садрже и огромну количину природног језика. Стога се могу ефикасно користити за решавање неких NLP задатака у zero-shot или few-shot окружењима.

## [Квиз после предавања](https://ff-quizzes.netlify.app/en/ai/quiz/40)

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.