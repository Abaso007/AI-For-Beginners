<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T14:47:05+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "sr"
}
-->
# Унапред обучени велики језички модели

У свим претходним задацима, тренирали смо неуронску мрежу да изврши одређени задатак користећи означене скупове података. Са великим трансформер моделима, као што је BERT, користимо језичко моделирање у самостално надгледаном режиму да изградимо језички модел, који се затим специјализује за специфичан задатак уз додатну обуку у одређеној области. Међутим, показано је да велики језички модели могу решити многе задатке и без било какве обуке специфичне за домен. Породица модела способних за то назива се **GPT**: Генеративни унапред обучени трансформер.

## [Квиз пре предавања](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## Генерисање текста и перплексност

Идеја да неуронска мрежа може обављати опште задатке без додатне обуке представљена је у раду [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Главна идеја је да се многи други задаци могу моделирати помоћу **генерисања текста**, јер разумевање текста суштински значи способност његовог стварања. Пошто је модел обучен на огромној количини текста која обухвата људско знање, он постаје упућен у широк спектар тема.

> Разумевање и способност стварања текста подразумева и познавање света око нас. Људи такође уче читањем у великој мери, а GPT мрежа је слична у том погледу.

Мреже за генерисање текста раде тако што предвиђају вероватноћу следеће речи $$P(w_N)$$. Међутим, безусловна вероватноћа следеће речи једнака је учесталости те речи у корпусу текста. GPT нам даје **условну вероватноћу** следеће речи, узимајући у обзир претходне речи: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Више о вероватноћама можете прочитати у нашем [курикулуму за почетнике у области науке о подацима](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Квалитет модела за генерисање текста може се дефинисати помоћу **перплексности**. То је унутрашња метрика која нам омогућава да измеримо квалитет модела без употребе скупа података специфичног за задатак. Заснована је на концепту *вероватноће реченице* - модел додељује високу вероватноћу реченици која је вероватно стварна (тј. модел није **збуњен** њоме), и ниску вероватноћу реченицама које мање имају смисла (нпр. *Може ли то шта урадити?*). Када моделу дамо реченице из стварног корпуса текста, очекујемо да имају високу вероватноћу и ниску **перплексност**. Математички, она се дефинише као нормализована инверзна вероватноћа тест скупа:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Можете експериментисати са генерисањем текста користећи [GPT-омогућени текстуални едитор од Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. У овом едитору, започните писање текста, а притиском на **[TAB]** добићете неколико опција за наставак. Ако су превише кратке или нисте задовољни њима - притисните [TAB] поново, и добићете више опција, укључујући и дуже делове текста.

## GPT је породица

GPT није један модел, већ колекција модела које је развио и обучио [OpenAI](https://openai.com).

У оквиру GPT модела, имамо:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|Језички модел са до 1.5 милијарди параметара. | Језички модел са до 175 милијарди параметара | 100T параметара, прихвата и слике и текст као улаз, а излаз је текст. |

Модели GPT-3 и GPT-4 доступни су [као когнитивна услуга преко Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste), и као [OpenAI API](https://openai.com/api/).

## Инжењеринг упита

Пошто је GPT обучен на огромним количинама података да разуме језик и код, он пружа одговоре на уносе (упите). Упити су уноси или захтеви за GPT где се моделу дају инструкције о задацима које треба да изврши. Да бисте добили жељени резултат, потребан вам је најефикаснији упит, што подразумева одабир правих речи, формата, фраза или чак симбола. Овај приступ се назива [инжењеринг упита](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Ова документација](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) пружа вам више информација о инжењерингу упита.

## ✍️ Пример бележнице: [Играње са OpenAI-GPT](GPT-PyTorch.ipynb)

Наставите учење у следећим бележницама:

* [Генерисање текста са OpenAI-GPT и Hugging Face Transformers](GPT-PyTorch.ipynb)

## Закључак

Нови општи унапред обучени језички модели не само да моделирају структуру језика, већ садрже и огромну количину природног језика. Због тога се могу ефикасно користити за решавање неких NLP задатака у zero-shot или few-shot режиму.

## [Квиз после предавања](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

