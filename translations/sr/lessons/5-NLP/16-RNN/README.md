<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2273cc150380a5e191903cea858f021",
  "translation_date": "2025-09-23T14:47:46+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "sr"
}
-->
# Рекурентне неуронске мреже

## [Квиз пре предавања](https://ff-quizzes.netlify.app/en/ai/quiz/31)

У претходним одељцима користили смо богате семантичке репрезентације текста и једноставни линеарни класификатор на врху уградњи. Ова архитектура хвата агрегирано значење речи у реченици, али не узима у обзир **редослед** речи, јер операција агрегирања на врху уградњи уклања ову информацију из оригиналног текста. Пошто ови модели не могу да моделирају редослед речи, они не могу решавати сложеније или двосмислене задатке као што су генерисање текста или одговарање на питања.

Да бисмо ухватили значење секвенце текста, потребно је да користимо другу архитектуру неуронске мреже, која се назива **рекурентна неуронска мрежа**, или RNN. У RNN-у, реченицу пропуштамо кроз мрежу један симбол по један, а мрежа производи неко **стање**, које затим поново прослеђујемо мрежи са следећим симболом.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.sr.png)

> Слика аутора

С обзиром на улазну секвенцу токена X<sub>0</sub>,...,X<sub>n</sub>, RNN креира секвенцу блокова неуронске мреже и тренира ову секвенцу од почетка до краја користећи бацкпропагацију. Сваки блок мреже узима пар (X<sub>i</sub>,S<sub>i</sub>) као улаз и производи S<sub>i+1</sub> као резултат. Коначно стање S<sub>n</sub> или (излаз Y<sub>n</sub>) иде у линеарни класификатор да би произвело резултат. Сви блокови мреже деле исте тежине и тренирају се од почетка до краја користећи један бацкпропагацијски пролаз.

Пошто се вектори стања S<sub>0</sub>,...,S<sub>n</sub> прослеђују кроз мрежу, она је у стању да научи секвенцијалне зависности између речи. На пример, када се реч *не* појави негде у секвенци, мрежа може научити да негира одређене елементе унутар вектора стања, што резултира негацијом.

> ✅ Пошто су тежине свих RNN блокова на слици изнад заједничке, иста слика може бити представљена као један блок (десно) са рекурентном повратном петљом, која враћа излазно стање мреже назад на улаз.

## Анатомија RNN ћелије

Хајде да видимо како је организована једноставна RNN ћелија. Она прихвата претходно стање S<sub>i-1</sub> и тренутни симбол X<sub>i</sub> као улазе и мора да произведе излазно стање S<sub>i</sub> (а понекад нас занима и неки други излаз Y<sub>i</sub>, као у случају генеративних мрежа).

Једноставна RNN ћелија има две матрице тежина унутар себе: једна трансформише улазни симбол (назовимо је W), а друга трансформише улазно стање (H). У овом случају излаз мреже се рачуна као &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), где је &sigma; активациона функција, а b је додатна пристрасност.

<img alt="Анатомија RNN ћелије" src="images/rnn-anatomy.png" width="50%"/>

> Слика аутора

У многим случајевима, улазни токени се прослеђују кроз слој уградње пре него што уђу у RNN ради смањења димензионалности. У овом случају, ако је димензија улазних вектора *emb_size*, а вектор стања *hid_size* - величина W је *emb_size*&times;*hid_size*, а величина H је *hid_size*&times;*hid_size*.

## Дугорочна краткорочна меморија (LSTM)

Један од главних проблема класичних RNN-ова је такозвани проблем **нестајућих градијената**. Пошто се RNN-ови тренирају од почетка до краја у једном бацкпропагацијском пролазу, тешко је пренети грешку до првих слојева мреже, па мрежа не може да научи односе између удаљених токена. Један од начина да се избегне овај проблем је увођење **експлицитног управљања стањем** коришћењем такозваних **врата**. Постоје две добро познате архитектуре овог типа: **Дугорочна краткорочна меморија** (LSTM) и **Јединица са контролисаним пролазом** (GRU).

![Слика која приказује пример ћелије дугорочне краткорочне меморије](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Извор слике TBD

LSTM мрежа је организована на начин сличан RNN-у, али постоје два стања која се преносе из слоја у слој: стварно стање C и скривени вектор H. У свакој јединици, скривени вектор H<sub>i</sub> се конкатенише са улазом X<sub>i</sub>, и они контролишу шта се дешава са стањем C преко **врата**. Свака врата су неуронска мрежа са сигмоид активацијом (излаз у опсегу [0,1]), која се може сматрати битовском маском када се помножи са вектором стања. Постоје следећа врата (слева надесно на слици изнад):

* **Врата заборава** узимају скривени вектор и одређују које компоненте вектора C треба да заборавимо, а које да пропустимо.
* **Врата уноса** узимају неке информације из улазних и скривених вектора и убацују их у стање.
* **Врата излаза** трансформишу стање преко линеарног слоја са *tanh* активацијом, а затим бирају неке од његових компоненти користећи скривени вектор H<sub>i</sub> да би произвели ново стање C<sub>i+1</sub>.

Компоненте стања C могу се сматрати неким заставицама које се могу укључити и искључити. На пример, када у секвенци наиђемо на име *Алиса*, можемо претпоставити да се односи на женски лик и подићи заставицу у стању да имамо женску именицу у реченици. Када даље наиђемо на фразу *и Том*, подићи ћемо заставицу да имамо множину. Тако манипулисањем стањем можемо наводно пратити граматичке особине делова реченице.

> ✅ Одличан ресурс за разумевање унутрашњости LSTM-а је овај сјајан чланак [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) аутора Кристофера Олаха.

## Двосмерни и вишеслојни RNN-ови

Разматрали смо рекурентне мреже које раде у једном смеру, од почетка секвенце до краја. То изгледа природно, јер подсећа на начин на који читамо и слушамо говор. Међутим, пошто у многим практичним случајевима имамо случајни приступ улазној секвенци, можда има смисла покренути рекурентни прорачун у оба смера. Такве мреже се називају **двосмерне** RNN-ове. Када радимо са двосмерном мрежом, биће нам потребна два скривена вектора стања, по један за сваки смер.

Рекурентна мрежа, било једносмерна или двосмерна, хвата одређене обрасце унутар секвенце и може их складиштити у вектор стања или проследити у излаз. Као и код конволуционих мрежа, можемо изградити још један рекурентни слој на врху првог да ухватимо обрасце вишег нивоа и изградимо од образаца нижег нивоа које је извукао први слој. Ово нас доводи до концепта **вишеслојног RNN-а**, који се састоји од два или више рекурентних мрежа, где се излаз претходног слоја прослеђује следећем слоју као улаз.

![Слика која приказује вишеслојни LSTM RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.sr.jpg)

*Слика из [овог дивног чланка](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) аутора Фернанда Лопеза*

## ✍️ Вежбе: Уградње

Наставите учење у следећим нотебуковима:

* [RNN-ови са PyTorch-ом](RNNPyTorch.ipynb)
* [RNN-ови са TensorFlow-ом](RNNTF.ipynb)

## Закључак

У овој јединици видели смо да се RNN-ови могу користити за класификацију секвенци, али у ствари, они могу обрађивати много више задатака, као што су генерисање текста, машински превод и други. Те задатке ћемо разматрати у следећој јединици.

## 🚀 Изазов

Прочитајте неку литературу о LSTM-овима и размислите о њиховим применама:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Квиз после предавања](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Преглед и самостално учење

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) аутора Кристофера Олаха.

## [Задатак: Нотебукови](assignment.md)

---

