<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:51:20+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "sr"
}
-->
# Представљање текста као тензора

## [Квиз пре предавања](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Класификација текста

Током првог дела овог одељка, фокусираћемо се на задатак **класификације текста**. Користићемо [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) скуп података, који садржи вести попут следећих:

* Категорија: Наука/Технологија  
* Наслов: Компанија из Кентакија добила грант за проучавање пептида (AP)  
* Тело: AP - Компанија коју је основао истраживач хемије са Универзитета у Луисвилу добила је грант за развој...

Наш циљ ће бити да класификујемо вест у једну од категорија на основу текста.

## Представљање текста

Ако желимо да решавамо задатке обраде природног језика (NLP) помоћу неуронских мрежа, потребан нам је начин да представимо текст као тензоре. Рачунари већ представљају текстуалне карактере као бројеве који се мапирају на фонтове на вашем екрану користећи кодирања као што су ASCII или UTF-8.

<img alt="Слика која приказује дијаграм мапирања карактера на ASCII и бинарну репрезентацију" src="images/ascii-character-map.png" width="50%"/>

> [Извор слике](https://www.seobility.net/en/wiki/ASCII)

Људи разумеју шта сваки карактер **представља** и како се сви карактери спајају да би формирали речи у реченици. Међутим, рачунари сами по себи немају такво разумевање, и неуронска мрежа мора да научи значење током тренинга.

Због тога можемо користити различите приступе за представљање текста:

* **Репрезентација на нивоу карактера**, где текст представљамо тако што сваки карактер третирамо као број. Ако имамо *C* различитих карактера у нашем корпусу текста, реч *Hello* би била представљена као 5x*C* тензор. Сваки карактер би одговарао колони тензора у one-hot кодирању.  
* **Репрезентација на нивоу речи**, где креирамо **речник** свих речи у тексту и затим представљамо речи користећи one-hot кодирање. Овај приступ је донекле бољи, јер сваки карактер сам по себи нема много значења, па коришћењем семантички вишег нивоа - речи - поједностављујемо задатак за неуронску мрежу. Међутим, због велике величине речника, морамо се носити са високо-димензионалним ретким тензорима.

Без обзира на репрезентацију, прво морамо конвертовати текст у секвенцу **токена**, где је један токен или карактер, реч, или понекад чак део речи. Затим, токен конвертујемо у број, обично користећи **речник**, и тај број се може унети у неуронску мрежу користећи one-hot кодирање.

## N-грамови

У природном језику, прецизно значење речи може се одредити само у контексту. На пример, значења *неуронска мрежа* и *риболовна мрежа* су потпуно различита. Један од начина да се ово узме у обзир је изградња модела на основу парова речи, третирајући парове речи као засебне токене у речнику. На овај начин, реченица *Волим да идем у риболов* би била представљена следећом секвенцом токена: *Волим да*, *да идем*, *идем у*, *у риболов*. Проблем са овим приступом је што величина речника значајно расте, а комбинације попут *идем у риболов* и *идем у куповину* су представљене различитим токенима, који не деле никакву семантичку сличност упркос истом глаголу.

У неким случајевима, можемо размотрити коришћење три-грамова -- комбинација од три речи -- такође. Због тога се овај приступ често назива **n-грамови**. Такође, има смисла користити n-грамове са репрезентацијом на нивоу карактера, у ком случају n-грамови отприлике одговарају различитим слоговима.

## Bag-of-Words и TF/IDF

Када решавамо задатке попут класификације текста, потребно је да текст представимо једним вектором фиксне величине, који ћемо користити као улаз за финални густ класификатор. Један од најједноставнијих начина за то је комбиновање свих појединачних репрезентација речи, нпр. њиховим сабирањем. Ако саберемо one-hot кодирања сваке речи, добићемо вектор фреквенција, који показује колико се пута свака реч појављује у тексту. Оваква репрезентација текста се назива **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Слика аутора

BoW у суштини представља које речи се појављују у тексту и у којим количинама, што заиста може бити добар показатељ о чему се текст ради. На пример, чланак о политици вероватно садржи речи као што су *председник* и *држава*, док би научна публикација имала нешто попут *колајдер*, *откривено*, итд. Стога, фреквенције речи могу у многим случајевима бити добар индикатор садржаја текста.

Проблем са BoW је што се одређене уобичајене речи, као што су *и*, *је*, итд. појављују у већини текстова и имају највише фреквенције, прикривајући речи које су заиста важне. Можемо смањити значај тих речи узимајући у обзир фреквенцију појављивања речи у целокупној колекцији докумената. Ово је главна идеја иза TF/IDF приступа, који је детаљније обрађен у нотебуцима приложеним уз ову лекцију.

Међутим, ниједан од ових приступа не може у потпуности узети у обзир **семантику** текста. За то су нам потребни моћнији модели неуронских мрежа, о којима ћемо говорити касније у овом одељку.

## ✍️ Вежбе: Репрезентација текста

Наставите своје учење у следећим нотебуцима:

* [Репрезентација текста са PyTorch-ом](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Репрезентација текста са TensorFlow-ом](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## Закључак

До сада смо проучавали технике које могу додати тежину фреквенције различитим речима. Међутим, оне нису у стању да представе значење или редослед. Како је познати лингвиста Џ. Р. Фирт рекао 1935. године: "Потпуно значење речи је увек контекстуално, и ниједно проучавање значења ван контекста не може се озбиљно схватити." Касније ћемо у курсу научити како да ухватимо контекстуалне информације из текста користећи моделирање језика.

## 🚀 Изазов

Испробајте неке друге вежбе користећи bag-of-words и различите моделе података. Можда ћете бити инспирисани овим [такмичењем на Kaggle-у](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Квиз после предавања](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Преглед и самостално учење

Вежбајте своје вештине са текстуалним уграђивањима и техникама bag-of-words на [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Задатак: Нотебуци](assignment.md)

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.