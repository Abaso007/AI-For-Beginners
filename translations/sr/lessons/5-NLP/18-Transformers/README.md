<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f335dfcb4a993920504c387973a36957",
  "translation_date": "2025-09-23T14:46:35+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "sr"
}
-->
# Механизми пажње и Трансформери

## [Квиз пре предавања](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Један од најважнијих проблема у области обраде природног језика (NLP) је **машински превод**, суштински задатак који је основа алата као што је Google Translate. У овом делу ћемо се фокусирати на машински превод, или, уопштеније, на било који задатак *секвенца-у-секвенцу* (који се такође назива **трансдукција реченица**).

Са РНМ-овима (RNNs), секвенца-у-секвенцу се имплементира помоћу две рекурентне мреже, где једна мрежа, **енкодер**, сабија улазну секвенцу у скривено стање, док друга мрежа, **декодер**, развија то скривено стање у преведени резултат. Постоји неколико проблема са овим приступом:

* Крајње стање енкодер мреже тешко памти почетак реченице, што доводи до слабијег квалитета модела за дуге реченице.
* Све речи у секвенци имају исти утицај на резултат. У стварности, међутим, одређене речи у улазној секвенци често имају већи утицај на излазну секвенцу од других.

**Механизми пажње** пружају начин за одређивање тежине контекстуалног утицаја сваког улазног вектора на сваку предикцију излазног РНМ-а. Ово се имплементира стварањем пречица између међустојања улазног РНМ-а и излазног РНМ-а. На овај начин, када генеришемо излазни симбол y<sub>t</sub>, узимамо у обзир сва улазна скривена стања h<sub>i</sub>, са различитим тежинским коефицијентима &alpha;<sub>t,i</sub>.

![Слика која приказује енкодер/декодер модел са адитивним слојем пажње](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.sr.png)

> Енкодер-декодер модел са адитивним механизмом пажње у [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитиран из [овог блога](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Матрица пажње {&alpha;<sub>i,j</sub>} представља степен у којем одређене улазне речи утичу на генерисање одређене речи у излазној секвенци. Испод је пример такве матрице:

![Слика која приказује пример поравнања пронађеног помоћу RNNsearch-50, преузета из Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.sr.png)

> Фигура из [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Сл.3)

Механизми пажње су одговорни за велики део тренутног или скоро тренутног стања уметности у NLP-у. Међутим, додавање пажње значајно повећава број параметара модела, што је довело до проблема са скалирањем РНМ-ова. Кључно ограничење скалирања РНМ-ова је то што рекурентна природа модела отежава груписање и паралелизацију тренинга. У РНМ-у сваки елемент секвенце мора бити обрађен редоследно, што значи да се не може лако паралелизовати.

![Енкодер Декодер са Пажњом](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Фигура из [Google-овог блога](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Усвајање механизама пажње у комбинацији са овим ограничењем довело је до стварања садашњих трансформер модела, као што су BERT и Open-GPT3, који представљају стање уметности.

## Трансформер модели

Једна од главних идеја иза трансформера је избегавање секвенцијалне природе РНМ-ова и стварање модела који је паралелизован током тренинга. Ово се постиже имплементацијом две идеје:

* позиционо кодирање
* коришћење механизма само-пажње за хватање образаца уместо РНМ-ова (или ЦНМ-ова) (због чега се рад који уводи трансформере зове *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Позиционо кодирање/уграђивање

Идеја позиционог кодирања је следећа:  
1. Када користимо РНМ-ове, релативна позиција токена је представљена бројем корака и стога не мора бити експлицитно представљена.  
2. Међутим, када пређемо на пажњу, морамо знати релативне позиције токена унутар секвенце.  
3. Да бисмо добили позиционо кодирање, проширујемо нашу секвенцу токена са секвенцом позиција токена у секвенци (тј. секвенцом бројева 0, 1, ...).  
4. Затим мешамо позицију токена са вектором уграђивања токена. Да бисмо трансформисали позицију (цео број) у вектор, можемо користити различите приступе:

* Уграђивање које се тренира, слично уграђивању токена. Ово је приступ који овде разматрамо. Примењујемо слојеве уграђивања на токене и њихове позиције, што резултира векторима уграђивања истих димензија, које затим сабирамо.
* Фиксна функција позиционог кодирања, како је предложено у оригиналном раду.

<img src="images/pos-embedding.png" width="50%"/>

> Слика аутора

Резултат који добијамо са позиционим уграђивањем укључује и оригинални токен и његову позицију унутар секвенце.

### Вишеглава само-пажња

Следеће, потребно је ухватити неке обрасце унутар наше секвенце. Да бисмо то урадили, трансформери користе механизам **само-пажње**, који је у суштини пажња примењена на исту секвенцу као улаз и излаз. Примена само-пажње омогућава нам да узмемо у обзир **контекст** унутар реченице и видимо које речи су међусобно повезане. На пример, омогућава нам да видимо на које речи се односе кореференце, као што је *оно*, и такође узмемо у обзир контекст:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.sr.png)

> Слика из [Google блога](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

У трансформерима користимо **вишеглаву пажњу** како бисмо мрежи дали моћ да ухвати неколико различитих типова зависности, нпр. дугорочне наспрам краткорочних односа између речи, кореференце наспрам нечег другог итд.

[TensorFlow Notebook](TransformersTF.ipynb) садржи више детаља о имплементацији слојева трансформера.

### Пажња између енкодера и декодера

У трансформерима, пажња се користи на два места:

* За хватање образаца унутар улазног текста помоћу само-пажње
* За извођење превођења секвенци - то је слој пажње између енкодера и декодера.

Пажња између енкодера и декодера је веома слична механизму пажње који се користи у РНМ-овима, како је описано на почетку овог дела. Ова анимирана дијаграм објашњава улогу пажње између енкодера и декодера.

![Анимирани ГИФ који приказује како се процене изводе у трансформер моделима.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Пошто се свака улазна позиција независно мапира на сваку излазну позицију, трансформери могу боље паралелизовати од РНМ-ова, што омогућава много веће и изражајније језичке моделе. Свака глава пажње може се користити за учење различитих односа између речи, што побољшава задатке обраде природног језика.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) је веома велика трансформер мрежа са више слојева, са 12 слојева за *BERT-base* и 24 за *BERT-large*. Модел се прво претходно тренира на великом корпусу текстуалних података (Wikipedia + књиге) користећи ненадгледано учење (предвиђање маскираних речи у реченици). Током претходног тренинга, модел апсорбује значајне нивое разумевања језика, који се затим могу искористити са другим скуповима података кроз фино подешавање. Овај процес се назива **трансфер учење**.

![слика са http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.sr.png)

> Слика [извор](http://jalammar.github.io/illustrated-bert/)

## ✍️ Вежбе: Трансформери

Наставите са учењем у следећим бележницама:

* [Трансформери у PyTorch-у](TransformersPyTorch.ipynb)
* [Трансформери у TensorFlow-у](TransformersTF.ipynb)

## Закључак

У овој лекцији сте научили о трансформерима и механизмима пажње, свим суштинским алатима у NLP алатници. Постоји много варијација трансформер архитектура, укључујући BERT, DistilBERT, BigBird, OpenGPT3 и друге, које се могу фино подесити. Пакет [HuggingFace](https://github.com/huggingface/) пружа репозиторијум за тренирање многих од ових архитектура са PyTorch-ом и TensorFlow-ом.

## 🚀 Изазов

## [Квиз након предавања](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Преглед и самостално учење

* [Блог пост](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), који објашњава класичан рад [Attention is all you need](https://arxiv.org/abs/1706.03762) о трансформерима.
* [Серија блог постова](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) о трансформерима, која детаљно објашњава архитектуру.

## [Задатак](assignment.md)

---

