<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-25T23:32:22+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "sr"
}
-->
# Дубоко појачано учење

Појачано учење (RL) се сматра једним од основних парадигми машинског учења, поред надгледаног и ненадгледаног учења. Док се у надгледаном учењу ослањамо на скуп података са познатим резултатима, RL се заснива на **учењу кроз рад**. На пример, када први пут видимо компјутерску игру, почињемо да играмо, чак и без познавања правила, и убрзо побољшавамо своје вештине кроз сам процес играња и прилагођавања понашања.

## [Квиз пре предавања](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Да бисмо спровели RL, потребно је:

* **Окружење** или **симулатор** који поставља правила игре. Требало би да можемо да изводимо експерименте у симулатору и посматрамо резултате.
* Нека **функција награде**, која указује на то колико је наш експеримент био успешан. У случају учења играња компјутерске игре, награда би била наш коначни резултат.

На основу функције награде, требало би да будемо у могућности да прилагодимо своје понашање и побољшамо своје вештине, тако да следећи пут играмо боље. Главна разлика између других типова машинског учења и RL-а је у томе што у RL-у обично не знамо да ли побеђујемо или губимо све док не завршимо игру. Стога, не можемо рећи да ли је одређени потез сам по себи добар или не - награду добијамо тек на крају игре.

Током RL-а, обично изводимо много експеримената. Током сваког експеримента, морамо балансирати између праћења оптималне стратегије коју смо до сада научили (**експлоатација**) и истраживања нових могућих стања (**истраживање**).

## OpenAI Gym

Одличан алат за RL је [OpenAI Gym](https://gym.openai.com/) - **симулационо окружење**, које може симулирати многа различита окружења, од Atari игара до физике иза балансирања шипке. То је једно од најпопуларнијих симулационих окружења за тренирање алгоритама појачаног учења, а одржава га [OpenAI](https://openai.com/).

> **Note**: Сва окружења доступна у OpenAI Gym можете видети [овде](https://gym.openai.com/envs/#classic_control).

## Балансирање шипке (CartPole)

Вероватно сте сви видели модерне уређаје за балансирање као што су *Segway* или *Гироскутери*. Они су у стању да се аутоматски балансирају прилагођавањем точкова у одговору на сигнал са акцелерометра или жироскопа. У овом одељку ћемо научити како да решимо сличан проблем - балансирање шипке. Ово је слично ситуацији када циркуски извођач треба да балансира шипку на својој руци - али ово балансирање шипке се дешава само у једној димензији.

Поједностављена верзија балансирања позната је као проблем **CartPole**. У свету CartPole-а, имамо хоризонтални клизач који се може кретати лево или десно, а циљ је балансирати вертикалну шипку на врху клизача док се он креће.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Да бисмо креирали и користили ово окружење, потребно је неколико линија Python кода:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Свака окружења се могу приступити на исти начин:
* `env.reset` започиње нови експеримент
* `env.step` изводи корак симулације. Прима **акцију** из **простора акција**, и враћа **посматрање** (из простора посматрања), као и награду и заставицу за завршетак.

У горњем примеру изводимо насумичну акцију у сваком кораку, због чега је живот експеримента веома кратак:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Циљ RL алгоритма је да обучи модел - такозвану **политику** π - која ће враћати акцију као одговор на дато стање. Такође можемо сматрати да је политика вероватносна, нпр. за било које стање *s* и акцију *a* она ће враћати вероватноћу π(*a*|*s*) да треба да предузмемо *a* у стању *s*.

## Алгоритам градијента политике

Најочигледнији начин за моделирање политике је креирање неуронске мреже која ће узимати стања као улаз и враћати одговарајуће акције (или тачније вероватноће свих акција). У извесном смислу, то би било слично нормалном задатку класификације, са једном великом разликом - унапред не знамо које акције треба да предузмемо у сваком кораку.

Идеја је овде да проценимо те вероватноће. Градимо вектор **кумулативних награда** који показује нашу укупну награду у сваком кораку експеримента. Такође примењујемо **дисконтовање награде** множењем ранијих награда са неким коефицијентом γ=0.99, како бисмо умањили улогу ранијих награда. Затим, појачавамо оне кораке дуж путање експеримента који доносе веће награде.

> Сазнајте више о алгоритму градијента политике и погледајте га у акцији у [примеру у бележници](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Алгоритам Actor-Critic

Унапређена верзија приступа градијентима политике назива се **Actor-Critic**. Главна идеја иза овог приступа је да се неуронска мрежа обучава да врати две ствари:

* Политику, која одређује коју акцију треба предузети. Овај део се назива **глумац (actor)**.
* Процену укупне награде коју можемо очекивати у том стању - овај део се назива **критичар (critic)**.

У извесном смислу, ова архитектура подсећа на [GAN](../../4-ComputerVision/10-GANs/README.md), где имамо две мреже које се обучавају једна против друге. У моделу Actor-Critic, глумац предлаже акцију коју треба предузети, а критичар покушава да буде критичан и процени резултат. Међутим, наш циљ је да обучимо те мреже у хармонији.

Пошто знамо и стварне кумулативне награде и резултате које је критичар вратио током експеримента, релативно је лако изградити функцију губитка која ће минимизирати разлику између њих. То би нам дало **губитак критичара**. **Губитак глумца** можемо израчунати користећи исти приступ као у алгоритму градијента политике.

Након покретања једног од ових алгоритама, можемо очекивати да се наш CartPole понаша овако:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Вежбе: Градијенти политике и Actor-Critic RL

Наставите са учењем у следећим бележницама:

* [RL у TensorFlow-у](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL у PyTorch-у](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Други RL задаци

Појачано учење је данас брзорастуће поље истраживања. Неки од занимљивих примера појачаног учења су:

* Учење рачунара да игра **Atari игре**. Изазов у овом проблему је што немамо једноставно стање представљено као вектор, већ снимак екрана - и морамо користити CNN да бисмо претворили слику екрана у вектор карактеристика или извукли информације о награди. Atari игре су доступне у Gym-у.
* Учење рачунара да игра друштвене игре, као што су шах и го. Недавно су програми попут **Alpha Zero** постигли врхунске резултате тако што су обучавани од нуле, са два агента који играју један против другог и побољшавају се на сваком кораку.
* У индустрији, RL се користи за креирање система управљања из симулације. Услуга звана [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) је посебно дизајнирана за то.

## Закључак

Сада смо научили како да обучимо агенте да постигну добре резултате само пружањем функције награде која дефинише жељено стање игре и давањем могућности да интелигентно истражују простор претраге. Успешно смо испробали два алгоритма и постигли добар резултат у релативно кратком временском периоду. Међутим, ово је само почетак вашег путовања у RL, и свакако би требало да размотрите похађање посебног курса ако желите дубље да истражите.

## 🚀 Изазов

Истражите апликације наведене у одељку „Други RL задаци“ и покушајте да имплементирате једну!

## [Квиз након предавања](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Преглед и самостално учење

Сазнајте више о класичном појачаном учењу у нашем [курикулуму за почетнике у машинском учењу](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Погледајте [овај одличан видео](https://www.youtube.com/watch?v=qv6UVOQ0F44) који говори о томе како рачунар може научити да игра Super Mario.

## Задатак: [Обучите Mountain Car](lab/README.md)

Ваш циљ током овог задатка биће да обучите друго Gym окружење - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.