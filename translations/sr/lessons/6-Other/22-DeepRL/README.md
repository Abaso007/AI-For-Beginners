<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T14:37:29+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "sr"
}
-->
# Дубоко појачано учење

Појачано учење (RL) се сматра једним од основних парадигми машинског учења, поред надгледаног и ненадгледаног учења. Док се у надгледаном учењу ослањамо на скуп података са познатим исходима, RL се заснива на **учењу кроз рад**. На пример, када први пут видимо компјутерску игру, почнемо да играмо, чак и без познавања правила, и убрзо успевамо да побољшамо своје вештине само кроз процес играња и прилагођавања нашег понашања.

## [Квиз пре предавања](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Да бисмо спровели RL, потребно нам је:

* **Окружење** или **симулатор** који поставља правила игре. Требало би да можемо да изводимо експерименте у симулатору и посматрамо резултате.
* Нека **функција награде**, која указује на то колико је наш експеримент био успешан. У случају учења играња компјутерске игре, награда би била наш коначни резултат.

На основу функције награде, требало би да можемо да прилагодимо своје понашање и побољшамо своје вештине, тако да следећи пут играмо боље. Главна разлика између других типова машинског учења и RL је у томе што у RL обично не знамо да ли побеђујемо или губимо све док не завршимо игру. Стога, не можемо рећи да ли је одређени потез сам по себи добар или не - награду добијамо тек на крају игре.

Током RL, обично изводимо много експеримената. Током сваког експеримента, потребно је да балансирамо између праћења оптималне стратегије коју смо до сада научили (**експлоатација**) и истраживања нових могућих стања (**експлорација**).

## OpenAI Gym

Одличан алат за RL је [OpenAI Gym](https://gym.openai.com/) - **симулационо окружење**, које може симулирати многа различита окружења, од Atari игара до физике иза балансирања шипке. То је једно од најпопуларнијих симулационих окружења за тренирање алгоритама појачаног учења, и одржава га [OpenAI](https://openai.com/).

> **Напомена**: Сва окружења доступна у OpenAI Gym можете видети [овде](https://gym.openai.com/envs/#classic_control).

## Балансирање CartPole-а

Вероватно сте сви видели модерне уређаје за балансирање као што су *Segway* или *Gyroscooters*. Они су у стању да се аутоматски балансирају прилагођавањем својих точкова као одговор на сигнал са акцелерометра или жироскопа. У овом делу ћемо научити како да решимо сличан проблем - балансирање шипке. То је слично ситуацији када циркуски извођач треба да балансира шипку на својој руци - али ово балансирање шипке се дешава само у 1Д.

Поједностављена верзија балансирања је позната као **CartPole** проблем. У свету CartPole-а, имамо хоризонтални клизач који може да се помера лево или десно, а циљ је да се вертикална шипка одржи у равнотежи на врху клизача док се он помера.

<img alt="CartPole" src="images/cartpole.png" width="200"/>

Да бисмо креирали и користили ово окружење, потребно је неколико линија Python кода:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Свака окружења се могу приступити на исти начин:
* `env.reset` започиње нови експеримент
* `env.step` изводи корак симулације. Прима **акцију** из **простора акција**, и враћа **опсервацију** (из простора опсервација), као и награду и заставицу за завршетак.

У горњем примеру изводимо насумичну акцију у сваком кораку, због чега је живот експеримента веома кратак:

![CartPole без балансирања](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Циљ RL алгоритма је да обучи модел - такозвану **политику** &pi; - која ће враћати акцију као одговор на дато стање. Такође можемо сматрати да је политика вероватносна, нпр. за било које стање *s* и акцију *a* она ће враћати вероватноћу &pi;(*a*|*s*) да треба да предузмемо *a* у стању *s*.

## Алгоритам Policy Gradients

Најочигледнији начин за моделирање политике је креирање неуронске мреже која ће узимати стања као улаз, и враћати одговарајуће акције (или боље речено вероватноће свих акција). У извесном смислу, то би било слично нормалном задатку класификације, са једном великом разликом - унапред не знамо које акције треба да предузмемо у сваком кораку.

Идеја је овде да проценимо те вероватноће. Градимо вектор **кумулативних награда** који показује нашу укупну награду у сваком кораку експеримента. Такође примењујемо **дисконтовање награда** множењем ранијих награда са неким коефицијентом &gamma;=0.99, како бисмо умањили улогу ранијих награда. Затим, ојачавамо оне кораке дуж пута експеримента који доносе веће награде.

> Сазнајте више о алгоритму Policy Gradient и погледајте га у акцији у [примеру у нотебуку](CartPole-RL-TF.ipynb).

## Алгоритам Actor-Critic

Побољшана верзија приступа Policy Gradients назива се **Actor-Critic**. Главна идеја иза овог алгоритма је да би неуронска мрежа требало да буде обучена да враћа две ствари:

* Политику, која одређује коју акцију треба предузети. Овај део се назива **глумац** (actor).
* Процену укупне награде коју можемо очекивати у овом стању - овај део се назива **критичар** (critic).

У извесном смислу, ова архитектура подсећа на [GAN](../../4-ComputerVision/10-GANs/README.md), где имамо две мреже које се обучавају једна против друге. У моделу Actor-Critic, глумац предлаже акцију коју треба предузети, а критичар покушава да буде критичан и процени резултат. Међутим, наш циљ је да обучимо те мреже у хармонији.

Пошто знамо и стварне кумулативне награде и резултате које критичар враћа током експеримента, релативно је лако изградити функцију губитка која ће минимизирати разлику између њих. То нам даје **губитак критичара**. Можемо израчунати **губитак глумца** користећи исти приступ као у алгоритму Policy Gradient.

Након покретања једног од ових алгоритама, можемо очекивати да наш CartPole изгледа овако:

![CartPole у равнотежи](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Вежбе: Policy Gradients и Actor-Critic RL

Наставите своје учење у следећим нотебуцима:

* [RL у TensorFlow-у](CartPole-RL-TF.ipynb)
* [RL у PyTorch-у](CartPole-RL-PyTorch.ipynb)

## Други RL задаци

Појачано учење данас је брзо растуће поље истраживања. Неки од занимљивих примера појачаног учења су:

* Учење компјутера да игра **Atari игре**. Изазовни део овог проблема је што немамо једноставно стање представљено као вектор, већ снимак екрана - и морамо користити CNN да бисмо конвертовали ову слику екрана у вектор карактеристика или извукли информације о награди. Atari игре су доступне у Gym-у.
* Учење компјутера да игра друштвене игре, као што су Шах и Го. Недавно су програми попут **Alpha Zero** достигли врхунске резултате тако што су обучени од нуле, са два агента који играју један против другог и побољшавају се у сваком кораку.
* У индустрији, RL се користи за креирање система контроле из симулације. Услуга под називом [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) је посебно дизајнирана за то.

## Закључак

Сада смо научили како да обучимо агенте да постигну добре резултате само пружањем функције награде која дефинише жељено стање игре и давањем могућности да интелигентно истражују простор претраге. Успешно смо испробали два алгоритма и постигли добар резултат у релативно кратком временском периоду. Међутим, ово је само почетак вашег путовања у RL, и свакако би требало да размислите о похађању посебног курса ако желите да дубље истражите.

## 🚀 Изазов

Истражите апликације наведене у одељку 'Други RL задаци' и покушајте да имплементирате једну!

## [Квиз после предавања](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Преглед и самостално учење

Сазнајте више о класичном појачаном учењу у нашем [курикулуму Машинско учење за почетнике](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Погледајте [овај одличан видео](https://www.youtube.com/watch?v=qv6UVOQ0F44) који говори о томе како компјутер може научити да игра Super Mario.

## Задатак: [Обучите Mountain Car](lab/README.md)

Ваш циљ током овог задатка биће да обучите друго Gym окружење - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

