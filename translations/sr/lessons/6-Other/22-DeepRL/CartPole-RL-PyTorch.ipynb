{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренирање RL за балансирање Cartpole-а\n",
    "\n",
    "Овај нотебук је део [AI for Beginners Curriculum](http://aka.ms/ai-beginners). Инспирисан је [званичним PyTorch туторијалом](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) и [овом имплементацијом Cartpole-а у PyTorch-у](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "У овом примеру, користићемо RL да обучимо модел за балансирање шипке на колицима која могу да се крећу лево и десно на хоризонталној скали. Користићемо окружење [OpenAI Gym](https://www.gymlibrary.ml/) за симулацију шипке.\n",
    "\n",
    "> **Напомена**: Код из овог часа можете покренути локално (нпр. из Visual Studio Code-а), у ком случају ће се симулација отворити у новом прозору. Приликом покретања кода онлајн, можда ће бити потребно да направите неке измене у коду, као што је описано [овде](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Почећемо тако што ћемо се уверити да је Gym инсталиран:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сада ћемо креирати окружење CartPole и видети како да радимо са њим. Окружење има следеће особине:\n",
    "\n",
    "* **Простор акција** је скуп могућих акција које можемо извршити у сваком кораку симулације  \n",
    "* **Простор опажања** је простор опажања која можемо направити  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хајде да видимо како симулација функционише. Следећа петља покреће симулацију све док `env.step` не врати заставицу за завршетак `done`. Насумично ћемо бирати акције користећи `env.action_space.sample()`, што значи да ће експеримент вероватно врло брзо пропасти (CartPole окружење се завршава када брзина CartPole-а, његова позиција или угао пређу одређене границе).\n",
    "\n",
    "> Симулација ће се отворити у новом прозору. Можете покренути код више пута и видети како се понаша.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можете приметити да запажања садрже 4 броја. То су:\n",
    "- Позиција колица\n",
    "- Брзина колица\n",
    "- Угао шипке\n",
    "- Стопа ротације шипке\n",
    "\n",
    "`rew` је награда коју добијамо на сваком кораку. У окружењу CartPole добијате 1 поен за сваки корак симулације, а циљ је максимизовати укупну награду, односно време током којег CartPole може да одржи равнотежу без пада.\n",
    "\n",
    "Током учења путем појачања, наш циљ је да обучимо **политику** $\\pi$, која ће нам за свако стање $s$ рећи коју акцију $a$ треба да предузмемо, што у суштини значи $a = \\pi(s)$.\n",
    "\n",
    "Ако желите вероватносно решење, можете замислити политику као скуп вероватноћа за сваку акцију, односно $\\pi(a|s)$ би значило вероватноћу да треба да предузмемо акцију $a$ у стању $s$.\n",
    "\n",
    "## Метод градијента политике\n",
    "\n",
    "У најједноставнијем RL алгоритму, који се зове **Градијент политике**, обучићемо неуронску мрежу да предвиди следећу акцију.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми ћемо тренирати мрежу извођењем многих експеримената и ажурирањем наше мреже након сваког извођења. Хајде да дефинишемо функцију која ће изводити експеримент и враћати резултате (тзв. **траг**) - сва стања, акције (и њихове препоручене вероватноће) и награде:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можете покренути једну епизоду са необученом мрежом и приметити да је укупна награда (позната и као дужина епизоде) веома ниска:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Један од изазовних аспеката алгоритма градијента политике је коришћење **дисконтованих награда**. Идеја је да израчунамо вектор укупних награда на сваком кораку игре, и током тог процеса дисконтоваћемо ране награде користећи неки коефицијент $gamma$. Такође нормализујемо резултујући вектор, јер ћемо га користити као тежину да утичемо на нашу обуку:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хајде да започнемо са правим тренингом! Извршићемо 300 епизода, а у свакој епизоди ћемо урадити следеће:\n",
    "\n",
    "1. Покренути експеримент и прикупити траг.\n",
    "2. Израчунати разлику (`градијенте`) између предузетих акција и предвиђених вероватноћа. Што је разлика мања, то смо сигурнији да смо предузели исправну акцију.\n",
    "3. Израчунати дисконтоване награде и помножити градијенте са дисконтованим наградама - то ће осигурати да кораци са већим наградама имају већи утицај на коначни резултат од оних са мањим наградама.\n",
    "4. Очекиване циљне акције за нашу неуронску мрежу делимично ће бити преузете из предвиђених вероватноћа током извршавања, а делимично из израчунатих градијената. Користићемо параметар `alpha` да одредимо у којој мери се градијенти и награде узимају у обзир - ово се назива *стопа учења* алгоритма за појачано учење.\n",
    "5. На крају, тренирамо нашу мрежу на основама стања и очекиваних акција, и понављамо процес.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сада хајде да покренемо епизоду са рендеровањем да видимо резултат:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надамо се да сада можете видети да штап прилично добро одржава равнотежу!\n",
    "\n",
    "## Модел Актор-Критичар\n",
    "\n",
    "Модел Актор-Критичар представља даљи развој метода градијената политике, у коме градимо неуронску мрежу која учи и политику и процењене награде. Мрежа ће имати два излаза (или се може посматрати као две одвојене мреже):\n",
    "* **Актор** ће препоручивати коју акцију треба предузети тако што ће нам дати расподелу вероватноће стања, као у моделу градијента политике.\n",
    "* **Критичар** ће процењивати каква би награда могла бити од тих акција. Он враћа укупно процењене награде у будућности за дато стање.\n",
    "\n",
    "Хајде да дефинишемо такав модел:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Морали бисмо мало изменити наше функције `discounted_rewards` и `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сада ћемо покренути главну петљу за тренирање. Користићемо процес ручног тренирања мреже рачунањем одговарајућих функција губитка и ажурирањем параметара мреже:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Закључак\n",
    "\n",
    "У овом демо примеру видели смо два алгоритма за учење путем појачања: једноставан градијент политике и сложенији актер-критичар. Можете приметити да ти алгоритми раде са апстрактним појмовима као што су стање, акција и награда - што значи да се могу применити у веома различитим окружењима.\n",
    "\n",
    "Учење путем појачања нам омогућава да научимо најбољу стратегију за решавање проблема само на основу посматрања коначне награде. Чињеница да нам нису потребни означени скупови података омогућава нам да више пута понављамо симулације како бисмо оптимизовали наше моделе. Ипак, још увек постоји много изазова у области учења путем појачања, које можете истражити ако одлучите да се више посветите овом занимљивом делу вештачке интелигенције.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Одрицање од одговорности**:  \nОвај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако настојимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T23:00:15+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "sr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}