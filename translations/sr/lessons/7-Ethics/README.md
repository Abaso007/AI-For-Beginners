<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "437c988596e751072e41a5aad3fcc5d9",
  "translation_date": "2025-08-25T21:25:21+00:00",
  "source_file": "lessons/7-Ethics/README.md",
  "language_code": "sr"
}
-->
# Етичка и одговорна вештачка интелигенција

Скоро сте завршили овај курс, и надам се да сада јасно видите да је вештачка интелигенција заснована на низу формалних математичких метода који нам омогућавају да пронађемо односе у подацима и обучимо моделе да реплицирају неке аспекте људског понашања. У овом тренутку историје, сматрамо да је вештачка интелигенција веома моћан алат за извлачење образаца из података и примену тих образаца за решавање нових проблема.

## [Квиз пре предавања](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5/)

Међутим, у научној фантастици често видимо приче у којима вештачка интелигенција представља опасност за човечанство. Обично су те приче усредсређене на неку врсту побуне вештачке интелигенције, када она одлучи да се супротстави људима. Ово имплицира да вештачка интелигенција има неку врсту емоција или може доносити одлуке које њени програмери нису предвидели.

Врста вештачке интелигенције коју смо изучавали у овом курсу није ништа више од велике матричне аритметике. То је веома моћан алат који нам помаже да решимо наше проблеме, и као сваки други моћан алат - може се користити у добре и лоше сврхе. Важно је напоменути да може бити *злоупотребљена*.

## Принципи одговорне вештачке интелигенције

Да бисмо избегли ову случајну или намерну злоупотребу вештачке интелигенције, Microsoft је дефинисао важне [Принципе одговорне вештачке интелигенције](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-77998-cacaste). Следећи концепти су основа ових принципа:

* **Правичност** је повезана са важним проблемом *пристрасности модела*, која може настати коришћењем пристрасних података за обуку. На пример, када покушавамо да предвидимо вероватноћу да нека особа добије посао програмера, модел ће вероватно дати већу предност мушкарцима - само зато што је скуп података за обуку вероватно био пристрасан према мушкој популацији. Морамо пажљиво балансирати податке за обуку и истражити модел како бисмо избегли пристрасности и осигурали да модел узима у обзир релевантније карактеристике.
* **Поузданост и безбедност**. По својој природи, модели вештачке интелигенције могу правити грешке. Неуронска мрежа враћа вероватноће, и то морамо узети у обзир приликом доношења одлука. Сваки модел има одређену прецизност и осетљивост, и то морамо разумети како бисмо спречили штету коју погрешан савет може изазвати.
* **Приватност и безбедност** имају неке специфичне импликације у контексту вештачке интелигенције. На пример, када користимо неке податке за обуку модела, ти подаци на неки начин постају "интегрисани" у модел. С једне стране, то повећава безбедност и приватност, али с друге стране - морамо се сетити који су подаци коришћени за обуку модела.
* **Инклузивност** значи да не градимо вештачку интелигенцију да замени људе, већ да их унапреди и учини наш рад креативнијим. Ово је такође повезано са правичношћу, јер када се бавимо недовољно заступљеним заједницама, већина скупова података које прикупљамо вероватно ће бити пристрасна, и морамо осигурати да те заједнице буду укључене и правилно обрађене од стране вештачке интелигенције.
* **Транспарентност**. Ово укључује осигурање да увек јасно ставимо до знања да се користи вештачка интелигенција. Такође, где год је могуће, желимо да користимо системе вештачке интелигенције који су *интерпретабилни*.
* **Одговорност**. Када модели вештачке интелигенције доносе неке одлуке, није увек јасно ко је одговоран за те одлуке. Морамо осигурати да разумемо где лежи одговорност за одлуке вештачке интелигенције. У већини случајева желимо да укључимо људе у процес доношења важних одлука, како би стварни људи били одговорни.

## Алати за одговорну вештачку интелигенцију

Microsoft је развио [Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox) који садржи сет алата:

* Табла за интерпретабилност (InterpretML)
* Табла за правичност (FairLearn)
* Табла за анализу грешака
* Табла за одговорну вештачку интелигенцију која укључује:

   - EconML - алат за каузалну анализу, који се фокусира на питања "шта ако"
   - DiCE - алат за контрафактуалну анализу који вам омогућава да видите које карактеристике треба променити да би се утицало на одлуку модела

За више информација о етици вештачке интелигенције, посетите [ову лекцију](https://github.com/microsoft/ML-For-Beginners/tree/main/1-Introduction/3-fairness?WT.mc_id=academic-77998-cacaste) у оквиру курикулума машинског учења која укључује задатке.

## Преглед и самостално учење

Пратите овај [пут учења](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77998-cacaste) да бисте сазнали више о одговорној вештачкој интелигенцији.

## [Квиз након предавања](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6/)

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на изворном језику треба сматрати ауторитативним извором. За критичне информације препоручује се професионални превод од стране људи. Не сносимо одговорност за било каква погрешна тумачења или неспоразуме који могу произаћи из коришћења овог превода.