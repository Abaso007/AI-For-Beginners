<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "717775c4050ccbffbe0c961ad8bf7bf7",
  "translation_date": "2025-08-25T23:08:33+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/README.md",
  "language_code": "sr"
}
-->
# Унапред обучене мреже и трансфер учење

Обучавање CNN-а може да траје дуго и захтева много података. Међутим, велики део времена се троши на учење најбољих ниско-нивоских филтера које мрежа може да користи за издвајање образаца из слика. Поставља се природно питање - можемо ли користити неуронску мрежу обучену на једном скупу података и прилагодити је за класификацију различитих слика без потребе за комплетним процесом обучавања?

## [Квиз пре предавања](https://ff-quizzes.netlify.app/en/ai/quiz/15)

Овај приступ се назива **трансфер учење**, јер преносимо одређено знање са једног модела неуронске мреже на други. У трансфер учењу, обично почињемо са унапред обученим моделом, који је обучен на неком великом скупу слика, као што је **ImageNet**. Ти модели већ добро раде на издвајању различитих карактеристика из општих слика, а у многим случајевима само изградња класификатора на основу тих издвојених карактеристика може дати добар резултат.

> ✅ Трансфер учење је термин који се користи и у другим академским областима, као што је образовање. Односи се на процес узимања знања из једне области и његову примену у другој.

## Унапред обучени модели као екстрактори карактеристика

Конволуционе мреже о којима смо говорили у претходном делу садрже бројне слојеве, од којих је сваки намењен издвајању одређених карактеристика из слике, почевши од ниско-нивоских комбинација пиксела (као што су хоризонталне/вертикалне линије или потези), па до виших комбинација карактеристика које одговарају стварима као што је око пламена. Ако обучимо CNN на довољно великом скупу општих и разноврсних слика, мрежа би требало да научи да издваја те уобичајене карактеристике.

И Keras и PyTorch садрже функције за лако учитавање унапред обучених тежина неуронских мрежа за неке уобичајене архитектуре, од којих је већина обучена на ImageNet сликама. Најчешће коришћене описане су на страници [Архитектуре CNN-а](../07-ConvNets/CNN_Architectures.md) из претходног часа. Конкретно, можете размотрити коришћење једног од следећих:

* **VGG-16/VGG-19**, који су релативно једноставни модели, али и даље дају добру тачност. Често је коришћење VGG-а као првог покушаја добар избор да се види како трансфер учење функционише.
* **ResNet** је породица модела коју је предложио Microsoft Research 2015. године. Имају више слојева, па самим тим захтевају више ресурса.
* **MobileNet** је породица модела смањене величине, погодна за мобилне уређаје. Користите их ако имате ограничене ресурсе и можете жртвовати мало тачности.

Ево примера карактеристика издвојених из слике мачке помоћу VGG-16 мреже:

![Карактеристике издвојене помоћу VGG-16](../../../../../translated_images/features.6291f9c7ba3a0b951af88fc9864632b9115365410765680680d30c927dd67354.sr.png)

## Скуп података: Мачке и пси

У овом примеру, користићемо скуп података [Мачке и пси](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), који је веома близак стварном сценарију класификације слика.

## ✍️ Вежба: Трансфер учење

Хајде да видимо трансфер учење у пракси у одговарајућим нотебуковима:

* [Трансфер учење - PyTorch](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb)
* [Трансфер учење - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningTF.ipynb)

## Визуализација идеалне мачке

Унапред обучена неуронска мрежа садржи различите обрасце у свом *мозгу*, укључујући концепте **идеалне мачке** (као и идеалног пса, идеалне зебре, итд.). Било би занимљиво некако **визуализовати ову слику**. Међутим, то није једноставно, јер су обрасци распоређени широм тежина мреже и организовани у хијерархијску структуру.

Један приступ који можемо применити је да почнемо са насумичном сликом, а затим покушамо да користимо технику **оптимизације градијентног спуштања** како бисмо прилагодили ту слику на такав начин да мрежа почне да мисли да је то мачка.

![Циклус оптимизације слике](../../../../../translated_images/ideal-cat-loop.999fbb8ff306e044f997032f4eef9152b453e6a990e449bbfb107de2493cc37e.sr.png)

Међутим, ако то урадимо, добићемо нешто веома слично насумичном шуму. То је зато што *постоји много начина да мрежа помисли да је улазна слика мачка*, укључујући неке који визуелно немају смисла. Иако те слике садрже много образаца типичних за мачку, ништа их не ограничава да буду визуелно препознатљиве.

Да бисмо побољшали резултат, можемо додати још један термин у функцију губитка, који се назива **губитак варијације**. То је метрика која показује колико су слични суседни пиксели слике. Минимизирање губитка варијације чини слику глаткијом и уклања шум - чиме открива визуелно привлачније обрасце. Ево примера таквих "идеалних" слика, које су класификоване као мачка и као зебра са високом вероватноћом:

![Идеална мачка](../../../../../translated_images/ideal-cat.203dd4597643d6b0bd73038b87f9c0464322725e3a06ab145d25d4a861c70592.sr.png) | ![Идеална зебра](../../../../../translated_images/ideal-zebra.7f70e8b54ee15a7a314000bb5df38a6cfe086ea04d60df4d3ef313d046b98a2b.sr.png)
-----|-----
 *Идеална мачка* | *Идеална зебра*

Сличан приступ може се користити за извођење такозваних **адверзаријалних напада** на неуронску мрежу. Претпоставимо да желимо да преваримо неуронску мрежу и учинимо да пас изгледа као мачка. Ако узмемо слику пса, коју мрежа препознаје као пса, можемо је мало изменити помоћу оптимизације градијентног спуштања, док мрежа не почне да је класификује као мачку:

![Слика пса](../../../../../translated_images/original-dog.8f68a67d2fe0911f33041c0f7fce8aa4ea919f9d3917ec4b468298522aeb6356.sr.png) | ![Слика пса класификована као мачка](../../../../../translated_images/adversarial-dog.d9fc7773b0142b89752539bfbf884118de845b3851c5162146ea0b8809fc820f.sr.png)
-----|-----
*Оригинална слика пса* | *Слика пса класификована као мачка*

Погледајте код за репродукцију горе наведених резултата у следећем нотебуку:

* [Идеална и адверзаријална мачка - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/AdversarialCat_TF.ipynb)

## Закључак

Коришћењем трансфер учења, можете брзо саставити класификатор за задатак класификације прилагођених објеката и постићи високу тачност. Можете видети да сложенији задаци које сада решавамо захтевају већу рачунарску снагу и не могу се лако решити на CPU-у. У следећој јединици, покушаћемо да користимо лакшу имплементацију за обучавање истог модела користећи мање ресурсе, што резултира само мало нижом тачношћу.

## 🚀 Изазов

У пратећим нотебуцима, на дну се налазе белешке о томе како трансфер знања најбоље функционише са донекле сличним подацима за обучавање (можда нова врста животиње). Урадите експерименте са потпуно новим типовима слика да видите колико добро или лоше ваши модели трансфер знања функционишу.

## [Квиз после предавања](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Преглед и самостално учење

Прочитајте [TrainingTricks.md](TrainingTricks.md) да бисте продубили своје знање о неким другим начинима обучавања модела.

## [Задатак](lab/README.md)

У овом лабораторијском раду, користићемо стварни [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) скуп података о кућним љубимцима са 35 раса мачака и паса, и направићемо класификатор за трансфер учење.

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.