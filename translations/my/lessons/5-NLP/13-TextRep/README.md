<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:53:28+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "my"
}
-->
# စာသားကို Tensor အဖြစ် ကိုယ်စားပြုခြင်း

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## စာသား အမျိုးအစားခွဲခြားခြင်း

ဤအပိုင်း၏ ပထမပိုင်းတစ်လျှောက်လုံးတွင် **စာသား အမျိုးအစားခွဲခြားခြင်း** လုပ်ငန်းကို အဓိကထားဆောင်ရွက်မည်ဖြစ်သည်။ ဤအတွက် [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) Dataset ကို အသုံးပြုမည်ဖြစ်ပြီး၊ ၎င်းတွင် အောက်ပါအတိုင်း သတင်းဆောင်းပါးများ ပါဝင်သည်-

* အမျိုးအစား - Sci/Tech  
* ခေါင်းစဉ် - Ky. Company Wins Grant to Study Peptides (AP)  
* အကြောင်းအရာ - AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop...  

ကျွန်ုပ်တို့၏ ရည်မှန်းချက်မှာ စာသားအပေါ်အခြေခံပြီး သတင်းအကြောင်းအရာကို အမျိုးအစားတစ်ခုသို့ ခွဲခြားခြင်းဖြစ်သည်။

## စာသားကို ကိုယ်စားပြုခြင်း

Neural Network များဖြင့် သဘာဝဘာသာစကား အ処理 (NLP) လုပ်ငန်းများကို ဖြေရှင်းလိုပါက၊ စာသားကို Tensor အဖြစ် ကိုယ်စားပြုရန် နည်းလမ်းတစ်ခုလိုအပ်သည်။ ကွန်ပျူတာများသည် ASCII သို့မဟုတ် UTF-8 ကဲ့သို့သော Encoding များကို အသုံးပြု၍ စာသားအက္ခရာများကို နံပါတ်များအဖြစ် ရှိပြီးသားဖြစ်သည်။

<img alt="အက္ခရာတစ်ခုကို ASCII နှင့် binary ကိုယ်စားပြုမှုသို့ mapping ပြုလုပ်ထားသော ပုံ" src="images/ascii-character-map.png" width="50%"/>

> [ပုံရင်းအရင်းအမြစ်](https://www.seobility.net/en/wiki/ASCII)

လူသားများအနေဖြင့် အက္ခရာတစ်ခုစီ၏ **ကိုယ်စားပြုမှု** ကို နားလည်နိုင်ပြီး၊ အက္ခရာအားလုံးပေါင်းစပ်၍ စာကြောင်းတစ်ကြောင်း၏ စကားလုံးများကို ဖွဲ့စည်းပုံကို နားလည်နိုင်သည်။ သို့သော် ကွန်ပျူတာများသည် ၎င်းကို မသိနိုင်သလို၊ Neural Network သည် သင်ကြားမှုအတွင်း အဓိပ္ပါယ်ကို သင်ယူရမည်ဖြစ်သည်။

ထို့ကြောင့် စာသားကို ကိုယ်စားပြုရာတွင် အောက်ပါနည်းလမ်းများကို အသုံးပြုနိုင်သည်-

* **Character-level ကိုယ်စားပြုမှု** - စာသားကို အက္ခရာတစ်ခုစီကို နံပါတ်တစ်ခုအဖြစ် ကိုယ်စားပြုခြင်း။ စာသား corpus တွင် *C* အက္ခရာများရှိပါက၊ *Hello* စကားလုံးကို 5x*C* Tensor အဖြစ် ကိုယ်စားပြုမည်။ အက္ခရာတစ်ခုစီသည် One-hot encoding တွင် Tensor column တစ်ခုနှင့် ကိုက်ညီမည်။
* **Word-level ကိုယ်စားပြုမှု** - စာသားရှိ စကားလုံးအားလုံးကို **vocabulary** တစ်ခုအဖြစ် ဖန်တီးပြီး၊ စကားလုံးများကို One-hot encoding ဖြင့် ကိုယ်စားပြုခြင်း။ ဤနည်းလမ်းသည် ပိုမိုကောင်းမွန်သော်လည်း၊ စကားလုံးများ၏ အဓိပ္ပါယ်ကို Neural Network အတွက် လွယ်ကူစေသည်။ သို့သော် Dictionary အရွယ်အစားကြီးမားမှုကြောင့်၊ High-dimensional sparse tensors ကို ကိုင်တွယ်ရမည်။

မည်သည့် ကိုယ်စားပြုမှုကိုမဆို စတင်ရန်အတွက် စာသားကို **tokens** အစီအစဉ်တစ်ခုအဖြစ် ပြောင်းလဲရမည်။ Token တစ်ခုသည် အက္ခရာ၊ စကားလုံး သို့မဟုတ် စကားလုံး၏ အစိတ်အပိုင်းတစ်ခုဖြစ်နိုင်သည်။ ထို့နောက် Token ကို **vocabulary** အသုံးပြု၍ နံပါတ်တစ်ခုအဖြစ် ပြောင်းလဲပြီး၊ ၎င်းနံပါတ်ကို Neural Network သို့ One-hot encoding ဖြင့် ထည့်သွင်းနိုင်သည်။

## N-Grams

သဘာဝဘာသာစကားတွင် စကားလုံး၏ အဓိပ္ပါယ်ကို အကြောင်းအရာအပေါ်မူတည်၍သာ သတ်မှတ်နိုင်သည်။ ဥပမာအားဖြင့် *neural network* နှင့် *fishing network* ၏ အဓိပ္ပါယ်များမှာ လုံးဝကွဲပြားသည်။ ၎င်းကို ထည့်သွင်းစဉ်းစားရန် နည်းလမ်းတစ်ခုမှာ စကားလုံးနှစ်လုံးစီကို Vocabulary token အဖြစ် သတ်မှတ်ခြင်းဖြစ်သည်။ ဥပမာအားဖြင့် *I like to go fishing* စာကြောင်းကို *I like*, *like to*, *to go*, *go fishing* အဖြစ် Tokens အဖြစ် ကိုယ်စားပြုမည်။ သို့သော် Dictionary အရွယ်အစားကြီးမားလာပြီး၊ *go fishing* နှင့် *go shopping* ကဲ့သို့သော စကားလုံးပေါင်းစပ်မှုများသည် Semantic ဆက်စပ်မှုမရှိသော်လည်း ကွဲပြားသော Tokens အဖြစ် သတ်မှတ်မည်။

တစ်ခါတစ်ရံတွင် Tri-grams (စကားလုံးသုံးလုံးပေါင်းစပ်မှု) ကိုလည်း အသုံးပြုနိုင်သည်။ ထို့ကြောင့် ဤနည်းလမ်းကို **n-grams** ဟု ခေါ်သည်။ Character-level ကိုယ်စားပြုမှုတွင်လည်း N-grams ကို အသုံးပြုနိုင်ပြီး၊ ၎င်းသည် အများအားဖြင့် သရစဉ်များနှင့် ကိုက်ညီသည်။

## Bag-of-Words နှင့် TF/IDF

စာသား အမျိုးအစားခွဲခြားခြင်းကဲ့သို့သော လုပ်ငန်းများကို ဖြေရှင်းရာတွင်၊ စာသားကို တစ်ခုတည်းသော အရွယ်အစားရှိ Vector အဖြစ် ကိုယ်စားပြုနိုင်ရမည်။ ၎င်းကို နောက်ဆုံး Dense Classifier သို့ ထည့်သွင်းမည်ဖြစ်သည်။ အလွယ်ဆုံးနည်းလမ်းတစ်ခုမှာ စကားလုံးတစ်ခုစီ၏ ကိုယ်စားပြုမှုအားလုံးကို ပေါင်းစည်းခြင်းဖြစ်သည်။ စကားလုံးတစ်ခုစီ၏ One-hot encoding ကို ပေါင်းစည်းပါက၊ စကားလုံးတစ်ခုစီ၏ Frequency ကို ပြသသော Vector တစ်ခုရရှိမည်။ ဤကိုယ်စားပြုမှုကို **Bag of Words** (BoW) ဟု ခေါ်သည်။

<img src="images/bow.png" width="90%"/>

> ပုံရေးသားသူမှ

BoW သည် စာသားတွင် ဘယ်စကားလုံးများ ပါဝင်ပြီး၊ ၎င်းတို့၏ အရေအတွက်ကို ကိုယ်စားပြုသည်။ ဤနည်းလမ်းသည် စာသား၏ အကြောင်းအရာကို သတ်မှတ်ရန် ကောင်းမွန်သော အညွှန်းတစ်ခုဖြစ်နိုင်သည်။ ဥပမာအားဖြင့် နိုင်ငံရေးသတင်းဆောင်းပါးတွင် *president* နှင့် *country* ကဲ့သို့သော စကားလုံးများ ပါဝင်နိုင်ပြီး၊ သိပ္ပံဆောင်းပါးတွင် *collider*, *discovered* စသည်ဖြင့် ပါဝင်နိုင်သည်။

BoW ၏ ပြဿနာမှာ *and*, *is* ကဲ့သို့သော စကားလုံးများသည် စာသားအများစုတွင် ပါဝင်ပြီး၊ အမြင့်ဆုံး Frequency ရှိသဖြင့် အရေးပါသော စကားလုံးများကို ဖုံးကွယ်သွားနိုင်သည်။ ၎င်းစကားလုံးများ၏ အရေးပါမှုကို လျော့ချရန် စာတမ်းစုစုပေါင်းတွင် စကားလုံးများ ပေါ်ပေါက်မှု Frequency ကို ထည့်သွင်းစဉ်းစားရမည်။ ဤနည်းလမ်းကို TF/IDF ဟု ခေါ်ပြီး၊ ဤသင်ခန်းစာတွင် ပါဝင်သော Notebook များတွင် အသေးစိတ်ဖော်ပြထားသည်။

သို့သော် ဤနည်းလမ်းများသည် စာသား၏ **အဓိပ္ပါယ်** ကို အပြည့်အဝ ကိုယ်စားပြုနိုင်ခြင်းမရှိပါ။ ၎င်းကို Neural Network မော်ဒယ်များဖြင့်သာ ပြုလုပ်နိုင်ပြီး၊ ၎င်းကို ကျွန်ုပ်တို့သည် နောက်ပိုင်းတွင် ဆွေးနွေးမည်ဖြစ်သည်။

## ✍️ လေ့ကျင့်မှုများ - စာသားကို ကိုယ်စားပြုခြင်း

အောက်ပါ Notebook များတွင် သင်ကြားမှုကို ဆက်လက်လုပ်ဆောင်ပါ-

* [Text Representation with PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Text Representation with TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## အနှိပ်ချုပ်

ယခုအချိန်ထိ ကျွန်ုပ်တို့သည် စကားလုံးများကို Frequency အလေးချိန် ပေးနိုင်သော နည်းလမ်းများကို လေ့လာခဲ့ပါသည်။ သို့သော် ၎င်းတို့သည် အဓိပ္ပါယ် သို့မဟုတ် အစီအစဉ်ကို ကိုယ်စားပြုနိုင်ခြင်းမရှိပါ။ 1935 ခုနှစ်တွင် နာမည်ကြီး ဘာသာဗေဒပညာရှင် J. R. Firth က "စကားလုံး၏ အဓိပ္ပါယ်သည် အမြဲတမ်း အကြောင်းအရာနှင့် ဆက်စပ်ပြီး၊ အကြောင်းအရာမှ ကင်းလွတ်သော အဓိပ္ပါယ်ကို လေ့လာခြင်းသည် အလေးထားစဉ်းစားရန် မဖြစ်နိုင်ပါ" ဟု ဆိုခဲ့သည်။ ကျွန်ုပ်တို့သည် နောက်ပိုင်းသင်ခန်းစာများတွင် စာသားမှ အကြောင်းအရာဆိုင်ရာ အချက်အလက်များကို Language Modeling အသုံးပြု၍ ဖမ်းယူနည်းကို လေ့လာမည်ဖြစ်သည်။

## 🚀 စိန်ခေါ်မှု

Bag-of-Words နှင့် အခြား Data Model များကို အသုံးပြု၍ အခြားသော လေ့ကျင့်မှုများကို စမ်းသပ်ကြည့်ပါ။ ဤ [Kaggle ပြိုင်ပွဲ](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) မှ အကြံဉာဏ်ရနိုင်ပါသည်။

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## ပြန်လည်သုံးသပ်ခြင်းနှင့် ကိုယ်တိုင်လေ့လာခြင်း

[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) တွင် စာသား Embedding နှင့် Bag-of-Words နည်းလမ်းများကို လေ့ကျင့်ပါ။

## [Assignment: Notebooks](assignment.md)

**အကြောင်းကြားချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်မှုများတွင် အမှားများ သို့မဟုတ် မတိကျမှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းစာရွက်စာတမ်းကို ၎င်း၏ မူရင်းဘာသာစကားဖြင့် အာဏာတရားရှိသော အရင်းအမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များမှ ပရော်ဖက်ရှင်နယ် ဘာသာပြန်မှုကို အကြံပြုပါသည်။ ဤဘာသာပြန်မှုကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအလွတ်များ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။