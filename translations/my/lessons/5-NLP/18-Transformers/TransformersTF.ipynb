{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# အာရုံစူးစိုက်မှု မော်ကွန်းများနှင့် Transformer မော်ဒယ်များ\n",
    "\n",
    "Recurrent Network (RNN) များ၏ အဓိကအားနည်းချက်တစ်ခုမှာ စာကြောင်းအတွင်းရှိ စကားလုံးအားလုံးသည် ရလဒ်အပေါ် တူညီသော သက်ရောက်မှုရှိနေခြင်းဖြစ်သည်။ ဒီအချက်ကြောင့် Named Entity Recognition (NER) နှင့် Machine Translation (MT) ကဲ့သို့သော စဉ်ဆက်မပြတ်အလုပ်များအတွက် ပုံမှန် LSTM encoder-decoder မော်ဒယ်များတွင် စွမ်းဆောင်ရည်မပြည့်စုံမှု ဖြစ်ပေါ်စေသည်။ အမှန်တကယ်တွင် input စာကြောင်းအတွင်းရှိ စကားလုံးတစ်ချို့သည် အခြားစကားလုံးများထက် sequential output များအပေါ် သက်ရောက်မှုပိုမိုရှိသည်။\n",
    "\n",
    "Machine Translation ကဲ့သို့သော sequence-to-sequence မော်ဒယ်ကို စဉ်းစားကြည့်ပါ။ ၎င်းကို recurrent network နှစ်ခုဖြင့် အကောင်အထည်ဖော်ထားပြီး၊ network တစ်ခု (**encoder**) သည် input စာကြောင်းကို hidden state အဖြစ်သို့ ပြောင်းလဲပြီး၊ အခြား network (**decoder**) သည် ထို hidden state ကို ပြန်လည်ဖွင့်လှစ်ကာ ဘာသာပြန်ရလဒ်အဖြစ် ထုတ်ပေးသည်။ ဒီနည်းလမ်း၏ ပြဿနာမှာ network ၏ နောက်ဆုံး state သည် စာကြောင်းအစကို မှတ်မိရန် အခက်အခဲရှိပြီး၊ ဒါကြောင့် ရှည်လျားသော စာကြောင်းများတွင် မော်ဒယ်အရည်အသွေးကျဆင်းစေသည်။\n",
    "\n",
    "**အာရုံစူးစိုက်မှု မော်ကွန်းများ** သည် RNN ၏ output များကို ခန့်မှန်းရာတွင် input vector တစ်ခုချင်းစီ၏ context သက်ရောက်မှုကို အလေးပေးနိုင်စေသော နည်းလမ်းတစ်ခုဖြစ်သည်။ ၎င်းကို အကောင်အထည်ဖော်ရာတွင် input RNN ၏ အလယ်အလတ် states များနှင့် output RNN အကြား shortcut များ ဖန်တီးခြင်းဖြင့် ပြုလုပ်သည်။ ဒီနည်းလမ်းဖြင့် output symbol $y_t$ ကို ထုတ်လုပ်စဉ်တွင် input hidden states $h_i$ အားလုံးကို အလေးချိန် coefficient များ $\\alpha_{t,i}$ ဖြင့် သက်ဆိုင်စွာ ထည့်သွင်းစဉ်းစားမည်ဖြစ်သည်။\n",
    "\n",
    "![Image showing an encoder/decoder model with an additive attention layer](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.my.png)\n",
    "*Additive attention mechanism ပါဝင်သည့် encoder-decoder မော်ဒယ် ([Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)) ကို [ဒီ blog post](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) မှ ရယူထားသည်*\n",
    "\n",
    "Attention matrix $\\{\\alpha_{i,j}\\}$ သည် output စာကြောင်းအတွင်း စကားလုံးတစ်လုံးကို ဖန်တီးရာတွင် သက်ဆိုင်သော input စကားလုံးများ၏ သက်ရောက်မှုအဆင့်ကို ကိုယ်စားပြုသည်။ အောက်တွင် ထို matrix ၏ ဥပမာကို ဖော်ပြထားသည်-\n",
    "\n",
    "![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.my.png)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) မှ ရယူထားသော ပုံ]*\n",
    "\n",
    "အာရုံစူးစိုက်မှု မော်ကွန်းများသည် ယနေ့ခေတ် Natural Language Processing (NLP) တွင် အဆင့်မြင့်ဆုံး နည်းလမ်းများ၏ အခြေခံအဖြစ် အရေးပါသည်။ သို့သော် အာရုံစူးစိုက်မှုကို ထည့်သွင်းခြင်းက မော်ဒယ်၏ parameter အရေအတွက်ကို အလွန်များစေပြီး၊ RNN များတွင် scale ပြုလုပ်ရာတွင် ပြဿနာများဖြစ်ပေါ်စေသည်။ RNN မော်ဒယ်များ၏ အဓိက အားနည်းချက်မှာ စဉ်ဆက်မပြတ်သော လုပ်ဆောင်မှုကြောင့် training ကို batch နှင့် parallelize ပြုလုပ်ရန် အခက်အခဲဖြစ်စေခြင်းဖြစ်သည်။ RNN တွင် စဉ်ဆက်မပြတ်သော sequence ၏ အစိတ်အပိုင်းတစ်ခုချင်းစီကို အစဉ်လိုက် လုပ်ဆောင်ရမည်ဖြစ်ပြီး၊ ၎င်းသည် parallelize ပြုလုပ်ရန် မလွယ်ကူစေပါ။\n",
    "\n",
    "အာရုံစူးစိုက်မှု မော်ကွန်းများကို အသုံးပြုခြင်းနှင့် RNN မော်ဒယ်များ၏ အကန့်အသတ်ကြောင့် ယနေ့ခေတ်တွင် အသုံးများသော BERT မှ OpenGPT3 အထိ Transformer မော်ဒယ်များကို ဖန်တီးခဲ့သည်။\n",
    "\n",
    "## Transformer မော်ဒယ်များ\n",
    "\n",
    "Prediction တစ်ခုချင်းစီ၏ context ကို နောက်တစ်ဆင့်သို့ ပို့ပေးခြင်းအစား၊ **Transformer မော်ဒယ်များ** သည် **positional encodings** နှင့် **attention** ကို အသုံးပြုကာ ပေးထားသော စာသား window အတွင်းရှိ context ကို ဖမ်းဆီးသည်။ အောက်ပါပုံသည် positional encodings နှင့် attention ကို အသုံးပြု၍ context ကို window အတွင်း ဖမ်းဆီးပုံကို ဖော်ပြထားသည်။\n",
    "\n",
    "![Animated GIF showing how the evaluations are performed in transformer models.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Input position တစ်ခုချင်းစီကို Output position တစ်ခုချင်းစီနှင့် လွတ်လပ်စွာ mapping ပြုလုပ်နိုင်သောကြောင့် Transformer မော်ဒယ်များသည် RNN မော်ဒယ်များထက် ပိုမို parallelize ပြုလုပ်နိုင်ပြီး၊ ပိုမိုကြီးမားပြီး အထိရောက်ဆုံးသော ဘာသာစကားမော်ဒယ်များကို ဖန်တီးနိုင်စေသည်။ Attention head တစ်ခုချင်းစီသည် စကားလုံးများအကြား ဆက်စပ်မှုများကို သင်ယူရန် အသုံးပြုနိုင်ပြီး၊ ၎င်းသည် Natural Language Processing (NLP) အလုပ်များတွင် ပိုမိုကောင်းမွန်စေသည်။\n",
    "\n",
    "## ရိုးရှင်းသော Transformer မော်ဒယ် တည်ဆောက်ခြင်း\n",
    "\n",
    "Keras တွင် built-in Transformer layer မပါဝင်သော်လည်း၊ ကိုယ်တိုင် တည်ဆောက်နိုင်သည်။ ယခင်ကဲ့သို့ AG News dataset ကို အသုံးပြု၍ စာသားအမျိုးအစားခွဲခြားမှုအပေါ် အာရုံစိုက်မည်ဖြစ်သော်လည်း၊ Transformer မော်ဒယ်များသည် ပိုမိုခက်ခဲသော NLP အလုပ်များတွင် အကောင်းဆုံးရလဒ်များကို ပြသသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras တွင် layer အသစ်များသည် `Layer` class ကို subclass လုပ်ရမည်ဖြစ်ပြီး `call` method ကို implement လုပ်ရမည်။ **Positional Embedding** layer ဖြင့် စတင်ကြမည်။ [Keras documentation မှ code အချို့](https://keras.io/examples/nlp/text_classification_with_transformer/) ကို အသုံးပြုမည်။ input sequences အားလုံးကို `maxlen` အရှည်သို့ pad လုပ်ထားသည်ဟု ချက်ချင်းယူဆမည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ဤအလွှာတွင် `Embedding` အလွှာနှစ်ခုပါဝင်သည်။ Token များကို embed လုပ်ရန်အတွက် (ယခင်ကဆွေးနွေးခဲ့သည့်နည်းလမ်းဖြင့်) နှင့် token ရဲ့နေရာများကို embed လုပ်ရန်အတွက်ဖြစ်သည်။ Token ရဲ့နေရာများကို `tf.range` ကိုအသုံးပြုပြီး 0 မှ `maxlen` အထိ သဘာဝကိန်းဂဏန်းများအဖြစ်ဖန်တီးပြီးနောက် `Embedding` အလွှာထဲသို့ပို့သည်။ ထိုအလွှာမှထွက်လာသော embedding vectors နှစ်ခုကိုပေါင်းပြီး `maxlen`$\\times$`embed_dim` ပုံစံရှိသော input ၏နေရာအခြေပြု representation ကိုဖန်တီးသည်။\n",
    "\n",
    "အခုတော့ transformer block ကို implement လုပ်ကြမယ်။ ဒါဟာ ယခင်ဖော်ပြထားတဲ့ embedding layer ရဲ့ output ကိုယူပါမယ်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အခုတော့ အပြည့်အစုံသော transformer မော်ဒယ်ကို သတ်မှတ်ဖို့ အဆင်သင့်ဖြစ်ပါပြီ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Models\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) သည် *BERT-base* အတွက် အလွှာ 12 လွှာနှင့် *BERT-large* အတွက် အလွှာ 24 လွှာပါဝင်သော အလွန်ကြီးမားသော multi-layer transformer network တစ်ခုဖြစ်သည်။ ဤမော်ဒယ်ကို ပထမဦးစွာ အကြီးမားသော စာသားဒေတာများ (WikiPedia + စာအုပ်များ) ကို အသုံးပြု၍ unsupervised training (ဝါကျအတွင်းရှိ masked စကားလုံးများကို ခန့်မှန်းခြင်း) ဖြင့် pre-training ပြုလုပ်သည်။ Pre-training လုပ်စဉ်အတွင်း မော်ဒယ်သည် ဘာသာစကားနားလည်မှုအဆင့်အတန်းများကို အလွန်အမင်း စွမ်းဆောင်နိုင်စွမ်း ရရှိလာပြီး၊ ထို့နောက် အခြားသောဒေတာများနှင့်အတူ fine tuning ဖြင့် အသုံးချနိုင်သည်။ ဤလုပ်ငန်းစဉ်ကို **transfer learning** ဟုခေါ်သည်။\n",
    "\n",
    "![picture from http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.my.png)\n",
    "\n",
    "BERT, DistilBERT, BigBird, OpenGPT3 စသည်တို့အပါအဝင် Transformer architecture များ၏ အမျိုးအစားများစွာရှိပြီး၊ ထိုမော်ဒယ်များကို fine tuning ပြုလုပ်နိုင်သည်။\n",
    "\n",
    "ယခု pre-trained BERT မော်ဒယ်ကို အသုံးပြု၍ ကျွန်ုပ်တို့၏ ရိုးရာ sequence classification ပြဿနာကို ဖြေရှင်းနိုင်ပုံကို ကြည့်ကြမည်။ [တရားဝင်စာရွက်စာတမ်း](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) မှ အကြံဉာဏ်နှင့် အချို့သောကုဒ်များကို ချေးယူမည်။\n",
    "\n",
    "Pre-trained မော်ဒယ်များကို load ပြုလုပ်ရန် **Tensorflow hub** ကို အသုံးပြုမည်။ ပထမဦးစွာ BERT-specific vectorizer ကို load ပြုလုပ်ကြစို့:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "မူရင်း network ကို သင်ကြားခဲ့သော vectorizer ကိုပဲ အသုံးပြုရမည်ဟု အရေးကြီးပါသည်။ ထို့အပြင် BERT vectorizer သည် အောက်ပါ component သုံးခုကို ပြန်ပေးသည် -\n",
    "\n",
    "* `input_word_ids` - input စာကြောင်းအတွက် token နံပါတ်များ၏ အစီအစဉ်\n",
    "* `input_mask` - အစီအစဉ်၏ ဘယ်အပိုင်းသည် အမှန်တကယ် input ပါဝင်သည်ကို ပြသပြီး ဘယ်အပိုင်းသည် padding ဖြစ်သည်ကို ပြသသည်။ ၎င်းသည် `Masking` layer မှ ထုတ်လုပ်သော mask နှင့် ဆင်တူသည်။\n",
    "* `input_type_ids` - ဘာသာစကား မော်ဒယ်လုပ်ငန်းများအတွက် အသုံးပြုပြီး အစီအစဉ်တစ်ခုတွင် input စာကြောင်းနှစ်ခုကို သတ်မှတ်ရန် ခွင့်ပြုသည်။\n",
    "\n",
    "ထို့နောက် BERT feature extractor ကို အောက်ပါအတိုင်း instantiate လုပ်နိုင်ပါသည် -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT layer သည် အောက်ပါ အသုံးဝင်သော ရလဒ်များကို ပြန်ပေးသည် -\n",
    "\n",
    "* `pooled_output` သည် sequence အတွင်းရှိ token အားလုံးကို ပျမ်းမျှတွက်ချက်ထားသော ရလဒ်ဖြစ်သည်။ ၎င်းကို network တစ်ခုလုံး၏ အဓိပ္ပါယ်ဆိုင်ရာ embedding အဖြစ် သတ်မှတ်နိုင်သည်။ ၎င်းသည် ယခင်မော်ဒယ်တွင် အသုံးပြုခဲ့သော `GlobalAveragePooling1D` layer ၏ output နှင့် တူညီသည်။\n",
    "* `sequence_output` သည် နောက်ဆုံး transformer layer ၏ output ဖြစ်သည် (၎င်းသည် အထက်တွင် ဖော်ပြထားသော မော်ဒယ်အတွင်း `TransformerBlock` ၏ output နှင့် ကိုက်ညီသည်)။\n",
    "* `encoder_outputs` သည် transformer layer အားလုံး၏ output များဖြစ်သည်။ ၎င်းတွင် 4-layer BERT မော်ဒယ်ကို load လုပ်ထားသောကြောင့် (အမည်တွင် `4_H` ပါဝင်နေသည်ကို သင်ခန့်မှန်းနိုင်သည်ဟု ထင်ပါသည်)၊ ၎င်းတွင် tensor 4 ခု ပါဝင်သည်။ နောက်ဆုံး tensor သည် `sequence_output` နှင့် တူညီသည်။\n",
    "\n",
    "ယခုတွင် end-to-end classification မော်ဒယ်ကို သတ်မှတ်မည်ဖြစ်သည်။ *functional model definition* ကို အသုံးပြုမည်ဖြစ်ပြီး၊ မော်ဒယ် input ကို သတ်မှတ်ပြီးနောက်၊ output ကိုတွက်ချက်ရန် expression များစဉ်ဆက်ပေးသွားမည်ဖြစ်သည်။ ထို့အပြင် BERT မော်ဒယ်၏ weight များကို train မလုပ်ဘဲ၊ နောက်ဆုံး classifier ကိုသာ train လုပ်မည် - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "သင်ကြားနိုင်သော parameters အနည်းငယ်သာရှိသော်လည်း၊ BERT feature extractor သည် တွက်ချက်မှုအလွန်လေးလံသောကြောင့် လုပ်ငန်းစဉ်သည် အတော်လေးနှေးကွေးနေပါသည်။ သင်ကြားမှုမလုံလောက်ခြင်း သို့မဟုတ် မော်ဒယ် parameters မလုံလောက်ခြင်းကြောင့် ကျွန်ုပ်တို့သည် သင့်လျော်သောတိကျမှန်ကန်မှုကို မရရှိနိုင်ခဲ့သကဲ့သို့ပင် ထင်ရပါသည်။\n",
    "\n",
    "ယခု BERT weights ကို အခဲဖြေပြီး၊ ထိုအတူတူ သင်ကြားကြည့်ပါစို့။ ဤအရာသည် အလွန်သေးငယ်သော learning rate တစ်ခုလိုအပ်ပြီး၊ **warmup** နှင့်အတူ **AdamW** optimizer ကို အသုံးပြု၍ သေချာစွာ သင်ကြားမှုမဟာဗျူဟာကိုလည်းလိုအပ်ပါသည်။ Optimizer ကို ဖန်တီးရန် `tf-models-official` package ကို အသုံးပြုမည်ဖြစ်သည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "သင်မြင်နိုင်သည့်အတိုင်း၊ သင်တန်းပေးခြင်းသည် အတော်လေးနှေးကွေးသည် - သို့သော် သင်မတိုင်မီ အသုံးပြုပြီး မော်ဒယ်ကို အနည်းဆုံး ၅-၁၀ အကြိမ် (epochs) သင်ကြားကြည့်ပြီး ယခင်တွင် အသုံးပြုခဲ့သော နည်းလမ်းများနှင့် နှိုင်းယှဉ်၍ အကောင်းဆုံးရလဒ်ရနိုင်မလား စမ်းသပ်ကြည့်လိုနိုင်သည်။\n",
    "\n",
    "## Huggingface Transformers Library\n",
    "\n",
    "Transformer မော်ဒယ်များကို အသုံးပြုရန် အလွန်ရိုးရှင်းပြီး လူသိများသော နည်းလမ်းတစ်ခုမှာ [HuggingFace package](https://github.com/huggingface/) ဖြစ်ပြီး၊ ၎င်းသည် အမျိုးမျိုးသော သဘာဝဘာသာစကားလုပ်ငန်းများ (NLP tasks) အတွက် ရိုးရှင်းသော အဆောက်အအုံများကို ပံ့ပိုးပေးသည်။ ၎င်းကို Tensorflow နှင့် PyTorch (နောက်ထပ် လူကြိုက်များသော နယူးရယ်နက်ဝက်ဖရိမ်းဝေါ့ခ်) နှစ်ခုစလုံးအတွက် အသုံးပြုနိုင်သည်။\n",
    "\n",
    "> **Note**: သင် Transformers library အလုပ်လုပ်ပုံကို မကြည့်လိုပါက - ဒီ notebook ၏ အဆုံးသို့ ကျော်သွားနိုင်သည်၊ အကြောင်းမှာ ယခင်တွင် ကျွန်ုပ်တို့ ပြုလုပ်ခဲ့သည့်အရာများနှင့် မတူညီသော အဓိကအချက်များ မတွေ့ရလိမ့်မည်။ ကျွန်ုပ်တို့သည် BERT မော်ဒယ်ကို သင်ကြားခြင်းအဆင့်များကို ထပ်မံလုပ်ဆောင်မည်ဖြစ်ပြီး၊ ကွဲပြားသော library နှင့် အလွန်ကြီးမားသော မော်ဒယ်ကို အသုံးပြုမည်ဖြစ်သည်။ ထို့ကြောင့်၊ ၎င်းလုပ်ငန်းစဉ်တွင် အတော်လေးကြာမြင့်သော သင်ကြားမှုများ ပါဝင်မည်ဖြစ်ပြီး၊ သင်သည် ကုဒ်ကိုသာ ကြည့်ရှုလိုနိုင်သည်။ \n",
    "\n",
    "အခုတော့ [Huggingface Transformers](http://huggingface.co) ကို အသုံးပြု၍ ကျွန်ုပ်တို့၏ ပြဿနာကို မည်သို့ ဖြေရှင်းနိုင်မည်ကို ကြည့်ကြရအောင်။\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ပထမဆုံး ကျွန်တော်တို့ အသုံးပြုမယ့် မော်ဒယ်ကို ရွေးချယ်ဖို့ လိုအပ်ပါတယ်။ Built-in မော်ဒယ်အချို့အပြင် Huggingface မှာ [အွန်လိုင်းမော်ဒယ်ရေပိုစစ်](https://huggingface.co/models) ရှိပြီး၊ အဲဒီမှာ community က ပြင်ဆင်ပြီးသား မော်ဒယ်များစွာကို ရှာဖွေတွေ့ရှိနိုင်ပါတယ်။ အဲဒီမော်ဒယ်တွေကို မော်ဒယ်နာမည်ပေးရုံနဲ့ load လုပ်ပြီး အသုံးပြုနိုင်ပါတယ်။ မော်ဒယ်အတွက် လိုအပ်တဲ့ binary ဖိုင်တွေကို အလိုအလျောက် download လုပ်ပေးပါမယ်။\n",
    "\n",
    "တစ်ချို့အချိန်တွေမှာ ကိုယ်ပိုင်မော်ဒယ်တွေကို load လုပ်ဖို့ လိုအပ်နိုင်ပါတယ်၊ အဲဒီအခါမှာ tokenizer အတွက် parameters, `config.json` ဖိုင် (မော်ဒယ် parameters ပါဝင်တဲ့), binary weights စတဲ့ လိုအပ်တဲ့ ဖိုင်တွေပါဝင်တဲ့ directory ကို သတ်မှတ်ပေးနိုင်ပါတယ်။\n",
    "\n",
    "မော်ဒယ်နာမည်ကနေ မော်ဒယ်နဲ့ tokenizer နှစ်ခုလုံးကို instantiate လုပ်နိုင်ပါတယ်။ အရင်ဆုံး tokenizer နဲ့ စလိုက်ကြမယ်:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` အရာဝတ္ထုတွင် `encode` အလုပ်ဆောင်မှုပါရှိပြီး၊ စာသားကိုတိုက်ရိုက် encode လုပ်ရန်အသုံးပြုနိုင်သည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ကျွန်ုပ်တို့သည် tokenizer ကို အသုံးပြု၍ `token_ids`, `input_mask` fields စသည်တို့ကို အပါအဝင် မော်ဒယ်သို့ ပေးပို့ရန် သင့်လျော်သော နည်းလမ်းဖြင့် အစီအစဉ်တစ်ခုကို encode လုပ်နိုင်ပါသည်။ ထို့အပြင် `return_tensors='tf'` argument ကို ပေးသွင်းခြင်းဖြင့် Tensorflow tensors ကို ရယူလိုကြောင်း သတ်မှတ်နိုင်ပါသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ကျွန်တော်တို့အနေဖြင့် `bert-base-uncased` ဟုခေါ်သော pre-trained BERT မော်ဒယ်ကို အသုံးပြုမည်ဖြစ်သည်။ *Uncased* ဆိုသည်မှာ မော်ဒယ်သည် အကြောင်းအရာအထိခိုက်မှုမရှိကြောင်းကို ဖော်ပြသည်။\n",
    "\n",
    "မော်ဒယ်ကို လေ့ကျင့်စဉ်တွင် tokenized sequence ကို input အဖြစ်ပေးရန်လိုအပ်ပြီး၊ ထို့ကြောင့် data processing pipeline ကို ဒီဇိုင်းဆွဲမည်ဖြစ်သည်။ `tokenizer.encode` သည် Python function ဖြစ်သောကြောင့်၊ နောက်ဆုံးယူနစ်တွင် အသုံးပြုခဲ့သည့်နည်းလမ်းတူပင် `py_function` ကို ခေါ်သုံး၍ အသုံးပြုမည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အခုတော့ `BertForSequenceClassification` ပက်ကေ့ဂျ်ကို အသုံးပြုပြီး အမှန်တကယ်မော်ဒယ်ကို load လုပ်နိုင်ပါပြီ။ ဒါဟာ classification အတွက် လိုအပ်တဲ့ architecture ကို မော်ဒယ်မှာ ရှိပြီးသားဖြစ်စေပြီး၊ final classifier ကိုပါ ထည့်သွင်းပေးထားပါတယ်။ သင်တွေ့ရမယ့် warning message က final classifier ရဲ့ weight တွေ initialize မလုပ်ထားသေးတာနဲ့ မော်ဒယ်ကို pre-training လိုအပ်တယ်ဆိုတာကို ပြောပါလိမ့်မယ် - ဒါက အဆင်ပြေပါတယ်၊ အကြောင်းကတော့ အခုလိုလုပ်ဖို့ပဲ ကျွန်တော်တို့လုပ်နေတဲ့အရာဖြစ်လို့ပါ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary()` မှာမြင်နိုင်သည့်အတိုင်း၊ အဆိုပါမော်ဒယ်တွင် ၁၁၀ သန်းနီးပါးသော parameters ပါဝင်ပါတယ်! သဘောတရားအားဖြင့်၊ သေးငယ်သော dataset ပေါ်တွင် ရိုးရှင်းသော classification task လုပ်ဆောင်လိုပါက BERT base layer ကို training မလုပ်ချင်ပါ။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ယခု ကျွန်ုပ်တို့ သင်ကြားမှုကို စတင်ရန် အဆင်သင့်ဖြစ်ပါပြီ!\n",
    "\n",
    "> **Note**: အပြည့်အဝ BERT မော်ဒယ်ကို သင်ကြားခြင်းသည် အချိန်အလွန်များစွာ လိုအပ်နိုင်ပါသည်! ထို့ကြောင့် ကျွန်ုပ်တို့သည် ပထမ 32 batches အတွက်သာ သင်ကြားမည်ဖြစ်သည်။ ဤသည်မှာ မော်ဒယ်သင်ကြားမှုကို စီစဉ်ပုံကို ပြသရန်သာ ဖြစ်သည်။ အပြည့်အဝ သင်ကြားမှုကို စမ်းသပ်လိုပါက - `steps_per_epoch` နှင့် `validation_steps` parameters ကို ဖယ်ရှားပြီး စောင့်ဆိုင်းရန် ပြင်ဆင်ပါ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အကယ်၍ သင် iteration အရေအတွက်ကို တိုးမြှင့်ပြီး အချိန်လုံလုံလောက်လောက်စောင့်ပြီး၊ epoch အတော်များများအထိ လေ့ကျင့်ပါက BERT classification က အကောင်းဆုံးတိကျမှန်ကန်မှုကို ပေးနိုင်မယ်လို့ မျှော်လင့်နိုင်ပါတယ်! အကြောင်းကတော့ BERT ဟာ ဘာသာစကား၏ ဖွဲ့စည်းပုံကို အတော်လေး နားလည်ပြီးသားဖြစ်ပြီး၊ နောက်ဆုံး classifier ကို fine-tune လုပ်ရုံသာ လိုအပ်လို့ပါ။ သို့သော် BERT ဟာ မော်ဒယ်ကြီးတစ်ခုဖြစ်တဲ့အတွက် လေ့ကျင့်မှု အပြည့်အစုံလုပ်ရတဲ့ အချိန်က အတော်ကြာပြီး၊ အတော်လေး အားကောင်းတဲ့ ကွန်ပျူတာစွမ်းအား (GPU၊ အထူးသဖြင့် တစ်ခုထက်ပိုများ) လိုအပ်ပါတယ်။\n",
    "\n",
    "> **Note:** ဥပမာအနေနဲ့ ကျွန်တော်တို့ အသုံးပြုနေတဲ့ မော်ဒယ်က BERT pre-trained မော်ဒယ်တွေထဲက အငယ်ဆုံးတစ်ခုဖြစ်ပါတယ်။ ပိုကြီးတဲ့ မော်ဒယ်တွေက ပိုမိုကောင်းမွန်တဲ့ ရလဒ်တွေကို ပေးနိုင်ဖို့ အလားအလာရှိပါတယ်။\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## အဓိကအချက်\n",
    "\n",
    "ဤယူနစ်တွင် **transformers** အခြေခံထားသော နောက်ဆုံးပေါ်မော်ဒယ်ဖွဲ့စည်းမှုများကို ကြည့်ရှုခဲ့ပါသည်။ ကျွန်ုပ်တို့၏စာသားအမျိုးအစားခွဲခြားမှုအလုပ်တွင် ထိုမော်ဒယ်များကို အသုံးပြုခဲ့ပြီး၊ BERT မော်ဒယ်များကို entity extraction, question answering နှင့် အခြားသော NLP အလုပ်များတွင်လည်း တူညီသည့်ပုံစံဖြင့် အသုံးပြုနိုင်ပါသည်။\n",
    "\n",
    "Transformer မော်ဒယ်များသည် NLP တွင် နောက်ဆုံးပေါ်နည်းပညာကို ကိုယ်စားပြုထားပြီး၊ မိမိ၏ custom NLP ဖြေရှင်းချက်များကို စတင်စမ်းသပ်မည်ဆိုပါက အဓိကရွေးချယ်မှုအဖြစ် သုံးသင့်ပါသည်။ သို့သော်၊ ဤ module တွင် ဆွေးနွေးထားသော recurrent neural networks ၏ အခြေခံအယူအဆများကို နားလည်ခြင်းသည် အဆင့်မြင့် neural မော်ဒယ်များကို တည်ဆောက်လိုပါက အလွန်အရေးကြီးပါသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**အကြောင်းကြားချက်**:  \nဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှန်ကန်မှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်မှုများတွင် အမှားများ သို့မဟုတ် မမှန်ကန်မှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းဘာသာစကားဖြင့် ရေးသားထားသော စာရွက်စာတမ်းကို အာဏာရှိသော ရင်းမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များမှ ပရော်ဖက်ရှင်နယ် ဘာသာပြန်မှုကို အကြံပြုပါသည်။ ဤဘာသာပြန်မှုကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအလွတ်များ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-30T10:23:46+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "my"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}