{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# အာရုံစူးစိုက်မှု မော်ကွန်းများနှင့် Transformer မော်ဒယ်များ\n",
    "\n",
    "Recurrent networks (ပြန်လည်ဆက်သွယ်မှုကွန်ရက်များ) ရဲ့ အဓိကအားနည်းချက်တစ်ခုကတော့ စာကြောင်းတစ်ကြောင်းရဲ့ စကားလုံးအားလုံးဟာ ရလဒ်အပေါ်မှာ တူညီတဲ့ သက်ရောက်မှုရှိတယ်ဆိုတာပါပဲ။ ဒါကြောင့် Named Entity Recognition (နာမည်ပုဂ္ဂိုလ်သတ်မှတ်မှု) နဲ့ Machine Translation (စက်ဖြင့်ဘာသာပြန်ခြင်း) လိုမျိုး sequence-to-sequence (အဆက်မပြတ်အချက်အလက်) တာဝန်တွေမှာ စံပုံ LSTM encoder-decoder မော်ဒယ်တွေက အကောင်းဆုံးစွမ်းဆောင်ရည်မရနိုင်တာဖြစ်ပါတယ်။ အမှန်တရားမှာတော့ input sequence (အဝင်အဆက်မပြတ်အချက်အလက်) ရဲ့ အချို့စကားလုံးတွေက output (အထွက်) အပေါ်မှာ ပိုမိုသက်ရောက်မှုရှိတတ်ပါတယ်။\n",
    "\n",
    "Machine translation (စက်ဖြင့်ဘာသာပြန်ခြင်း) လိုမျိုး sequence-to-sequence မော်ဒယ်တစ်ခုကို စဉ်းစားကြည့်ပါ။ ဒါကို recurrent networks နှစ်ခုနဲ့ အကောင်အထည်ဖော်ထားပါတယ်။ network တစ်ခု (**encoder**) က input sequence ကို hidden state (ဖုံးလွှမ်းထားသောအခြေအနေ) အဖြစ်သို့ ပြောင်းလဲပြီး၊ နောက်တစ်ခု (**decoder**) က အဲ့ဒီ hidden state ကို ဘာသာပြန်ရလဒ်အဖြစ် ပြန်လည်ဖော်ထုတ်ပါတယ်။ ဒီနည်းလမ်းရဲ့ ပြဿနာကတော့ network ရဲ့ နောက်ဆုံးအခြေအနေက စာကြောင်းရဲ့ အစပိုင်းကို မှတ်မိဖို့ အခက်အခဲရှိတာကြောင့် ရှည်လျားတဲ့ စာကြောင်းတွေမှာ မော်ဒယ်ရဲ့ အရည်အသွေးကျဆင်းစေတတ်ပါတယ်။\n",
    "\n",
    "**Attention Mechanisms (အာရုံစူးစိုက်မှုစနစ်များ)** ကတော့ RNN ရဲ့ output prediction (အထွက်ခန့်မှန်းမှု) တစ်ခုစီအပေါ် input vector (အဝင်ဗက်တာ) တစ်ခုစီရဲ့ အကြောင်းအရာသက်ရောက်မှုကို အလေးပေးနိုင်တဲ့ နည်းလမ်းတစ်ခုကို ပံ့ပိုးပေးပါတယ်။ ဒီစနစ်ကို အကောင်အထည်ဖော်တဲ့နည်းလမ်းက input RNN ရဲ့ အလယ်အလတ်အခြေအနေတွေနဲ့ output RNN အကြား shortcut (တိုက်ရိုက်လမ်းကြောင်း) တွေ ဖန်တီးခြင်းဖြစ်ပါတယ်။ ဒီနည်းလမ်းနဲ့ $y_t$ output symbol ကို ဖန်တီးတဲ့အခါမှာ input hidden states $h_i$ အားလုံးကို အလေးချိန်ကွဲပြားမှု $\\alpha_{t,i}$ နဲ့အတူ စဉ်းစားပါမယ်။\n",
    "\n",
    "![Image showing an encoder/decoder model with an additive attention layer](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.my.png)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) မှ additive attention mechanism ပါတဲ့ encoder-decoder မော်ဒယ်ကို [ဒီ blog post](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) မှ ရယူထားသည်။*\n",
    "\n",
    "Attention matrix $\\{\\alpha_{i,j}\\}$ က output sequence (အထွက်အဆက်မပြတ်အချက်အလက်) ရဲ့ စကားလုံးတစ်လုံးကို ဖန်တီးရာမှာ input words (အဝင်စကားလုံး) တစ်ချို့ရဲ့ သက်ရောက်မှုအဆင့်ကို ကိုယ်စားပြုပါတယ်။ အောက်မှာ ဒီလို matrix ရဲ့ ဥပမာကို ကြည့်နိုင်ပါတယ်။\n",
    "\n",
    "![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.my.png)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) မှ (ပုံ ၃) ကို ရယူထားသည်။*\n",
    "\n",
    "Attention mechanisms တွေက Natural Language Processing (ဘာသာစကားအလုပ်လုပ်စနစ်) ရဲ့ လက်ရှိ state-of-the-art (အဆင့်မြင့်နည်းပညာ) မော်ဒယ်တွေမှာ အရေးပါတဲ့ အခန်းကဏ္ဍကို ထမ်းဆောင်နေပါတယ်။ သို့သော် attention ကို ထည့်သွင်းခြင်းက မော်ဒယ် parameter (မော်ဒယ်အချက်အလက်) အရေအတွက်ကို အလွန်များစေပြီး၊ RNN တွေမှာ scaling (အရွယ်အစားချဲ့ထွင်မှု) ပြဿနာတွေ ဖြစ်ပေါ်စေပါတယ်။ RNN တွေမှာ အဆက်မပြတ်အချက်အလက်တစ်ခုစီကို အစဉ်လိုက် ဆက်တိုက်လုပ်ဆောင်ရတာကြောင့် training (လေ့ကျင့်မှု) ကို batch နဲ့ parallelize (တပြိုင်နက်) လုပ်ဖို့ အခက်အခဲရှိပါတယ်။\n",
    "\n",
    "Attention mechanisms တွေကို အသုံးပြုမှုနဲ့ အထက်ပါ အကန့်အသတ်ကြောင့် ယနေ့ကျွန်တော်တို့သိပြီး အသုံးပြုနေတဲ့ BERT ကနေ OpenGPT3 အထိရှိတဲ့ လက်ရှိ state-of-the-art Transformer မော်ဒယ်တွေကို ဖန်တီးနိုင်ခဲ့ပါတယ်။\n",
    "\n",
    "## Transformer မော်ဒယ်များ\n",
    "\n",
    "တစ်ခုချင်းစီရဲ့ အရင်ဆုံးခန့်မှန်းချက်ရဲ့ context (အကြောင်းအရာ) ကို နောက်တစ်ဆင့်သို့ ပို့ပေးခြင်းအစား၊ **Transformer မော်ဒယ်များ** က **positional encodings** နဲ့ attention ကို အသုံးပြုပြီး ပေးထားတဲ့ စာသား window (အကန့်အသတ်) အတွင်းမှာ အကြောင်းအရာကို ဖမ်းဆီးပါတယ်။ အောက်ပါပုံက positional encodings နဲ့ attention က window အတွင်းမှာ context ကို ဘယ်လိုဖမ်းဆီးနိုင်တယ်ဆိုတာ ပြထားပါတယ်။\n",
    "\n",
    "![Animated GIF showing how the evaluations are performed in transformer models.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Input position တစ်ခုချင်းစီကို output position တစ်ခုချင်းစီနဲ့ လွတ်လပ်စွာ mapping လုပ်နိုင်တဲ့အတွက်၊ transformers တွေက RNN တွေထက် ပိုမို parallelize လုပ်နိုင်ပြီး၊ ပိုကြီးမားပြီး ပိုထိရောက်တဲ့ ဘာသာစကားမော်ဒယ်တွေကို ဖန်တီးနိုင်ပါတယ်။ Attention head တစ်ခုချင်းစီက စကားလုံးတွေကြားက ဆက်စပ်မှုအမျိုးမျိုးကို သင်ယူနိုင်ပြီး၊ ဒါက Downstream Natural Language Processing (နောက်ဆင့်ဘာသာစကားအလုပ်လုပ်စနစ်) တာဝန်တွေကို တိုးတက်စေပါတယ်။\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) က အလွှာ ၁၂ လွှာပါတဲ့ *BERT-base* နဲ့ အလွှာ ၂၄ လွှာပါတဲ့ *BERT-large* တို့ဖြင့် ဖွဲ့စည်းထားတဲ့ အလွန်ကြီးမားတဲ့ multi-layer transformer network တစ်ခုဖြစ်ပါတယ်။ ဒီမော်ဒယ်ကို စာကြောင်းထဲက masked words (ဖုံးထားသောစကားလုံးများ) ကို ခန့်မှန်းတဲ့ unsupervised training (မကြီးကြပ်သောလေ့ကျင့်မှု) နည်းလမ်းနဲ့ စာကြောင်းအများအပြား (WikiPedia + စာအုပ်များ) အပေါ်မှာ အရင်ဆုံး pre-train လုပ်ထားပါတယ်။ Pre-training လုပ်စဉ်မှာ မော်ဒယ်က ဘာသာစကားနားလည်မှုအဆင့်မြင့်တစ်ခုကို စုပ်ယူထားပြီး၊ အဲ့ဒီနားလည်မှုကို အခြား dataset တွေနဲ့ fine-tuning (အသေးစိတ်ချိန်ညှိမှု) လုပ်နိုင်ပါတယ်။ ဒီလုပ်ငန်းစဉ်ကို **transfer learning** လို့ ခေါ်ပါတယ်။\n",
    "\n",
    "![picture from http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.my.png)\n",
    "\n",
    "Transformer architecture တွေမှာ BERT, DistilBERT, BigBird, OpenGPT3 စတဲ့ မော်ဒယ်အမျိုးအစားအများအပြားရှိပြီး၊ အဲ့ဒီမော်ဒယ်တွေကို fine-tune လုပ်နိုင်ပါတယ်။ [HuggingFace package](https://github.com/huggingface/) က PyTorch နဲ့ အဲ့ဒီ architecture တွေကို training လုပ်ဖို့ repository တစ်ခုကို ပံ့ပိုးပေးထားပါတယ်။\n",
    "\n",
    "## BERT ကို အသုံးပြု၍ စာသားအမျိုးအစားခွဲခြားခြင်း\n",
    "\n",
    "Pre-trained BERT မော်ဒယ်ကို အသုံးပြုပြီး ကျွန်တော်တို့ရဲ့ ရိုးရာတာဝန်ဖြစ်တဲ့ sequence classification (အဆက်မပြတ်အချက်အလက်ခွဲခြားခြင်း) ကို ဘယ်လိုဖြေရှင်းမလဲဆိုတာ ကြည့်ကြရအောင်။ ကျွန်တော်တို့ရဲ့ မူရင်း AG News dataset ကို ခွဲခြားသတ်မှတ်သွားမှာ ဖြစ်ပါတယ်။\n",
    "\n",
    "ပထမဆုံး HuggingFace library နဲ့ dataset ကို load လုပ်ရအောင်:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ကျွန်ုပ်တို့သည် pre-trained BERT မော်ဒယ်ကို အသုံးပြုမည်ဖြစ်သောကြောင့် သီးခြား tokenizer တစ်ခုကို အသုံးပြုရန် လိုအပ်ပါသည်။ ပထမဦးစွာ pre-trained BERT မော်ဒယ်နှင့် ဆက်စပ်သော tokenizer ကို load လုပ်ပါမည်။\n",
    "\n",
    "HuggingFace library တွင် pre-trained မော်ဒယ်များ၏ repository တစ်ခု ပါဝင်ပြီး၊ မော်ဒယ်အမည်များကို `from_pretrained` function များတွင် argument အဖြစ် သတ်မှတ်ခြင်းဖြင့် အသုံးပြုနိုင်ပါသည်။ မော်ဒယ်အတွက် လိုအပ်သော binary ဖိုင်များအားလုံးကို အလိုအလျောက် download လုပ်ပေးပါမည်။\n",
    "\n",
    "သို့သော် တစ်ချိန်ချိန်တွင် မိမိ၏ မော်ဒယ်များကို load လုပ်ရန် လိုအပ်နိုင်ပြီး၊ ထိုအခါတွင် tokenizer အတွက် parameters, မော်ဒယ် parameters ပါဝင်သော `config.json` ဖိုင်၊ binary weights စသည်တို့ပါဝင်သည့် directory ကို သတ်မှတ်နိုင်ပါသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` အရာဝတ္ထုတွင် `encode` အလုပ်ဆောင်မှုပါရှိပြီး၊ စာသားကိုတိုက်ရိုက် encode လုပ်ရန်အသုံးပြုနိုင်သည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ထို့နောက်၊ သင်ကြားမှုအတွင်း ဒေတာများကို ဝင်ရောက်အသုံးပြုရန်အတွက် အသုံးပြုမည့် iterators များကို ဖန်တီးကြမည်။ BERT သည် သူ၏ကိုယ်ပိုင် encoding function ကို အသုံးပြုသောကြောင့်၊ ယခင်က သတ်မှတ်ထားသော `padify` နှင့် ဆင်တူသော padding function ကို သတ်မှတ်ရန် လိုအပ်ပါမည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ကျွန်ုပ်တို့၏ကိစ္စတွင် `bert-base-uncased` ဟုခေါ်သော pre-trained BERT မော်ဒယ်ကို အသုံးပြုမည်ဖြစ်သည်။ `BertForSequenceClassfication` package ကို အသုံးပြု၍ မော်ဒယ်ကို load လုပ်ပါမည်။ ဤသည်မှာ classification အတွက် လိုအပ်သော architecture ကို မော်ဒယ်တွင် ရှိပြီးဖြစ်စေသည်၊ အပြီးသတ် classifier ကိုပါ ထည့်သွင်းထားသည်။ မော်ဒယ်၏ အပြီးသတ် classifier ၏ weight များကို initialize မလုပ်ထားသောကြောင်းနှင့် မော်ဒယ်ကို pre-training လိုအပ်မည်ဖြစ်ကြောင်းကို ပြသသော warning message ကို တွေ့မြင်ရမည် - ဤသည်မှာ အလုံးစုံအဆင်ပြေပါသည်၊ အကြောင်းမူကား ကျွန်ုပ်တို့လုပ်ဆောင်မည့်အရာမှာ အတိအကျဤအရာပင်ဖြစ်သည်!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ယခုကျွန်ုပ်တို့ သင်ကြားမှုကို စတင်ရန် အဆင်သင့်ဖြစ်ပါပြီ! BERT သည် အစဉ်အလာအတိုင်း အကြိုသင်ကြားပြီးသားဖြစ်သောကြောင့်၊ မူလအလေးချိန်များကို ပျက်စီးမသွားစေရန် သင်ကြားမှုအတွက် အလွန်သေးငယ်သော learning rate ဖြင့် စတင်လိုပါသည်။\n",
    "\n",
    "`BertForSequenceClassification` မော်ဒယ်က အဓိကအလုပ်အားလုံးကို လုပ်ဆောင်ပေးပါသည်။ သင်ကြားမှုဒေတာပေါ်တွင် မော်ဒယ်ကို ခေါ်သုံးသောအခါ၊ input minibatch အတွက် loss နှင့် network output နှစ်ခုလုံးကို ပြန်ပေးပါသည်။ loss ကို parameter optimization (`loss.backward()` သည် backward pass ကို လုပ်ဆောင်သည်) အတွက် အသုံးပြုပြီး၊ `out` ကို training accuracy ကိုတွက်ချက်ရန် အသုံးပြုပါသည်။ accuracy ကိုတွက်ချက်ရာတွင် ရရှိသော labels `labs` (ဤသည်ကို `argmax` အသုံးပြု၍ တွက်ချက်သည်) နှင့် မျှော်မှန်းထားသော `labels` ကို နှိုင်းယှဉ်ပါသည်။\n",
    "\n",
    "လုပ်ငန်းစဉ်ကို ထိန်းချုပ်နိုင်ရန်၊ loss နှင့် accuracy ကို iteration အတော်များများအတွင်း စုဆောင်းပြီး၊ `report_freq` သင်ကြားမှုစက်ဝိုင်းတိုင်းတွင် ထုတ်ပြပါသည်။\n",
    "\n",
    "ဤသင်ကြားမှုသည် အချိန်အတော်ကြာနိုင်သောကြောင့်၊ iteration အရေအတွက်ကို ကန့်သတ်ထားပါသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT ကို အသုံးပြု၍ ခွဲခြားသတ်မှတ်မှုလုပ်ဆောင်ရာတွင် (အထူးသဖြင့် iteration အရေအတွက်ကို တိုးမြှင့်ပြီး အချိန်အတော်ကြာစောင့်ပါက) တော်တော်ကောင်းမွန်သော တိကျမှုရလဒ်ကို ရရှိနိုင်သည်ကို တွေ့နိုင်ပါသည်။ ဒါဟာ BERT သည် ဘာသာစကား၏ ဖွဲ့စည်းပုံကို ရှင်းလင်းစွာ နားလည်ပြီးသားဖြစ်သောကြောင့် ဖြစ်ပြီး၊ ကျွန်ုပ်တို့အနေဖြင့် နောက်ဆုံး classifier ကိုသာ fine-tune လုပ်ရန် လိုအပ်သည်။ သို့သော် BERT သည် အရွယ်အစားကြီးမားသော မော်ဒယ်တစ်ခုဖြစ်သည့်အတွက်၊ လေ့ကျင့်မှု လုပ်ငန်းစဉ်တစ်ခုလုံးသည် အချိန်အတော်ကြာပြီး၊ ကြီးမားသော ကွန်ပျူတာစွမ်းအား (GPU၊ အကောင်းဆုံးအားဖြင့် တစ်ခုထက်ပိုမိုသော GPU) လိုအပ်ပါသည်။\n",
    "\n",
    "> **Note:** ကျွန်ုပ်တို့၏ ဥပမာတွင်၊ အရွယ်အစားအငယ်ဆုံးသော pre-trained BERT မော်ဒယ်တစ်ခုကို အသုံးပြုထားပါသည်။ ပိုမိုကောင်းမွန်သော ရလဒ်များကို ရရှိစေနိုင်မည့် အရွယ်အစားကြီးမားသော မော်ဒယ်များလည်း ရှိပါသည်။\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## မော်ဒယ်၏လုပ်ဆောင်မှုကိုအကဲဖြတ်ခြင်း\n",
    "\n",
    "အခုတော့ မော်ဒယ်၏လုပ်ဆောင်မှုကို စမ်းသပ်ဒေတာအစုပေါ်မှာ အကဲဖြတ်နိုင်ပါပြီ။ အကဲဖြတ်မှုလုပ်ငန်းစဉ်သည် လေ့ကျင့်မှုလုပ်ငန်းစဉ်နှင့် ဆင်တူပေမယ့် `model.eval()` ကိုခေါ်ပြီး မော်ဒယ်ကို အကဲဖြတ်မှုအနေအထားသို့ ပြောင်းရန် မမေ့ရပါ။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## အဓိကအချက်\n",
    "\n",
    "ဒီယူနစ်မှာ **transformers** စာကြောင်းများကို အသုံးပြုပြီး အလွယ်တကူ သင်ခန်းစာအတွက် စာသားအမျိုးအစား ခွဲခြားမှုလုပ်ဆောင်နိုင်ပုံကို ကြည့်ရှုခဲ့ပါတယ်။ အတူတူပဲ BERT မော်ဒယ်များကို entity extraction, question answering နဲ့ အခြားသော NLP လုပ်ငန်းများအတွက် အသုံးပြုနိုင်ပါတယ်။\n",
    "\n",
    "Transformer မော်ဒယ်များသည် NLP ရှေ့ဆောင်နည်းပညာကို ကိုယ်စားပြုထားပြီး၊ အများဆုံးအခြေအနေများတွင် သင့်ရဲ့ custom NLP ဖြေရှင်းချက်များကို စမ်းသပ်စဉ်မှာ စတင်အသုံးပြုသင့်တဲ့ ပထမဆုံးနည်းလမ်းဖြစ်ပါတယ်။ သို့သော်၊ ဒီ module မှာ ဆွေးနွေးထားတဲ့ recurrent neural networks ရဲ့ အခြေခံအယူအဆများကို နားလည်ထားတာက အဆင့်မြင့် neural မော်ဒယ်များ တည်ဆောက်ချင်တဲ့အခါ အရေးကြီးပါတယ်။\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**အကြောင်းကြားချက်**:  \nဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်မှုများတွင် အမှားများ သို့မဟုတ် မတိကျမှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းစာရွက်စာတမ်းကို ၎င်း၏ မူရင်းဘာသာစကားဖြင့် အာဏာတရားရှိသော အရင်းအမြစ်အဖြစ် ရှုလေ့လာသင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူက ဘာသာပြန်မှု ဝန်ဆောင်မှုကို အသုံးပြုရန် အကြံပြုပါသည်။ ဤဘာသာပြန်မှုကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအလွတ်များ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-30T10:19:52+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "my"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}