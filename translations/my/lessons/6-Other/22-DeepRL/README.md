<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-25T23:34:34+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "my"
}
-->
# အနက်ရှိုင်းသော အတိုးအကျိုးသင်ယူမှု

အတိုးအကျိုးသင်ယူမှု (Reinforcement Learning - RL) သည် စောင့်ကြည့်သင်ယူမှု (Supervised Learning) နှင့် မစောင့်ကြည့်သင်ယူမှု (Unsupervised Learning) တို့နှင့်အတူ အခြေခံစက်လေ့လာမှု ပုံစံများထဲမှ တစ်ခုအဖြစ် သတ်မှတ်နိုင်သည်။ စောင့်ကြည့်သင်ယူမှုတွင် ကျွန်ုပ်တို့သည် ရလဒ်များကို ကြိုတင်သိထားသော ဒေတာအစုအဖွဲ့ကို အခြေခံသည်။ သို့သော် RL တွင် **လုပ်ဆောင်ခြင်းမှ သင်ယူခြင်း** ကို အခြေခံသည်။ ဥပမာအားဖြင့် ပထမဆုံးအကြိမ် ကွန်ပျူတာဂိမ်းကို မြင်သောအခါ၊ စည်းမျဉ်းများကို မသိဘဲဖြစ်စေ ကစားရန် စတင်ပြီး၊ ကစားနေစဉ်တွင် ကျွန်ုပ်တို့၏ အတတ်ပညာများကို တိုးတက်အောင် ပြုလုပ်နိုင်သည်။

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RL ကို လုပ်ဆောင်ရန် ကျွန်ုပ်တို့အတွက် လိုအပ်သည်မှာ -

* **ပတ်ဝန်းကျင်** သို့မဟုတ် **အတုအယောင်စက်** တစ်ခုဖြစ်ပြီး၊ ဂိမ်း၏ စည်းမျဉ်းများကို သတ်မှတ်ပေးရမည်။ ကျွန်ုပ်တို့သည် အတုအယောင်စက်တွင် စမ်းသပ်မှုများကို ပြုလုပ်ပြီး ရလဒ်များကို ကြည့်ရှုနိုင်ရမည်။
* **ဆုချီးမြှင့်မှုအလုပ်ဆောင်မှု (Reward function)** တစ်ခုလိုအပ်ပြီး၊ ကျွန်ုပ်တို့၏ စမ်းသပ်မှုအောင်မြင်မှုကို ပြသပေးရမည်။ ဥပမာအားဖြင့် ကွန်ပျူတာဂိမ်းကစားခြင်းကို သင်ယူနေစဉ်၊ ဆုချီးမြှင့်မှုမှာ ကျွန်ုပ်တို့ရရှိသော နောက်ဆုံးအမှတ်ဖြစ်နိုင်သည်။

ဆုချီးမြှင့်မှုအလုပ်ဆောင်မှုကို အခြေခံ၍ ကျွန်ုပ်တို့၏ အပြုအမူကို ပြင်ဆင်ပြီး အတတ်ပညာများကို တိုးတက်အောင် ပြုလုပ်နိုင်ရမည်။ ဒါကြောင့် နောက်တစ်ကြိမ်ကစားသောအခါ ပိုမိုကောင်းမွန်စွာ ပြုလုပ်နိုင်မည်။ အခြားစက်လေ့လာမှုအမျိုးအစားများနှင့် RL အကြား အဓိကကွာခြားချက်မှာ RL တွင် ဂိမ်းပြီးဆုံးသည်အထိ ကျွန်ုပ်တို့ အနိုင်ရမလား အရှုံးပေးမလား မသိနိုင်ခြင်းဖြစ်သည်။ ထို့ကြောင့် တစ်ခုချင်းစီသော လှုပ်ရှားမှုသည် ကောင်းမွန်သလား မကောင်းသလား မသိနိုင်ပါ - ကျွန်ုပ်တို့သည် ဆုချီးမြှင့်မှုကို ဂိမ်းပြီးဆုံးချိန်တွင်သာ ရရှိမည်ဖြစ်သည်။

RL လုပ်ဆောင်စဉ်တွင် ကျွန်ုပ်တို့သည် စမ်းသပ်မှုများစွာ ပြုလုပ်ရမည်။ စမ်းသပ်မှုတစ်ခုစီတွင် ကျွန်ုပ်တို့သည် ယခင်တစ်ခါက သင်ယူထားသော အကောင်းဆုံးမဟာဗျူဟာကို လိုက်နာခြင်း (**exploitation**) နှင့် အသစ်သော အခြေအနေများကို စူးစမ်းခြင်း (**exploration**) အကြား ကိုက်ညီမှုရှိစေရန် လိုအပ်သည်။

## OpenAI Gym

RL အတွက် အထောက်အကူဖြစ်သော ကိရိယာတစ်ခုမှာ [OpenAI Gym](https://gym.openai.com/) ဖြစ်သည်။ ၎င်းသည် အမျိုးမျိုးသော ပတ်ဝန်းကျင်များကို အတုအယောင်ပြုလုပ်နိုင်သည့် **အတုအယောင်ပတ်ဝန်းကျင်** တစ်ခုဖြစ်ပြီး၊ Atari ဂိမ်းများမှ စ၍ တိုင်တည်မှုရုပ်ဗေဒအထိ အမျိုးမျိုးသော ပတ်ဝန်းကျင်များကို အတုအယောင်ပြုလုပ်နိုင်သည်။ ၎င်းသည် အတိုးအကျိုးသင်ယူမှု အယ်လဂိုရီသမ်များကို လေ့ကျင့်ရန် အများဆုံးအသုံးပြုသော အတုအယောင်ပတ်ဝန်းကျင်များထဲမှ တစ်ခုဖြစ်ပြီး၊ [OpenAI](https://openai.com/) မှ ထိန်းသိမ်းထားသည်။

> **Note**: OpenAI Gym မှ ရရှိနိုင်သည့် ပတ်ဝန်းကျင်များအားလုံးကို [ဒီမှာ](https://gym.openai.com/envs/#classic_control) ကြည့်ရှုနိုင်ပါသည်။

## CartPole Balancing

ခေတ်သစ်တိုင်တည်မှုစက်များဖြစ်သည့် *Segway* သို့မဟုတ် *Gyroscooters* တို့ကို သင်တွေ့ဖူးကြမည်ဟု ထင်ပါသည်။ ၎င်းတို့သည် accelerometer သို့မဟုတ် gyroscope မှ လက္ခဏာကို ဖြေရှင်းရန် ၎င်းတို့၏ ဘီးများကို ချိန်ညှိခြင်းဖြင့် အလိုအလျောက် တိုင်တည်နိုင်သည်။ ဤအပိုင်းတွင် ကျွန်ုပ်တို့သည် တိုင်တည်မှုဆိုင်ရာ ပြဿနာတစ်ခုကို ဖြေရှင်းရန် သင်ယူမည် - တိုင်တည်မှုတိုင်တည်မှုဖြစ်သည်။ ၎င်းသည် စက်ကွင်းပညာရှင်တစ်ဦးသည် ၎င်း၏လက်ပေါ်တွင် တိုင်တည်ရန် လိုအပ်သည့် အခြေအနေနှင့် ဆင်တူသည် - သို့သော် ဤတိုင်တည်မှုသည် 1D တွင်သာ ဖြစ်ပေါ်သည်။

တိုင်တည်မှု၏ ရိုးရှင်းသောဗားရှင်းကို **CartPole** ပြဿနာဟု သိရှိကြသည်။ CartPole ကမ္ဘာတွင် ကျွန်ုပ်တို့တွင် ဘယ်ဘက် သို့မဟုတ် ညာဘက်သို့ ရွေ့နိုင်သည့် အလျားလိုက် slider တစ်ခုရှိပြီး၊ ရည်မှန်းချက်မှာ slider အပေါ်တွင် တိုင်တည်နေသော တိုင်တစ်ခုကို တိုင်တည်စေရန် ဖြစ်သည်။

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

ဤပတ်ဝန်းကျင်ကို ဖန်တီးပြီး အသုံးပြုရန်အတွက် Python ကုဒ်လိုင်းအချို့လိုအပ်သည် -

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

ပတ်ဝန်းကျင်တစ်ခုစီကို အတူတူပင် အသုံးပြုနိုင်သည် -
* `env.reset` သည် စမ်းသပ်မှုအသစ်ကို စတင်သည်။
* `env.step` သည် အတုအယောင်အဆင့်တစ်ခုကို ပြုလုပ်သည်။ ၎င်းသည် **action space** မှ **action** တစ်ခုကို လက်ခံပြီး၊ **observation space** မှ **observation** တစ်ခုနှင့်အတူ ဆုချီးမြှင့်မှုနှင့် အဆုံးသတ်အလံကို ပြန်ပေးသည်။

အထက်ပါဥပမာတွင် ကျွန်ုပ်တို့သည် အဆင့်တစ်ခုစီတွင် အမှတ်မရှိသော လှုပ်ရှားမှုကို ပြုလုပ်သောကြောင့် စမ်းသပ်မှု၏ အသက်ရှည်မှုသည် အလွန်တိုတောင်းသည် -

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL အယ်လဂိုရီသမ်၏ ရည်မှန်းချက်မှာ မော်ဒယ်တစ်ခုကို လေ့ကျင့်ခြင်းဖြစ်သည် - ၎င်းကို **policy** π ဟု ခေါ်သည် - ၎င်းသည် အခြေအနေတစ်ခုအတွက် လိုက်နာရန် လှုပ်ရှားမှုကို ပြန်ပေးမည်။ policy ကို သတ်မှတ်ချက်တစ်ခုအဖြစ်လည်း သတ်မှတ်နိုင်သည်၊ ဥပမာအားဖြင့် အခြေအနေ *s* နှင့် လှုပ်ရှားမှု *a* တို့အတွက် ၎င်းသည် *s* အခြေအနေတွင် *a* ကို လုပ်ဆောင်သင့်ကြောင်း ပြသသည့် probability π(*a*|*s*) ကို ပြန်ပေးမည်။

## Policy Gradients Algorithm

policy ကို မော်ဒယ်ဖော်ဆောင်ရန် အလွယ်ဆုံးနည်းလမ်းမှာ အခြေအနေများကို input အဖြစ် လက်ခံပြီး၊ ဆက်စပ်သော လှုပ်ရှားမှုများ (သို့မဟုတ် လှုပ်ရှားမှုအားလုံး၏ probability များ) ကို ပြန်ပေးမည့် နယူးရယ်နက်ဝက်တစ်ခုကို ဖန်တီးခြင်းဖြစ်သည်။ ၎င်းသည် ပုံမှန် အမျိုးအစားခွဲခြားမှုလုပ်ငန်းနှင့် ဆင်တူဖြစ်မည်၊ သို့သော် အဓိကကွာခြားချက်မှာ - ကျွန်ုပ်တို့သည် အဆင့်တစ်ခုစီတွင် ဘယ်လှုပ်ရှားမှုကို ပြုလုပ်သင့်ကြောင်း ကြိုတင်မသိနိုင်ခြင်းဖြစ်သည်။

ဤနေရာတွင် အဓိကအကြံဉာဏ်မှာ ၎င်းတို့၏ probability များကို ခန့်မှန်းခြင်းဖြစ်သည်။ ကျွန်ုပ်တို့သည် စမ်းသပ်မှု၏ အဆင့်တစ်ခုစီတွင် ရရှိသော စုစုပေါင်းဆုချီးမြှင့်မှုများကို ပြသသည့် **cumulative rewards** ဗက်တာတစ်ခုကို တည်ဆောက်သည်။ ထို့နောက် **reward discounting** ကို အသုံးပြု၍ ယခင်ဆုချီးမြှင့်မှုများကို γ=0.99 ဟု သတ်မှတ်ထားသော ကိန်းဂဏန်းဖြင့် မျှတစွာ လျော့နည်းစေရန် ပြုလုပ်သည်။ ထို့နောက်၊ ပိုမိုကြီးမားသော ဆုချီးမြှင့်မှုများကို ရရှိစေသည့် အဆင့်များကို အားဖြည့်ပေးသည်။

> Policy Gradient အယ်လဂိုရီသမ်နှင့် ၎င်း၏ လုပ်ဆောင်မှုကို [ဥပမာ notebook](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) တွင် ပိုမိုလေ့လာပါ။

## Actor-Critic Algorithm

Policy Gradients နည်းလမ်း၏ တိုးတက်သောဗားရှင်းကို **Actor-Critic** ဟု ခေါ်သည်။ ၎င်း၏ အဓိကအကြံဉာဏ်မှာ နယူးရယ်နက်ဝက်ကို အောက်ပါအရာနှစ်ခုကို ပြန်ပေးရန် လေ့ကျင့်ခြင်းဖြစ်သည် -

* policy, ၎င်းသည် ဘယ်လှုပ်ရှားမှုကို ပြုလုပ်ရမည်ကို သတ်မှတ်သည်။ ၎င်းကို **actor** ဟု ခေါ်သည်။
* အခြေအနေတစ်ခုတွင် ရရှိနိုင်မည့် စုစုပေါင်းဆုချီးမြှင့်မှုကို ခန့်မှန်းခြင်း - ၎င်းကို **critic** ဟု ခေါ်သည်။

ဤ architecture သည် [GAN](../../4-ComputerVision/10-GANs/README.md) နှင့် ဆင်တူသည်၊ ၎င်းတွင် နက်ဝက်နှစ်ခုကို တစ်ဦးနှင့်တစ်ဦး အပြိုင်လေ့ကျင့်သည်။ Actor-Critic မော်ဒယ်တွင် actor သည် ကျွန်ုပ်တို့ လုပ်ဆောင်ရမည့် လှုပ်ရှားမှုကို အကြံပြုသည်၊ Critic သည် အကဲဖြတ်ပြီး ရလဒ်ကို ခန့်မှန်းသည်။ သို့သော် ကျွန်ုပ်တို့၏ ရည်မှန်းချက်မှာ ၎င်းတို့ကို တစ်ပြိုင်တည်း လေ့ကျင့်ခြင်းဖြစ်သည်။

စမ်းသပ်မှုအတွင်းတွင် ကျွန်ုပ်တို့သည် စုစုပေါင်းဆုချီးမြှင့်မှုများနှင့် Critic မှ ပြန်ပေးသော ရလဒ်များကို သိရှိထားသောကြောင့်၊ ၎င်းတို့အကြား ကွာဟမှုကို လျှော့ချရန် Loss Function တစ်ခုကို တည်ဆောက်ရန် အလွန်လွယ်ကူသည်။ ၎င်းသည် **critic loss** ကို ပေးမည်။ **actor loss** ကို Policy Gradient အယ်လဂိုရီသမ်တွင် အသုံးပြုသည့် နည်းလမ်းတူတူဖြင့် တွက်ချက်နိုင်သည်။

ဤအယ်လဂိုရီများထဲမှ တစ်ခုကို လုပ်ဆောင်ပြီးနောက် ကျွန်ုပ်တို့၏ CartPole သည် အောက်ပါအတိုင်း လုပ်ဆောင်မည်ဟု မျှော်လင့်နိုင်သည် -

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ လေ့ကျင့်မှုများ: Policy Gradients နှင့် Actor-Critic RL

အောက်ပါ notebook များတွင် သင်ကြားမှုကို ဆက်လက်လုပ်ဆောင်ပါ -

* [RL in TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL in PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## အခြား RL လုပ်ငန်းများ

ယနေ့အခါ Reinforcement Learning သည် အလွန်မြန်ဆန်စွာ တိုးတက်နေသည့် သုတေသနနယ်ပယ်တစ်ခုဖြစ်သည်။ Reinforcement Learning ၏ စိတ်ဝင်စားဖွယ် ဥပမာအချို့မှာ -

* **Atari Games** ကစားရန် ကွန်ပျူတာကို သင်ကြားခြင်း။ ဤပြဿနာ၏ စိန်ခေါ်မှုမှာ အခြေအနေကို ဗက်တာအဖြစ် ရိုးရှင်းစွာ ကိုယ်စားပြုထားခြင်းမဟုတ်ဘဲ၊ screenshot တစ်ခုဖြစ်ပြီး - CNN ကို အသုံးပြု၍ ဤ screen ပုံရိပ်ကို အင်္ဂါရပ်ဗက်တာသို့ ပြောင်းရန် သို့မဟုတ် ဆုချီးမြှင့်မှုအချက်အလက်ကို ထုတ်ယူရန် လိုအပ်သည်။ Atari ဂိမ်းများကို Gym တွင် ရရှိနိုင်သည်။
* Chess နှင့် Go ကဲ့သို့သော စားပွဲပေါ်ဂိမ်းများကို ကွန်ပျူတာကို သင်ကြားခြင်း။ မကြာသေးမီက **Alpha Zero** ကဲ့သို့သော နည်းလမ်းများကို စတင်ကတည်းက အေးဂျင့်နှစ်ဦး တစ်ဦးနှင့်တစ်ဦး အပြိုင်ကစားခြင်းဖြင့် လေ့ကျင့်ခဲ့ပြီး၊ အဆင့်တစ်ခုစီတွင် တိုးတက်မှုရရှိခဲ့သည်။
* စက်မှုလုပ်ငန်းတွင် RL ကို အတုအယောင်မှ ထိန်းချုပ်မှုစနစ်များ ဖန်တီးရန် အသုံးပြုသည်။ [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) ဟုခေါ်သော ဝန်ဆောင်မှုတစ်ခုသည် အထူးသဖြင့် ဤအတွက် ဖန်တီးထားသည်။

## နိဂုံး

ကျွန်ုပ်တို့သည် ယခုအခါ ဂိမ်း၏ ရည်မှန်းထားသော အခြေအနေကို သတ်မှတ်ပေးသည့် ဆုချီးမြှင့်မှုအလုပ်ဆောင်မှုတစ်ခုသာ ပေးခြင်းဖြင့်၊ ၎င်းတို့ကို ရှာဖွေမှုအကျိုးရှိစွာ ပြုလုပ်ရန် အခွင့်အရေးပေးခြင်းဖြင့်၊ အေးဂျင့်များကို ကောင်းမွန်သောရလဒ်ရရှိရန် လေ့ကျင့်ပေးနိုင်သည်ကို သင်ယူခဲ့ပါသည်။ ကျွန်ုပ်တို့သည် အယ်လဂိုရီနှစ်ခုကို အောင်မြင်စွာ စမ်းသပ်ခဲ့ပြီး၊ အချိန်တိုအတွင်း ကောင်းမွန်သောရလဒ်ရရှိခဲ့သည်။ သို့သော် RL သို့ ခရီးစဉ်၏ စတင်ခြင်းသာဖြစ်ပြီး၊ ပိုမိုနက်ရှိုင်းစွာ လေ့လာလိုပါက သီးခြားသင်တန်းတစ်ခုကို စဉ်းစားသင့်သည်။

## 🚀 စိန်ခေါ်မှု

'အခြား RL လုပ်ငန်းများ' အပိုင်းတွင် ဖော်ပြထားသော လျှောက်လွှာများကို စူးစမ်းပြီး၊ တစ်ခုကို အကောင်အထည်ဖော်ကြည့်ပါ။

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## ပြန်လည်သုံးသပ်ခြင်းနှင့် ကိုယ်တိုင်လေ့လာခြင်း

ကျွန်ုပ်တို့၏ [စတင်သူများအတွက် စက်လေ့လာမှု သင်ရိုး](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) တွင် ရိုးရာအတိုးအကျိုးသင်ယူမှုအကြောင်း ပိုမိုလေ့လာပါ။

Super Mario ကို ကွန်ပျူတာက ဘယ်လို သင်ယူကစားနိုင်သလဲဆိုသည်ကို ရှင်းပြသည့် [ဤဗီဒီယို](https://www.youtube.com/watch?v=qv6UVOQ0F44) ကို ကြည့်ပါ။

## လုပ်ငန်းစဉ်: [တောင်တက်မော်တင်

**အကြောင်းကြားချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်မှုများတွင် အမှားများ သို့မဟုတ် မမှန်ကန်မှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းစာရွက်စာတမ်းကို ၎င်း၏ မူရင်းဘာသာစကားဖြင့် အာဏာရှိသောအရင်းအမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များမှ ပရော်ဖက်ရှင်နယ် ဘာသာပြန်မှုကို အကြံပြုပါသည်။ ဤဘာသာပြန်မှုကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအလွတ်များ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။