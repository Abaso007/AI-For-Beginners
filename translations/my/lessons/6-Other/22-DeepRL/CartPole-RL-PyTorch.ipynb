{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL ကို Cartpole Balancing အတွက် လေ့ကျင့်ခြင်း\n",
    "\n",
    "ဒီ notebook က [AI for Beginners Curriculum](http://aka.ms/ai-beginners) ရဲ့ အစိတ်အပိုင်းတစ်ခုဖြစ်ပြီး [PyTorch ရဲ့ တရားဝင်လမ်းညွှန်](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) နဲ့ [ဒီ Cartpole PyTorch အကောင်အထည်](https://github.com/yc930401/Actor-Critic-pytorch) မှ အကြောင်းအရာများကို အခြေခံထားပါတယ်။\n",
    "\n",
    "ဒီဥပမာမှာ RL ကို အသုံးပြုပြီး မော်ဒယ်တစ်ခုကို လေ့ကျင့်ကာ လှေကားပေါ်မှာ တိုင်တစ်ခုကို မျဉ်းပြိုင်အတိုင်း ဘယ်ညာလှုပ်ရှားနိုင်တဲ့ ကားပေါ်မှာ ထိန်းညှိနိုင်အောင် လုပ်ဆောင်ပါမယ်။ [OpenAI Gym](https://www.gymlibrary.ml/) environment ကို အသုံးပြုပြီး simulation ကို ဖန်တီးပါမယ်။\n",
    "\n",
    "> **Note**: ဒီသင်ခန်းစာရဲ့ code ကို ဒေသတွင်းမှာ (ဥပမာ Visual Studio Code မှ) အလုပ်လုပ်နိုင်ပါတယ်၊ ဒီအခါမှာ simulation က window အသစ်မှာ ဖွင့်ပါမယ်။ အွန်လိုင်းမှာ code ကို အလုပ်လုပ်စေချင်ရင် [ဒီမှာ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) ဖော်ပြထားတဲ့အတိုင်း code ကို အနည်းငယ် ပြင်ဆင်ဖို့ လိုအပ်နိုင်ပါတယ်။\n",
    "\n",
    "အရင်ဆုံး Gym ကို install လုပ်ထားတာ သေချာအောင် စတင်ပါမယ်:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အခုတော့ CartPole ပတ်ဝန်းကျင်ကို ဖန်တီးပြီး၊ အဲဒီပတ်ဝန်းကျင်ကို ဘယ်လိုအလုပ်လုပ်ရမလဲဆိုတာ ကြည့်ကြမယ်။ ပတ်ဝန်းကျင်တစ်ခုမှာ အောက်ပါအင်္ဂါရပ်တွေ ပါဝင်ပါတယ် -\n",
    "\n",
    "* **Action space** ဆိုတာက ဆင်ဆာလျှောက်လှမ်းမှုတစ်ခုစီမှာ ကျွန်တော်တို့လုပ်ဆောင်နိုင်တဲ့ လုပ်ဆောင်ချက်တွေရဲ့ အစုအဝေးဖြစ်ပါတယ်။\n",
    "* **Observation space** ဆိုတာက ကျွန်တော်တို့ စောင့်ကြည့်နိုင်တဲ့ အချက်အလက်တွေရဲ့ အစုအဝေးဖြစ်ပါတယ်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အတုလုပ်ဆောင်မှုကဘယ်လိုအလုပ်လုပ်သလဲဆိုတာကြည့်ကြမယ်။ အောက်မှာရှိတဲ့ loop က `env.step` က `done` ဆိုတဲ့ termination flag ကိုမပေးအထိ အတုလုပ်ဆောင်မှုကို run လုပ်ပေးမှာဖြစ်ပါတယ်။ ကျွန်တော်တို့ `env.action_space.sample()` ကိုသုံးပြီး အရေးယူမှုတွေကို အလွတ်ရွေးချယ်မှာဖြစ်ပြီး၊ ဒါက အတုလုပ်ဆောင်မှုဟာ အလွန်မြန်မြန်ပျက်သွားနိုင်ပါတယ် (CartPole environment က CartPole ရဲ့ အရှိန်၊ တည်နေရာ၊ ဒေါင်လိုက်ထောင့်တွေက သတ်မှတ်ထားတဲ့ကန့်သတ်ချက်တွေကိုကျော်လွန်သွားတဲ့အခါ terminate လုပ်ပါတယ်)။\n",
    "\n",
    "> အတုလုပ်ဆောင်မှုဟာ window အသစ်မှာဖွင့်ပါလိမ့်မယ်။ သင် code ကို အကြိမ်ကြိမ် run လုပ်ပြီး ဘယ်လိုအပြုအမူတွေရှိလဲဆိုတာကြည့်နိုင်ပါတယ်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "သင်တွေ့နိုင်ပါတယ်၊ အချက်အလက်တွေမှာ နံပါတ် ၄ ခု ပါဝင်ပါတယ်။ အဲဒါတွေက:\n",
    "\n",
    "- လှေကားရဲ့ တည်နေရာ\n",
    "- လှေကားရဲ့ အရှိန်\n",
    "- တိုင်ရဲ့ ထောင့်\n",
    "- တိုင်ရဲ့ လှည့်ပတ်နှုန်း\n",
    "\n",
    "`rew` ကတော့ အဆင့်တစ်ခုစီမှာ ရရှိတဲ့ ဆုလာဘ်ဖြစ်ပါတယ်။ CartPole ပတ်ဝန်းကျင်မှာ simulation အဆင့်တစ်ခုစီအတွက် ၁ မှတ်ရရှိပြီး၊ ရည်မှန်းချက်ကတော့ စုစုပေါင်းဆုလာဘ်ကို အများဆုံးရရှိအောင်လုပ်ဖို့၊ အတိုင်မကျဘဲ CartPole ကို ချိန်ခွင့်ပေးနိုင်တဲ့ အချိန်ကို တိုးမြှင့်ဖို့ ဖြစ်ပါတယ်။\n",
    "\n",
    "Reinforcement learning လုပ်စဉ်အတွင်းမှာ၊ ကျွန်တော်တို့ရဲ့ ရည်မှန်းချက်က **policy** $\\pi$ ကို လေ့ကျင့်ဖို့ဖြစ်ပြီး၊ အခြေအနေတစ်ခု $s$ အတွက် ဘယ်အရေးယူမှု $a$ ကို လုပ်ရမယ်ဆိုတာကို ပြောပြပေးမှာဖြစ်ပါတယ်။ အဓိကအားဖြင့် $a = \\pi(s)$ ဖြစ်ပါတယ်။\n",
    "\n",
    "အကွေ့အကွင်းဖြေရှင်းမှုကို သင်လိုချင်ရင်၊ policy ကို အရေးယူမှုတစ်ခုစီအတွက် probability တွေကို ပြန်ပေးတဲ့အနေနဲ့ စဉ်းစားနိုင်ပါတယ်။ ဥပမာ $\\pi(a|s)$ ဆိုတာကတော့ အခြေအနေ $s$ မှာ အရေးယူမှု $a$ ကို လုပ်သင့်တဲ့ probability ဖြစ်ပါတယ်။\n",
    "\n",
    "## Policy Gradient Method\n",
    "\n",
    "အလွယ်ဆုံး RL algorithm ဖြစ်တဲ့ **Policy Gradient** မှာတော့၊ နောက်ထပ် အရေးယူမှုကို ခန့်မှန်းပေးဖို့ neural network ကို လေ့ကျင့်သွားမှာ ဖြစ်ပါတယ်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ကျွန်ုပ်တို့သည် အများအပြား စမ်းသပ်မှုများကို လုပ်ဆောင်ခြင်းဖြင့် ကွန်ယက်ကို လေ့ကျင့်မည်ဖြစ်ပြီး၊ စမ်းသပ်မှုတစ်ခုစီပြီးဆုံးသောအခါတွင် ကွန်ယက်ကို အပ်ဒိတ်လုပ်မည်ဖြစ်သည်။ စမ်းသပ်မှုကို လုပ်ဆောင်ပြီး ရလဒ်များ (အခြေအနေများ၊ လုပ်ဆောင်မှုများ (နှင့်၎င်းတို့၏ အကြံပြုထားသော အလားအလာများ)၊ နှင့် ဆုများ) ကို ပြန်ပေးမည့် အလုပ်ဆောင်မှုတစ်ခုကို သတ်မှတ်ကြစို့:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "သင်မလေ့ကျင့်ထားသောကွန်ရက်ဖြင့်တစ်ပိုင်းဆောင်ရွက်နိုင်ပြီး အကျိုးအမြတ်စုစုပေါင်း (အပိုင်း၏အရှည်အဖြစ်လည်းသိထားသော) သို့မဟုတ်အလွန်နည်းပါးသည်ကိုကြည့်ရှုနိုင်သည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ပေါ်လစီဂရေဒီယင့်အယ်လ်ဂိုရီသမ်၏ခက်ခဲသောအပိုင်းများထဲမှတစ်ခုမှာ **လျော့ချထားသောဆုများ** ကိုအသုံးပြုခြင်းဖြစ်သည်။ အကြံဉာဏ်မှာဂိမ်း၏အဆင့်တစ်ခုစီတွင်စုစုပေါင်းဆုများ၏ဗက်တာကိုတွက်ချက်ပြီး၊ ဤလုပ်ငန်းစဉ်အတွင်းမှာ $gamma$ ဆိုသောကိန်းဂဏန်းတစ်ခုကိုအသုံးပြုပြီးစောစောဆုများကိုလျော့ချခြင်းဖြစ်သည်။ ထို့အပြင်၊ ကျွန်ုပ်တို့၏လေ့ကျင့်မှုကိုသက်ရောက်စေရန်အလေးချိန်အဖြစ်အသုံးပြုမည့်အတွက်ရလဒ်ဗက်တာကိုလည်းပုံမှန်အောင်ပြုလုပ်ပါသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အခုတော့ လေ့ကျင့်မှုကို စတင်ကြရအောင်! ကျွန်တော်တို့ ၃၀၀ ပတ်လည် လေ့ကျင့်မှုကို ပြုလုပ်မယ်၊ အဲဒီမှာ တစ်ပတ်လည်စီမှာ အောက်ပါအတိုင်း လုပ်ဆောင်သွားမယ် -\n",
    "\n",
    "1. စမ်းသပ်မှုကို ပြုလုပ်ပြီး trace ကို စုဆောင်းပါ။\n",
    "2. လုပ်ဆောင်ခဲ့တဲ့ လှုပ်ရှားမှုများနဲ့ ခန့်မှန်းထားတဲ့ probability များအကြား ကွာဟမှု (`gradients`) ကိုတွက်ချက်ပါ။ ကွာဟမှု နည်းလျှင်, ကျွန်တော်တို့ မှန်ကန်တဲ့ လှုပ်ရှားမှုကို လုပ်ဆောင်ခဲ့တယ်ဆိုတာ ပိုမိုသေချာနိုင်ပါတယ်။\n",
    "3. Discounted rewards ကိုတွက်ချက်ပြီး gradients နဲ့ ပေါင်းစပ်ပါ - ဒါကတော့ reward မြင့်တဲ့ လှုပ်ရှားမှုတွေက နောက်ဆုံးရလဒ်မှာ ပိုမိုအရေးပါလာစေပြီး, reward နည်းတဲ့ လှုပ်ရှားမှုတွေက သက်သာသွားစေမှာ ဖြစ်ပါတယ်။\n",
    "4. ကျွန်တော်တို့ရဲ့ neural network အတွက် မျှော်မှန်းထားတဲ့ လုပ်ဆောင်မှုများကို, စမ်းသပ်မှုအတွင်း ခန့်မှန်းထားတဲ့ probability များနဲ့, gradient တွေကို တွက်ချက်ထားတဲ့ အချက်အလက်များမှ အစိတ်အပိုင်းတစ်ချို့ကို ယူပါမယ်။ `alpha` parameter ကို အသုံးပြုပြီး gradient နဲ့ reward များကို ဘယ်လောက်အထိ အရေးထားမလဲဆိုတာ ဆုံးဖြတ်မယ် - ဒါကို reinforcement algorithm ရဲ့ *learning rate* လို့ ခေါ်ပါတယ်။\n",
    "5. နောက်ဆုံးမှာတော့, state များနဲ့ မျှော်မှန်းထားတဲ့ လုပ်ဆောင်မှုများကို အသုံးပြုပြီး network ကို လေ့ကျင့်ပါ၊ ပြီးတော့ ဒီလုပ်ငန်းစဉ်ကို ထပ်မံလုပ်ဆောင်ပါ။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အခုတော့ ရလဒ်ကိုကြည့်ရန် rendering ဖြင့် အပိုင်းကို ပြေးကြည့်ရအောင်:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အခုတော့ သင်မြင်နိုင်ပြီလား၊ တိုင်တစ်ခုဟာ အတော်လေး မျှတစွာ ထိန်းထားနိုင်ပြီဆိုတာကို!\n",
    "\n",
    "## Actor-Critic မော်ဒယ်\n",
    "\n",
    "Actor-Critic မော်ဒယ်ကတော့ policy gradients ရဲ့ နောက်ထပ် တိုးတက်မှုတစ်ခုဖြစ်ပြီး၊ ဒီမှာတော့ policy နဲ့ ခန့်မှန်းထားတဲ့ rewards နှစ်ခုလုံးကို သင်ယူနိုင်ဖို့အတွက် နယူးရယ်နက်ဝက်တစ်ခုကို တည်ဆောက်ပါတယ်။ ဒီနက်ဝက်မှာ output နှစ်ခု (သို့မဟုတ် သီးခြားနက်ဝက်နှစ်ခုလိုလည်း တွေးနိုင်ပါတယ်) ရှိမှာဖြစ်ပါတယ်။\n",
    "* **Actor** ကတော့ policy gradient မော်ဒယ်မှာလိုပဲ state probability distribution ကို ပေးပြီး ဘယ်လို လုပ်ဆောင်မှုကို လုပ်မလဲဆိုတာ အကြံပြုပေးမှာဖြစ်ပါတယ်။\n",
    "* **Critic** ကတော့ အဲ့ဒီ လုပ်ဆောင်မှုတွေကနေ ရရှိမယ့် reward ကို ခန့်မှန်းပေးမှာဖြစ်ပါတယ်။ ဒါကတော့ ပေးထားတဲ့ state မှာ အနာဂတ်မှာ ရရှိနိုင်မယ့် စုစုပေါင်း ခန့်မှန်းထားတဲ့ rewards ကို ပြန်ပေးမှာဖြစ်ပါတယ်။\n",
    "\n",
    "ဒီလိုမော်ဒယ်တစ်ခုကို သတ်မှတ်ကြည့်ရအောင်:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ကျွန်တော်တို့ `discounted_rewards` နှင့် `run_episode` လုပ်ဆောင်မှုများကို အနည်းငယ်ပြင်ဆင်ရန် လိုအပ်ပါမည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အခုကျွန်တော်တို့ အဓိကလေ့ကျင့်မှု loop ကို run မယ်။ Loss function တွေကို မှန်ကန်စွာတွက်ချက်ပြီး network parameters တွေကို update လုပ်ခြင်းဖြင့် လက်ဖြင့် network လေ့ကျင့်မှုလုပ်ငန်းစဉ်ကို အသုံးပြုမယ်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## အဓိကအချက်\n",
    "\n",
    "ဒီဒင်္ဂါးပြားမှာ RL အယ်လဂိုရီသုံးမျိုးကို ကြည့်ရှုခဲ့ပါတယ် - ရိုးရှင်းတဲ့ policy gradient နဲ့ ပိုမိုတိုးတက်တဲ့ actor-critic ဖြစ်ပါတယ်။ ဒီအယ်လဂိုရီသုံးမျိုးဟာ state, action နဲ့ reward ဆိုတဲ့ အကြမ်းဖျင်းအယူအဆတွေနဲ့ အလုပ်လုပ်ကြောင်းကို သတိထားမိနိုင်ပါတယ် - ဒါကြောင့် အလွန်ကွဲပြားတဲ့ ပတ်ဝန်းကျင်တွေမှာတောင် အသုံးချနိုင်ပါတယ်။\n",
    "\n",
    "Reinforcement learning က ပြဿနာကို ဖြေရှင်းဖို့အတွက် အကောင်းဆုံးမဟာဗျူဟာကို နောက်ဆုံးရရှိတဲ့ reward ကိုသာကြည့်ပြီး သင်ယူနိုင်စေပါတယ်။ Label တပ်ထားတဲ့ datasets မလိုအပ်တာကြောင့် မော်ဒယ်တွေကို အကောင်းဆုံးဖြစ်အောင် simulation တွေကို မကြိမ်မရှည် ပြန်လုပ်နိုင်ပါတယ်။ သို့သော် RL မှာ အခက်အခဲတွေ အများကြီးရှိဆဲဖြစ်ပြီး၊ ဒီ AI ရဲ့ စိတ်ဝင်စားဖွယ်ရာနယ်ပယ်မှာ ပိုမိုအာရုံစိုက်ဖို့ ဆုံးဖြတ်ရင် သင်လေ့လာနိုင်မယ့် အရာတွေ အများကြီးရှိပါတယ်။\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**အကြောင်းကြားချက်**:  \nဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်ခြင်းတွင် အမှားများ သို့မဟုတ် မမှန်ကန်မှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းဘာသာစကားဖြင့် ရေးသားထားသော စာရွက်စာတမ်းကို အာဏာတရ အရင်းအမြစ်အဖြစ် ရှုလေ့လာသင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များမှ ပရော်ဖက်ရှင်နယ် ဘာသာပြန်ခြင်းကို အကြံပြုပါသည်။ ဤဘာသာပြန်ကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအမှားများ သို့မဟုတ် အနားယူမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-30T08:53:49+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "my"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}