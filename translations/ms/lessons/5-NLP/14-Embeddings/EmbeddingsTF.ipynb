{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pembenaman\n",
    "\n",
    "Dalam contoh sebelumnya, kita menggunakan vektor bag-of-words berdimensi tinggi dengan panjang `vocab_size`, dan kita secara eksplisit menukar vektor representasi kedudukan berdimensi rendah kepada representasi satu-hot yang jarang. Representasi satu-hot ini tidak cekap dari segi memori. Selain itu, setiap perkataan dianggap secara bebas antara satu sama lain, jadi vektor yang dikodkan satu-hot tidak menggambarkan persamaan semantik antara perkataan.\n",
    "\n",
    "Dalam unit ini, kita akan terus meneroka dataset **News AG**. Untuk memulakan, mari kita muatkan data dan dapatkan beberapa definisi daripada unit sebelumnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apa itu embedding?\n",
    "\n",
    "Idea **embedding** adalah untuk mewakili perkataan menggunakan vektor padat berdimensi rendah yang mencerminkan makna semantik perkataan tersebut. Kita akan bincangkan kemudian bagaimana untuk membina embedding perkataan yang bermakna, tetapi buat masa ini anggaplah embedding sebagai cara untuk mengurangkan dimensi vektor perkataan.\n",
    "\n",
    "Jadi, lapisan embedding mengambil perkataan sebagai input, dan menghasilkan vektor output dengan `embedding_size` yang ditentukan. Dalam satu aspek, ia sangat mirip dengan lapisan `Dense`, tetapi bukannya mengambil vektor one-hot encoded sebagai input, ia mampu mengambil nombor perkataan.\n",
    "\n",
    "Dengan menggunakan lapisan embedding sebagai lapisan pertama dalam rangkaian kita, kita boleh beralih daripada model bag-of-words kepada model **embedding bag**, di mana kita mula-mula menukar setiap perkataan dalam teks kita kepada embedding yang sepadan, dan kemudian mengira beberapa fungsi agregat ke atas semua embedding tersebut, seperti `sum`, `average` atau `max`.\n",
    "\n",
    "![Imej menunjukkan pengelasan embedding untuk lima perkataan dalam urutan.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ms.png)\n",
    "\n",
    "Rangkaian neural pengelasan kita terdiri daripada lapisan-lapisan berikut:\n",
    "\n",
    "* Lapisan `TextVectorization`, yang mengambil string sebagai input, dan menghasilkan tensor nombor token. Kita akan menentukan saiz perbendaharaan kata `vocab_size` yang munasabah, dan mengabaikan perkataan yang jarang digunakan. Bentuk input akan menjadi 1, dan bentuk output akan menjadi $n$, kerana kita akan mendapatkan $n$ token sebagai hasilnya, setiap satu mengandungi nombor dari 0 hingga `vocab_size`.\n",
    "* Lapisan `Embedding`, yang mengambil $n$ nombor, dan mengurangkan setiap nombor kepada vektor padat dengan panjang tertentu (100 dalam contoh kita). Oleh itu, tensor input dengan bentuk $n$ akan ditukar kepada tensor $n\\times 100$.\n",
    "* Lapisan agregasi, yang mengambil purata tensor ini sepanjang paksi pertama, iaitu ia akan mengira purata semua $n$ tensor input yang sepadan dengan perkataan yang berbeza. Untuk melaksanakan lapisan ini, kita akan menggunakan lapisan `Lambda`, dan memasukkan fungsi untuk mengira purata. Output akan mempunyai bentuk 100, dan ia akan menjadi representasi numerik bagi keseluruhan urutan input.\n",
    "* Pengelasan linear `Dense` terakhir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam cetakan `summary`, dalam lajur **output shape**, dimensi tensor pertama `None` merujuk kepada saiz minibatch, dan dimensi kedua merujuk kepada panjang urutan token. Semua urutan token dalam minibatch mempunyai panjang yang berbeza. Kita akan bincangkan cara mengatasinya dalam bahagian seterusnya.\n",
    "\n",
    "Sekarang mari kita latih rangkaian:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Nota** bahawa kami sedang membina penvektoran berdasarkan subset data. Ini dilakukan untuk mempercepatkan proses, dan ia mungkin menyebabkan situasi di mana tidak semua token daripada teks kami terdapat dalam perbendaharaan kata. Dalam kes ini, token tersebut akan diabaikan, yang mungkin mengakibatkan ketepatan yang sedikit lebih rendah. Walau bagaimanapun, dalam kehidupan sebenar, subset teks sering memberikan anggaran perbendaharaan kata yang baik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menguruskan saiz urutan pembolehubah\n",
    "\n",
    "Mari kita fahami bagaimana latihan berlaku dalam minibatch. Dalam contoh di atas, tensor input mempunyai dimensi 1, dan kita menggunakan minibatch sepanjang 128, jadi saiz sebenar tensor adalah $128 \\times 1$. Walau bagaimanapun, bilangan token dalam setiap ayat adalah berbeza. Jika kita menggunakan lapisan `TextVectorization` pada satu input, bilangan token yang dikembalikan adalah berbeza, bergantung pada bagaimana teks itu ditokenkan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namun, apabila kita menggunakan penvektor pada beberapa jujukan, ia perlu menghasilkan tensor berbentuk segi empat, jadi ia mengisi elemen yang tidak digunakan dengan token PAD (yang dalam kes kita adalah sifar):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di sini kita dapat melihat penjelmaan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: Untuk meminimumkan jumlah padding, dalam beberapa kes adalah masuk akal untuk menyusun semua jujukan dalam dataset mengikut urutan panjang yang meningkat (atau, lebih tepat lagi, bilangan token). Ini akan memastikan setiap minibatch mengandungi jujukan dengan panjang yang serupa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perwakilan Semantik: Word2Vec\n",
    "\n",
    "Dalam contoh sebelumnya, lapisan embedding belajar memetakan perkataan kepada perwakilan vektor, namun perwakilan ini tidak mempunyai makna semantik. Akan lebih baik jika kita dapat belajar perwakilan vektor di mana perkataan yang serupa atau sinonim mempunyai vektor yang dekat antara satu sama lain berdasarkan jarak vektor tertentu (contohnya jarak Euclidean).\n",
    "\n",
    "Untuk mencapai itu, kita perlu melatih model embedding kita terlebih dahulu pada koleksi teks yang besar menggunakan teknik seperti [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ia berdasarkan dua seni bina utama yang digunakan untuk menghasilkan perwakilan teragih bagi perkataan:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), di mana kita melatih model untuk meramal satu perkataan berdasarkan konteks sekeliling. Diberikan ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, matlamat model adalah untuk meramal $W_0$ daripada $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** adalah bertentangan dengan CBoW. Model menggunakan tetingkap perkataan konteks sekeliling untuk meramal perkataan semasa.\n",
    "\n",
    "CBoW lebih pantas, manakala skip-gram lebih perlahan tetapi lebih baik dalam mewakili perkataan yang jarang digunakan.\n",
    "\n",
    "![Imej menunjukkan algoritma CBoW dan Skip-Gram untuk menukar perkataan kepada vektor.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ms.png)\n",
    "\n",
    "Untuk mencuba embedding Word2Vec yang telah dilatih terlebih dahulu pada dataset Google News, kita boleh menggunakan pustaka **gensim**. Di bawah ini, kita mencari perkataan yang paling serupa dengan 'neural'.\n",
    "\n",
    "> **Nota:** Apabila anda mula-mula mencipta vektor perkataan, memuat turun mereka mungkin mengambil masa!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga boleh mengekstrak penjelmaan vektor daripada perkataan, untuk digunakan dalam melatih model klasifikasi. Penjelmaan tersebut mempunyai 300 komponen, tetapi di sini kami hanya menunjukkan 20 komponen pertama vektor untuk kejelasan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perkara hebat tentang pengekodan semantik ialah anda boleh memanipulasi pengekodan vektor berdasarkan semantik. Sebagai contoh, kita boleh meminta untuk mencari perkataan yang pengekodan vektornya sedekat mungkin dengan perkataan *raja* dan *wanita*, dan sejauh mungkin dari perkataan *lelaki*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Contoh di atas menggunakan beberapa keajaiban dalaman GenSym, tetapi logik asasnya sebenarnya agak mudah. Satu perkara menarik tentang embedding ialah anda boleh melakukan operasi vektor biasa pada vektor embedding, dan itu akan mencerminkan operasi pada **makna** perkataan. Contoh di atas boleh dinyatakan dalam bentuk operasi vektor: kita mengira vektor yang sepadan dengan **KING-MAN+WOMAN** (operasi `+` dan `-` dilakukan pada representasi vektor perkataan yang sepadan), dan kemudian mencari perkataan paling hampir dalam kamus kepada vektor tersebut:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTA**: Kami terpaksa menambah pekali kecil pada vektor *man* dan *woman* - cuba keluarkan pekali tersebut untuk melihat apa yang berlaku.\n",
    "\n",
    "Untuk mencari vektor yang paling hampir, kami menggunakan mekanisme TensorFlow untuk mengira vektor jarak antara vektor kami dan semua vektor dalam perbendaharaan kata, dan kemudian mencari indeks perkataan minimum menggunakan `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walaupun Word2Vec kelihatan seperti cara yang hebat untuk menyatakan semantik perkataan, ia mempunyai banyak kelemahan, termasuk yang berikut:\n",
    "\n",
    "* Kedua-dua model CBoW dan skip-gram adalah **predictive embeddings**, dan mereka hanya mengambil kira konteks tempatan. Word2Vec tidak memanfaatkan konteks global.\n",
    "* Word2Vec tidak mengambil kira **morfologi** perkataan, iaitu hakikat bahawa makna perkataan boleh bergantung pada bahagian-bahagian tertentu dalam perkataan, seperti akar perkataan.\n",
    "\n",
    "**FastText** cuba mengatasi kelemahan kedua ini, dan membina atas Word2Vec dengan mempelajari representasi vektor untuk setiap perkataan dan n-gram aksara yang terdapat dalam setiap perkataan. Nilai-nilai representasi ini kemudian dirata-rata menjadi satu vektor pada setiap langkah latihan. Walaupun ini menambah banyak pengiraan tambahan semasa prapemodelan, ia membolehkan embeddings perkataan menyandikan maklumat sub-perkataan.\n",
    "\n",
    "Kaedah lain, **GloVe**, menggunakan pendekatan yang berbeza untuk embeddings perkataan, berdasarkan faktorisasi matriks konteks-perkataan. Pertama, ia membina matriks besar yang mengira bilangan kejadian perkataan dalam pelbagai konteks, dan kemudian ia cuba mewakili matriks ini dalam dimensi yang lebih rendah dengan cara yang meminimumkan kehilangan rekonstruksi.\n",
    "\n",
    "Perpustakaan gensim menyokong embeddings perkataan ini, dan anda boleh mencuba dengan menukar kod pemuatan model di atas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan embedding pralatih dalam Keras\n",
    "\n",
    "Kita boleh mengubah contoh di atas untuk mengisi matriks dalam lapisan embedding kita dengan embedding semantik, seperti Word2Vec. Perbendaharaan kata embedding pralatih dan korpus teks kemungkinan besar tidak sepadan, jadi kita perlu memilih salah satu. Di sini kita meneroka dua pilihan yang mungkin: menggunakan perbendaharaan kata tokenizer, dan menggunakan perbendaharaan kata daripada embedding Word2Vec.\n",
    "\n",
    "### Menggunakan perbendaharaan kata tokenizer\n",
    "\n",
    "Apabila menggunakan perbendaharaan kata tokenizer, beberapa perkataan daripada perbendaharaan kata akan mempunyai embedding Word2Vec yang sepadan, dan beberapa akan tiada. Memandangkan saiz perbendaharaan kata kita adalah `vocab_size`, dan panjang vektor embedding Word2Vec adalah `embed_size`, lapisan embedding akan diwakili oleh matriks berat dengan bentuk `vocab_size`$\\times$`embed_size`. Kita akan mengisi matriks ini dengan melalui perbendaharaan kata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk perkataan yang tidak terdapat dalam perbendaharaan kata Word2Vec, kita boleh sama ada meninggalkannya sebagai sifar, atau menghasilkan vektor rawak.\n",
    "\n",
    "Sekarang kita boleh mendefinisikan lapisan embedding dengan berat yang telah dilatih:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: Perhatikan bahawa kami menetapkan `trainable=False` semasa mencipta `Embedding`, yang bermaksud kami tidak melatih semula lapisan Embedding. Ini mungkin menyebabkan ketepatan sedikit lebih rendah, tetapi ia mempercepatkan proses latihan.\n",
    "\n",
    "### Menggunakan perbendaharaan kata embedding\n",
    "\n",
    "Satu masalah dengan pendekatan sebelumnya ialah perbendaharaan kata yang digunakan dalam TextVectorization dan Embedding adalah berbeza. Untuk mengatasi masalah ini, kita boleh menggunakan salah satu daripada penyelesaian berikut:\n",
    "* Melatih semula model Word2Vec pada perbendaharaan kata kita.\n",
    "* Memuatkan dataset kita dengan perbendaharaan kata daripada model Word2Vec yang telah dilatih. Perbendaharaan kata yang digunakan untuk memuatkan dataset boleh ditentukan semasa proses pemuatan.\n",
    "\n",
    "Pendekatan kedua kelihatan lebih mudah, jadi mari kita laksanakan. Pertama sekali, kita akan mencipta lapisan `TextVectorization` dengan perbendaharaan kata yang ditentukan, diambil daripada embedding Word2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perpustakaan pemetaan perkataan gensim mengandungi fungsi yang mudah, `get_keras_embeddings`, yang akan secara automatik mencipta lapisan pemetaan Keras yang sepadan untuk anda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salah satu sebab kita tidak melihat ketepatan yang lebih tinggi adalah kerana beberapa perkataan daripada set data kita tiada dalam kosa kata GloVe yang telah dilatih, dan oleh itu ia pada dasarnya diabaikan. Untuk mengatasi ini, kita boleh melatih penjelmaan kita sendiri berdasarkan set data kita.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pembenaman Kontekstual\n",
    "\n",
    "Satu kelemahan utama dalam representasi pembenaman pralatih tradisional seperti Word2Vec adalah hakikat bahawa, walaupun ia dapat menangkap sebahagian makna sesuatu perkataan, ia tidak dapat membezakan antara makna yang berbeza. Ini boleh menyebabkan masalah dalam model hiliran.\n",
    "\n",
    "Sebagai contoh, perkataan 'play' mempunyai makna yang berbeza dalam dua ayat berikut:\n",
    "- Saya pergi ke sebuah **play** di teater.\n",
    "- John mahu **play** dengan kawan-kawannya.\n",
    "\n",
    "Pembenaman pralatih yang kita bincangkan sebelum ini mewakili kedua-dua makna perkataan 'play' dalam pembenaman yang sama. Untuk mengatasi kelemahan ini, kita perlu membina pembenaman berdasarkan **model bahasa**, yang dilatih menggunakan korpus teks yang besar, dan *tahu* bagaimana perkataan boleh disusun dalam konteks yang berbeza. Perbincangan tentang pembenaman kontekstual adalah di luar skop tutorial ini, tetapi kita akan kembali kepada topik ini apabila membincangkan model bahasa dalam unit seterusnya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-29T16:24:29+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "ms"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}