{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mekanisme Perhatian dan Transformer\n",
    "\n",
    "Satu kelemahan utama rangkaian berulang (recurrent networks) ialah semua perkataan dalam satu urutan mempunyai kesan yang sama terhadap hasilnya. Ini menyebabkan prestasi yang kurang optimum dengan model LSTM encoder-decoder standard untuk tugas urutan ke urutan, seperti Pengecaman Entiti Bernama dan Terjemahan Mesin. Dalam realiti, perkataan tertentu dalam urutan input sering kali mempunyai lebih banyak pengaruh terhadap output berurutan berbanding yang lain.\n",
    "\n",
    "Pertimbangkan model urutan ke urutan, seperti terjemahan mesin. Ia dilaksanakan oleh dua rangkaian berulang, di mana satu rangkaian (**encoder**) akan memampatkan urutan input ke dalam keadaan tersembunyi, dan satu lagi, **decoder**, akan mengembangkan keadaan tersembunyi ini menjadi hasil terjemahan. Masalah dengan pendekatan ini ialah keadaan akhir rangkaian akan menghadapi kesukaran untuk mengingati permulaan ayat, menyebabkan kualiti model yang rendah pada ayat yang panjang.\n",
    "\n",
    "**Mekanisme Perhatian** menyediakan cara untuk memberi pemberat kepada kesan kontekstual setiap vektor input terhadap setiap ramalan output RNN. Cara ia dilaksanakan adalah dengan mencipta jalan pintas antara keadaan perantaraan RNN input dan RNN output. Dengan cara ini, apabila menghasilkan simbol output $y_t$, kita akan mengambil kira semua keadaan tersembunyi input $h_i$, dengan pekali pemberat yang berbeza $\\alpha_{t,i}$. \n",
    "\n",
    "![Imej menunjukkan model encoder/decoder dengan lapisan perhatian tambahan](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.ms.png)\n",
    "*Model encoder-decoder dengan mekanisme perhatian tambahan dalam [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), dipetik daripada [catatan blog ini](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matriks perhatian $\\{\\alpha_{i,j}\\}$ akan mewakili tahap di mana perkataan input tertentu memainkan peranan dalam penjanaan perkataan tertentu dalam urutan output. Di bawah adalah contoh matriks seperti itu:\n",
    "\n",
    "![Imej menunjukkan penjajaran sampel yang ditemui oleh RNNsearch-50, diambil daripada Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.ms.png)\n",
    "\n",
    "*Rajah diambil daripada [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Rajah 3)*\n",
    "\n",
    "Mekanisme perhatian bertanggungjawab untuk sebahagian besar kemajuan terkini atau hampir terkini dalam Pemprosesan Bahasa Semula Jadi. Walau bagaimanapun, penambahan perhatian secara signifikan meningkatkan bilangan parameter model, yang membawa kepada isu penskalaan dengan RNN. Satu kekangan utama dalam penskalaan RNN ialah sifat berulang model ini menjadikannya mencabar untuk melatih secara kelompok dan selari. Dalam RNN, setiap elemen dalam urutan perlu diproses secara berurutan, yang bermaksud ia tidak boleh diselarikan dengan mudah.\n",
    "\n",
    "Penggunaan mekanisme perhatian bersama kekangan ini membawa kepada penciptaan Model Transformer yang kini menjadi standard terkini yang kita kenali dan gunakan hari ini, daripada BERT hingga OpenGPT3.\n",
    "\n",
    "## Model Transformer\n",
    "\n",
    "Daripada meneruskan konteks setiap ramalan sebelumnya ke langkah penilaian seterusnya, **model transformer** menggunakan **pengekodan kedudukan** dan **perhatian** untuk menangkap konteks input tertentu dalam tetingkap teks yang disediakan. Imej di bawah menunjukkan bagaimana pengekodan kedudukan dengan perhatian dapat menangkap konteks dalam tetingkap tertentu.\n",
    "\n",
    "![GIF animasi menunjukkan bagaimana penilaian dilakukan dalam model transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Oleh kerana setiap kedudukan input dipetakan secara bebas ke setiap kedudukan output, transformer dapat diselarikan dengan lebih baik berbanding RNN, yang membolehkan model bahasa yang lebih besar dan lebih ekspresif. Setiap kepala perhatian boleh digunakan untuk mempelajari hubungan yang berbeza antara perkataan, yang meningkatkan tugas Pemprosesan Bahasa Semula Jadi hiliran.\n",
    "\n",
    "## Membina Model Transformer Ringkas\n",
    "\n",
    "Keras tidak mengandungi lapisan Transformer terbina dalam, tetapi kita boleh membinanya sendiri. Seperti sebelumnya, kita akan memberi tumpuan kepada pengelasan teks dataset AG News, tetapi perlu disebutkan bahawa model Transformer menunjukkan hasil terbaik pada tugas NLP yang lebih sukar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lapisan baharu dalam Keras perlu mewarisi kelas `Layer`, dan melaksanakan kaedah `call`. Mari kita mulakan dengan lapisan **Positional Embedding**. Kita akan menggunakan [beberapa kod daripada dokumentasi rasmi Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Kita akan menganggap bahawa kita melapik semua jujukan input kepada panjang `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lapisan ini terdiri daripada dua lapisan `Embedding`: untuk pengekodan token (dengan cara yang telah kita bincangkan sebelum ini) dan kedudukan token. Kedudukan token dicipta sebagai urutan nombor semula jadi dari 0 hingga `maxlen` menggunakan `tf.range`, dan kemudian dihantar melalui lapisan embedding. Dua vektor embedding yang dihasilkan kemudian ditambah, menghasilkan representasi input yang tertanam secara kedudukan dengan bentuk `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Sekarang, mari kita laksanakan blok transformer. Ia akan mengambil output daripada lapisan embedding yang telah ditakrifkan sebelum ini:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer menggunakan `MultiHeadAttention` pada input yang telah dikodkan dengan posisi untuk menghasilkan vektor perhatian dengan dimensi `maxlen`$\\times$`embed_dim`, yang kemudian dicampur dengan input dan dinormalisasi menggunakan `LayerNormalization`.\n",
    "\n",
    "> **Nota**: `LayerNormalization` adalah serupa dengan `BatchNormalization` yang dibincangkan dalam bahagian *Computer Vision* dalam laluan pembelajaran ini, tetapi ia menormalkan output lapisan sebelumnya untuk setiap sampel latihan secara bebas, untuk membawanya ke julat [-1..1].\n",
    "\n",
    "Output daripada lapisan ini kemudian dihantar melalui rangkaian `Dense` (dalam kes kita - perceptron dua lapisan), dan hasilnya ditambah kepada output akhir (yang sekali lagi menjalani normalisasi). \n",
    "\n",
    "Sekarang, kita bersedia untuk mentakrifkan model transformer lengkap:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Transformer BERT\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) ialah rangkaian transformer berbilang lapisan yang sangat besar dengan 12 lapisan untuk *BERT-base*, dan 24 lapisan untuk *BERT-large*. Model ini mula-mula dilatih awal menggunakan korpus teks yang besar (WikiPedia + buku) melalui latihan tanpa pengawasan (meramalkan perkataan yang disembunyikan dalam ayat). Semasa latihan awal, model ini menyerap tahap pemahaman bahasa yang signifikan yang kemudiannya boleh dimanfaatkan dengan dataset lain melalui penalaan halus. Proses ini dipanggil **pembelajaran pemindahan**.\n",
    "\n",
    "![gambar dari http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.ms.png)\n",
    "\n",
    "Terdapat banyak variasi seni bina Transformer termasuk BERT, DistilBERT, BigBird, OpenGPT3 dan banyak lagi yang boleh ditala halus.\n",
    "\n",
    "Mari kita lihat bagaimana kita boleh menggunakan model BERT yang telah dilatih awal untuk menyelesaikan masalah klasifikasi jujukan tradisional kita. Kita akan meminjam idea dan beberapa kod daripada [dokumentasi rasmi](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Untuk memuatkan model yang telah dilatih awal, kita akan menggunakan **Tensorflow hub**. Pertama, mari kita muatkan penvektor khusus BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adalah penting untuk menggunakan penvektor yang sama seperti yang digunakan oleh rangkaian asal semasa dilatih. Selain itu, penvektor BERT mengembalikan tiga komponen:\n",
    "* `input_word_ids`, iaitu satu urutan nombor token untuk ayat input\n",
    "* `input_mask`, menunjukkan bahagian mana dalam urutan yang mengandungi input sebenar, dan bahagian mana yang merupakan padding. Ia serupa dengan topeng yang dihasilkan oleh lapisan `Masking`\n",
    "* `input_type_ids` digunakan untuk tugas pemodelan bahasa, dan membolehkan untuk menentukan dua ayat input dalam satu urutan.\n",
    "\n",
    "Kemudian, kita boleh memulakan pengekstrak ciri BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jadi, lapisan BERT mengembalikan beberapa hasil yang berguna:\n",
    "* `pooled_output` adalah hasil daripada purata semua token dalam urutan. Anda boleh menganggapnya sebagai embedding semantik pintar untuk keseluruhan rangkaian. Ia setara dengan output lapisan `GlobalAveragePooling1D` dalam model kita sebelum ini.\n",
    "* `sequence_output` adalah output daripada lapisan transformer terakhir (bersamaan dengan output `TransformerBlock` dalam model kita di atas).\n",
    "* `encoder_outputs` adalah output daripada semua lapisan transformer. Oleh kerana kita telah memuatkan model BERT 4-lapisan (seperti yang mungkin anda teka daripada namanya, yang mengandungi `4_H`), ia mempunyai 4 tensor. Tensor terakhir adalah sama seperti `sequence_output`.\n",
    "\n",
    "Sekarang kita akan mentakrifkan model klasifikasi hujung-ke-hujung. Kita akan menggunakan *definisi model fungsional*, di mana kita mentakrifkan input model, dan kemudian menyediakan beberapa ekspresi untuk mengira outputnya. Kita juga akan menjadikan berat model BERT tidak boleh dilatih, dan hanya melatih pengklasifikasi terakhir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walaupun terdapat sedikit parameter yang boleh dilatih, prosesnya agak perlahan kerana pengekstrak ciri BERT memerlukan pengiraan yang berat. Nampaknya kita tidak dapat mencapai ketepatan yang munasabah, sama ada disebabkan kekurangan latihan atau kekurangan parameter model.\n",
    "\n",
    "Mari kita cuba membuka pembekuan berat BERT dan melatihnya juga. Ini memerlukan kadar pembelajaran yang sangat kecil, serta strategi latihan yang lebih teliti dengan **warmup**, menggunakan pengoptimum **AdamW**. Kita akan menggunakan pakej `tf-models-official` untuk mencipta pengoptimum:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti yang anda lihat, latihan berjalan agak perlahan - tetapi anda mungkin ingin mencuba dan melatih model selama beberapa epoch (5-10) dan lihat jika anda boleh mendapatkan hasil terbaik berbanding pendekatan yang telah kita gunakan sebelum ini.\n",
    "\n",
    "## Perpustakaan Huggingface Transformers\n",
    "\n",
    "Satu lagi cara yang sangat biasa (dan sedikit lebih mudah) untuk menggunakan model Transformer adalah [pakej HuggingFace](https://github.com/huggingface/), yang menyediakan blok binaan mudah untuk pelbagai tugas NLP. Ia tersedia untuk Tensorflow dan PyTorch, satu lagi rangka kerja rangkaian neural yang sangat popular.\n",
    "\n",
    "> **Nota**: Jika anda tidak berminat untuk melihat bagaimana perpustakaan Transformers berfungsi - anda boleh melangkau ke penghujung notebook ini, kerana anda tidak akan melihat apa-apa yang sangat berbeza daripada apa yang telah kita lakukan di atas. Kita akan mengulangi langkah-langkah yang sama untuk melatih model BERT menggunakan perpustakaan yang berbeza dan model yang jauh lebih besar. Oleh itu, proses ini melibatkan latihan yang agak panjang, jadi anda mungkin hanya ingin melihat kodnya.\n",
    "\n",
    "Mari kita lihat bagaimana masalah kita boleh diselesaikan menggunakan [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perkara pertama yang perlu kita lakukan ialah memilih model yang akan kita gunakan. Selain beberapa model terbina dalam, Huggingface mempunyai [repositori model dalam talian](https://huggingface.co/models), di mana anda boleh menemui banyak lagi model pra-latihan daripada komuniti. Semua model tersebut boleh dimuatkan dan digunakan hanya dengan menyediakan nama model. Semua fail binari yang diperlukan untuk model tersebut akan dimuat turun secara automatik.\n",
    "\n",
    "Pada masa tertentu, anda mungkin perlu memuatkan model anda sendiri, dalam hal ini anda boleh menentukan direktori yang mengandungi semua fail berkaitan, termasuk parameter untuk tokenizer, fail `config.json` dengan parameter model, berat binari, dan sebagainya.\n",
    "\n",
    "Daripada nama model, kita boleh mencipta kedua-dua model dan tokenizer. Mari kita mulakan dengan tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objek `tokenizer` mengandungi fungsi `encode` yang boleh digunakan secara langsung untuk mengekod teks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga boleh menggunakan pengekod token untuk mengekod satu jujukan dengan cara yang sesuai untuk dihantar kepada model, iaitu termasuk medan `token_ids`, `input_mask`, dan sebagainya. Kita juga boleh menentukan bahawa kita mahu tensor Tensorflow dengan menyediakan argumen `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam kes kita, kita akan menggunakan model BERT yang telah dilatih terlebih dahulu yang dipanggil `bert-base-uncased`. *Uncased* menunjukkan bahawa model ini tidak sensitif kepada huruf besar atau kecil.\n",
    "\n",
    "Semasa melatih model, kita perlu menyediakan urutan yang telah ditokenkan sebagai input, dan oleh itu kita akan mereka bentuk saluran pemprosesan data. Oleh kerana `tokenizer.encode` adalah fungsi Python, kita akan menggunakan pendekatan yang sama seperti dalam unit sebelumnya dengan memanggilnya menggunakan `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita boleh memuatkan model sebenar menggunakan pakej `BertForSequenceClassfication`. Ini memastikan bahawa model kita sudah mempunyai seni bina yang diperlukan untuk klasifikasi, termasuk pengklasifikasi akhir. Anda akan melihat mesej amaran yang menyatakan bahawa berat pengklasifikasi akhir tidak diinisialisasi, dan model memerlukan pra-latihan - itu adalah perkara yang biasa, kerana itulah yang akan kita lakukan!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti yang anda boleh lihat daripada `summary()`, model ini mengandungi hampir 110 juta parameter! Mungkin, jika kita mahu tugas klasifikasi mudah pada set data yang agak kecil, kita tidak mahu melatih lapisan asas BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita bersedia untuk memulakan latihan!\n",
    "\n",
    "> **Nota**: Melatih model BERT berskala penuh boleh mengambil masa yang sangat lama! Oleh itu, kita hanya akan melatihnya untuk 32 batch pertama. Ini hanya untuk menunjukkan bagaimana latihan model disediakan. Jika anda berminat untuk mencuba latihan berskala penuh - hanya keluarkan parameter `steps_per_epoch` dan `validation_steps`, dan bersedialah untuk menunggu!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika anda meningkatkan bilangan iterasi dan menunggu cukup lama, serta melatih untuk beberapa epoch, anda boleh menjangkakan bahawa klasifikasi BERT memberikan ketepatan terbaik! Ini kerana BERT sudah memahami struktur bahasa dengan baik, dan kita hanya perlu menyesuaikan pengklasifikasi akhir. Walau bagaimanapun, kerana BERT adalah model yang besar, keseluruhan proses latihan mengambil masa yang lama dan memerlukan kuasa pengkomputeran yang tinggi! (GPU, dan sebaiknya lebih daripada satu).\n",
    "\n",
    "> **Note:** Dalam contoh kami, kami telah menggunakan salah satu model BERT pra-latih yang paling kecil. Terdapat model yang lebih besar yang berkemungkinan memberikan hasil yang lebih baik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perkara Penting\n",
    "\n",
    "Dalam unit ini, kita telah melihat seni bina model terkini berdasarkan **transformer**. Kita telah menggunakannya untuk tugas klasifikasi teks, tetapi model BERT juga boleh digunakan untuk pengekstrakan entiti, menjawab soalan, dan tugas NLP lain.\n",
    "\n",
    "Model transformer mewakili tahap terkini dalam NLP, dan dalam kebanyakan kes, ia seharusnya menjadi penyelesaian pertama yang anda cuba apabila melaksanakan penyelesaian NLP tersuai. Walau bagaimanapun, memahami prinsip asas rangkaian neural berulang yang dibincangkan dalam modul ini adalah sangat penting jika anda ingin membina model neural yang lebih maju.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-29T16:02:17+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "ms"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}