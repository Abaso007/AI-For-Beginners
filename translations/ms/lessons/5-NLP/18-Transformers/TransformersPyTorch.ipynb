{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mekanisme Perhatian dan Transformer\n",
    "\n",
    "Satu kelemahan utama rangkaian berulang (recurrent networks) ialah semua perkataan dalam satu urutan mempunyai kesan yang sama terhadap hasilnya. Ini menyebabkan prestasi yang kurang optimum dengan model encoder-decoder LSTM standard untuk tugas urutan ke urutan, seperti Pengecaman Entiti Bernama (Named Entity Recognition) dan Terjemahan Mesin. Dalam realiti, perkataan tertentu dalam urutan input selalunya mempunyai lebih banyak kesan terhadap output berurutan berbanding yang lain.\n",
    "\n",
    "Pertimbangkan model urutan ke urutan, seperti terjemahan mesin. Ia dilaksanakan oleh dua rangkaian berulang, di mana satu rangkaian (**encoder**) akan memampatkan urutan input ke dalam keadaan tersembunyi, dan satu lagi, **decoder**, akan mengembangkan keadaan tersembunyi ini menjadi hasil terjemahan. Masalah dengan pendekatan ini ialah keadaan akhir rangkaian akan menghadapi kesukaran untuk mengingati permulaan ayat, menyebabkan kualiti model yang rendah pada ayat yang panjang.\n",
    "\n",
    "**Mekanisme Perhatian** menyediakan cara untuk memberi pemberat kepada kesan kontekstual setiap vektor input terhadap setiap ramalan output RNN. Cara ia dilaksanakan adalah dengan mencipta pintasan antara keadaan perantaraan RNN input dan RNN output. Dengan cara ini, apabila menghasilkan simbol output $y_t$, kita akan mengambil kira semua keadaan tersembunyi input $h_i$, dengan pekali pemberat yang berbeza $\\alpha_{t,i}$.\n",
    "\n",
    "![Imej menunjukkan model encoder/decoder dengan lapisan perhatian tambahan](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.ms.png)\n",
    "*Model encoder-decoder dengan mekanisme perhatian tambahan dalam [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), dipetik daripada [catatan blog ini](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matriks perhatian $\\{\\alpha_{i,j}\\}$ akan mewakili tahap di mana perkataan input tertentu memainkan peranan dalam penjanaan perkataan tertentu dalam urutan output. Di bawah adalah contoh matriks seperti itu:\n",
    "\n",
    "![Imej menunjukkan penjajaran sampel yang ditemui oleh RNNsearch-50, diambil daripada Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.ms.png)\n",
    "\n",
    "*Rajah diambil daripada [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Rajah 3)*\n",
    "\n",
    "Mekanisme perhatian bertanggungjawab untuk sebahagian besar keadaan seni semasa atau hampir semasa dalam Pemprosesan Bahasa Semula Jadi. Walau bagaimanapun, menambah perhatian secara signifikan meningkatkan bilangan parameter model yang membawa kepada isu penskalaan dengan RNN. Satu kekangan utama penskalaan RNN ialah sifat berulang model menjadikannya mencabar untuk melaksanakan latihan secara batch dan selari. Dalam RNN, setiap elemen dalam satu urutan perlu diproses secara berurutan, yang bermaksud ia tidak boleh dengan mudah diparalelkan.\n",
    "\n",
    "Penggunaan mekanisme perhatian yang digabungkan dengan kekangan ini membawa kepada penciptaan Model Transformer yang kini menjadi Keadaan Seni (State of the Art) yang kita kenal dan gunakan hari ini, daripada BERT hingga OpenGPT3.\n",
    "\n",
    "## Model Transformer\n",
    "\n",
    "Daripada meneruskan konteks setiap ramalan sebelumnya ke langkah penilaian seterusnya, **model transformer** menggunakan **pengekodan kedudukan** dan perhatian untuk menangkap konteks input tertentu dalam tetingkap teks yang disediakan. Imej di bawah menunjukkan bagaimana pengekodan kedudukan dengan perhatian dapat menangkap konteks dalam tetingkap tertentu.\n",
    "\n",
    "![GIF animasi menunjukkan bagaimana penilaian dilakukan dalam model transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Oleh kerana setiap kedudukan input dipetakan secara bebas ke setiap kedudukan output, transformer dapat diparalelkan dengan lebih baik berbanding RNN, yang membolehkan model bahasa yang lebih besar dan lebih ekspresif. Setiap kepala perhatian boleh digunakan untuk mempelajari hubungan yang berbeza antara perkataan yang meningkatkan tugas Pemprosesan Bahasa Semula Jadi hiliran.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) ialah rangkaian transformer berlapis besar dengan 12 lapisan untuk *BERT-base*, dan 24 untuk *BERT-large*. Model ini mula-mula dilatih awal pada korpus teks yang besar (WikiPedia + buku) menggunakan latihan tanpa pengawasan (meramalkan perkataan yang disembunyikan dalam satu ayat). Semasa latihan awal, model menyerap tahap pemahaman bahasa yang signifikan yang kemudiannya boleh dimanfaatkan dengan set data lain menggunakan penalaan halus. Proses ini dipanggil **pembelajaran pemindahan**.\n",
    "\n",
    "![gambar dari http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.ms.png)\n",
    "\n",
    "Terdapat banyak variasi seni bina Transformer termasuk BERT, DistilBERT, BigBird, OpenGPT3 dan banyak lagi yang boleh ditala halus. Pakej [HuggingFace](https://github.com/huggingface/) menyediakan repositori untuk melatih banyak seni bina ini dengan PyTorch.\n",
    "\n",
    "## Menggunakan BERT untuk pengelasan teks\n",
    "\n",
    "Mari kita lihat bagaimana kita boleh menggunakan model BERT yang telah dilatih awal untuk menyelesaikan tugas tradisional kita: pengelasan urutan. Kita akan mengelaskan set data AG News asal kita.\n",
    "\n",
    "Pertama, mari kita muatkan perpustakaan HuggingFace dan set data kita:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oleh kerana kita akan menggunakan model BERT yang telah dilatih, kita perlu menggunakan penanda token tertentu. Pertama, kita akan memuatkan penanda token yang berkaitan dengan model BERT yang telah dilatih.\n",
    "\n",
    "Perpustakaan HuggingFace mengandungi repositori model yang telah dilatih, yang boleh anda gunakan hanya dengan menentukan nama mereka sebagai argumen kepada fungsi `from_pretrained`. Semua fail binari yang diperlukan untuk model akan dimuat turun secara automatik.\n",
    "\n",
    "Namun, pada masa tertentu anda mungkin perlu memuatkan model anda sendiri, di mana anda boleh menentukan direktori yang mengandungi semua fail yang berkaitan, termasuk parameter untuk penanda token, fail `config.json` dengan parameter model, berat binari, dan sebagainya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objek `tokenizer` mengandungi fungsi `encode` yang boleh digunakan secara langsung untuk mengekod teks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian, mari kita buat iterator yang akan kita gunakan semasa latihan untuk mengakses data. Oleh kerana BERT menggunakan fungsi pengekodannya sendiri, kita perlu menentukan fungsi padding yang serupa dengan `padify` yang telah kita tentukan sebelum ini:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam kes kita, kita akan menggunakan model BERT yang telah dilatih terlebih dahulu yang dipanggil `bert-base-uncased`. Mari kita muatkan model menggunakan pakej `BertForSequenceClassfication`. Ini memastikan bahawa model kita sudah mempunyai seni bina yang diperlukan untuk klasifikasi, termasuk pengklasifikasi akhir. Anda akan melihat mesej amaran yang menyatakan bahawa berat pengklasifikasi akhir tidak diinisialisasi, dan model memerlukan latihan awal - itu adalah perkara yang sangat baik, kerana itulah yang akan kita lakukan!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita bersedia untuk memulakan latihan! Oleh kerana BERT sudah dilatih terlebih dahulu, kita ingin memulakan dengan kadar pembelajaran yang agak kecil supaya tidak merosakkan berat awal.\n",
    "\n",
    "Semua kerja keras dilakukan oleh model `BertForSequenceClassification`. Apabila kita memanggil model pada data latihan, ia mengembalikan kedua-dua kehilangan (loss) dan output rangkaian untuk minibatch input. Kita menggunakan kehilangan untuk pengoptimuman parameter (`loss.backward()` melakukan laluan ke belakang), dan `out` untuk mengira ketepatan latihan dengan membandingkan label yang diperoleh `labs` (dikira menggunakan `argmax`) dengan `labels` yang dijangkakan.\n",
    "\n",
    "Untuk mengawal proses, kita mengumpul kehilangan dan ketepatan sepanjang beberapa iterasi, dan mencetaknya setiap kitaran latihan `report_freq`.\n",
    "\n",
    "Latihan ini mungkin akan mengambil masa yang agak lama, jadi kita menghadkan bilangan iterasi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda boleh lihat (terutamanya jika anda meningkatkan bilangan iterasi dan menunggu cukup lama) bahawa klasifikasi BERT memberikan ketepatan yang cukup baik! Ini kerana BERT sudah memahami struktur bahasa dengan baik, dan kita hanya perlu melaras pengklasifikasi akhir. Walau bagaimanapun, kerana BERT adalah model yang besar, keseluruhan proses latihan mengambil masa yang lama dan memerlukan kuasa pengkomputeran yang tinggi! (GPU, dan sebaiknya lebih daripada satu).\n",
    "\n",
    "> **Note:** Dalam contoh kita, kita telah menggunakan salah satu model BERT pra-latih yang paling kecil. Terdapat model yang lebih besar yang berkemungkinan memberikan hasil yang lebih baik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menilai prestasi model\n",
    "\n",
    "Sekarang kita boleh menilai prestasi model kita pada set data ujian. Gelung penilaian adalah hampir serupa dengan gelung latihan, tetapi kita tidak boleh lupa untuk menukar model kepada mod penilaian dengan memanggil `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perkara Penting\n",
    "\n",
    "Dalam unit ini, kita telah melihat betapa mudahnya mengambil model bahasa yang telah dilatih dari perpustakaan **transformers** dan menyesuaikannya untuk tugas klasifikasi teks kita. Begitu juga, model BERT boleh digunakan untuk pengekstrakan entiti, menjawab soalan, dan tugas NLP lain.\n",
    "\n",
    "Model transformer mewakili teknologi terkini dalam NLP, dan dalam kebanyakan kes, ia seharusnya menjadi penyelesaian pertama yang anda cuba apabila melaksanakan penyelesaian NLP tersuai. Walau bagaimanapun, memahami prinsip asas rangkaian neural berulang yang dibincangkan dalam modul ini adalah sangat penting jika anda ingin membina model neural yang lebih maju.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-29T15:56:22+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "ms"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}