{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melatih RL untuk Menyeimbangkan Cartpole\n",
    "\n",
    "Notebook ini adalah sebahagian daripada [Kurikulum AI untuk Pemula](http://aka.ms/ai-beginners). Ia diilhamkan oleh [tutorial rasmi PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) dan [implementasi Cartpole PyTorch ini](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Dalam contoh ini, kita akan menggunakan RL untuk melatih model agar dapat menyeimbangkan tiang pada kereta yang boleh bergerak ke kiri dan kanan pada skala mendatar. Kita akan menggunakan persekitaran [OpenAI Gym](https://www.gymlibrary.ml/) untuk mensimulasikan tiang tersebut.\n",
    "\n",
    "> **Nota**: Anda boleh menjalankan kod pelajaran ini secara tempatan (contohnya dari Visual Studio Code), di mana simulasi akan dibuka dalam tetingkap baru. Apabila menjalankan kod secara dalam talian, anda mungkin perlu membuat beberapa penyesuaian pada kod, seperti yang diterangkan [di sini](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Kita akan bermula dengan memastikan Gym telah dipasang:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita cipta persekitaran CartPole dan lihat bagaimana untuk mengendalikannya. Sesebuah persekitaran mempunyai ciri-ciri berikut:\n",
    "\n",
    "* **Ruang tindakan** ialah set tindakan yang boleh kita lakukan pada setiap langkah simulasi\n",
    "* **Ruang pemerhatian** ialah ruang pemerhatian yang boleh kita buat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita lihat bagaimana simulasi ini berfungsi. Gelung berikut menjalankan simulasi sehingga `env.step` tidak lagi mengembalikan bendera penamatan `done`. Kita akan memilih tindakan secara rawak menggunakan `env.action_space.sample()`, yang bermaksud eksperimen ini mungkin akan gagal dengan sangat cepat (persekitaran CartPole akan tamat apabila kelajuan CartPole, posisinya, atau sudutnya berada di luar had tertentu).\n",
    "\n",
    "> Simulasi akan dibuka dalam tetingkap baru. Anda boleh menjalankan kod ini beberapa kali dan melihat bagaimana ia berkelakuan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda boleh perhatikan bahawa pemerhatian mengandungi 4 nombor. Nombor-nombor tersebut adalah:\n",
    "- Kedudukan troli\n",
    "- Kelajuan troli\n",
    "- Sudut tiang\n",
    "- Kadar putaran tiang\n",
    "\n",
    "`rew` ialah ganjaran yang kita terima pada setiap langkah. Dalam persekitaran CartPole, anda akan menerima 1 mata untuk setiap langkah simulasi, dan matlamatnya adalah untuk memaksimumkan jumlah ganjaran, iaitu masa CartPole dapat menyeimbangkan tanpa jatuh.\n",
    "\n",
    "Semasa pembelajaran pengukuhan, matlamat kita adalah untuk melatih **dasar** $\\pi$, yang untuk setiap keadaan $s$ akan memberitahu kita tindakan $a$ yang perlu diambil, jadi pada asasnya $a = \\pi(s)$.\n",
    "\n",
    "Jika anda mahukan penyelesaian secara kebarangkalian, anda boleh menganggap dasar sebagai mengembalikan satu set kebarangkalian untuk setiap tindakan, iaitu $\\pi(a|s)$ akan bermaksud kebarangkalian bahawa kita harus mengambil tindakan $a$ pada keadaan $s$.\n",
    "\n",
    "## Kaedah Policy Gradient\n",
    "\n",
    "Dalam algoritma RL yang paling mudah, dipanggil **Policy Gradient**, kita akan melatih rangkaian neural untuk meramalkan tindakan seterusnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kami akan melatih rangkaian dengan menjalankan banyak eksperimen, dan mengemas kini rangkaian kami selepas setiap percubaan. Mari kita definisikan satu fungsi yang akan menjalankan eksperimen dan mengembalikan hasilnya (dikenali sebagai **jejak**) - semua keadaan, tindakan (dan kebarangkalian yang disyorkan), dan ganjaran:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda boleh menjalankan satu episod dengan rangkaian yang tidak terlatih dan perhatikan bahawa ganjaran keseluruhan (AKA panjang episod) adalah sangat rendah:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salah satu aspek sukar dalam algoritma polisi kecerunan adalah menggunakan **ganjaran terdiskaun**. Idea di sebaliknya adalah kita mengira vektor jumlah ganjaran pada setiap langkah permainan, dan semasa proses ini kita mendiskaunkan ganjaran awal menggunakan beberapa koefisien $gamma$. Kita juga menormalkan vektor yang terhasil, kerana kita akan menggunakannya sebagai berat untuk mempengaruhi latihan kita:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita mulakan latihan sebenar! Kita akan menjalankan 300 episod, dan pada setiap episod kita akan melakukan perkara berikut:\n",
    "\n",
    "1. Jalankan eksperimen dan kumpulkan jejak\n",
    "1. Kira perbezaan (`gradients`) antara tindakan yang diambil dan kebarangkalian yang diramalkan. Semakin kecil perbezaannya, semakin yakin kita bahawa tindakan yang diambil adalah betul.\n",
    "1. Kira ganjaran yang didiskaunkan dan darabkan `gradients` dengan ganjaran yang didiskaunkan - ini akan memastikan langkah dengan ganjaran yang lebih tinggi memberi kesan lebih besar pada hasil akhir berbanding langkah dengan ganjaran yang lebih rendah.\n",
    "1. Tindakan sasaran yang dijangka untuk rangkaian neural kita sebahagiannya akan diambil daripada kebarangkalian yang diramalkan semasa eksperimen dijalankan, dan sebahagiannya daripada `gradients` yang dikira. Kita akan menggunakan parameter `alpha` untuk menentukan sejauh mana `gradients` dan ganjaran diambil kira - ini dipanggil *kadar pembelajaran* dalam algoritma pengukuhan.\n",
    "1. Akhir sekali, kita melatih rangkaian kita berdasarkan keadaan dan tindakan yang dijangka, dan ulangi proses tersebut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari jalankan episod dengan rendering untuk melihat hasilnya:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semoga anda dapat melihat bahawa tiang kini boleh seimbang dengan agak baik!\n",
    "\n",
    "## Model Actor-Critic\n",
    "\n",
    "Model Actor-Critic adalah perkembangan lanjut daripada policy gradients, di mana kita membina rangkaian neural untuk mempelajari kedua-dua polisi dan ganjaran yang dianggarkan. Rangkaian ini akan mempunyai dua output (atau anda boleh melihatnya sebagai dua rangkaian berasingan):\n",
    "* **Actor** akan mencadangkan tindakan yang perlu diambil dengan memberikan kita taburan kebarangkalian keadaan, seperti dalam model policy gradient.\n",
    "* **Critic** akan menganggarkan apa ganjaran yang mungkin diperoleh daripada tindakan tersebut. Ia mengembalikan jumlah ganjaran yang dianggarkan pada masa hadapan dalam keadaan tertentu.\n",
    "\n",
    "Mari kita definisikan model seperti ini:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita perlu sedikit mengubah fungsi `discounted_rewards` dan `run_episode` kita:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita akan menjalankan gelung latihan utama. Kita akan menggunakan proses latihan rangkaian secara manual dengan mengira fungsi kehilangan yang sesuai dan mengemas kini parameter rangkaian:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intipati\n",
    "\n",
    "Kita telah melihat dua algoritma RL dalam demo ini: simple policy gradient, dan actor-critic yang lebih canggih. Anda boleh lihat bahawa algoritma-algoritma tersebut beroperasi dengan konsep abstrak seperti keadaan, tindakan, dan ganjaran - oleh itu, ia boleh digunakan dalam persekitaran yang sangat berbeza.\n",
    "\n",
    "Pembelajaran pengukuhan membolehkan kita mempelajari strategi terbaik untuk menyelesaikan masalah hanya dengan melihat ganjaran akhir. Hakikat bahawa kita tidak memerlukan dataset berlabel membolehkan kita mengulangi simulasi berkali-kali untuk mengoptimumkan model kita. Walau bagaimanapun, masih terdapat banyak cabaran dalam RL, yang mungkin anda pelajari jika anda memutuskan untuk memberi tumpuan lebih kepada bidang AI yang menarik ini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat penting, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T13:04:00+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "ms"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}