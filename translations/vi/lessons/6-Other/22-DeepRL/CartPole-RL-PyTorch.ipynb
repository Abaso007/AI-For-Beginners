{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện RL để cân bằng Cartpole\n",
    "\n",
    "Notebook này là một phần của [Chương trình học AI cho người mới bắt đầu](http://aka.ms/ai-beginners). Nó được lấy cảm hứng từ [hướng dẫn chính thức của PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) và [triển khai Cartpole bằng PyTorch này](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Trong ví dụ này, chúng ta sẽ sử dụng RL để huấn luyện một mô hình cân bằng một cây cột trên một xe đẩy có thể di chuyển sang trái và phải trên một trục ngang. Chúng ta sẽ sử dụng môi trường [OpenAI Gym](https://www.gymlibrary.ml/) để mô phỏng cây cột.\n",
    "\n",
    "> **Note**: Bạn có thể chạy mã của bài học này trên máy cục bộ (ví dụ: từ Visual Studio Code), trong trường hợp đó, mô phỏng sẽ mở trong một cửa sổ mới. Khi chạy mã trực tuyến, bạn có thể cần thực hiện một số điều chỉnh mã, như được mô tả [ở đây](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Chúng ta sẽ bắt đầu bằng cách đảm bảo Gym đã được cài đặt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ, hãy tạo môi trường CartPole và xem cách thao tác trên nó. Một môi trường có các thuộc tính sau:\n",
    "\n",
    "* **Action space** là tập hợp các hành động có thể thực hiện tại mỗi bước của mô phỏng  \n",
    "* **Observation space** là không gian các quan sát mà chúng ta có thể thực hiện  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy xem cách mô phỏng hoạt động. Vòng lặp sau sẽ chạy mô phỏng cho đến khi `env.step` không trả về cờ kết thúc `done`. Chúng ta sẽ chọn hành động một cách ngẫu nhiên bằng cách sử dụng `env.action_space.sample()`, điều này có nghĩa là thí nghiệm có thể thất bại rất nhanh (môi trường CartPole sẽ kết thúc khi tốc độ của CartPole, vị trí hoặc góc của nó vượt quá các giới hạn nhất định).\n",
    "\n",
    "> Mô phỏng sẽ mở trong cửa sổ mới. Bạn có thể chạy mã nhiều lần và quan sát cách nó hoạt động.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn có thể nhận thấy rằng các quan sát bao gồm 4 con số. Chúng là:  \n",
    "- Vị trí của xe đẩy  \n",
    "- Vận tốc của xe đẩy  \n",
    "- Góc của cột  \n",
    "- Tốc độ quay của cột  \n",
    "\n",
    "`rew` là phần thưởng mà chúng ta nhận được ở mỗi bước. Bạn có thể thấy rằng trong môi trường CartPole, bạn được thưởng 1 điểm cho mỗi bước mô phỏng, và mục tiêu là tối đa hóa tổng phần thưởng, tức là thời gian mà CartPole có thể giữ thăng bằng mà không bị ngã.\n",
    "\n",
    "Trong quá trình học tăng cường, mục tiêu của chúng ta là huấn luyện một **chính sách** $\\pi$, mà đối với mỗi trạng thái $s$ sẽ cho chúng ta biết hành động $a$ nào cần thực hiện, về cơ bản là $a = \\pi(s)$.\n",
    "\n",
    "Nếu bạn muốn một giải pháp mang tính xác suất, bạn có thể nghĩ rằng chính sách sẽ trả về một tập hợp các xác suất cho mỗi hành động, tức là $\\pi(a|s)$ sẽ biểu thị xác suất rằng chúng ta nên thực hiện hành động $a$ tại trạng thái $s$.\n",
    "\n",
    "## Phương pháp Gradient Chính sách\n",
    "\n",
    "Trong thuật toán RL đơn giản nhất, được gọi là **Gradient Chính sách**, chúng ta sẽ huấn luyện một mạng nơ-ron để dự đoán hành động tiếp theo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sẽ huấn luyện mạng bằng cách thực hiện nhiều thí nghiệm và cập nhật mạng sau mỗi lần chạy. Hãy định nghĩa một hàm để thực hiện thí nghiệm và trả về kết quả (gọi là **trace**) - tất cả các trạng thái, hành động (và xác suất được đề xuất của chúng), và phần thưởng:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn có thể chạy một tập với mạng chưa được huấn luyện và quan sát rằng tổng phần thưởng (hay còn gọi là độ dài của tập) rất thấp:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một trong những khía cạnh khó khăn của thuật toán gradient chính sách là sử dụng **phần thưởng chiết khấu**. Ý tưởng là chúng ta tính toán vector tổng phần thưởng tại mỗi bước của trò chơi, và trong quá trình này chúng ta chiết khấu các phần thưởng ban đầu bằng một hệ số $gamma$. Chúng ta cũng chuẩn hóa vector kết quả, vì chúng ta sẽ sử dụng nó như một trọng số để ảnh hưởng đến quá trình huấn luyện của mình:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ hãy bắt đầu quá trình huấn luyện thực sự! Chúng ta sẽ chạy 300 tập, và trong mỗi tập, chúng ta sẽ thực hiện các bước sau:\n",
    "\n",
    "1. Chạy thí nghiệm và thu thập dấu vết.\n",
    "2. Tính toán sự chênh lệch (`gradients`) giữa các hành động đã thực hiện và các xác suất dự đoán. Sự chênh lệch càng nhỏ, chúng ta càng chắc chắn rằng hành động đã thực hiện là đúng.\n",
    "3. Tính toán phần thưởng chiết khấu và nhân `gradients` với phần thưởng chiết khấu - điều này đảm bảo rằng các bước có phần thưởng cao hơn sẽ có ảnh hưởng lớn hơn đến kết quả cuối cùng so với các bước có phần thưởng thấp hơn.\n",
    "4. Các hành động mục tiêu kỳ vọng cho mạng nơ-ron của chúng ta sẽ được lấy một phần từ các xác suất dự đoán trong quá trình chạy, và một phần từ các `gradients` đã tính toán. Chúng ta sẽ sử dụng tham số `alpha` để xác định mức độ mà `gradients` và phần thưởng được tính đến - đây được gọi là *tốc độ học* của thuật toán tăng cường.\n",
    "5. Cuối cùng, chúng ta huấn luyện mạng nơ-ron trên các trạng thái và hành động kỳ vọng, sau đó lặp lại quy trình.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ hãy chạy tập phim với kết xuất để xem kết quả:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hy vọng bạn có thể thấy rằng cây cột giờ đã có thể cân bằng khá tốt!\n",
    "\n",
    "## Mô hình Actor-Critic\n",
    "\n",
    "Mô hình Actor-Critic là một bước phát triển tiếp theo của policy gradients, trong đó chúng ta xây dựng một mạng nơ-ron để học cả chính sách và phần thưởng ước tính. Mạng này sẽ có hai đầu ra (hoặc bạn có thể xem nó như hai mạng riêng biệt):\n",
    "* **Actor** sẽ đề xuất hành động cần thực hiện bằng cách cung cấp phân phối xác suất trạng thái, giống như trong mô hình policy gradient.\n",
    "* **Critic** sẽ ước tính phần thưởng có thể nhận được từ những hành động đó. Nó trả về tổng phần thưởng ước tính trong tương lai tại trạng thái hiện tại.\n",
    "\n",
    "Hãy định nghĩa một mô hình như vậy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sẽ cần chỉnh sửa một chút các hàm `discounted_rewards` và `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ chạy vòng lặp huấn luyện chính. Chúng ta sẽ sử dụng quy trình huấn luyện mạng thủ công bằng cách tính toán các hàm mất mát phù hợp và cập nhật các tham số của mạng:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Điểm chính\n",
    "\n",
    "Chúng ta đã tìm hiểu hai thuật toán Học tăng cường (RL) trong bản demo này: gradient chính sách đơn giản và actor-critic phức tạp hơn. Bạn có thể thấy rằng các thuật toán này hoạt động với các khái niệm trừu tượng như trạng thái, hành động và phần thưởng - do đó chúng có thể được áp dụng cho nhiều môi trường rất khác nhau.\n",
    "\n",
    "Học tăng cường cho phép chúng ta học chiến lược tốt nhất để giải quyết vấn đề chỉ bằng cách quan sát phần thưởng cuối cùng. Việc không cần các tập dữ liệu được gắn nhãn cho phép chúng ta lặp lại các mô phỏng nhiều lần để tối ưu hóa mô hình của mình. Tuy nhiên, vẫn còn nhiều thách thức trong RL, mà bạn có thể tìm hiểu nếu quyết định tập trung nhiều hơn vào lĩnh vực thú vị này của AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Tuyên bố miễn trừ trách nhiệm**:  \nTài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn tham khảo chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp từ con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T13:02:38+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "vi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}