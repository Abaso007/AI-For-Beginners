{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cơ chế Attention và mô hình Transformer\n",
    "\n",
    "Một nhược điểm lớn của mạng hồi quy (recurrent networks) là tất cả các từ trong một chuỗi đều có tác động như nhau đến kết quả. Điều này dẫn đến hiệu suất không tối ưu với các mô hình LSTM encoder-decoder tiêu chuẩn cho các nhiệm vụ chuỗi sang chuỗi, chẳng hạn như Nhận diện Thực thể Được đặt tên (Named Entity Recognition) và Dịch Máy. Trong thực tế, một số từ cụ thể trong chuỗi đầu vào thường có ảnh hưởng lớn hơn đến các đầu ra tuần tự so với các từ khác.\n",
    "\n",
    "Hãy xem xét mô hình chuỗi sang chuỗi, chẳng hạn như dịch máy. Mô hình này được triển khai bằng hai mạng hồi quy, trong đó một mạng (**encoder**) sẽ nén chuỗi đầu vào thành trạng thái ẩn, và mạng còn lại (**decoder**) sẽ mở rộng trạng thái ẩn này thành kết quả đã dịch. Vấn đề với cách tiếp cận này là trạng thái cuối cùng của mạng sẽ gặp khó khăn trong việc ghi nhớ phần đầu của câu, dẫn đến chất lượng mô hình kém đối với các câu dài.\n",
    "\n",
    "**Cơ chế Attention** cung cấp một phương pháp để gán trọng số cho tác động ngữ cảnh của từng vector đầu vào lên từng dự đoán đầu ra của RNN. Cách triển khai là tạo các đường tắt giữa các trạng thái trung gian của RNN đầu vào và RNN đầu ra. Theo cách này, khi tạo ra ký hiệu đầu ra $y_t$, chúng ta sẽ xem xét tất cả các trạng thái ẩn đầu vào $h_i$, với các hệ số trọng số khác nhau $\\alpha_{t,i}$.\n",
    "\n",
    "![Hình ảnh mô tả mô hình encoder/decoder với lớp attention cộng tính](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.vi.png)\n",
    "*Mô hình encoder-decoder với cơ chế attention cộng tính trong [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), được trích dẫn từ [bài viết blog này](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Ma trận Attention $\\{\\alpha_{i,j}\\}$ sẽ biểu thị mức độ mà các từ đầu vào cụ thể ảnh hưởng đến việc tạo ra một từ nhất định trong chuỗi đầu ra. Dưới đây là ví dụ về một ma trận như vậy:\n",
    "\n",
    "![Hình ảnh minh họa một mẫu căn chỉnh được tìm thấy bởi RNNsearch-50, lấy từ Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.vi.png)\n",
    "\n",
    "*Hình ảnh được lấy từ [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Hình 3)*\n",
    "\n",
    "Cơ chế Attention chịu trách nhiệm cho phần lớn các thành tựu hiện tại hoặc gần đây trong Xử lý Ngôn ngữ Tự nhiên. Tuy nhiên, việc thêm Attention làm tăng đáng kể số lượng tham số của mô hình, dẫn đến các vấn đề về khả năng mở rộng với RNNs. Một hạn chế chính của việc mở rộng RNNs là tính chất hồi quy của các mô hình khiến việc xử lý theo lô (batch) và song song hóa (parallelize) trở nên khó khăn. Trong RNN, mỗi phần tử của một chuỗi cần được xử lý theo thứ tự tuần tự, điều này có nghĩa là không thể dễ dàng song song hóa.\n",
    "\n",
    "Việc áp dụng cơ chế Attention kết hợp với hạn chế này đã dẫn đến sự ra đời của các Mô hình Transformer hiện đại mà chúng ta biết và sử dụng ngày nay, từ BERT đến OpenGPT3.\n",
    "\n",
    "## Mô hình Transformer\n",
    "\n",
    "Thay vì chuyển tiếp ngữ cảnh của từng dự đoán trước vào bước đánh giá tiếp theo, **mô hình Transformer** sử dụng **mã hóa vị trí** (positional encodings) và **attention** để nắm bắt ngữ cảnh của đầu vào trong một cửa sổ văn bản được cung cấp. Hình ảnh dưới đây minh họa cách mã hóa vị trí kết hợp với attention có thể nắm bắt ngữ cảnh trong một cửa sổ nhất định.\n",
    "\n",
    "![GIF động minh họa cách các đánh giá được thực hiện trong mô hình Transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Vì mỗi vị trí đầu vào được ánh xạ độc lập đến mỗi vị trí đầu ra, các mô hình Transformer có thể song song hóa tốt hơn so với RNNs, điều này cho phép tạo ra các mô hình ngôn ngữ lớn hơn và biểu đạt hơn. Mỗi attention head có thể được sử dụng để học các mối quan hệ khác nhau giữa các từ, cải thiện các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên.\n",
    "\n",
    "## Xây dựng Mô hình Transformer Đơn giản\n",
    "\n",
    "Keras không chứa lớp Transformer tích hợp sẵn, nhưng chúng ta có thể tự xây dựng. Như trước đây, chúng ta sẽ tập trung vào phân loại văn bản của tập dữ liệu AG News, nhưng cần lưu ý rằng các mô hình Transformer cho kết quả tốt nhất ở các nhiệm vụ NLP khó hơn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các lớp mới trong Keras cần kế thừa lớp `Layer`, và triển khai phương thức `call`. Hãy bắt đầu với lớp **Positional Embedding**. Chúng ta sẽ sử dụng [một số mã từ tài liệu chính thức của Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Chúng ta sẽ giả định rằng tất cả các chuỗi đầu vào đều được đệm đến độ dài `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lớp này bao gồm hai lớp `Embedding`: một lớp để nhúng các token (theo cách chúng ta đã thảo luận trước đó) và một lớp để nhúng vị trí của các token. Vị trí của các token được tạo thành một dãy số tự nhiên từ 0 đến `maxlen` bằng cách sử dụng `tf.range`, sau đó được đưa qua lớp nhúng. Hai vector nhúng kết quả sau đó được cộng lại, tạo ra biểu diễn nhúng theo vị trí của đầu vào với hình dạng `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Bây giờ, chúng ta hãy triển khai khối transformer. Nó sẽ nhận đầu ra từ lớp nhúng đã được định nghĩa trước đó:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer áp dụng `MultiHeadAttention` lên đầu vào đã được mã hóa vị trí để tạo ra vector attention với kích thước `maxlen`$\\times$`embed_dim`, sau đó được trộn với đầu vào và chuẩn hóa bằng cách sử dụng `LayerNormalization`.\n",
    "\n",
    "> **Lưu ý**: `LayerNormalization` tương tự như `BatchNormalization` đã được thảo luận trong phần *Thị giác Máy tính* của lộ trình học này, nhưng nó chuẩn hóa đầu ra của lớp trước đó cho từng mẫu huấn luyện một cách độc lập, để đưa chúng về phạm vi [-1..1].\n",
    "\n",
    "Đầu ra của lớp này sau đó được truyền qua mạng `Dense` (trong trường hợp của chúng ta - perceptron hai lớp), và kết quả được cộng vào đầu ra cuối cùng (đầu ra này lại được chuẩn hóa một lần nữa).\n",
    "\n",
    "Bây giờ, chúng ta đã sẵn sàng để định nghĩa mô hình transformer hoàn chỉnh:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các Mô Hình Transformer BERT\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) là một mạng transformer nhiều lớp rất lớn với 12 lớp cho *BERT-base*, và 24 lớp cho *BERT-large*. Mô hình này được huấn luyện trước trên một tập dữ liệu văn bản lớn (WikiPedia + sách) bằng cách sử dụng phương pháp huấn luyện không giám sát (dự đoán các từ bị che trong câu). Trong quá trình huấn luyện trước, mô hình hấp thụ một mức độ hiểu biết ngôn ngữ đáng kể, sau đó có thể được tận dụng với các tập dữ liệu khác thông qua việc tinh chỉnh. Quá trình này được gọi là **học chuyển giao**.\n",
    "\n",
    "![hình ảnh từ http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.vi.png)\n",
    "\n",
    "Có nhiều biến thể của kiến trúc Transformer bao gồm BERT, DistilBERT, BigBird, OpenGPT3 và nhiều hơn nữa có thể được tinh chỉnh.\n",
    "\n",
    "Hãy cùng xem cách chúng ta có thể sử dụng mô hình BERT đã được huấn luyện trước để giải quyết vấn đề phân loại chuỗi truyền thống. Chúng ta sẽ mượn ý tưởng và một số đoạn mã từ [tài liệu chính thức](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Để tải các mô hình đã được huấn luyện trước, chúng ta sẽ sử dụng **Tensorflow hub**. Đầu tiên, hãy tải vectorizer dành riêng cho BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Điều quan trọng là bạn phải sử dụng cùng bộ vector hóa như bộ mà mạng ban đầu đã được huấn luyện. Ngoài ra, bộ vector hóa BERT trả về ba thành phần:\n",
    "* `input_word_ids`, là một chuỗi các số token cho câu đầu vào\n",
    "* `input_mask`, cho biết phần nào của chuỗi chứa đầu vào thực tế và phần nào là padding. Nó tương tự như mặt nạ được tạo bởi lớp `Masking`\n",
    "* `input_type_ids` được sử dụng cho các nhiệm vụ mô hình ngôn ngữ, và cho phép chỉ định hai câu đầu vào trong một chuỗi.\n",
    "\n",
    "Sau đó, chúng ta có thể khởi tạo bộ trích xuất đặc trưng BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy, lớp BERT trả về một số kết quả hữu ích:\n",
    "\n",
    "* `pooled_output` là kết quả của việc tính trung bình tất cả các token trong chuỗi. Bạn có thể xem nó như một embedding ngữ nghĩa thông minh của toàn bộ mạng. Nó tương đương với đầu ra của lớp `GlobalAveragePooling1D` trong mô hình trước của chúng ta.\n",
    "* `sequence_output` là đầu ra của lớp transformer cuối cùng (tương ứng với đầu ra của `TransformerBlock` trong mô hình ở trên).\n",
    "* `encoder_outputs` là đầu ra của tất cả các lớp transformer. Vì chúng ta đã tải mô hình BERT 4 lớp (như bạn có thể đoán từ tên gọi, chứa `4_H`), nên nó có 4 tensor. Tensor cuối cùng giống với `sequence_output`.\n",
    "\n",
    "Bây giờ chúng ta sẽ định nghĩa mô hình phân loại end-to-end. Chúng ta sẽ sử dụng *định nghĩa mô hình chức năng*, khi chúng ta định nghĩa đầu vào của mô hình, và sau đó cung cấp một loạt các biểu thức để tính toán đầu ra của nó. Chúng ta cũng sẽ đặt trọng số của mô hình BERT ở trạng thái không thể huấn luyện, và chỉ huấn luyện bộ phân loại cuối cùng:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù có rất ít tham số có thể huấn luyện, quá trình vẫn khá chậm vì bộ trích xuất đặc trưng của BERT tiêu tốn nhiều tài nguyên tính toán. Có vẻ như chúng ta không thể đạt được độ chính xác hợp lý, có thể do thiếu huấn luyện hoặc thiếu tham số của mô hình.\n",
    "\n",
    "Hãy thử mở khóa các trọng số của BERT và huấn luyện nó. Điều này yêu cầu tốc độ học rất nhỏ, cùng với chiến lược huấn luyện cẩn thận hơn với **warmup**, sử dụng trình tối ưu hóa **AdamW**. Chúng ta sẽ sử dụng gói `tf-models-official` để tạo trình tối ưu hóa:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như bạn có thể thấy, quá trình huấn luyện diễn ra khá chậm - nhưng bạn có thể thử nghiệm và huấn luyện mô hình trong vài epoch (5-10) để xem liệu bạn có thể đạt được kết quả tốt nhất so với các phương pháp mà chúng ta đã sử dụng trước đây hay không.\n",
    "\n",
    "## Thư viện Huggingface Transformers\n",
    "\n",
    "Một cách khác rất phổ biến (và đơn giản hơn một chút) để sử dụng các mô hình Transformer là [gói HuggingFace](https://github.com/huggingface/), cung cấp các khối xây dựng đơn giản cho các tác vụ NLP khác nhau. Thư viện này có sẵn cho cả Tensorflow và PyTorch, một framework mạng nơ-ron rất phổ biến khác.\n",
    "\n",
    "> **Lưu ý**: Nếu bạn không quan tâm đến việc tìm hiểu cách thư viện Transformers hoạt động - bạn có thể bỏ qua phần cuối của notebook này, vì bạn sẽ không thấy điều gì khác biệt đáng kể so với những gì chúng ta đã làm ở trên. Chúng ta sẽ lặp lại các bước tương tự để huấn luyện mô hình BERT bằng một thư viện khác và một mô hình lớn hơn đáng kể. Do đó, quá trình này bao gồm một số bước huấn luyện khá dài, vì vậy bạn có thể chỉ cần xem qua mã.\n",
    "\n",
    "Hãy cùng xem cách chúng ta có thể giải quyết vấn đề của mình bằng [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Điều đầu tiên chúng ta cần làm là chọn mô hình mà chúng ta sẽ sử dụng. Ngoài một số mô hình tích hợp sẵn, Huggingface còn có một [kho lưu trữ mô hình trực tuyến](https://huggingface.co/models), nơi bạn có thể tìm thấy rất nhiều mô hình đã được huấn luyện trước bởi cộng đồng. Tất cả các mô hình này đều có thể được tải và sử dụng chỉ bằng cách cung cấp tên mô hình. Tất cả các tệp nhị phân cần thiết cho mô hình sẽ tự động được tải xuống.\n",
    "\n",
    "Trong một số trường hợp, bạn sẽ cần tải mô hình của riêng mình, khi đó bạn có thể chỉ định thư mục chứa tất cả các tệp liên quan, bao gồm các tham số cho tokenizer, tệp `config.json` với các tham số mô hình, trọng số nhị phân, v.v.\n",
    "\n",
    "Từ tên mô hình, chúng ta có thể khởi tạo cả mô hình và tokenizer. Hãy bắt đầu với tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đối tượng `tokenizer` chứa hàm `encode` có thể được sử dụng trực tiếp để mã hóa văn bản:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta cũng có thể sử dụng tokenizer để mã hóa một chuỗi theo cách phù hợp để truyền vào mô hình, tức là bao gồm các trường `token_ids`, `input_mask`, v.v. Chúng ta cũng có thể chỉ định rằng chúng ta muốn các tensor của Tensorflow bằng cách cung cấp đối số `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong trường hợp này, chúng ta sẽ sử dụng mô hình BERT đã được huấn luyện trước, gọi là `bert-base-uncased`. *Uncased* cho biết rằng mô hình không phân biệt chữ hoa chữ thường.\n",
    "\n",
    "Khi huấn luyện mô hình, chúng ta cần cung cấp chuỗi đã được mã hóa làm đầu vào, và do đó chúng ta sẽ thiết kế quy trình xử lý dữ liệu. Vì `tokenizer.encode` là một hàm Python, chúng ta sẽ sử dụng cách tiếp cận tương tự như trong bài học trước bằng cách gọi nó thông qua `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta có thể tải mô hình thực tế bằng gói `BertForSequenceClassification`. Điều này đảm bảo rằng mô hình của chúng ta đã có kiến trúc cần thiết cho phân loại, bao gồm cả bộ phân loại cuối cùng. Bạn sẽ thấy thông báo cảnh báo cho biết rằng các trọng số của bộ phân loại cuối cùng chưa được khởi tạo, và mô hình sẽ cần được huấn luyện trước - điều đó hoàn toàn ổn, vì đó chính xác là những gì chúng ta sắp làm!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như bạn có thể thấy từ `summary()`, mô hình chứa gần 110 triệu tham số! Có lẽ, nếu chúng ta muốn thực hiện nhiệm vụ phân loại đơn giản trên tập dữ liệu tương đối nhỏ, chúng ta không muốn huấn luyện lớp cơ sở BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẵn sàng bắt đầu huấn luyện!\n",
    "\n",
    "> **Lưu ý**: Việc huấn luyện mô hình BERT toàn diện có thể mất rất nhiều thời gian! Vì vậy, chúng ta sẽ chỉ huấn luyện nó trong 32 lô đầu tiên. Đây chỉ là để minh họa cách thiết lập huấn luyện mô hình. Nếu bạn muốn thử huấn luyện toàn diện - chỉ cần loại bỏ các tham số `steps_per_epoch` và `validation_steps`, và chuẩn bị chờ đợi!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu bạn tăng số lượng vòng lặp và chờ đủ lâu, đồng thời huấn luyện trong vài epoch, bạn có thể kỳ vọng rằng phân loại bằng BERT sẽ mang lại độ chính xác tốt nhất! Điều này là do BERT đã hiểu khá tốt cấu trúc của ngôn ngữ, và chúng ta chỉ cần tinh chỉnh bộ phân loại cuối cùng. Tuy nhiên, vì BERT là một mô hình lớn, toàn bộ quá trình huấn luyện mất rất nhiều thời gian và đòi hỏi sức mạnh tính toán đáng kể! (GPU, và tốt nhất là nhiều hơn một GPU).\n",
    "\n",
    "> **Lưu ý:** Trong ví dụ của chúng ta, chúng ta đã sử dụng một trong những mô hình BERT nhỏ nhất đã được huấn luyện sẵn. Có những mô hình lớn hơn có khả năng mang lại kết quả tốt hơn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kết luận\n",
    "\n",
    "Trong bài học này, chúng ta đã tìm hiểu về các kiến trúc mô hình rất mới dựa trên **transformers**. Chúng ta đã áp dụng chúng cho nhiệm vụ phân loại văn bản, nhưng tương tự, các mô hình BERT cũng có thể được sử dụng cho việc trích xuất thực thể, trả lời câu hỏi, và các nhiệm vụ NLP khác.\n",
    "\n",
    "Các mô hình transformer hiện đang đại diện cho trạng thái tiên tiến nhất trong NLP, và trong hầu hết các trường hợp, đây nên là giải pháp đầu tiên bạn bắt đầu thử nghiệm khi triển khai các giải pháp NLP tùy chỉnh. Tuy nhiên, việc hiểu các nguyên tắc cơ bản của mạng nơ-ron hồi quy được thảo luận trong module này là vô cùng quan trọng nếu bạn muốn xây dựng các mô hình nơ-ron tiên tiến.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Tuyên bố miễn trừ trách nhiệm**:  \nTài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp từ con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-29T16:00:07+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "vi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}