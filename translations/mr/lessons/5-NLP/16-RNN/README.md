<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-26T08:09:30+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "mr"
}
-->
# पुनरावृत्ती तंत्रिका जाळी (Recurrent Neural Networks)

## [पूर्व-व्याख्यान प्रश्नमंजुषा](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

मागील विभागांमध्ये, आपण मजकूराच्या समृद्ध अर्थपूर्ण सादरीकरणांचा वापर केला आणि एम्बेडिंग्जच्या वर एक साधा रेषीय वर्गीकरणकर्ता वापरला. ही आर्किटेक्चर वाक्यातील शब्दांचा एकत्रित अर्थ पकडते, परंतु ती शब्दांच्या **क्रम** विचारात घेत नाही, कारण एम्बेडिंग्जवरील एकत्रीकरण क्रियेमुळे मूळ मजकूरातील ही माहिती गमावली जाते. हे मॉडेल्स शब्दांच्या क्रमाचे मॉडेलिंग करण्यात अक्षम असल्यामुळे, ते मजकूर निर्मिती किंवा प्रश्नोत्तर यांसारख्या अधिक जटिल किंवा संदिग्ध कार्ये सोडवू शकत नाहीत.

मजकूर अनुक्रमाचा अर्थ पकडण्यासाठी, आपल्याला आणखी एका तंत्रिका जाळी आर्किटेक्चरचा वापर करावा लागतो, ज्याला **पुनरावृत्ती तंत्रिका जाळी** किंवा RNN म्हणतात. RNN मध्ये, आपण आपले वाक्य जाळीमधून एकावेळी एक चिन्ह पाठवतो, आणि जाळी काही **स्थिती** तयार करते, जी नंतर पुढील चिन्हासह पुन्हा जाळीमध्ये पाठवली जाते.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.mr.png)

> लेखकाने तयार केलेली प्रतिमा

टोकन X<sub>0</sub>,...,X<sub>n</sub> च्या इनपुट अनुक्रम दिल्यास, RNN तंत्रिका जाळी ब्लॉक्सचा अनुक्रम तयार करते आणि हा अनुक्रम एंड-टू-एंड बॅकप्रोपोगेशन वापरून प्रशिक्षित करते. प्रत्येक जाळी ब्लॉक (X<sub>i</sub>,S<sub>i</sub>) ही जोडी इनपुट म्हणून घेतो आणि S<sub>i+1</sub> तयार करतो. अंतिम स्थिती S<sub>n</sub> किंवा (आउटपुट Y<sub>n</sub>) रेषीय वर्गीकरणकर्त्यामध्ये जाते आणि परिणाम तयार करते. सर्व जाळी ब्लॉक्स समान वजन सामायिक करतात आणि एकाच बॅकप्रोपोगेशन पासद्वारे एंड-टू-एंड प्रशिक्षित केले जातात.

कारण स्थिती वेक्टर S<sub>0</sub>,...,S<sub>n</sub> जाळीमधून पास होतात, त्यामुळे ते शब्दांमधील अनुक्रमिक अवलंबित्व शिकण्यास सक्षम असतात. उदाहरणार्थ, जेव्हा *नॉट* हा शब्द अनुक्रमात कुठेतरी दिसतो, तेव्हा तो स्थिती वेक्टरमधील विशिष्ट घटक नकारात्मक करण्यासाठी शिकतो, ज्यामुळे नकार तयार होतो.

> ✅ वरील चित्रातील सर्व RNN ब्लॉक्सचे वजन सामायिक असल्यामुळे, तेच चित्र एका ब्लॉकसारखे (उजवीकडे) दर्शवले जाऊ शकते, ज्यामध्ये पुनरावृत्ती फीडबॅक लूप आहे, जो जाळीच्या आउटपुट स्थितीला पुन्हा इनपुटकडे पाठवतो.

## RNN सेलची रचना

आता आपण पाहूया की एक साधा RNN सेल कसा आयोजित केला जातो. तो मागील स्थिती S<sub>i-1</sub> आणि वर्तमान चिन्ह X<sub>i</sub> इनपुट म्हणून स्वीकारतो, आणि आउटपुट स्थिती S<sub>i</sub> तयार करतो (आणि, कधी कधी, जसे की जनरेटिव्ह जाळ्यांमध्ये, आपल्याला काही इतर आउटपुट Y<sub>i</sub> मध्येही रस असतो).

एक साध्या RNN सेलमध्ये दोन वजन मॅट्रिसेस असतात: एक इनपुट चिन्ह रूपांतरित करते (त्याला W म्हणूया), आणि दुसरे इनपुट स्थिती रूपांतरित करते (H). या प्रकरणात, जाळीचे आउटपुट σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b) म्हणून गणना केली जाते, जिथे σ सक्रियता फंक्शन आहे आणि b अतिरिक्त बायस आहे.

<img alt="RNN सेलची रचना" src="images/rnn-anatomy.png" width="50%"/>

> लेखकाने तयार केलेली प्रतिमा

अनेक प्रकरणांमध्ये, इनपुट टोकन्स RNN मध्ये प्रवेश करण्यापूर्वी एम्बेडिंग लेयरमधून पास होतात, ज्यामुळे परिमाण कमी होते. या प्रकरणात, जर इनपुट वेक्टरचा परिमाण *emb_size* असेल, आणि स्थिती वेक्टरचा परिमाण *hid_size* असेल - तर W चा आकार *emb_size*×*hid_size* असेल, आणि H चा आकार *hid_size*×*hid_size* असेल.

## लाँग शॉर्ट टर्म मेमरी (LSTM)

क्लासिकल RNN च्या मुख्य समस्यांपैकी एक म्हणजे **वॅनिशिंग ग्रेडियंट्स** समस्या. कारण RNN एकाच बॅकप्रोपोगेशन पासमध्ये एंड-टू-एंड प्रशिक्षित केले जातात, त्यामुळे नेटवर्कला पहिल्या स्तरांपर्यंत त्रुटी पोहोचवण्यात अडचण येते, आणि त्यामुळे नेटवर्क लांबच्या टोकन्समधील संबंध शिकू शकत नाही. ही समस्या टाळण्यासाठी **स्पष्ट स्थिती व्यवस्थापन** गेट्सच्या वापराने सादर केले जाते. यापैकी दोन प्रसिद्ध आर्किटेक्चर्स म्हणजे **लाँग शॉर्ट टर्म मेमरी** (LSTM) आणि **गेटेड रिले युनिट** (GRU).

![लाँग शॉर्ट टर्म मेमरी सेलचे उदाहरण दर्शवणारी प्रतिमा](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> प्रतिमा स्रोत TBD

LSTM नेटवर्क RNN प्रमाणेच आयोजित केले जाते, परंतु येथे दोन स्थिती स्तर असतात: वास्तविक स्थिती C, आणि लपलेला वेक्टर H. प्रत्येक युनिटमध्ये, लपलेला वेक्टर H<sub>i</sub> इनपुट X<sub>i</sub> सह जोडला जातो, आणि ते **गेट्स** च्या माध्यमातून स्थिती C मध्ये काय घडते ते नियंत्रित करतात. प्रत्येक गेट सिग्मॉइड सक्रियतेसह एक तंत्रिका जाळी आहे (आउटपुट श्रेणी [0,1]), जी स्थिती वेक्टरने गुणिले गेल्यावर बिटवाइज मास्कसारखी विचार केली जाऊ शकते. खालील गेट्स आहेत (वरील चित्रात डावीकडून उजवीकडे):

* **फॉरगेट गेट** लपलेल्या वेक्टरचा वापर करून ठरवते की C वेक्टरच्या कोणत्या घटकांना विसरायचे आहे, आणि कोणते पास करायचे आहे.
* **इनपुट गेट** इनपुट आणि लपलेल्या वेक्टरमधून काही माहिती घेते आणि ती स्थितीत घालते.
* **आउटपुट गेट** स्थितीला *tanh* सक्रियतेसह रेषीय स्तराद्वारे रूपांतरित करते, नंतर लपलेल्या वेक्टर H<sub>i</sub> चा वापर करून त्याच्या काही घटकांची निवड करते आणि नवीन स्थिती C<sub>i+1</sub> तयार करते.

स्थिती C चे घटक काही फ्लॅग्ससारखे विचारले जाऊ शकतात, जे चालू किंवा बंद केले जाऊ शकतात. उदाहरणार्थ, जेव्हा आपण अनुक्रमात *Alice* नाव पाहतो, तेव्हा आपण गृहीत धरतो की ते एका स्त्री पात्राला संदर्भित करते, आणि स्थितीत फ्लॅग उचलतो की वाक्यात स्त्रीलिंगी नाम आहे. जेव्हा आपण पुढे *and Tom* वाक्यांश पाहतो, तेव्हा आपण फ्लॅग उचलतो की आपल्याकडे अनेकवचनी नाम आहे. अशा प्रकारे स्थितीचे व्यवस्थापन करून आपण वाक्याच्या व्याकरणात्मक गुणधर्मांचा मागोवा ठेवू शकतो.

> ✅ LSTM चे अंतर्गत कार्य समजून घेण्यासाठी एक उत्कृष्ट स्रोत म्हणजे क्रिस्टोफर ओलाह यांचा [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) हा अप्रतिम लेख.

## द्विदिशात्मक आणि बहुस्तरीय RNNs

आपण अशा पुनरावृत्ती जाळ्यांबद्दल चर्चा केली आहे ज्या एका दिशेने कार्य करतात, अनुक्रमाच्या सुरुवातीपासून शेवटापर्यंत. हे नैसर्गिक वाटते, कारण ते आपण वाचन किंवा भाषण ऐकण्याच्या पद्धतीसारखे दिसते. तथापि, अनेक व्यावहारिक प्रकरणांमध्ये आपल्याला इनपुट अनुक्रमात यादृच्छिक प्रवेश असल्यामुळे, पुनरावृत्ती गणना दोन्ही दिशेने चालवणे अर्थपूर्ण ठरू शकते. अशा जाळ्यांना **द्विदिशात्मक** RNNs म्हणतात. द्विदिशात्मक नेटवर्क हाताळताना, आपल्याला प्रत्येक दिशेसाठी दोन लपलेले स्थिती वेक्टर आवश्यक असतात.

पुनरावृत्ती जाळे, एकदिशात्मक किंवा द्विदिशात्मक, अनुक्रमातील विशिष्ट नमुने कॅप्चर करते, आणि त्यांना स्थिती वेक्टरमध्ये साठवते किंवा आउटपुटमध्ये पास करते. जसे की कन्व्होल्यूशनल जाळ्यांमध्ये, आपण पहिल्या स्तराद्वारे काढलेल्या कमी-स्तरीय नमुन्यांवरून उच्च-स्तरीय नमुने कॅप्चर करण्यासाठी दुसऱ्या स्तरावर आणखी एक पुनरावृत्ती स्तर तयार करू शकतो. यामुळे **बहुस्तरीय RNN** ची संकल्पना तयार होते, ज्यामध्ये दोन किंवा अधिक पुनरावृत्ती जाळ्यांचा समावेश असतो, जिथे मागील स्तराचे आउटपुट पुढील स्तरासाठी इनपुट म्हणून पास केले जाते.

![बहुस्तरीय लाँग-शॉर्ट-टर्म-मेमरी RNN दर्शवणारी प्रतिमा](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.mr.jpg)

*फर्नांडो लोपेझ यांच्या [या अप्रतिम पोस्ट](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) मधून घेतलेली प्रतिमा*

## ✍️ सराव: एम्बेडिंग्ज

खालील नोटबुक्समध्ये आपले शिक्षण सुरू ठेवा:

* [PyTorch सह RNNs](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow सह RNNs](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## निष्कर्ष

या युनिटमध्ये, आपण पाहिले की RNNs अनुक्रम वर्गीकरणासाठी वापरले जाऊ शकतात, परंतु प्रत्यक्षात, ते मजकूर निर्मिती, यंत्र अनुवाद, आणि बरेच काही यांसारखी अनेक कार्ये हाताळू शकतात. आपण पुढील युनिटमध्ये ही कार्ये विचारात घेऊ.

## 🚀 आव्हान

LSTMs बद्दल काही साहित्य वाचा आणि त्यांच्या अनुप्रयोगांचा विचार करा:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [व्याख्यानानंतरची प्रश्नमंजुषा](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## पुनरावलोकन आणि स्व-अभ्यास

- क्रिस्टोफर ओलाह यांचा [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) हा लेख.

## [असाइनमेंट: नोटबुक्स](assignment.md)

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी कृपया लक्षात ठेवा की स्वयंचलित भाषांतरे त्रुटी किंवा अचूकतेच्या अभावाने युक्त असू शकतात. मूळ भाषेतील दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी, व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर करून उद्भवलेल्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार नाही.