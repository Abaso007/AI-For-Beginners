{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# लक्षवेधी यंत्रणा आणि ट्रान्सफॉर्मर्स\n",
    "\n",
    "पुनरावृत्ती नेटवर्क्सचा एक मोठा तोटा म्हणजे एका अनुक्रमातील सर्व शब्दांचा परिणामावर समान प्रभाव असतो. यामुळे नामनिर्देशन ओळख (Named Entity Recognition) आणि यंत्राद्वारे भाषांतर (Machine Translation) यांसारख्या अनुक्रम-ते-अनुक्रम कार्यांसाठी मानक LSTM एन्कोडर-डिकोडर मॉडेल्सची कार्यक्षमता कमी होते. प्रत्यक्षात, इनपुट अनुक्रमातील विशिष्ट शब्दांचा अनुक्रमिक आउटपुटवर इतरांपेक्षा अधिक प्रभाव असतो.\n",
    "\n",
    "यंत्राद्वारे भाषांतरासारख्या अनुक्रम-ते-अनुक्रम मॉडेलचा विचार करा. हे दोन पुनरावृत्ती नेटवर्क्सद्वारे अंमलात आणले जाते, जिथे एक नेटवर्क (**एन्कोडर**) इनपुट अनुक्रमाला लपलेल्या स्थितीत (hidden state) रूपांतरित करते, आणि दुसरे नेटवर्क (**डिकोडर**) त्या लपलेल्या स्थितीला भाषांतरित परिणामात बदलते. या दृष्टिकोनातील समस्या अशी आहे की नेटवर्कची अंतिम स्थिती वाक्याच्या सुरुवातीचा भाग लक्षात ठेवण्यात अडचण निर्माण करते, ज्यामुळे लांब वाक्यांवर मॉडेलची गुणवत्ता कमी होते.\n",
    "\n",
    "**लक्षवेधी यंत्रणा (Attention Mechanisms)** RNN च्या प्रत्येक आउटपुट अंदाजावर प्रत्येक इनपुट वेक्टरच्या संदर्भात्मक प्रभावाचे वजन करण्याचा एक मार्ग प्रदान करतात. हे अंमलात आणण्याचा मार्ग म्हणजे इनपुट RNN च्या मध्यवर्ती स्थिती आणि आउटपुट RNN यांच्यात शॉर्टकट तयार करणे. अशा प्रकारे, आउटपुट चिन्ह $y_t$ तयार करताना, आपण सर्व इनपुट लपलेल्या स्थिती $h_i$ विचारात घेऊ, वेगवेगळ्या वजन गुणांक $\\alpha_{t,i}$ सह.\n",
    "\n",
    "![एन्कोडर/डिकोडर मॉडेल आणि अ‍ॅडिटिव्ह लक्षवेधी स्तर दर्शविणारी प्रतिमा](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.mr.png)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) मधील अ‍ॅडिटिव्ह लक्षवेधी यंत्रणेसह एन्कोडर-डिकोडर मॉडेल, [या ब्लॉग पोस्टमधून](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) उद्धृत]*\n",
    "\n",
    "लक्षवेधी मॅट्रिक्स $\\{\\alpha_{i,j}\\}$ हे दर्शवेल की आउटपुट अनुक्रमातील विशिष्ट शब्द तयार करण्यात कोणत्या इनपुट शब्दांचा किती प्रभाव आहे. खाली अशा मॅट्रिक्सचे उदाहरण दिले आहे:\n",
    "\n",
    "![RNNsearch-50 ने सापडलेले नमुना संरेखन दर्शविणारी प्रतिमा, Bahdanau - arviz.org कडून घेतलेली](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.mr.png)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (आकृती 3) मधून घेतलेली आकृती]*\n",
    "\n",
    "लक्षवेधी यंत्रणा नैसर्गिक भाषा प्रक्रिया (Natural Language Processing) मध्ये सध्याच्या किंवा जवळपासच्या अत्याधुनिक तंत्रज्ञानासाठी मोठ्या प्रमाणावर जबाबदार आहेत. मात्र, लक्षवेधी यंत्रणा जोडल्याने मॉडेलच्या पॅरामीटर्सची संख्या मोठ्या प्रमाणात वाढते, ज्यामुळे RNNs सह स्केलिंग समस्या निर्माण होतात. RNNs स्केल करण्याचे एक महत्त्वाचे बंधन म्हणजे मॉडेल्सच्या पुनरावृत्ती स्वरूपामुळे प्रशिक्षण बॅचिंग आणि समांतरित करणे कठीण होते. RNN मध्ये अनुक्रमातील प्रत्येक घटक क्रमाने प्रक्रिया करणे आवश्यक असते, ज्यामुळे ते सहजपणे समांतरित करता येत नाही.\n",
    "\n",
    "लक्षवेधी यंत्रणांचा अवलंब आणि या बंधनामुळे आज आपण ओळखत असलेल्या अत्याधुनिक ट्रान्सफॉर्मर मॉडेल्सची निर्मिती झाली, जसे की BERT आणि OpenGPT3.\n",
    "\n",
    "## ट्रान्सफॉर्मर मॉडेल्स\n",
    "\n",
    "प्रत्येक मागील अंदाजाचा संदर्भ पुढील मूल्यांकन चरणात पाठविण्याऐवजी, **ट्रान्सफॉर्मर मॉडेल्स** **स्थिती कोडिंग्स (positional encodings)** आणि लक्षवेधी यंत्रणा वापरून दिलेल्या मजकूराच्या खिडकीत (window) इनपुटचा संदर्भ कॅप्चर करतात. खालील प्रतिमा दर्शवते की स्थिती कोडिंग्ससह लक्षवेधी यंत्रणा दिलेल्या खिडकीत संदर्भ कसा कॅप्चर करू शकते.\n",
    "\n",
    "![ट्रान्सफॉर्मर मॉडेल्समध्ये मूल्यांकन कसे केले जाते हे दर्शविणारा अ‍ॅनिमेटेड GIF.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "प्रत्येक इनपुट स्थिती स्वतंत्रपणे प्रत्येक आउटपुट स्थितीशी मॅप केल्यामुळे, ट्रान्सफॉर्मर्स RNNs पेक्षा चांगले समांतरित होऊ शकतात, ज्यामुळे खूप मोठी आणि अधिक अभिव्यक्तीक्षम भाषा मॉडेल्स सक्षम होतात. प्रत्येक लक्षवेधी हेड वेगवेगळ्या शब्दांमधील संबंध शिकण्यासाठी वापरला जाऊ शकतो, ज्यामुळे नैसर्गिक भाषा प्रक्रिया कार्ये सुधारतात.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) हे 12 स्तरांसाठी *BERT-base* आणि 24 स्तरांसाठी *BERT-large* असलेले खूप मोठे बहुस्तरीय ट्रान्सफॉर्मर नेटवर्क आहे. हे मॉडेल मोठ्या प्रमाणावर मजकूर डेटावर (विकिपीडिया + पुस्तके) असंरचित प्रशिक्षण (unsupervised training) वापरून आधी प्रशिक्षणित केले जाते (वाक्यातील लपवलेले शब्द ओळखणे). पूर्व-प्रशिक्षणादरम्यान मॉडेल महत्त्वपूर्ण भाषिक समज आत्मसात करते, ज्याचा नंतर इतर डेटासेट्ससह सूक्ष्म-ट्यूनिंगद्वारे उपयोग केला जाऊ शकतो. या प्रक्रियेला **स्थानांतरण शिक्षण (transfer learning)** म्हणतात.\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ येथून घेतलेली प्रतिमा](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.mr.png)\n",
    "\n",
    "BERT, DistilBERT, BigBird, OpenGPT3 आणि इतर अनेक ट्रान्सफॉर्मर आर्किटेक्चर्सच्या विविध आवृत्त्या आहेत, ज्या सूक्ष्म-ट्यूनिंगसाठी वापरता येतात. [HuggingFace पॅकेज](https://github.com/huggingface/) PyTorch सह या आर्किटेक्चर्सपैकी अनेकांचे प्रशिक्षण घेण्यासाठी रिपॉझिटरी प्रदान करते.\n",
    "\n",
    "## मजकूर वर्गीकरणासाठी BERT चा वापर\n",
    "\n",
    "आता आपण पूर्व-प्रशिक्षित BERT मॉडेलचा वापर करून आपले पारंपरिक कार्य कसे सोडवू शकतो ते पाहू: अनुक्रम वर्गीकरण. आपण आपल्या मूळ AG News डेटासेटचे वर्गीकरण करू.\n",
    "\n",
    "सुरुवातीला, HuggingFace लायब्ररी आणि आपला डेटासेट लोड करूया:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "कारण आपण पूर्व-प्रशिक्षित BERT मॉडेल वापरणार आहोत, त्यामुळे आपल्याला विशिष्ट टोकनायझर वापरावा लागेल. प्रथम, आपण पूर्व-प्रशिक्षित BERT मॉडेलशी संबंधित टोकनायझर लोड करू.\n",
    "\n",
    "HuggingFace लायब्ररीमध्ये पूर्व-प्रशिक्षित मॉडेल्सचा संग्रह आहे, ज्याचा आपण फक्त त्यांच्या नावांचा `from_pretrained` फंक्शन्समध्ये युक्तिवाद म्हणून उल्लेख करून वापर करू शकता. मॉडेलसाठी आवश्यक असलेली सर्व बायनरी फाइल्स आपोआप डाउनलोड केल्या जातील.\n",
    "\n",
    "तथापि, काही वेळा आपल्याला स्वतःचे मॉडेल्स लोड करावे लागतील, अशा परिस्थितीत आपण त्या डिरेक्टरीचा उल्लेख करू शकता ज्यामध्ये सर्व संबंधित फाइल्स असतील, जसे की टोकनायझरसाठीचे पॅरामीटर्स, मॉडेल पॅरामीटर्ससह `config.json` फाइल, बायनरी वेट्स इत्यादी.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` ऑब्जेक्टमध्ये `encode` फंक्शन असते जी थेट मजकूर एन्कोड करण्यासाठी वापरली जाऊ शकते:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "मग, प्रशिक्षणादरम्यान डेटा ऍक्सेस करण्यासाठी आपण ज्या इटरेटर्सचा वापर करू ते तयार करूया. कारण BERT त्याच्या स्वतःच्या एन्कोडिंग फंक्शनचा वापर करतो, त्यामुळे आपल्याला यापूर्वी परिभाषित केलेल्या `padify` सारखी एक पॅडिंग फंक्शन परिभाषित करावी लागेल:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आमच्या बाबतीत, आम्ही `bert-base-uncased` नावाचा पूर्व-प्रशिक्षित BERT मॉडेल वापरणार आहोत. चला `BertForSequenceClassfication` पॅकेज वापरून मॉडेल लोड करूया. यामुळे आपले मॉडेल वर्गीकरणासाठी आवश्यक असलेली आर्किटेक्चर, अंतिम वर्गीकरणकर्ता समावेशासह, आधीच तयार आहे याची खात्री होते. तुम्हाला एक चेतावणी संदेश दिसेल ज्यामध्ये म्हटले जाईल की अंतिम वर्गीकरणकर्त्याचे वजन प्रारंभिक केलेले नाही, आणि मॉडेलला पूर्व-प्रशिक्षणाची आवश्यकता असेल - ते पूर्णपणे ठीक आहे, कारण आपण नेमके तेच करणार आहोत!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आता आपण प्रशिक्षण सुरू करण्यासाठी तयार आहोत! कारण BERT आधीच प्री-ट्रेन केलेला आहे, त्यामुळे सुरुवातीला लहान learning rate वापरणे महत्त्वाचे आहे, जेणेकरून सुरुवातीचे weights खराब होऊ नयेत.\n",
    "\n",
    "संपूर्ण मेहनत `BertForSequenceClassification` मॉडेलद्वारे केली जाते. जेव्हा आपण प्रशिक्षण डेटावर मॉडेल कॉल करतो, तेव्हा ते इनपुट minibatch साठी loss आणि नेटवर्क output दोन्ही परत करते. आम्ही loss चा वापर parameter optimization साठी करतो (`loss.backward()` बॅकवर्ड पास करते), आणि `out` चा वापर प्रशिक्षण accuracy मोजण्यासाठी करतो, ज्यामध्ये मिळालेल्या labels `labs` (जे `argmax` वापरून गणना केले जातात) अपेक्षित `labels` शी तुलना केली जाते.\n",
    "\n",
    "प्रक्रियेवर नियंत्रण ठेवण्यासाठी, आम्ही loss आणि accuracy अनेक iterations वर जमा करतो आणि प्रत्येक `report_freq` प्रशिक्षण चक्रांनंतर त्यांना प्रिंट करतो.\n",
    "\n",
    "हे प्रशिक्षण कदाचित बराच वेळ घेईल, त्यामुळे आम्ही iterations ची संख्या मर्यादित ठेवतो.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आपण पाहू शकता (विशेषतः जर तुम्ही पुनरावृत्तीची संख्या वाढवली आणि थोडा वेळ थांबलात), की BERT वर्गीकरण आपल्याला खूप चांगली अचूकता देते! याचे कारण असे की BERT आधीच भाषेची रचना चांगल्या प्रकारे समजून घेतो, आणि आपल्याला फक्त अंतिम वर्गीकरणकर्ता सुधारित करायचा असतो. मात्र, BERT हा मोठा मॉडेल असल्यामुळे, संपूर्ण प्रशिक्षण प्रक्रिया खूप वेळखाऊ असते आणि ती करण्यासाठी प्रचंड संगणकीय शक्तीची आवश्यकता असते! (GPU, आणि शक्यतो एकापेक्षा जास्त).\n",
    "\n",
    "> **टीप:** आपल्या उदाहरणात, आपण सर्वात लहान पूर्व-प्रशिक्षित BERT मॉडेल्सपैकी एकाचा वापर केला आहे. मोठ्या मॉडेल्स उपलब्ध आहेत, जे कदाचित अधिक चांगले परिणाम देतील.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## मॉडेलच्या कार्यक्षमतेचे मूल्यमापन\n",
    "\n",
    "आता आपण चाचणी डेटासेटवर आपल्या मॉडेलच्या कार्यक्षमतेचे मूल्यमापन करू शकतो. मूल्यांकन लूप प्रशिक्षण लूपसारखाच असतो, परंतु `model.eval()` कॉल करून मॉडेलला मूल्यांकन मोडमध्ये स्विच करणे विसरू नये.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## मुख्य मुद्दा\n",
    "\n",
    "या युनिटमध्ये, आपण पाहिले की **transformers** लायब्ररीमधून पूर्व-प्रशिक्षित भाषा मॉडेल घेणे आणि ते आपल्या टेक्स्ट वर्गीकरण कार्यासाठी अनुकूल करणे किती सोपे आहे. त्याचप्रमाणे, BERT मॉडेल्सचा वापर घटक शोधणे, प्रश्नोत्तर, आणि इतर NLP कार्यांसाठी केला जाऊ शकतो.\n",
    "\n",
    "Transformer मॉडेल्स NLP मधील अत्याधुनिक तंत्रज्ञानाचे प्रतिनिधित्व करतात, आणि बहुतेक प्रकरणांमध्ये, कस्टम NLP सोल्यूशन्स अंमलात आणताना प्रयोग सुरू करण्यासाठी ही पहिली निवड असावी. तथापि, जर तुम्हाला प्रगत न्यूरल मॉडेल्स तयार करायचे असतील, तर या मॉड्यूलमध्ये चर्चिलेल्या पुनरावृत्ती न्यूरल नेटवर्क्सच्या मूलभूत तत्त्वांचे समजून घेणे अत्यंत महत्त्वाचे आहे.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nहा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात ठेवा की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील दस्तऐवज हा अधिकृत स्रोत मानावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर करून उद्भवलेल्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T09:22:53+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "mr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}