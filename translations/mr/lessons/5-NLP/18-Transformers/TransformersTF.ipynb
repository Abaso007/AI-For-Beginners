{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# अटेन्शन मेकॅनिझम्स आणि ट्रान्सफॉर्मर्स\n",
    "\n",
    "रीकरंट नेटवर्क्सचा एक मोठा तोटा म्हणजे एका अनुक्रमातील सर्व शब्दांचा परिणामावर समान प्रभाव असतो. यामुळे नेम्ड एंटिटी रिकग्निशन आणि मशीन ट्रान्सलेशनसारख्या अनुक्रम-ते-अनुक्रम कार्यांसाठी स्टँडर्ड LSTM एन्कोडर-डिकोडर मॉडेल्समध्ये कमी कार्यक्षमता दिसून येते. प्रत्यक्षात, इनपुट अनुक्रमातील विशिष्ट शब्दांचा अनुक्रमिक आउटपुटवर इतरांपेक्षा अधिक प्रभाव असतो.\n",
    "\n",
    "मशीन ट्रान्सलेशनसारख्या अनुक्रम-ते-अनुक्रम मॉडेलचा विचार करा. हे दोन रीकरंट नेटवर्क्सद्वारे अंमलात आणले जाते, जिथे एक नेटवर्क (**एन्कोडर**) इनपुट अनुक्रमाला हिडन स्टेटमध्ये रूपांतरित करते, आणि दुसरे नेटवर्क (**डिकोडर**) त्या हिडन स्टेटला ट्रान्सलेटेड आउटपुटमध्ये बदलते. या पद्धतीचा एक मोठा तोटा म्हणजे नेटवर्कचा अंतिम स्टेट वाक्याच्या सुरुवातीचा भाग लक्षात ठेवण्यात अडचण निर्माण करतो, ज्यामुळे लांब वाक्यांवर मॉडेलची गुणवत्ता कमी होते.\n",
    "\n",
    "**अटेन्शन मेकॅनिझम्स** RNN च्या प्रत्येक आउटपुट प्रेडिक्शनवर प्रत्येक इनपुट व्हेक्टरच्या संदर्भात्मक प्रभावाचे वजन करण्याचा एक मार्ग प्रदान करतात. हे अंमलात आणण्याचा मार्ग म्हणजे इनपुट RNN च्या इंटरमिजिएट स्टेट्स आणि आउटपुट RNN दरम्यान शॉर्टकट तयार करणे. अशा प्रकारे, जेव्हा आउटपुट चिन्ह $y_t$ तयार करतो, तेव्हा आपण सर्व इनपुट हिडन स्टेट्स $h_i$ विचारात घेऊ, वेगवेगळ्या वजन गुणांक $\\alpha_{t,i}$ सह.\n",
    "\n",
    "![एन्कोडर/डिकोडर मॉडेल अ‍ॅडिटिव्ह अटेन्शन लेयरसह दर्शविणारी प्रतिमा](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.mr.png)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) मधील अ‍ॅडिटिव्ह अटेन्शन मेकॅनिझमसह एन्कोडर-डिकोडर मॉडेल, [या ब्लॉग पोस्टमधून](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) घेतलेले*\n",
    "\n",
    "अटेन्शन मॅट्रिक्स $\\{\\alpha_{i,j}\\}$ हे दर्शवते की आउटपुट अनुक्रमातील विशिष्ट शब्द तयार करण्यात कोणत्या इनपुट शब्दांचा किती प्रभाव आहे. खाली अशा मॅट्रिक्सचे उदाहरण दिले आहे:\n",
    "\n",
    "![RNNsearch-50 ने सापडलेले नमुना अलाइनमेंट दर्शविणारी प्रतिमा, Bahdanau - arviz.org कडून घेतलेली](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.mr.png)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) मधून घेतलेली आकृती]*\n",
    "\n",
    "अटेन्शन मेकॅनिझम्स नैसर्गिक भाषा प्रक्रिया (NLP) मध्ये सध्याच्या किंवा जवळपासच्या अत्याधुनिक स्थितीसाठी जबाबदार आहेत. मात्र, अटेन्शन जोडल्याने मॉडेल पॅरामीटर्सची संख्या मोठ्या प्रमाणात वाढते, ज्यामुळे RNNs सह स्केलिंग समस्या निर्माण होतात. RNNs च्या स्केलिंगसाठी एक महत्त्वाची अडचण म्हणजे मॉडेल्सच्या रीकरंट स्वरूपामुळे प्रशिक्षण बॅच करणे आणि पॅरललाइज करणे कठीण होते. RNN मध्ये अनुक्रमातील प्रत्येक घटकाला अनुक्रमिक क्रमाने प्रक्रिया करावी लागते, ज्यामुळे ते सहजपणे पॅरललाइज करता येत नाही.\n",
    "\n",
    "अटेन्शन मेकॅनिझम्सचा अवलंब आणि या अडचणीमुळे आज आपण वापरत असलेल्या अत्याधुनिक ट्रान्सफॉर्मर मॉडेल्सचा जन्म झाला, जसे की BERT आणि OpenGPT3.\n",
    "\n",
    "## ट्रान्सफॉर्मर मॉडेल्स\n",
    "\n",
    "प्रत्येक मागील प्रेडिक्शनचा संदर्भ पुढील मूल्यांकन चरणात पाठविण्याऐवजी, **ट्रान्सफॉर्मर मॉडेल्स** **पोजिशनल एन्कोडिंग्स** आणि **अटेन्शन** वापरून दिलेल्या मजकूराच्या विंडोमध्ये इनपुटचा संदर्भ कॅप्चर करतात. खालील प्रतिमा दर्शवते की पोजिशनल एन्कोडिंग्ससह अटेन्शन दिलेल्या विंडोमध्ये संदर्भ कसा कॅप्चर करू शकतो.\n",
    "\n",
    "![ट्रान्सफॉर्मर मॉडेल्समध्ये मूल्यांकन कसे केले जाते हे दर्शविणारा अ‍ॅनिमेटेड GIF](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "प्रत्येक इनपुट पोझिशन स्वतंत्रपणे प्रत्येक आउटपुट पोझिशनशी मॅप केल्यामुळे, ट्रान्सफॉर्मर्स RNNs पेक्षा चांगले पॅरललाइज करू शकतात, ज्यामुळे खूप मोठी आणि अधिक अभिव्यक्तीक्षम भाषा मॉडेल्स सक्षम होतात. प्रत्येक अटेन्शन हेड वेगवेगळ्या शब्दांमधील संबंध शिकण्यासाठी वापरला जाऊ शकतो, ज्यामुळे नैसर्गिक भाषा प्रक्रिया कार्ये सुधारतात.\n",
    "\n",
    "## साधे ट्रान्सफॉर्मर मॉडेल तयार करणे\n",
    "\n",
    "Keras मध्ये अंगभूत ट्रान्सफॉर्मर लेयर नाही, परंतु आपण स्वतः तयार करू शकतो. यापूर्वीप्रमाणे, आपण AG News डेटासेटच्या मजकूर वर्गीकरणावर लक्ष केंद्रित करू, परंतु हे नमूद करणे महत्त्वाचे आहे की ट्रान्सफॉर्मर मॉडेल्स अधिक कठीण NLP कार्यांमध्ये सर्वोत्तम परिणाम दाखवतात.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "केरस मधील नवीन स्तरांना `Layer` वर्गाचे उपवर्ग बनवावे लागेल आणि `call` पद्धत लागू करावी लागेल. चला **Positional Embedding** स्तरासह सुरुवात करूया. आम्ही [अधिकृत केरस दस्तऐवजातील काही कोड](https://keras.io/examples/nlp/text_classification_with_transformer/) वापरणार आहोत. आम्ही गृहित धरतो की आम्ही सर्व इनपुट अनुक्रम `maxlen` लांबीपर्यंत पॅड करतो.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "या स्तरामध्ये दोन `Embedding` स्तरांचा समावेश आहे: टोकनसाठी एम्बेडिंग (जसा आपण यापूर्वी चर्चा केली आहे) आणि टोकनच्या स्थानांसाठी. टोकनची स्थानं 0 ते `maxlen` पर्यंत नैसर्गिक संख्यांच्या अनुक्रमाप्रमाणे `tf.range` वापरून तयार केली जातात आणि नंतर एम्बेडिंग स्तरामधून पाठवली जातात. परिणामी दोन एम्बेडिंग व्हेक्टर एकत्र केले जातात, ज्यामुळे `maxlen`$\\times$`embed_dim` आकाराच्या इनपुटचे स्थानिक-एम्बेडेड प्रतिनिधित्व तयार होते.\n",
    "\n",
    "आता, आपण ट्रान्सफॉर्मर ब्लॉक लागू करूया. हा ब्लॉक यापूर्वी परिभाषित केलेल्या एम्बेडिंग स्तराच्या आउटपुटला घेईल:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आता, आपण पूर्ण ट्रान्सफॉर्मर मॉडेल परिभाषित करण्यासाठी तयार आहोत:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Models\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) हा एक मोठा मल्टी लेयर ट्रान्सफॉर्मर नेटवर्क आहे ज्यामध्ये *BERT-base* साठी 12 स्तर आणि *BERT-large* साठी 24 स्तर आहेत. हा मॉडेल प्रथम मोठ्या प्रमाणावर मजकूर डेटावर (WikiPedia + पुस्तके) असुपरवाइज्ड ट्रेनिंगचा वापर करून (वाक्यातील मास्क केलेल्या शब्दांची भविष्यवाणी करणे) प्री-ट्रेन केला जातो. प्री-ट्रेनिंग दरम्यान मॉडेल महत्त्वपूर्ण भाषिक समज आत्मसात करते, ज्याचा नंतर इतर डेटासेटसह फाइन ट्यूनिंगद्वारे उपयोग केला जाऊ शकतो. या प्रक्रियेला **ट्रान्सफर लर्निंग** म्हणतात.\n",
    "\n",
    "![picture from http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.mr.png)\n",
    "\n",
    "BERT, DistilBERT, BigBird, OpenGPT3 आणि इतर अनेक ट्रान्सफॉर्मर आर्किटेक्चरचे प्रकार आहेत, जे फाइन ट्यून केले जाऊ शकतात.\n",
    "\n",
    "चला पाहूया की प्री-ट्रेन केलेल्या BERT मॉडेलचा उपयोग पारंपरिक सिक्वेन्स वर्गीकरण समस्येचे निराकरण करण्यासाठी कसा करता येईल. आपण [अधिकृत दस्तऐवज](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) मधून कल्पना आणि काही कोड उधार घेऊ.\n",
    "\n",
    "प्री-ट्रेन केलेले मॉडेल लोड करण्यासाठी, आपण **Tensorflow hub** वापरणार आहोत. प्रथम, BERT-विशिष्ट व्हेक्टरायझर लोड करूया:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "मूळ नेटवर्क प्रशिक्षणासाठी वापरलेला तोच व्हेक्टरायझर वापरणे महत्त्वाचे आहे. तसेच, BERT व्हेक्टरायझर तीन घटक परत करतो:\n",
    "* `input_word_ids`, जो इनपुट वाक्यांसाठी टोकन क्रमांकांची मालिका असतो\n",
    "* `input_mask`, जो दर्शवतो की मालिकेचा कोणता भाग वास्तविक इनपुट आहे आणि कोणता भाग पॅडिंग आहे. हे `Masking` स्तराद्वारे तयार केलेल्या मास्कसारखेच असते\n",
    "* `input_type_ids` भाषा मॉडेलिंग कार्यांसाठी वापरले जाते आणि एका मालिकेत दोन इनपुट वाक्ये निर्दिष्ट करण्याची परवानगी देते.\n",
    "\n",
    "यानंतर, आपण BERT वैशिष्ट्य एक्स्ट्रॅक्टर तयार करू शकतो:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "तर, BERT लेयर काही उपयुक्त परिणाम परत करते:\n",
    "* `pooled_output` हा अनुक्रमातील सर्व टोकनचा सरासरी काढण्याचा परिणाम आहे. तुम्ही याला संपूर्ण नेटवर्कचे एक बुद्धिमान सिमॅंटिक एम्बेडिंग म्हणून पाहू शकता. हे आपल्या आधीच्या मॉडेलमधील `GlobalAveragePooling1D` लेयरच्या आउटपुटसारखेच आहे.\n",
    "* `sequence_output` हा शेवटच्या ट्रान्सफॉर्मर लेयरचा आउटपुट आहे (आपल्या वरील मॉडेलमधील `TransformerBlock` च्या आउटपुटशी संबंधित आहे).\n",
    "* `encoder_outputs` हे सर्व ट्रान्सफॉर्मर लेयर्सचे आउटपुट आहेत. आपण 4-लेयर BERT मॉडेल लोड केले आहे (जसे तुम्ही नावावरून अंदाज लावू शकता, ज्यामध्ये `4_H` समाविष्ट आहे), त्यामुळे यामध्ये 4 टेन्सर्स आहेत. यातील शेवटचा `sequence_output` सारखाच आहे.\n",
    "\n",
    "आता आपण एंड-टू-एंड वर्गीकरण मॉडेल परिभाषित करू. आपण *फंक्शनल मॉडेल डिफिनिशन* वापरणार आहोत, जिथे आपण मॉडेल इनपुट परिभाषित करू, आणि नंतर त्याच्या आउटपुटची गणना करण्यासाठी एक मालिका अभिव्यक्ती प्रदान करू. आपण BERT मॉडेलचे वेट्स ट्रेन करण्यायोग्य ठेवणार नाही, आणि फक्त अंतिम वर्गीकरणकर्ता ट्रेन करू:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "जरी ट्रेन करण्यायोग्य पॅरामिटर्स कमी असले तरी, प्रक्रिया खूपच धीमी आहे, कारण BERT फीचर एक्स्ट्रॅक्टर संगणकीयदृष्ट्या जड आहे. असे दिसते की आम्ही योग्य अचूकता साध्य करण्यात अयशस्वी ठरलो, कदाचित प्रशिक्षणाच्या अभावामुळे किंवा मॉडेल पॅरामिटर्सच्या अभावामुळे.\n",
    "\n",
    "आता BERT चे वेट्स अनफ्रीज करून त्याला देखील ट्रेन करण्याचा प्रयत्न करूया. यासाठी खूपच कमी लर्निंग रेट आवश्यक आहे, तसेच **वॉर्मअप** आणि **AdamW** ऑप्टिमायझर वापरून अधिक काळजीपूर्वक प्रशिक्षण धोरण आवश्यक आहे. ऑप्टिमायझर तयार करण्यासाठी आम्ही `tf-models-official` पॅकेज वापरणार आहोत:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "जसे तुम्ही पाहू शकता, प्रशिक्षण प्रक्रिया खूपच हळूहळू चालते - पण तुम्हाला कदाचित काही epochs (5-10) साठी मॉडेल प्रशिक्षण देऊन पाहावेसे वाटेल आणि आपण यापूर्वी वापरलेल्या पद्धतींच्या तुलनेत सर्वोत्तम परिणाम मिळवू शकता का ते तपासावे.\n",
    "\n",
    "## Huggingface Transformers लायब्ररी\n",
    "\n",
    "Transformer मॉडेल्स वापरण्याचा आणखी एक खूप सामान्य (आणि थोडासा सोपा) मार्ग म्हणजे [HuggingFace पॅकेज](https://github.com/huggingface/), जे विविध NLP कार्यांसाठी सोपी बिल्डिंग ब्लॉक्स प्रदान करते. हे Tensorflow आणि PyTorch या आणखी एका लोकप्रिय न्यूरल नेटवर्क फ्रेमवर्कसाठी उपलब्ध आहे.\n",
    "\n",
    "> **टीप**: जर तुम्हाला Transformers लायब्ररी कशी कार्य करते हे पाहण्यात रस नसेल - तर तुम्ही या नोटबुकच्या शेवटी जाऊ शकता, कारण तुम्हाला वर केलेल्या गोष्टींपेक्षा काही वेगळे दिसणार नाही. आम्ही वेगळी लायब्ररी आणि तुलनेने मोठ्या मॉडेलचा वापर करून BERT मॉडेल प्रशिक्षणाची तीच पावले पुन्हा घेणार आहोत. त्यामुळे, प्रक्रियेमध्ये काहीसे दीर्घ प्रशिक्षण समाविष्ट आहे, त्यामुळे तुम्हाला फक्त कोड पाहणे अधिक सोयीचे वाटेल.\n",
    "\n",
    "चला पाहूया की आपली समस्या [Huggingface Transformers](http://huggingface.co) वापरून कशी सोडवता येईल.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "सर्वप्रथम, आपल्याला वापरण्यासाठी कोणता मॉडेल निवडायचा आहे ते ठरवावे लागेल. काही अंगभूत मॉडेल्स व्यतिरिक्त, Huggingface मध्ये [ऑनलाइन मॉडेल रिपॉझिटरी](https://huggingface.co/models) आहे, जिथे तुम्हाला समुदायाने तयार केलेली बरीच प्री-ट्रेन मॉडेल्स सापडतील. या सर्व मॉडेल्स केवळ मॉडेलचे नाव दिल्याने लोड आणि वापरता येतात. मॉडेलसाठी आवश्यक असलेली सर्व बायनरी फाइल्स आपोआप डाउनलोड केल्या जातील.\n",
    "\n",
    "काही वेळा तुम्हाला स्वतःचे मॉडेल्स लोड करायची गरज भासेल, अशा वेळी तुम्ही त्या डिरेक्टरीचा पत्ता देऊ शकता ज्यामध्ये संबंधित सर्व फाइल्स असतील, जसे की टोकनायझरसाठीचे पॅरामिटर्स, मॉडेल पॅरामिटर्ससह `config.json` फाइल, बायनरी वेट्स इत्यादी.\n",
    "\n",
    "मॉडेलच्या नावावरून, आपण मॉडेल आणि टोकनायझर दोन्ही तयार करू शकतो. चला टोकनायझरपासून सुरुवात करूया:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` ऑब्जेक्टमध्ये `encode` फंक्शन असते जी थेट मजकूर एन्कोड करण्यासाठी वापरली जाऊ शकते:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आम्ही टोकनायझरचा वापर करून अनुक्रम एका अशा प्रकारे एन्कोड करू शकतो जो मॉडेलला पास करण्यासाठी योग्य असेल, उदा. `token_ids`, `input_mask` फील्ड्स समाविष्ट करून. तसेच, आम्ही `return_tensors='tf'` युक्तिवाद प्रदान करून Tensorflow टेन्सर्स हवे आहेत हे निर्दिष्ट करू शकतो:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आपल्या बाबतीत, आपण `bert-base-uncased` नावाचा पूर्व-प्रशिक्षित BERT मॉडेल वापरणार आहोत. *Uncased* म्हणजे मॉडेल केस-संवेदनशील नाही.\n",
    "\n",
    "मॉडेल प्रशिक्षणाच्या वेळी, आपल्याला टोकनाइझ केलेली अनुक्रमे इनपुट म्हणून प्रदान करावी लागते, आणि म्हणूनच आपण डेटा प्रक्रिया पाइपलाइन डिझाइन करू. कारण `tokenizer.encode` ही एक Python फंक्शन आहे, आपण मागील युनिटमध्ये जसे केले होते त्याच पद्धतीचा वापर करून `py_function` वापरून ती कॉल करू:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आता आपण `BertForSequenceClassification` पॅकेज वापरून वास्तविक मॉडेल लोड करू शकतो. यामुळे आपले मॉडेल वर्गीकरणासाठी आवश्यक असलेली आर्किटेक्चर आधीच तयार असते, ज्यामध्ये अंतिम वर्गीकरणकर्ता समाविष्ट आहे. तुम्हाला एक चेतावणी संदेश दिसेल ज्यामध्ये म्हटले जाईल की अंतिम वर्गीकरणकर्त्याचे वेट्स प्रारंभिक केलेले नाहीत आणि मॉडेलला पूर्व-प्रशिक्षणाची आवश्यकता असेल - ते पूर्णपणे ठीक आहे, कारण आपण नेमके तेच करणार आहोत!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary()` वरून तुम्ही पाहू शकता की, मॉडेलमध्ये जवळजवळ 110 दशलक्ष पॅरामिटर्स आहेत! गृहित धरले तर, जर आपल्याला तुलनेने लहान डेटासेटवर साधे वर्गीकरण कार्य करायचे असेल, तर आपल्याला BERT बेस लेयर प्रशिक्षण द्यायची गरज नाही:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आता आपण प्रशिक्षण सुरू करण्यासाठी तयार आहोत!\n",
    "\n",
    "> **टीप**: पूर्ण-स्केल BERT मॉडेलचे प्रशिक्षण घेणे खूप वेळखाऊ असू शकते! त्यामुळे आपण फक्त पहिल्या 32 बॅचेससाठी प्रशिक्षण घेणार आहोत. हे फक्त मॉडेल प्रशिक्षण कसे सेट केले जाते हे दाखवण्यासाठी आहे. जर तुम्हाला पूर्ण-स्केल प्रशिक्षण करून पाहायचे असेल - तर `steps_per_epoch` आणि `validation_steps` पॅरामीटर्स काढा, आणि थोडा वेळ थांबण्याची तयारी करा!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "जर तुम्ही पुनरावृत्तींची संख्या वाढवली आणि पुरेसा वेळ दिला, तसेच अनेक epochs साठी प्रशिक्षण दिले, तर तुम्ही अपेक्षा करू शकता की BERT वर्गीकरण आपल्याला सर्वोत्तम अचूकता देईल! कारण BERT आधीच भाषेची रचना चांगल्या प्रकारे समजून घेतो, आणि आपल्याला फक्त अंतिम वर्गीकरणकर्ता fine-tune करायचा आहे. मात्र, BERT हा मोठा मॉडेल असल्यामुळे संपूर्ण प्रशिक्षण प्रक्रिया खूप वेळ घेते आणि त्यासाठी प्रचंड संगणकीय शक्तीची आवश्यकता असते! (GPU, आणि शक्यतो एकापेक्षा जास्त).\n",
    "\n",
    "> **Note:** आमच्या उदाहरणात, आम्ही सर्वात लहान pre-trained BERT मॉडेलपैकी एक वापरत आहोत. मोठे मॉडेल्स उपलब्ध आहेत जे अधिक चांगले परिणाम देण्याची शक्यता आहे.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## मुख्य मुद्दा\n",
    "\n",
    "या युनिटमध्ये, आपण **transformers** आधारित अत्याधुनिक मॉडेल आर्किटेक्चर पाहिले. आपण त्यांचा उपयोग आपल्या टेक्स्ट वर्गीकरण कार्यासाठी केला, परंतु त्याचप्रमाणे BERT मॉडेल्सचा उपयोग घटक शोधणे, प्रश्नोत्तर, आणि इतर NLP कार्यांसाठी केला जाऊ शकतो.\n",
    "\n",
    "Transformer मॉडेल्स NLP मध्ये सध्याच्या अत्याधुनिक तंत्रज्ञानाचे प्रतिनिधित्व करतात, आणि बहुतेक प्रकरणांमध्ये, कस्टम NLP सोल्यूशन्स अंमलात आणताना प्रयोग करण्यासाठी ही पहिली निवड असावी. तथापि, प्रगत न्यूरल मॉडेल्स तयार करायचे असल्यास, या मॉड्यूलमध्ये चर्चिलेल्या पुनरावृत्ती न्यूरल नेटवर्क्सच्या मूलभूत तत्त्वांचे समजून घेणे अत्यंत महत्त्वाचे आहे.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nहा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) चा वापर करून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील मूळ दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर केल्यामुळे उद्भवणाऱ्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T09:26:33+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "mr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}