<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-26T08:20:22+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "mr"
}
-->
# जनरेटिव नेटवर्क्स

## [पूर्व-व्याख्यान प्रश्नमंजूषा](https://ff-quizzes.netlify.app/en/ai/quiz/33)

रीकरंट न्यूरल नेटवर्क्स (RNNs) आणि त्यांचे गेटेड सेल प्रकार जसे की लाँग शॉर्ट टर्म मेमरी सेल्स (LSTMs) आणि गेटेड रीकरंट युनिट्स (GRUs) भाषेचे मॉडेलिंग करण्यासाठी एक यंत्रणा प्रदान करतात ज्यामुळे ते शब्दांची क्रमवारी शिकू शकतात आणि अनुक्रमातील पुढील शब्दासाठी अंदाज देऊ शकतात. यामुळे RNNs चा वापर **जनरेटिव कार्यांसाठी** करता येतो, जसे की सामान्य मजकूर निर्मिती, मशीन अनुवाद, आणि अगदी प्रतिमेचे वर्णन तयार करणे.

> ✅ तुम्ही मजकूर पूर्ण करण्यासारख्या जनरेटिव कार्यांमुळे किती वेळा फायदा घेतला आहे याचा विचार करा. तुमच्या आवडत्या अॅप्लिकेशन्सबद्दल संशोधन करा आणि त्यांनी RNNs चा उपयोग केला आहे का ते पहा.

आम्ही मागील युनिटमध्ये चर्चा केलेल्या RNN आर्किटेक्चरमध्ये, प्रत्येक RNN युनिटने पुढील हिडन स्टेट आउटपुट म्हणून तयार केली. तथापि, आम्ही प्रत्येक रीकरंट युनिटला आणखी एक आउटपुट जोडू शकतो, ज्यामुळे आम्हाला **अनुक्रम** आउटपुट करता येईल (जो मूळ अनुक्रमाच्या लांबीइतकाच असेल). याशिवाय, आम्ही RNN युनिट्स वापरू शकतो जे प्रत्येक टप्प्यावर इनपुट स्वीकारत नाहीत, फक्त काही प्रारंभिक स्टेट वेक्टर घेतात आणि नंतर आउटपुट्सचा अनुक्रम तयार करतात.

यामुळे खालील चित्रात दाखवलेल्या विविध न्यूरल आर्किटेक्चर शक्य होतात:

![रीकरंट न्यूरल नेटवर्क्सचे सामान्य नमुने दाखवणारे चित्र.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.mr.jpg)

> ब्लॉग पोस्ट [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) द्वारे [Andrej Karpaty](http://karpathy.github.io/) यांचे चित्र

* **वन-टू-वन** हे पारंपरिक न्यूरल नेटवर्क आहे ज्यामध्ये एक इनपुट आणि एक आउटपुट असतो
* **वन-टू-मेनी** हे जनरेटिव आर्किटेक्चर आहे जे एक इनपुट मूल्य स्वीकारते आणि आउटपुट मूल्यांचा अनुक्रम तयार करते. उदाहरणार्थ, जर आम्हाला **प्रतिमा वर्णन** नेटवर्क प्रशिक्षण द्यायचे असेल जे चित्राचे मजकूर वर्णन तयार करेल, तर आम्ही चित्र इनपुट म्हणून घेऊ शकतो, ते CNN मधून पास करून त्याचा हिडन स्टेट मिळवू शकतो आणि नंतर रीकरंट चेनने शब्द-प्रत्येक शब्दाने वर्णन तयार करावे
* **मेनी-टू-वन** हे RNN आर्किटेक्चरशी संबंधित आहे ज्याचे वर्णन आम्ही मागील युनिटमध्ये केले होते, जसे की मजकूर वर्गीकरण
* **मेनी-टू-मेनी**, किंवा **अनुक्रम-टू-अनुक्रम** हे **मशीन अनुवाद** सारख्या कार्यांशी संबंधित आहे, जिथे पहिला RNN इनपुट अनुक्रमातून सर्व माहिती हिडन स्टेटमध्ये गोळा करतो आणि दुसरा RNN चेन त्या स्टेटला आउटपुट अनुक्रमात बदलतो.

या युनिटमध्ये, आम्ही मजकूर तयार करण्यात मदत करणाऱ्या साध्या जनरेटिव मॉडेल्सवर लक्ष केंद्रित करू. सोपेपणासाठी, आम्ही कॅरेक्टर-लेव्हल टोकनायझेशन वापरू.

आम्ही या RNN ला चरण-दर-चरण मजकूर तयार करण्यासाठी प्रशिक्षण देऊ. प्रत्येक टप्प्यावर, आम्ही `nchars` लांबीचा कॅरेक्टर अनुक्रम घेऊ आणि नेटवर्कला प्रत्येक इनपुट कॅरेक्टरसाठी पुढील आउटपुट कॅरेक्टर तयार करण्यास सांगू:

!['HELLO' शब्द तयार करण्यासाठी RNN चा उदाहरण दाखवणारे चित्र.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.mr.png)

मजकूर तयार करताना (इन्फरन्स दरम्यान), आम्ही काही **प्रॉम्प्ट** ने सुरुवात करतो, जे RNN सेल्समधून पास होते आणि त्याचा इंटरमीडिएट स्टेट तयार होतो, आणि नंतर त्या स्टेटमधून निर्मिती सुरू होते. आम्ही एकावेळी एक कॅरेक्टर तयार करतो आणि स्टेट आणि तयार केलेला कॅरेक्टर दुसऱ्या RNN सेलला पास करतो जो पुढील कॅरेक्टर तयार करतो, जोपर्यंत पुरेसे कॅरेक्टर तयार होत नाहीत.

<img src="images/rnn-generate-inf.png" width="60%"/>

> लेखकाचे चित्र

## ✍️ सराव: जनरेटिव नेटवर्क्स

खालील नोटबुक्समध्ये तुमचे शिक्षण सुरू ठेवा:

* [PyTorch सह जनरेटिव नेटवर्क्स](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [TensorFlow सह जनरेटिव नेटवर्क्स](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## सॉफ्ट टेक्स्ट जनरेशन आणि तापमान

प्रत्येक RNN सेलचे आउटपुट कॅरेक्टर्सचे संभाव्यता वितरण असते. जर आम्ही नेहमीच सर्वाधिक संभाव्यता असलेल्या कॅरेक्टरला पुढील कॅरेक्टर म्हणून निवडले, तर तयार केलेला मजकूर अनेकदा "सायकल" होऊ शकतो, जसे की खालील उदाहरणात:

```
today of the second the company and a second the company ...
```

तथापि, जर आपण पुढील कॅरेक्टरसाठी संभाव्यता वितरण पाहिले, तर असे होऊ शकते की काही उच्च संभाव्यता असलेल्या कॅरेक्टर्समधील फरक फारसा मोठा नाही, उदा. एका कॅरेक्टरची संभाव्यता 0.2 असू शकते, तर दुसऱ्याची 0.19 असू शकते. उदाहरणार्थ, '*play*' अनुक्रमात पुढील कॅरेक्टर स्पेस किंवा **e** (जसे *player* शब्दात) असू शकतो.

यामुळे असे निष्कर्ष निघतो की नेहमीच उच्च संभाव्यता असलेल्या कॅरेक्टरला निवडणे "योग्य" नाही, कारण दुसऱ्या क्रमांकाच्या संभाव्यता असलेल्या कॅरेक्टरला निवडणे देखील अर्थपूर्ण मजकूर तयार करू शकते. नेटवर्क आउटपुटद्वारे दिलेल्या संभाव्यता वितरणातून कॅरेक्टर्स **नमुन्याने** निवडणे अधिक शहाणपणाचे आहे. आम्ही **तापमान** नावाचा पॅरामीटर देखील वापरू शकतो, जो संभाव्यता वितरण सपाट करतो, जर आम्हाला अधिक रँडमनेस जोडायचा असेल, किंवा अधिक तीव्र करतो, जर आम्हाला उच्च-सम्भाव्यता कॅरेक्टर्सकडे अधिक चिकटून राहायचे असेल.

वरील नोटबुक्समध्ये सॉफ्ट टेक्स्ट जनरेशन कसे अंमलात आणले जाते ते एक्सप्लोर करा.

## निष्कर्ष

मजकूर निर्मिती स्वतःच उपयुक्त असू शकते, परंतु मुख्य फायदे RNNs चा उपयोग करून मजकूर तयार करण्याच्या क्षमतेतून येतात, जे काही प्रारंभिक फीचर वेक्टरपासून सुरू होते. उदाहरणार्थ, मजकूर निर्मिती मशीन अनुवादाचा भाग म्हणून वापरली जाते (अनुक्रम-टू-अनुक्रम, या प्रकरणात *एन्कोडर* कडून स्टेट वेक्टर वापरून अनुवादित संदेश तयार किंवा *डिकोड* केला जातो), किंवा प्रतिमेचे मजकूर वर्णन तयार करण्यासाठी (ज्या प्रकरणात फीचर वेक्टर CNN एक्स्ट्रॅक्टरकडून येतो).

## 🚀 आव्हान

या विषयावर Microsoft Learn वर काही धडे घ्या

* [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste) सह मजकूर निर्मिती

## [व्याख्यानानंतरची प्रश्नमंजूषा](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## पुनरावलोकन आणि स्व-अभ्यास

तुमचे ज्ञान वाढवण्यासाठी काही लेख येथे दिले आहेत

* मार्कोव्ह चेन, LSTM आणि GPT-2 सह मजकूर निर्मितीचे वेगवेगळे दृष्टिकोन: [ब्लॉग पोस्ट](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Keras दस्तऐवज](https://keras.io/examples/generative/lstm_character_level_text_generation/) मध्ये मजकूर निर्मितीचे उदाहरण

## [असाइनमेंट](lab/README.md)

आपण कॅरेक्टर-बाय-कॅरेक्टर मजकूर कसा तयार करायचा ते पाहिले आहे. प्रयोगशाळेत, तुम्ही शब्द-स्तरीय मजकूर निर्मिती एक्सप्लोर कराल.

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील मूळ दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी, व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर केल्यामुळे उद्भवणाऱ्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.