<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T07:12:14+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "mr"
}
-->
# पूर्व-प्रशिक्षित मोठे भाषा मॉडेल्स

आपल्या मागील सर्व कार्यांमध्ये, आम्ही लेबल केलेल्या डेटासेटचा वापर करून विशिष्ट कार्य करण्यासाठी न्यूरल नेटवर्क प्रशिक्षण देत होतो. मोठ्या ट्रान्सफॉर्मर मॉडेल्ससह, जसे की BERT, आम्ही स्वयं-पर्यवेक्षित पद्धतीने भाषा मॉडेलिंगचा वापर करून भाषा मॉडेल तयार करतो, जे नंतर विशिष्ट डाउनस्ट्रीम कार्यासाठी डोमेन-विशिष्ट प्रशिक्षणाद्वारे विशेषीकृत केले जाते. तथापि, हे सिद्ध झाले आहे की मोठे भाषा मॉडेल्स अनेक कार्ये कोणत्याही डोमेन-विशिष्ट प्रशिक्षणाशिवाय सोडवू शकतात. अशा प्रकारच्या मॉडेल्सच्या कुटुंबाला **GPT**: जनरेटिव्ह प्री-ट्रेंड ट्रान्सफॉर्मर म्हणतात.

## [पूर्व-व्याख्यान प्रश्नमंजुषा](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## मजकूर निर्मिती आणि पेर्प्लेक्सिटी

डाउनस्ट्रीम प्रशिक्षणाशिवाय सामान्य कार्ये करण्यास सक्षम असलेल्या न्यूरल नेटवर्कची कल्पना [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) या पेपरमध्ये सादर केली आहे. मुख्य कल्पना अशी आहे की अनेक इतर कार्ये **मजकूर निर्मिती** वापरून मॉडेल केली जाऊ शकतात, कारण मजकूर समजणे म्हणजे तो तयार करण्यास सक्षम असणे. कारण मॉडेल मानवी ज्ञान समाविष्ट असलेल्या प्रचंड प्रमाणात मजकूरावर प्रशिक्षित केले जाते, त्यामुळे ते विविध विषयांबद्दल ज्ञान संपन्न होते.

> मजकूर समजणे आणि तयार करण्यास सक्षम असणे याचा अर्थ आपल्या सभोवतालच्या जगाबद्दल काहीतरी जाणून घेणे देखील आहे. लोक मोठ्या प्रमाणात वाचन करून शिकतात आणि GPT नेटवर्क या बाबतीत समान आहे.

मजकूर निर्मिती नेटवर्क पुढील शब्दाची $$P(w_N)$$ संभाव्यता अंदाज करून कार्य करतात. तथापि, पुढील शब्दाची अटीशिवाय संभाव्यता मजकूर कॉर्पस मधील त्या शब्दाच्या वारंवारतेच्या समान असते. GPT आपल्याला मागील शब्द दिल्यास पुढील शब्दाची **अटीसह संभाव्यता** देऊ शकते: $$P(w_N | w_{n-1}, ..., w_0)$$

> संभाव्यतेबद्दल अधिक वाचण्यासाठी आमच्या [डेटा सायन्स फॉर बिगिनर्स अभ्यासक्रम](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) ला भेट द्या.

भाषा निर्माण करणाऱ्या मॉडेलची गुणवत्ता **पेर्प्लेक्सिटी** वापरून परिभाषित केली जाऊ शकते. हे अंतर्गत मेट्रिक आहे जे आम्हाला कोणत्याही कार्य-विशिष्ट डेटासेटशिवाय मॉडेलची गुणवत्ता मोजण्याची परवानगी देते. *वाक्याची संभाव्यता* या संकल्पनेवर आधारित आहे - मॉडेल अशा वाक्याला उच्च संभाव्यता देते जे खरे असण्याची शक्यता आहे (म्हणजे मॉडेल त्याबद्दल **गोंधळलेले** नाही), आणि अशा वाक्यांना कमी संभाव्यता देते ज्यांचा अर्थ कमी लागतो (उदा. *Can it does what?*). जेव्हा आपण आपल्या मॉडेलला वास्तविक मजकूर कॉर्पस मधील वाक्ये देतो, तेव्हा त्यांना उच्च संभाव्यता आणि कमी **पेर्प्लेक्सिटी** असणे अपेक्षित आहे. गणितीयदृष्ट्या, हे चाचणी संचाच्या सामान्यीकृत उलट संभाव्यतेप्रमाणे परिभाषित केले जाते:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**आपण [Hugging Face च्या GPT-सक्षम मजकूर संपादक](https://transformer.huggingface.co/doc/gpt2-large) वापरून मजकूर निर्मितीचा प्रयोग करू शकता**. या संपादकात, आपण आपला मजकूर लिहायला सुरुवात करता आणि **[TAB]** दाबल्यावर आपल्याला अनेक पूर्णता पर्याय मिळतात. जर ते खूप लहान असतील किंवा आपल्याला ते पसंत नसतील - [TAB] पुन्हा दाबा, आणि आपल्याला अधिक पर्याय मिळतील, ज्यामध्ये लांब मजकूराचे तुकडे देखील असतील.

## GPT हे एक कुटुंब आहे

GPT हे एकच मॉडेल नाही, तर [OpenAI](https://openai.com) द्वारे विकसित आणि प्रशिक्षित केलेल्या मॉडेल्सचा संग्रह आहे.

GPT मॉडेल्स अंतर्गत, आपल्याकडे आहेत:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| 1.5 अब्ज पॅरामीटर्सपर्यंत भाषा मॉडेल. | 175 अब्ज पॅरामीटर्सपर्यंत भाषा मॉडेल | 100T पॅरामीटर्स आणि मजकूर तसेच प्रतिमा इनपुट स्वीकारते आणि मजकूर आउटपुट देते. |

GPT-3 आणि GPT-4 मॉडेल्स [Microsoft Azure च्या कॉग्निटिव्ह सर्व्हिस](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) म्हणून आणि [OpenAI API](https://openai.com/api/) म्हणून उपलब्ध आहेत.

## प्रॉम्प्ट इंजिनिअरिंग

GPT ला भाषा आणि कोड समजण्यासाठी प्रचंड प्रमाणात डेटा वर प्रशिक्षित केले गेले आहे, त्यामुळे ते इनपुट्स (प्रॉम्प्ट्स) ला प्रतिसाद म्हणून आउटपुट्स प्रदान करतात. प्रॉम्प्ट्स म्हणजे GPT इनपुट्स किंवा क्वेरीज जिथे एखादी व्यक्ती मॉडेल्सला पुढील कार्य पूर्ण करण्यासाठी सूचना देते. इच्छित परिणाम मिळवण्यासाठी, आपल्याला सर्वात प्रभावी प्रॉम्प्ट आवश्यक आहे ज्यामध्ये योग्य शब्द, स्वरूप, वाक्यांश किंवा अगदी चिन्हे निवडणे समाविष्ट आहे. या दृष्टिकोनाला [प्रॉम्प्ट इंजिनिअरिंग](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) म्हणतात.

[ही दस्तऐवज](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) प्रॉम्प्ट इंजिनिअरिंगबद्दल अधिक माहिती प्रदान करते.

## ✍️ उदाहरण नोटबुक: [OpenAI-GPT सह खेळणे](GPT-PyTorch.ipynb)

खालील नोटबुक्समध्ये आपले शिक्षण सुरू ठेवा:

* [OpenAI-GPT आणि Hugging Face Transformers सह मजकूर तयार करणे](GPT-PyTorch.ipynb)

## निष्कर्ष

नवीन सामान्य पूर्व-प्रशिक्षित भाषा मॉडेल्स केवळ भाषा संरचना मॉडेल करत नाहीत, तर प्रचंड प्रमाणात नैसर्गिक भाषा देखील समाविष्ट करतात. त्यामुळे, ते काही NLP कार्ये शून्य-शॉप किंवा फ्यू-शॉट सेटिंग्जमध्ये प्रभावीपणे सोडवण्यासाठी वापरले जाऊ शकतात.

## [व्याख्यानानंतर प्रश्नमंजुषा](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

