<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-26T08:43:27+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "mr"
}
-->
# पूर्व-प्रशिक्षित मोठे भाषा मॉडेल्स

आपल्या मागील सर्व कार्यांमध्ये, आम्ही लेबल केलेल्या डेटासेटचा वापर करून विशिष्ट कार्य करण्यासाठी न्यूरल नेटवर्क प्रशिक्षण देत होतो. मोठ्या ट्रान्सफॉर्मर मॉडेल्ससह, जसे की BERT, आम्ही स्व-पर्यवेक्षित पद्धतीने भाषा मॉडेलिंगचा वापर करून भाषा मॉडेल तयार करतो, जे नंतर विशिष्ट डाउनस्ट्रीम कार्यासाठी डोमेन-विशिष्ट प्रशिक्षणाद्वारे विशेषीकृत केले जाते. तथापि, हे सिद्ध झाले आहे की मोठे भाषा मॉडेल्स अनेक कार्ये कोणत्याही डोमेन-विशिष्ट प्रशिक्षणाशिवाय सोडवू शकतात. अशा प्रकारच्या मॉडेल्सच्या कुटुंबाला **GPT**: जनरेटिव्ह प्री-ट्रेनड ट्रान्सफॉर्मर म्हणतात.

## [पूर्व-व्याख्यान प्रश्नमंजुषा](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## मजकूर निर्मिती आणि पेर्प्लेक्सिटी

डाउनस्ट्रीम प्रशिक्षणाशिवाय सामान्य कार्ये करण्यास सक्षम असलेल्या न्यूरल नेटवर्कची कल्पना [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) या पेपरमध्ये सादर केली आहे. मुख्य कल्पना अशी आहे की अनेक इतर कार्ये **मजकूर निर्मिती** वापरून मॉडेल केली जाऊ शकतात, कारण मजकूर समजणे म्हणजे तो तयार करण्यास सक्षम असणे. कारण मॉडेल मानवी ज्ञान समाविष्ट असलेल्या प्रचंड प्रमाणात मजकूरावर प्रशिक्षण दिले जाते, त्यामुळे ते विविध विषयांबद्दलही ज्ञानवान बनते.

> मजकूर समजणे आणि तयार करण्यास सक्षम असणे याचा अर्थ आपल्या सभोवतालच्या जगाबद्दल काहीतरी जाणून घेणे देखील आहे. लोक मोठ्या प्रमाणात वाचून शिकतात, आणि GPT नेटवर्क या बाबतीत समान आहे.

मजकूर निर्मिती नेटवर्क पुढील शब्दाचा $$P(w_N)$$ संभाव्यता अंदाज करून कार्य करतात. तथापि, पुढील शब्दाची बिनशर्त संभाव्यता मजकूर कॉर्पस मधील त्या शब्दाच्या वारंवारतेच्या समान असते. GPT आपल्याला मागील शब्द दिल्यास पुढील शब्दाची **सशर्त संभाव्यता** देऊ शकते: $$P(w_N | w_{n-1}, ..., w_0)$$

> संभाव्यतेबद्दल अधिक वाचण्यासाठी आमच्या [डेटा सायन्स फॉर बिगिनर्स अभ्यासक्रम](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) ला भेट द्या.

भाषा निर्माण करणाऱ्या मॉडेलची गुणवत्ता **पेर्प्लेक्सिटी** वापरून परिभाषित केली जाऊ शकते. हे अंतर्गत मेट्रिक आहे जे आपल्याला कोणत्याही कार्य-विशिष्ट डेटासेटशिवाय मॉडेलची गुणवत्ता मोजण्याची परवानगी देते. हे *वाक्याची संभाव्यता* या संकल्पनेवर आधारित आहे - मॉडेल अशा वाक्याला उच्च संभाव्यता देते जे खरे असण्याची शक्यता आहे (म्हणजे मॉडेल त्याबद्दल **गोंधळलेले** नाही), आणि अशा वाक्यांना कमी संभाव्यता देते ज्यांचा अर्थ कमी लागतो (उदा. *Can it does what?*). जेव्हा आपण आपल्या मॉडेलला वास्तविक मजकूर कॉर्पस मधील वाक्ये देतो, तेव्हा आपल्याला त्यांना उच्च संभाव्यता आणि कमी **पेर्प्लेक्सिटी** असण्याची अपेक्षा असते. गणितीयदृष्ट्या, हे चाचणी संचाच्या सामान्यीकृत उलट संभाव्यतेसारखे परिभाषित केले जाते:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**आपण [Hugging Face च्या GPT-सक्षम मजकूर संपादक](https://transformer.huggingface.co/doc/gpt2-large) वापरून मजकूर निर्मितीचा प्रयोग करू शकता.** या संपादकात, आपण आपला मजकूर लिहायला सुरुवात करता, आणि **[TAB]** दाबल्यावर आपल्याला अनेक पूर्णता पर्याय मिळतात. जर ते खूप लहान असतील, किंवा आपण त्यांच्याशी समाधानी नसाल - [TAB] पुन्हा दाबा, आणि आपल्याला अधिक पर्याय मिळतील, ज्यामध्ये लांब मजकूराचे तुकडे समाविष्ट आहेत.

## GPT हे एक कुटुंब आहे

GPT हे एकच मॉडेल नाही, तर [OpenAI](https://openai.com) द्वारे विकसित आणि प्रशिक्षित केलेल्या मॉडेल्सचा संग्रह आहे.

GPT मॉडेल्स अंतर्गत, आपल्याकडे:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| 1.5 अब्ज पॅरामीटर्सपर्यंत असलेले भाषा मॉडेल. | 175 अब्ज पॅरामीटर्सपर्यंत असलेले भाषा मॉडेल | 100T पॅरामीटर्स आणि मजकूर तसेच प्रतिमा इनपुट स्वीकारते आणि मजकूर आउटपुट देते. |

GPT-3 आणि GPT-4 मॉडेल्स [Microsoft Azure मधील कॉग्निटिव्ह सर्व्हिस](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) म्हणून आणि [OpenAI API](https://openai.com/api/) म्हणून उपलब्ध आहेत.

## प्रॉम्प्ट इंजिनिअरिंग

GPT ला भाषा आणि कोड समजण्यासाठी प्रचंड प्रमाणात डेटा वर प्रशिक्षण दिले गेले आहे, त्यामुळे ते इनपुट्स (प्रॉम्प्ट्स) ला प्रतिसाद म्हणून आउटपुट्स प्रदान करतात. प्रॉम्प्ट्स म्हणजे GPT इनपुट्स किंवा क्वेरीज जिथे एखादी व्यक्ती मॉडेल्सना पुढील कार्य पूर्ण करण्याच्या सूचना देते. इच्छित परिणाम मिळवण्यासाठी, आपल्याला सर्वात प्रभावी प्रॉम्प्ट आवश्यक आहे ज्यामध्ये योग्य शब्द, स्वरूप, वाक्यांश किंवा अगदी चिन्हे निवडणे समाविष्ट आहे. या दृष्टिकोनाला [प्रॉम्प्ट इंजिनिअरिंग](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) म्हणतात.

[ही दस्तऐवज](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) प्रॉम्प्ट इंजिनिअरिंगबद्दल अधिक माहिती प्रदान करते.

## ✍️ उदाहरण नोटबुक: [OpenAI-GPT सह खेळणे](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

खालील नोटबुक्समध्ये आपले शिक्षण सुरू ठेवा:

* [OpenAI-GPT आणि Hugging Face Transformers सह मजकूर तयार करणे](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## निष्कर्ष

नवीन सामान्य पूर्व-प्रशिक्षित भाषा मॉडेल्स केवळ भाषा संरचना मॉडेल करत नाहीत, तर प्रचंड प्रमाणात नैसर्गिक भाषा देखील समाविष्ट करतात. त्यामुळे, ते काही NLP कार्ये शून्य-शॉट किंवा फ्यू-शॉट सेटिंग्जमध्ये प्रभावीपणे सोडवण्यासाठी वापरले जाऊ शकतात.

## [व्याख्यानानंतर प्रश्नमंजुषा](https://ff-quizzes.netlify.app/en/ai/quiz/40)

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) चा वापर करून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील मूळ दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी, व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर केल्यामुळे उद्भवणाऱ्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.