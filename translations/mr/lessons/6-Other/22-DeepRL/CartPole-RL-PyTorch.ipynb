{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# कार्टपोल संतुलनासाठी RL प्रशिक्षण\n",
    "\n",
    "हे नोटबुक [AI for Beginners Curriculum](http://aka.ms/ai-beginners) चा भाग आहे. हे [अधिकृत PyTorch ट्यूटोरियल](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) आणि [Cartpole PyTorch अंमलबजावणी](https://github.com/yc930401/Actor-Critic-pytorch) यापासून प्रेरित आहे.\n",
    "\n",
    "या उदाहरणात, आपण RL वापरून एक मॉडेल प्रशिक्षित करू जे एका गाड्यावर असलेल्या खांबाला संतुलित ठेवेल, जो आडव्या पट्टीवर डावीकडे आणि उजवीकडे हलू शकतो. आम्ही [OpenAI Gym](https://www.gymlibrary.ml/) पर्यावरणाचा वापर करून खांबाचे अनुकरण करू.\n",
    "\n",
    "> **Note**: तुम्ही या धड्याचा कोड स्थानिक पातळीवर (उदा. Visual Studio Code मधून) चालवू शकता, अशा परिस्थितीत अनुकरण एका नवीन विंडोमध्ये उघडेल. ऑनलाइन कोड चालवताना, तुम्हाला कोडमध्ये काही बदल करावे लागतील, जसे [येथे](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) वर्णन केले आहे.\n",
    "\n",
    "आपण सुरुवात जिम स्थापित असल्याची खात्री करून करू:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आता CartPole वातावरण तयार करूया आणि त्यावर कसे कार्य करायचे ते पाहूया. एका वातावरणामध्ये खालील गुणधर्म असतात:\n",
    "\n",
    "* **Action space** म्हणजे प्रत्येक सिम्युलेशनच्या टप्प्यावर आपण करू शकणाऱ्या संभाव्य क्रियांची श्रेणी  \n",
    "* **Observation space** म्हणजे आपण करू शकणाऱ्या निरीक्षणांची श्रेणी  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "चला पाहूया की सिम्युलेशन कसे कार्य करते. खालील लूप सिम्युलेशन चालवतो, जोपर्यंत `env.step` टर्मिनेशन फ्लॅग `done` परत करत नाही. आम्ही `env.action_space.sample()` वापरून कृती निवडू, ज्याचा अर्थ असा आहे की प्रयोग फार लवकर अपयशी होण्याची शक्यता आहे (CartPole वातावरण संपते जेव्हा CartPole चा वेग, त्याचे स्थान किंवा कोन विशिष्ट मर्यादांच्या बाहेर जातो).\n",
    "\n",
    "> सिम्युलेशन नवीन विंडोमध्ये उघडेल. तुम्ही कोड अनेक वेळा चालवू शकता आणि त्याचे वर्तन कसे आहे ते पाहू शकता.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आपण पाहू शकता की निरीक्षणांमध्ये 4 संख्या असतात. त्या आहेत:\n",
    "- गाडीची स्थिती\n",
    "- गाडीचा वेग\n",
    "- खांबाचा कोन\n",
    "- खांबाच्या फिरण्याचा दर\n",
    "\n",
    "`rew` हा प्रत्येक टप्प्यावर आपल्याला मिळणारा बक्षीस आहे. CartPole वातावरणात, प्रत्येक सिम्युलेशन टप्प्यासाठी तुम्हाला 1 गुण दिला जातो, आणि उद्दिष्ट म्हणजे एकूण बक्षीस जास्तीत जास्त करणे, म्हणजे CartPole पडल्याशिवाय तोल सांभाळण्याचा कालावधी वाढवणे.\n",
    "\n",
    "रिइनफोर्समेंट लर्निंग दरम्यान, आपले उद्दिष्ट म्हणजे एक **पॉलिसी** $\\pi$ तयार करणे, जी प्रत्येक स्थिती $s$ साठी आपल्याला कोणती क्रिया $a$ घ्यायची ते सांगेल, म्हणजेच मूलतः $a = \\pi(s)$.\n",
    "\n",
    "जर तुम्हाला संभाव्यताधारित उपाय हवा असेल, तर तुम्ही पॉलिसीला प्रत्येक क्रियेसाठी संभाव्यता परत करणारी प्रणाली म्हणून विचार करू शकता, म्हणजे $\\pi(a|s)$ याचा अर्थ असा होईल की स्थिती $s$ वर आपण क्रिया $a$ घ्यावी याची संभाव्यता.\n",
    "\n",
    "## पॉलिसी ग्रेडियंट पद्धत\n",
    "\n",
    "सर्वात सोप्या RL अल्गोरिदममध्ये, ज्याला **पॉलिसी ग्रेडियंट** म्हणतात, आपण पुढील क्रिया भाकीत करण्यासाठी एक न्यूरल नेटवर्क प्रशिक्षित करू.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आम्ही अनेक प्रयोग चालवून आणि प्रत्येक प्रयोगानंतर आमच्या नेटवर्कचे अद्यतन करून नेटवर्क प्रशिक्षण देऊ. चला एक फंक्शन परिभाषित करूया जे प्रयोग चालवेल आणि परिणाम परत करेल (तथाकथित **ट्रेस**) - सर्व स्थिती, क्रिया (आणि त्यांचे शिफारस केलेले संभाव्यता), आणि बक्षिसे:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आपण प्रशिक्षित न केलेल्या नेटवर्कसह एक एपिसोड चालवू शकता आणि लक्षात घेऊ शकता की एकूण बक्षीस (एपिसोडची लांबी म्हणून ओळखले जाते) खूप कमी आहे:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "धोरण ग्रेडियंट अल्गोरिदमच्या क्लिष्ट पैलूंमध्ये **सवलतीच्या बक्षिसांचा** वापर करणे आहे. कल्पना अशी आहे की आपण खेळाच्या प्रत्येक टप्प्यावर एकूण बक्षिसांचा वेक्टर गणना करतो, आणि या प्रक्रियेदरम्यान काही गुणांक $gamma$ वापरून सुरुवातीची बक्षिसे सवलत देतो. आम्ही परिणामी वेक्टर सामान्यीकृत देखील करतो, कारण आम्ही ते आमच्या प्रशिक्षणावर परिणाम करण्यासाठी वजन म्हणून वापरणार आहोत:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आता प्रत्यक्ष प्रशिक्षण सुरू करूया! आपण 300 एपिसोड चालवणार आहोत, आणि प्रत्येक एपिसोडमध्ये खालील गोष्टी करूया:\n",
    "\n",
    "1. प्रयोग चालवा आणि ट्रेस गोळा करा\n",
    "1. घेतलेल्या कृती आणि अंदाजित संभाव्यता यांच्यातील फरक (`gradients`) मोजा. फरक जितका कमी असेल, तितके आम्हाला खात्री आहे की योग्य कृती घेतली आहे.\n",
    "1. सवलतीच्या बक्षिसे मोजा आणि सवलतीच्या बक्षिसांद्वारे `gradients` गुणाकार करा - यामुळे उच्च बक्षिस असलेल्या टप्प्यांचा अंतिम निकालावर अधिक प्रभाव पडेल, तुलनेने कमी बक्षिस असलेल्या टप्प्यांपेक्षा.\n",
    "1. आमच्या न्यूरल नेटवर्कसाठी अपेक्षित लक्ष्य कृती काही प्रमाणात चालवताना अंदाजित संभाव्यतेमधून घेतल्या जातील, आणि काही प्रमाणात गणितीय `gradients` मधून. आम्ही `alpha` पॅरामीटर वापरून ठरवू की `gradients` आणि बक्षिसे किती प्रमाणात विचारात घेतली जातील - याला reinforcement algorithm चा *learning rate* म्हणतात.\n",
    "1. शेवटी, आम्ही आमच्या नेटवर्कला states आणि अपेक्षित कृतींवर प्रशिक्षण देतो, आणि प्रक्रिया पुन्हा सुरू करतो.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आता परिणाम पाहण्यासाठी रेंडरिंगसह एपिसोड चालवूया:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आशा आहे की आता तुम्हाला दिसत असेल की खांब चांगल्या प्रकारे संतुलित राहू शकतो!\n",
    "\n",
    "## अभिनेता-समालोचक मॉडेल\n",
    "\n",
    "अभिनेता-समालोचक मॉडेल हे पॉलिसी ग्रेडियंट्सचे पुढील विकसित रूप आहे, ज्यामध्ये आम्ही धोरण आणि अंदाजित बक्षिसे शिकण्यासाठी एक न्यूरल नेटवर्क तयार करतो. या नेटवर्कला दोन आउटपुट्स असतील (किंवा तुम्ही याला दोन स्वतंत्र नेटवर्क्स म्हणून पाहू शकता):\n",
    "* **अभिनेता (Actor)** पॉलिसी ग्रेडियंट मॉडेलप्रमाणेच, स्थितीची संभाव्यता वितरण देऊन कोणती कृती करायची याची शिफारस करतो.\n",
    "* **समालोचक (Critic)** त्या कृतींमधून मिळणाऱ्या बक्षिसांचा अंदाज लावतो. दिलेल्या स्थितीत भविष्यातील एकूण अंदाजित बक्षिसे परत करतो.\n",
    "\n",
    "चला असे मॉडेल परिभाषित करूया:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आम्हाला आमच्या `discounted_rewards` आणि `run_episode` फंक्शन्समध्ये थोडा बदल करावा लागेल:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आता आपण मुख्य प्रशिक्षण लूप चालवू. योग्य तोटा फंक्शन्स मोजून आणि नेटवर्क पॅरामीटर्स अद्यतनित करून आपण मॅन्युअल नेटवर्क प्रशिक्षण प्रक्रिया वापरू:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## मुख्य मुद्दा\n",
    "\n",
    "या डेमोमध्ये आपण दोन RL अल्गोरिदम पाहिले: साधा पॉलिसी ग्रेडियंट आणि अधिक प्रगत अ‍ॅक्टर-क्रिटिक. तुम्ही पाहू शकता की हे अल्गोरिदम स्टेट, अ‍ॅक्शन आणि रिवॉर्ड यांसारख्या अमूर्त संकल्पनांवर कार्य करतात - त्यामुळे ते खूप वेगवेगळ्या वातावरणांमध्ये लागू केले जाऊ शकतात.\n",
    "\n",
    "रिइनफोर्समेंट लर्निंग आपल्याला केवळ अंतिम रिवॉर्ड पाहून समस्या सोडवण्यासाठी सर्वोत्तम रणनीती शिकण्याची परवानगी देते. लेबल केलेल्या डेटासेटची गरज नसल्यामुळे, आपल्याला आपल्या मॉडेल्सचा ऑप्टिमायझेशन करण्यासाठी सिम्युलेशन्स अनेक वेळा पुन्हा पुन्हा चालवता येतात. तथापि, RL मध्ये अजूनही अनेक आव्हाने आहेत, जी तुम्हाला शिकता येतील जर तुम्ही या AI च्या रंजक क्षेत्रावर अधिक लक्ष केंद्रित करण्याचा निर्णय घेतला.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nहा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी कृपया लक्षात ठेवा की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर करून निर्माण होणाऱ्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T07:32:18+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "mr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}