<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-26T10:14:00+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "mr"
}
-->
# डीप रिइन्फोर्समेंट लर्निंग

रिइन्फोर्समेंट लर्निंग (RL) हे सुपरवाइज्ड लर्निंग आणि अनसुपरवाइज्ड लर्निंग यांच्यासोबत मशीन लर्निंगचे एक मूलभूत पॅराडाइम मानले जाते. सुपरवाइज्ड लर्निंगमध्ये आपल्याला ज्ञात परिणामांसह डेटासेटवर अवलंबून राहावे लागते, तर RL **करून शिकणे** या संकल्पनेवर आधारित आहे. उदाहरणार्थ, जेव्हा आपण प्रथम एखादे संगणक गेम पाहतो, तेव्हा आपण नियम माहित नसतानाही खेळायला सुरुवात करतो आणि लवकरच खेळत राहून आणि आपले वर्तन समायोजित करून आपले कौशल्य सुधारतो.

## [पूर्व-व्याख्यान प्रश्नमंजुषा](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RL करण्यासाठी आपल्याला आवश्यक आहे:

* **पर्यावरण** किंवा **सिम्युलेटर**, जे खेळाचे नियम सेट करते. आपल्याला सिम्युलेटरमध्ये प्रयोग चालवता यायला हवेत आणि त्याचे परिणाम पाहता यायला हवेत.
* काही **रिवॉर्ड फंक्शन**, जे आपला प्रयोग किती यशस्वी होता हे दर्शवते. संगणक गेम खेळायला शिकण्याच्या बाबतीत, रिवॉर्ड म्हणजे आपला अंतिम स्कोअर असेल.

रिवॉर्ड फंक्शनच्या आधारे, आपल्याला आपले वर्तन समायोजित करता यायला हवे आणि आपले कौशल्य सुधारता यायला हवे, जेणेकरून पुढच्या वेळी आपण चांगले खेळू शकू. इतर प्रकारच्या मशीन लर्निंग आणि RL यामधील मुख्य फरक म्हणजे RL मध्ये आपल्याला खेळ संपेपर्यंत आपण जिंकतो की हरतो हे सहसा माहित नसते. त्यामुळे, एखादी विशिष्ट चाल चांगली आहे की नाही हे आपण सांगू शकत नाही - आपल्याला खेळाच्या शेवटीच रिवॉर्ड मिळते.

RL दरम्यान, आपण अनेक प्रयोग करतो. प्रत्येक प्रयोगादरम्यान, आपण आतापर्यंत शिकलेल्या सर्वोत्तम रणनीतीचे अनुसरण करणे (**शोषण**) आणि नवीन संभाव्य स्थितींचा शोध घेणे (**अन्वेषण**) यामध्ये संतुलन साधणे आवश्यक आहे.

## OpenAI Gym

RL साठी एक उत्कृष्ट साधन म्हणजे [OpenAI Gym](https://gym.openai.com/) - एक **सिम्युलेशन पर्यावरण**, जे अटारी गेम्सपासून ते पोल बॅलन्सिंगच्या भौतिकशास्त्रापर्यंत अनेक वेगवेगळ्या पर्यावरणांचे अनुकरण करू शकते. हे रिइन्फोर्समेंट लर्निंग अल्गोरिदम्स प्रशिक्षणासाठी सर्वात लोकप्रिय सिम्युलेशन पर्यावरणांपैकी एक आहे आणि [OpenAI](https://openai.com/) द्वारे देखरेख केली जाते.

> **Note**: OpenAI Gym मधील सर्व उपलब्ध पर्यावरणे आपण [येथे](https://gym.openai.com/envs/#classic_control) पाहू शकता.

## CartPole बॅलन्सिंग

आपण सर्वांनी आधुनिक बॅलन्सिंग उपकरणे जसे की *Segway* किंवा *Gyroscooters* पाहिली असतील. हे उपकरणे अ‍ॅक्सेलरोमीटर किंवा गायरॉस्कोपकडून मिळालेल्या सिग्नलच्या प्रतिसादात आपले चाक समायोजित करून आपोआप संतुलन साधू शकतात. या विभागात, आपण एक समान समस्या सोडवायला शिकणार आहोत - पोल बॅलन्सिंग. हे एका सर्कस कलाकाराने आपल्या हातावर पोल संतुलित करण्याच्या परिस्थितीसारखे आहे - परंतु हे पोल बॅलन्सिंग फक्त 1D मध्ये होते.

संतुलन साधण्याचे एक साधे रूप **CartPole** समस्या म्हणून ओळखले जाते. CartPole जगात, आपल्याकडे एक आडवा स्लायडर आहे जो डावीकडे किंवा उजवीकडे हलवू शकतो, आणि उद्दिष्ट म्हणजे स्लायडरवर वरच्या बाजूस उभा असलेला पोल संतुलित ठेवणे.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

हे पर्यावरण तयार करण्यासाठी आणि वापरण्यासाठी, आपल्याला काही ओळींचा Python कोड आवश्यक आहे:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

प्रत्येक पर्यावरणाचा उपयोग नेमक्या याच प्रकारे करता येतो:
* `env.reset` नवीन प्रयोग सुरू करतो
* `env.step` सिम्युलेशन स्टेप पार पाडतो. हे **action space** मधून एक **action** प्राप्त करते आणि **observation space** मधून एक **observation**, तसेच रिवॉर्ड आणि टर्मिनेशन फ्लॅग परत करते.

वरील उदाहरणात, प्रत्येक स्टेपवर आम्ही एक रँडम action घेतो, त्यामुळे प्रयोगाचे आयुष्य खूपच कमी असते:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL अल्गोरिदमचे उद्दिष्ट म्हणजे एक मॉडेल - ज्याला **policy** π म्हणतात - प्रशिक्षण देणे, जे दिलेल्या स्थितीच्या प्रतिसादात action परत करेल. आपण पॉलिसीला संभाव्यत: विचार करू शकतो, उदा. कोणत्याही स्थिती *s* आणि action *a* साठी, ते *s* स्थितीत *a* घेण्याची शक्यता π(*a*|*s*) परत करेल.

## Policy Gradients अल्गोरिदम

पॉलिसी मॉडेल करण्याचा सर्वात स्पष्ट मार्ग म्हणजे एक न्यूरल नेटवर्क तयार करणे, जे states इनपुट म्हणून घेईल आणि संबंधित actions (किंवा सर्व actions च्या संभाव्यता) परत करेल. एका अर्थाने, हे सामान्य वर्गीकरण कार्यासारखे असेल, परंतु एक मोठा फरक म्हणजे - आपल्याला प्रत्येक स्टेपवर कोणते action घ्यावे हे आधीपासून माहित नसते.

येथे कल्पना अशी आहे की त्या संभाव्यता अंदाज करणे. आपण **cumulative rewards** चा एक व्हेक्टर तयार करतो, जो प्रयोगाच्या प्रत्येक स्टेपवर आपला एकूण रिवॉर्ड दर्शवतो. आपण **reward discounting** देखील लागू करतो, जिथे पूर्वीच्या रिवॉर्ड्सला काही गुणक γ=0.99 ने गुणून त्यांची भूमिका कमी करतो. नंतर, आपण त्या स्टेप्सला बळकट करतो जे अधिक रिवॉर्ड देतात.

> Policy Gradient अल्गोरिदमबद्दल अधिक जाणून घ्या आणि [उदाहरण नोटबुक](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) मध्ये ते कृतीत पाहा.

## Actor-Critic अल्गोरिदम

Policy Gradients पद्धतीचे एक सुधारित रूप **Actor-Critic** म्हणून ओळखले जाते. यामागील मुख्य कल्पना अशी आहे की न्यूरल नेटवर्क दोन गोष्टी परत करण्यासाठी प्रशिक्षित केले जाईल:

* पॉलिसी, जी कोणते action घ्यायचे ते ठरवते. या भागाला **actor** म्हणतात.
* आपण या स्थितीत मिळवू शकणाऱ्या एकूण रिवॉर्डचा अंदाज - या भागाला **critic** म्हणतात.

एका अर्थाने, ही रचना [GAN](../../4-ComputerVision/10-GANs/README.md) सारखी आहे, जिथे दोन नेटवर्क्स एकमेकांविरुद्ध प्रशिक्षित केली जातात. Actor-Critic मॉडेलमध्ये, actor आपल्याला घ्यायचे action प्रस्तावित करते, आणि critic त्यावर टीका करून परिणामाचा अंदाज लावतो. मात्र, आपले उद्दिष्ट हे नेटवर्क्स एकत्र प्रशिक्षित करणे आहे.

कारण आपल्याला प्रयोगादरम्यान वास्तविक cumulative rewards आणि critic द्वारे परत केलेले परिणाम दोन्ही माहित असतात, त्यामुळे त्यांच्यातील फरक कमी करण्यासाठी loss function तयार करणे तुलनेने सोपे आहे. यामुळे आपल्याला **critic loss** मिळते. **actor loss** आपण policy gradient अल्गोरिदमसारख्याच पद्धतीने गणना करू शकतो.

या अल्गोरिदम्सपैकी एक चालवल्यानंतर, आपला CartPole असे वागेल:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ सराव: Policy Gradients आणि Actor-Critic RL

खालील नोटबुक्समध्ये आपले शिक्षण सुरू ठेवा:

* [RL in TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL in PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## इतर RL कार्ये

आजकाल रिइन्फोर्समेंट लर्निंग हे संशोधनाचे वेगाने वाढणारे क्षेत्र आहे. रिइन्फोर्समेंट लर्निंगचे काही मनोरंजक उदाहरणे:

* **Atari Games** खेळायला संगणकाला शिकवणे. या समस्येतील आव्हानात्मक भाग म्हणजे आपल्याकडे व्हेक्टर म्हणून सोपी state नसते, तर स्क्रीनशॉट असतो - आणि आपल्याला CNN वापरून या स्क्रीन इमेजला फीचर व्हेक्टरमध्ये रूपांतरित करावे लागते किंवा रिवॉर्ड माहिती काढावी लागते. अटारी गेम्स Gym मध्ये उपलब्ध आहेत.
* संगणकाला Chess आणि Go सारखे बोर्ड गेम्स खेळायला शिकवणे. अलीकडे **Alpha Zero** सारख्या अत्याधुनिक प्रोग्राम्स दोन एजंट्स एकमेकांविरुद्ध खेळून आणि प्रत्येक स्टेपवर सुधारणा करून सुरुवातीपासून प्रशिक्षित केले गेले.
* उद्योगात, RL सिम्युलेशनमधून नियंत्रण प्रणाली तयार करण्यासाठी वापरले जाते. [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) नावाची सेवा यासाठी खास डिझाइन केली आहे.

## निष्कर्ष

आता आपण एजंट्सना चांगले परिणाम मिळवण्यासाठी प्रशिक्षण देणे शिकले आहे, फक्त त्यांना खेळाच्या इच्छित स्थितीची व्याख्या करणारे रिवॉर्ड फंक्शन प्रदान करून आणि त्यांना शोध जागा बुद्धिमत्तेने एक्सप्लोर करण्याची संधी देऊन. आपण यशस्वीरित्या दोन अल्गोरिदम्स वापरले आणि तुलनेने कमी कालावधीत चांगला परिणाम साध्य केला. मात्र, हे RL मध्ये आपली प्रवासाची फक्त सुरुवात आहे, आणि आपण अधिक खोलवर जाण्यासाठी स्वतंत्र कोर्स घेण्याचा विचार नक्कीच करावा.

## 🚀 आव्हान

'इतर RL कार्ये' विभागात सूचीबद्ध अनुप्रयोग एक्सप्लोर करा आणि एक अंमलात आणण्याचा प्रयत्न करा!

## [व्याख्यानानंतरची प्रश्नमंजुषा](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## पुनरावलोकन आणि स्व-अभ्यास

[Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) मध्ये क्लासिकल रिइन्फोर्समेंट लर्निंगबद्दल अधिक जाणून घ्या.

संगणक Super Mario खेळायला कसे शिकतो याबद्दल बोलणारे [हे उत्कृष्ट व्हिडिओ](https://www.youtube.com/watch?v=qv6UVOQ0F44) पहा.

## असाइनमेंट: [Train a Mountain Car](lab/README.md)

या असाइनमेंट दरम्यान आपले उद्दिष्ट वेगळ्या Gym पर्यावरणाला - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) - प्रशिक्षण देणे असेल.

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील मूळ दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी, व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर करून उद्भवलेल्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.