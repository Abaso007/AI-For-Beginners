<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T07:03:29+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "mr"
}
-->
# डीप रिइन्फोर्समेंट लर्निंग

रिइन्फोर्समेंट लर्निंग (RL) हे मशीन लर्निंगचे एक मूलभूत पॅराडाइम मानले जाते, ज्यामध्ये सुपरवाइज्ड लर्निंग आणि अनसुपरवाइज्ड लर्निंगचा समावेश होतो. सुपरवाइज्ड लर्निंगमध्ये आपल्याला ज्ञात परिणामांसह डेटासेटवर अवलंबून राहावे लागते, तर RL **करून शिकणे** या संकल्पनेवर आधारित आहे. उदाहरणार्थ, जेव्हा आपण प्रथम एखादे संगणक गेम पाहतो, तेव्हा आपण नियम न जाणता खेळायला सुरुवात करतो आणि लवकरच फक्त खेळण्याच्या प्रक्रियेद्वारे आणि आपले वर्तन समायोजित करून आपले कौशल्य सुधारतो.

## [पूर्व-व्याख्यान क्विझ](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL करण्यासाठी आपल्याला आवश्यक आहे:

* **पर्यावरण** किंवा **सिम्युलेटर**, जे खेळाचे नियम सेट करते. आपल्याला सिम्युलेटरमध्ये प्रयोग चालवता यायला हवेत आणि त्याचे परिणाम पाहता यायला हवेत.
* काही **रिवॉर्ड फंक्शन**, जे आपला प्रयोग किती यशस्वी होता हे दर्शवते. संगणक गेम खेळायला शिकण्याच्या बाबतीत, रिवॉर्ड म्हणजे आपला अंतिम स्कोअर असेल.

रिवॉर्ड फंक्शनच्या आधारे, आपल्याला आपले वर्तन समायोजित करता यायला हवे आणि आपले कौशल्य सुधारता यायला हवे, जेणेकरून पुढच्या वेळी आपण चांगले खेळू शकू. इतर प्रकारच्या मशीन लर्निंग आणि RL यामधील मुख्य फरक म्हणजे RL मध्ये आपल्याला खेळ संपेपर्यंत आपण जिंकतो की हरतो हे सहसा माहित नसते. त्यामुळे, एखादी विशिष्ट चाल चांगली आहे की नाही हे आपण सांगू शकत नाही - आपल्याला फक्त खेळाच्या शेवटी रिवॉर्ड मिळते.

RL दरम्यान, आपण सहसा अनेक प्रयोग करतो. प्रत्येक प्रयोगादरम्यान, आपण आतापर्यंत शिकलेल्या सर्वोत्तम रणनीतीचे अनुसरण करणे (**शोषण**) आणि नवीन संभाव्य स्थितींचा शोध घेणे (**अन्वेषण**) यामध्ये संतुलन साधणे आवश्यक आहे.

## OpenAI Gym

RL साठी एक उत्कृष्ट साधन म्हणजे [OpenAI Gym](https://gym.openai.com/) - एक **सिम्युलेशन पर्यावरण**, जे अटारी गेम्सपासून ते पोल बॅलन्सिंगच्या भौतिकशास्त्रापर्यंत अनेक वेगवेगळ्या पर्यावरणांचे अनुकरण करू शकते. हे रिइन्फोर्समेंट लर्निंग अल्गोरिदम्स प्रशिक्षणासाठी सर्वात लोकप्रिय सिम्युलेशन पर्यावरणांपैकी एक आहे आणि [OpenAI](https://openai.com/) द्वारे देखरेख केली जाते.

> **Note**: OpenAI Gym मधील सर्व उपलब्ध पर्यावरणे आपण [येथे](https://gym.openai.com/envs/#classic_control) पाहू शकता.

## CartPole बॅलन्सिंग

आपण सर्वांनी आधुनिक बॅलन्सिंग उपकरणे जसे की *Segway* किंवा *Gyroscooters* पाहिली असतील. हे उपकरणे आपोआप बॅलन्स करू शकतात, ज्यासाठी ते अॅक्सेलरोमीटर किंवा गायरोस्कोपमधून सिग्नल मिळाल्यावर त्यांच्या चाकांचे समायोजन करतात. या विभागात, आपण एक समान समस्या सोडवायला शिकणार आहोत - पोल बॅलन्सिंग. हे सर्कस कलाकाराने आपल्या हातावर पोल बॅलन्स करण्यासारखे आहे - परंतु हे पोल बॅलन्सिंग फक्त 1D मध्ये होते.

बॅलन्सिंगची एक साधी आवृत्ती **CartPole** समस्या म्हणून ओळखली जाते. CartPole जगात, आपल्याकडे एक आडवा स्लायडर असतो जो डावीकडे किंवा उजवीकडे हलवू शकतो, आणि उद्दिष्ट म्हणजे स्लायडरवर उभ्या पोलला बॅलन्स करणे.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

हे पर्यावरण तयार करण्यासाठी आणि वापरण्यासाठी, आपल्याला काही ओळींचा Python कोड आवश्यक आहे:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

प्रत्येक पर्यावरणाचा उपयोग नेमक्या याच प्रकारे केला जाऊ शकतो:
* `env.reset` नवीन प्रयोग सुरू करते
* `env.step` सिम्युलेशन स्टेप पार पाडते. हे **action space** मधून एक **action** प्राप्त करते आणि **observation space** मधून एक **observation**, तसेच रिवॉर्ड आणि टर्मिनेशन फ्लॅग परत करते.

वरील उदाहरणात, प्रत्येक स्टेपवर एक रँडम अॅक्शन घेतले जाते, ज्यामुळे प्रयोगाचे आयुष्य खूपच कमी असते:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL अल्गोरिदमचे उद्दिष्ट म्हणजे एक मॉडेल - ज्याला **policy** &pi; म्हणतात - प्रशिक्षण देणे, जे दिलेल्या स्थितीला प्रतिसाद म्हणून अॅक्शन परत करेल. आपण पॉलिसीला संभाव्यतात्मक मानू शकतो, उदा. कोणत्याही स्थिती *s* आणि अॅक्शन *a* साठी ते &pi;(*a*|*s*) संभाव्यता परत करेल की आपण *s* स्थितीत *a* घ्यायला हवे.

## पॉलिसी ग्रेडियंट्स अल्गोरिदम

पॉलिसी मॉडेल करण्याचा सर्वात स्पष्ट मार्ग म्हणजे एक न्यूरल नेटवर्क तयार करणे, जे इनपुट म्हणून स्टेट्स घेईल आणि संबंधित अॅक्शन (किंवा सर्व अॅक्शनची संभाव्यता) परत करेल. एका अर्थाने, हे सामान्य वर्गीकरण कार्यासारखे असेल, परंतु एक मोठा फरक म्हणजे - आपल्याला प्रत्येक स्टेपवर कोणते अॅक्शन घ्यावे हे आधीपासून माहित नसते.

येथे कल्पना अशी आहे की त्या संभाव्यता अंदाज करणे. आपण **क्युम्युलेटिव्ह रिवॉर्ड्स**चा एक व्हेक्टर तयार करतो, जो प्रयोगाच्या प्रत्येक स्टेपवर आपला एकूण रिवॉर्ड दर्शवतो. आपण **रिवॉर्ड डिस्काउंटिंग** देखील लागू करतो, जिथे पूर्वीच्या रिवॉर्ड्सला काही गुणक &gamma;=0.99 ने गुणून त्यांची भूमिका कमी केली जाते. नंतर, आपण त्या स्टेप्सला बळकट करतो जे अधिक रिवॉर्ड्स देतात.

> पॉलिसी ग्रेडियंट अल्गोरिदमबद्दल अधिक जाणून घ्या आणि [उदाहरण नोटबुक](CartPole-RL-TF.ipynb) मध्ये ते अॅक्शनमध्ये पाहा.

## अॅक्टर-क्रिटिक अल्गोरिदम

पॉलिसी ग्रेडियंट्स पद्धतीची सुधारित आवृत्ती **Actor-Critic** म्हणून ओळखली जाते. यामागील मुख्य कल्पना अशी आहे की न्यूरल नेटवर्क दोन गोष्टी परत करण्यासाठी प्रशिक्षित केले जाईल:

* पॉलिसी, जी कोणते अॅक्शन घ्यायचे ते ठरवते. या भागाला **actor** म्हणतात.
* आपण या स्थितीत मिळवू शकणाऱ्या एकूण रिवॉर्डचा अंदाज - या भागाला **critic** म्हणतात.

एका अर्थाने, ही आर्किटेक्चर [GAN](../../4-ComputerVision/10-GANs/README.md) सारखी आहे, जिथे दोन नेटवर्क्स एकमेकांविरुद्ध प्रशिक्षित केली जातात. Actor-Critic मॉडेलमध्ये, actor आपल्याला घ्यायचे अॅक्शन प्रस्तावित करते, आणि critic त्यावर टीका करून परिणामाचा अंदाज लावतो. मात्र, आपले उद्दिष्ट हे नेटवर्क्स एकत्रितपणे प्रशिक्षित करणे आहे.

कारण आपल्याला प्रयोगादरम्यान वास्तविक क्युम्युलेटिव्ह रिवॉर्ड्स आणि critic द्वारे परत केलेले परिणाम दोन्ही माहित आहेत, त्यामुळे त्यांच्यातील फरक कमी करण्यासाठी लॉस फंक्शन तयार करणे तुलनेने सोपे आहे. यामुळे आपल्याला **critic loss** मिळते. आपण पॉलिसी ग्रेडियंट अल्गोरिदमप्रमाणेच पद्धती वापरून **actor loss** देखील गणना करू शकतो.

या अल्गोरिदम्सपैकी एक चालवल्यानंतर, आपला CartPole असे वागेल:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ व्यायाम: पॉलिसी ग्रेडियंट्स आणि अॅक्टर-क्रिटिक RL

खालील नोटबुक्समध्ये आपले शिक्षण सुरू ठेवा:

* [TensorFlow मध्ये RL](CartPole-RL-TF.ipynb)
* [PyTorch मध्ये RL](CartPole-RL-PyTorch.ipynb)

## इतर RL कार्ये

आजकाल रिइन्फोर्समेंट लर्निंग हे संशोधनाचे वेगाने वाढणारे क्षेत्र आहे. रिइन्फोर्समेंट लर्निंगचे काही मनोरंजक उदाहरणे आहेत:

* संगणकाला **अटारी गेम्स** खेळायला शिकवणे. या समस्येतील आव्हानात्मक भाग म्हणजे आपल्याकडे साधी स्थिती व्हेक्टर म्हणून नाही, तर स्क्रीनशॉट असतो - आणि आपल्याला CNN वापरून या स्क्रीन इमेजला फीचर व्हेक्टरमध्ये रूपांतरित करावे लागते किंवा रिवॉर्ड माहिती काढावी लागते. अटारी गेम्स Gym मध्ये उपलब्ध आहेत.
* संगणकाला बोर्ड गेम्स जसे की Chess आणि Go खेळायला शिकवणे. अलीकडे **Alpha Zero** सारख्या अत्याधुनिक प्रोग्राम्स दोन एजंट्स एकमेकांविरुद्ध खेळून आणि प्रत्येक स्टेपवर सुधारणा करून सुरुवातीपासून प्रशिक्षित केले गेले.
* उद्योगात, सिम्युलेशनमधून नियंत्रण प्रणाली तयार करण्यासाठी RL वापरले जाते. [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) नावाची सेवा यासाठी विशेषतः डिझाइन केली गेली आहे.

## निष्कर्ष

आता आपण एजंट्सना फक्त रिवॉर्ड फंक्शन प्रदान करून चांगले परिणाम मिळवण्यासाठी प्रशिक्षण देणे शिकले आहे, जे खेळाची इच्छित स्थिती परिभाषित करते, आणि त्यांना शोध जागा बुद्धिमत्तेने एक्सप्लोर करण्याची संधी देतो. आपण यशस्वीरित्या दोन अल्गोरिदम्स वापरले आणि तुलनेने कमी कालावधीत चांगला परिणाम साध्य केला. मात्र, हे RL मध्ये आपली प्रवासाची फक्त सुरुवात आहे, आणि आपण अधिक खोलवर जाण्यासाठी स्वतंत्र कोर्स घेण्याचा विचार नक्कीच करावा.

## 🚀 आव्हान

'इतर RL कार्ये' विभागात सूचीबद्ध अनुप्रयोग एक्सप्लोर करा आणि त्यापैकी एक अंमलात आणण्याचा प्रयत्न करा!

## [व्याख्यानानंतरचा क्विझ](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## पुनरावलोकन आणि स्व-अभ्यास

आमच्या [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) मध्ये क्लासिकल रिइन्फोर्समेंट लर्निंगबद्दल अधिक जाणून घ्या.

सुपर मारिओ खेळायला संगणक कसे शिकू शकते याबद्दल बोलणारे [हे उत्कृष्ट व्हिडिओ](https://www.youtube.com/watch?v=qv6UVOQ0F44) पहा.

## असाइनमेंट: [Mountain Car प्रशिक्षित करा](lab/README.md)

या असाइनमेंट दरम्यान आपले उद्दिष्ट वेगळ्या Gym पर्यावरणाला प्रशिक्षित करणे असेल - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

