{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addestrare RL per bilanciare il Cartpole\n",
    "\n",
    "Questo notebook fa parte del [Curriculum AI per Principianti](http://aka.ms/ai-beginners). È stato ispirato dal [tutorial ufficiale di PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) e da [questa implementazione di Cartpole in PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "In questo esempio, utilizzeremo RL per addestrare un modello a bilanciare un'asta su un carrello che può muoversi a sinistra e a destra su una scala orizzontale. Useremo l'ambiente [OpenAI Gym](https://www.gymlibrary.ml/) per simulare l'asta.\n",
    "\n",
    "> **Nota**: Puoi eseguire il codice di questa lezione localmente (ad esempio, da Visual Studio Code), nel qual caso la simulazione si aprirà in una nuova finestra. Quando esegui il codice online, potrebbe essere necessario apportare alcune modifiche al codice, come descritto [qui](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Inizieremo assicurandoci che Gym sia installato:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora creiamo l'ambiente CartPole e vediamo come operare su di esso. Un ambiente ha le seguenti proprietà:\n",
    "\n",
    "* **Action space** è l'insieme delle azioni possibili che possiamo eseguire a ogni passo della simulazione\n",
    "* **Observation space** è lo spazio delle osservazioni che possiamo effettuare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come funziona la simulazione. Il seguente ciclo esegue la simulazione fino a quando `env.step` non restituisce il flag di terminazione `done`. Sceglieremo le azioni in modo casuale utilizzando `env.action_space.sample()`, il che significa che l'esperimento probabilmente fallirà molto rapidamente (l'ambiente CartPole termina quando la velocità del CartPole, la sua posizione o l'angolo superano certi limiti).\n",
    "\n",
    "> La simulazione si aprirà in una nuova finestra. Puoi eseguire il codice più volte e osservare come si comporta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puoi notare che le osservazioni contengono 4 numeri. Essi sono:\n",
    "- Posizione del carrello\n",
    "- Velocità del carrello\n",
    "- Angolo del palo\n",
    "- Velocità di rotazione del palo\n",
    "\n",
    "`rew` è la ricompensa che riceviamo a ogni passo. Puoi vedere che nell'ambiente CartPole si riceve 1 punto per ogni passo di simulazione, e l'obiettivo è massimizzare la ricompensa totale, ovvero il tempo in cui CartPole riesce a bilanciarsi senza cadere.\n",
    "\n",
    "Durante l'apprendimento per rinforzo, il nostro obiettivo è allenare una **politica** $\\pi$, che per ogni stato $s$ ci dirà quale azione $a$ intraprendere, quindi essenzialmente $a = \\pi(s)$.\n",
    "\n",
    "Se desideri una soluzione probabilistica, puoi pensare alla politica come a un insieme di probabilità per ogni azione, ovvero $\\pi(a|s)$ rappresenterebbe la probabilità che dovremmo intraprendere l'azione $a$ nello stato $s$.\n",
    "\n",
    "## Metodo del Gradiente di Politica\n",
    "\n",
    "Nel più semplice algoritmo di RL, chiamato **Gradiente di Politica**, alleneremo una rete neurale per prevedere la prossima azione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleneremo la rete eseguendo molti esperimenti e aggiornando la nostra rete dopo ogni esecuzione. Definiamo una funzione che eseguirà l'esperimento e restituirà i risultati (il cosiddetto **traccia**) - tutti gli stati, le azioni (e le loro probabilità consigliate) e le ricompense:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puoi eseguire un episodio con una rete non addestrata e osservare che il premio totale (ovvero la durata dell'episodio) è molto basso:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno degli aspetti complicati dell'algoritmo di policy gradient è l'uso delle **ricompense scontate**. L'idea è che calcoliamo il vettore delle ricompense totali a ogni passo del gioco e, durante questo processo, scontiamo le ricompense iniziali utilizzando un coefficiente $gamma$. Normalizziamo anche il vettore risultante, perché lo useremo come peso per influenzare il nostro allenamento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora iniziamo l'addestramento vero e proprio! Eseguiremo 300 episodi e, in ciascun episodio, faremo quanto segue:\n",
    "\n",
    "1. Esegui l'esperimento e raccogli la traccia.\n",
    "2. Calcola la differenza (`gradients`) tra le azioni intraprese e le probabilità previste. Minore è la differenza, più siamo sicuri di aver preso l'azione corretta.\n",
    "3. Calcola le ricompense scontate e moltiplica i gradienti per le ricompense scontate - questo assicurerà che i passi con ricompense più alte abbiano un impatto maggiore sul risultato finale rispetto a quelli con ricompense più basse.\n",
    "4. Le azioni target previste per la nostra rete neurale saranno in parte prese dalle probabilità previste durante l'esecuzione e in parte dai gradienti calcolati. Utilizzeremo il parametro `alpha` per determinare in che misura considerare gradienti e ricompense - questo è chiamato *tasso di apprendimento* dell'algoritmo di rinforzo.\n",
    "5. Infine, addestriamo la nostra rete sugli stati e sulle azioni previste, e ripetiamo il processo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora eseguiamo l'episodio con il rendering per vedere il risultato:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speriamo che tu possa vedere che ora l'asta riesce a bilanciarsi piuttosto bene!\n",
    "\n",
    "## Modello Actor-Critic\n",
    "\n",
    "Il modello Actor-Critic è un ulteriore sviluppo dei gradienti di politica, in cui costruiamo una rete neurale per apprendere sia la politica che le ricompense stimate. La rete avrà due output (o puoi considerarla come due reti separate):\n",
    "* **Actor** raccomanderà l'azione da intraprendere fornendoci la distribuzione di probabilità dello stato, come nel modello a gradiente di politica.\n",
    "* **Critic** stimerebbe quale sarebbe la ricompensa derivante da quelle azioni. Restituisce le ricompense totali stimate nel futuro nello stato dato.\n",
    "\n",
    "Definiamo un modello di questo tipo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avremmo bisogno di modificare leggermente le nostre funzioni `discounted_rewards` e `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora eseguiremo il ciclo principale di addestramento. Utilizzeremo il processo di addestramento manuale della rete calcolando le funzioni di perdita appropriate e aggiornando i parametri della rete:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "Abbiamo visto due algoritmi di apprendimento per rinforzo in questa demo: il semplice policy gradient e l'attore-critico più sofisticato. Puoi notare che questi algoritmi operano con nozioni astratte di stato, azione e ricompensa - per questo motivo possono essere applicati a ambienti molto diversi.\n",
    "\n",
    "L'apprendimento per rinforzo ci permette di apprendere la strategia migliore per risolvere un problema semplicemente osservando la ricompensa finale. Il fatto che non abbiamo bisogno di dataset etichettati ci consente di ripetere le simulazioni molte volte per ottimizzare i nostri modelli. Tuttavia, ci sono ancora molte sfide nell'apprendimento per rinforzo, che potresti approfondire se decidi di concentrarti maggiormente su questa interessante area dell'IA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T12:48:32+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}