{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporazioni\n",
    "\n",
    "Nel nostro esempio precedente, abbiamo lavorato su vettori bag-of-words ad alta dimensionalità con lunghezza `vocab_size`, e stavamo convertendo esplicitamente da vettori di rappresentazione posizionale a bassa dimensionalità in rappresentazioni sparse one-hot. Questa rappresentazione one-hot non è efficiente in termini di memoria, inoltre, ogni parola viene trattata indipendentemente dalle altre, ovvero i vettori codificati one-hot non esprimono alcuna somiglianza semantica tra le parole.\n",
    "\n",
    "In questa unità, continueremo a esplorare il dataset **News AG**. Per iniziare, carichiamo i dati e recuperiamo alcune definizioni dal notebook precedente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos'è l'embedding?\n",
    "\n",
    "L'idea di **embedding** è rappresentare le parole attraverso vettori densi a dimensione ridotta, che in qualche modo riflettono il significato semantico di una parola. Più avanti discuteremo come costruire embedding di parole significativi, ma per ora pensiamo agli embedding come un modo per ridurre la dimensionalità di un vettore di parole.\n",
    "\n",
    "Quindi, il livello di embedding prende una parola come input e produce un vettore di output con una dimensione specificata `embedding_size`. In un certo senso, è molto simile al livello `Linear`, ma invece di prendere un vettore codificato one-hot, sarà in grado di accettare un numero di parola come input.\n",
    "\n",
    "Utilizzando il livello di embedding come primo livello nella nostra rete, possiamo passare dal modello bag-of-words al modello **embedding bag**, dove prima convertiamo ogni parola nel nostro testo nel corrispondente embedding e poi calcoliamo una funzione aggregata su tutti questi embedding, come `sum`, `average` o `max`.\n",
    "\n",
    "![Immagine che mostra un classificatore embedding per cinque parole in sequenza.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.it.png)\n",
    "\n",
    "La nostra rete neurale classificatrice inizierà con un livello di embedding, seguito da un livello di aggregazione e un classificatore lineare sopra di esso:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestire la dimensione variabile delle sequenze\n",
    "\n",
    "A causa di questa architettura, i minibatch per la nostra rete dovranno essere creati in un certo modo. Nell'unità precedente, utilizzando il bag-of-words, tutti i tensori BoW in un minibatch avevano una dimensione uguale `vocab_size`, indipendentemente dalla lunghezza effettiva della sequenza di testo. Una volta passati agli embedding di parole, ci troveremo con un numero variabile di parole in ogni campione di testo, e quando combiniamo questi campioni in minibatch dovremo applicare un po' di padding.\n",
    "\n",
    "Questo può essere fatto utilizzando la stessa tecnica di fornire la funzione `collate_fn` alla sorgente dati:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addestrare il classificatore di embedding\n",
    "\n",
    "Ora che abbiamo definito un dataloader adeguato, possiamo addestrare il modello utilizzando la funzione di training che abbiamo definito nell'unità precedente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: Qui stiamo addestrando solo per 25k record (meno di un'epoca completa) per risparmiare tempo, ma puoi continuare l'addestramento, scrivere una funzione per addestrare per diverse epoche e sperimentare con il parametro del tasso di apprendimento per ottenere una maggiore accuratezza. Dovresti essere in grado di raggiungere un'accuratezza di circa il 90%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Livello EmbeddingBag e Rappresentazione di Sequenze a Lunghezza Variabile\n",
    "\n",
    "Nell'architettura precedente, era necessario riempire tutte le sequenze fino alla stessa lunghezza per adattarle a un minibatch. Questo non è il modo più efficiente per rappresentare sequenze a lunghezza variabile - un altro approccio sarebbe utilizzare un vettore di **offset**, che contiene gli offset di tutte le sequenze memorizzate in un unico grande vettore.\n",
    "\n",
    "![Immagine che mostra una rappresentazione di sequenza con offset](../../../../../translated_images/offset-sequence-representation.eb73fcefb29b46eecfbe74466077cfeb7c0f93a4f254850538a2efbc63517479.it.png)\n",
    "\n",
    "> **Nota**: Nell'immagine sopra, mostriamo una sequenza di caratteri, ma nel nostro esempio stiamo lavorando con sequenze di parole. Tuttavia, il principio generale di rappresentare le sequenze con un vettore di offset rimane lo stesso.\n",
    "\n",
    "Per lavorare con la rappresentazione tramite offset, utilizziamo il livello [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html). È simile a `Embedding`, ma prende come input un vettore di contenuto e un vettore di offset, e include anche uno strato di aggregazione, che può essere `mean`, `sum` o `max`.\n",
    "\n",
    "Ecco una rete modificata che utilizza `EmbeddingBag`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per preparare il dataset per l'addestramento, dobbiamo fornire una funzione di conversione che preparerà il vettore di offset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota che, a differenza di tutti gli esempi precedenti, la nostra rete ora accetta due parametri: il vettore dei dati e il vettore degli offset, che sono di dimensioni diverse. Allo stesso modo, il nostro data loader ci fornisce 3 valori invece di 2: sia il vettore dei testi che quello degli offset sono forniti come caratteristiche. Pertanto, dobbiamo leggermente adattare la nostra funzione di training per gestire questa situazione:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Embeddings: Word2Vec\n",
    "\n",
    "Nel nostro esempio precedente, il livello di embedding del modello ha imparato a mappare le parole in rappresentazioni vettoriali, tuttavia questa rappresentazione non aveva molto significato semantico. Sarebbe utile apprendere una rappresentazione vettoriale tale che parole simili o sinonimi corrispondano a vettori vicini tra loro in termini di una certa distanza vettoriale (ad esempio, distanza euclidea).\n",
    "\n",
    "Per fare ciò, dobbiamo pre-addestrare il nostro modello di embedding su una vasta collezione di testi in un modo specifico. Uno dei primi metodi per addestrare embedding semantici si chiama [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Si basa su due principali architetture utilizzate per produrre una rappresentazione distribuita delle parole:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW) — in questa architettura, addestriamo il modello a prevedere una parola dal contesto circostante. Dato l'ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, l'obiettivo del modello è prevedere $W_0$ a partire da $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** è l'opposto del CBoW. Il modello utilizza la finestra di parole di contesto circostanti per prevedere la parola corrente.\n",
    "\n",
    "CBoW è più veloce, mentre skip-gram è più lento, ma rappresenta meglio le parole meno frequenti.\n",
    "\n",
    "![Immagine che mostra entrambi gli algoritmi CBoW e Skip-Gram per convertire parole in vettori.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.it.png)\n",
    "\n",
    "Per sperimentare con embedding word2vec pre-addestrati sul dataset Google News, possiamo utilizzare la libreria **gensim**. Di seguito troviamo le parole più simili a 'neural'.\n",
    "\n",
    "> **Nota:** Quando crei per la prima volta i vettori di parole, il download potrebbe richiedere un po' di tempo!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo anche calcolare gli embedding vettoriali dalla parola, da utilizzare nell'addestramento del modello di classificazione (mostriamo solo i primi 20 componenti del vettore per chiarezza):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cosa fantastica degli embedding semantici è che puoi manipolare la codifica dei vettori per cambiare la semantica. Ad esempio, possiamo chiedere di trovare una parola, la cui rappresentazione vettoriale sia il più vicino possibile alle parole *re* e *donna*, e il più lontano possibile dalla parola *uomo*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sia CBoW che Skip-Grams sono embedding \"predittivi\", poiché tengono conto solo dei contesti locali. Word2Vec non sfrutta il contesto globale.\n",
    "\n",
    "**FastText** si basa su Word2Vec imparando rappresentazioni vettoriali per ogni parola e per i n-grammi di caratteri trovati all'interno di ciascuna parola. I valori delle rappresentazioni vengono poi mediati in un unico vettore a ogni passo di addestramento. Sebbene ciò aggiunga un notevole carico computazionale durante il pre-addestramento, consente agli embedding di parole di codificare informazioni sui sotto-elementi delle parole.\n",
    "\n",
    "Un altro metodo, **GloVe**, sfrutta l'idea della matrice di co-occorrenza e utilizza metodi neurali per decomporre la matrice di co-occorrenza in vettori di parole più espressivi e non lineari.\n",
    "\n",
    "Puoi sperimentare con l'esempio cambiando gli embedding in FastText e GloVe, poiché gensim supporta diversi modelli di embedding di parole.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizzo di Embedding Pre-Addestrati in PyTorch\n",
    "\n",
    "Possiamo modificare l'esempio sopra per pre-popolare la matrice nel nostro livello di embedding con embedding semantici, come Word2Vec. Dobbiamo tenere conto del fatto che i vocabolari degli embedding pre-addestrati e del nostro corpus di testo probabilmente non corrisponderanno, quindi inizializzeremo i pesi per le parole mancanti con valori casuali:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel nostro caso, non osserviamo un grande aumento di accuratezza, probabilmente a causa dei vocabolari piuttosto diversi.  \n",
    "Per superare il problema dei vocabolari differenti, possiamo utilizzare una delle seguenti soluzioni:  \n",
    "* Riaddestrare il modello word2vec sul nostro vocabolario  \n",
    "* Caricare il nostro dataset utilizzando il vocabolario del modello word2vec pre-addestrato. Il vocabolario utilizzato per caricare il dataset può essere specificato durante il caricamento.  \n",
    "\n",
    "L'ultimo approccio sembra più semplice, soprattutto perché il framework `torchtext` di PyTorch contiene un supporto integrato per gli embeddings. Possiamo, ad esempio, istanziare un vocabolario basato su GloVe nel seguente modo:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il vocabolario caricato ha le seguenti operazioni di base:\n",
    "* Il dizionario `vocab.stoi` ci permette di convertire una parola nel suo indice nel dizionario\n",
    "* `vocab.itos` fa l'opposto - converte un numero in una parola\n",
    "* `vocab.vectors` è l'array di vettori di embedding, quindi per ottenere l'embedding di una parola `s` dobbiamo usare `vocab.vectors[vocab.stoi[s]]`\n",
    "\n",
    "Ecco un esempio di manipolazione degli embedding per dimostrare l'equazione **kind-man+woman = queen** (ho dovuto modificare un po' il coefficiente per farlo funzionare):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per addestrare il classificatore utilizzando quei embeddings, dobbiamo prima codificare il nostro dataset utilizzando il vocabolario GloVe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo visto sopra, tutti gli embedding dei vettori sono memorizzati nella matrice `vocab.vectors`. Questo rende estremamente facile caricare quei pesi nei pesi del livello di embedding utilizzando una semplice copia:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno dei motivi per cui non stiamo osservando un aumento significativo dell'accuratezza è dovuto al fatto che alcune parole del nostro dataset sono assenti nel vocabolario GloVe pre-addestrato e, di conseguenza, vengono essenzialmente ignorate. Per superare questo problema, possiamo addestrare i nostri embedding sul nostro dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contesti Contestuali\n",
    "\n",
    "Una delle principali limitazioni delle rappresentazioni di embedding pre-addestrati tradizionali, come Word2Vec, è il problema della disambiguazione del significato delle parole. Sebbene gli embedding pre-addestrati possano catturare parte del significato delle parole nel contesto, ogni possibile significato di una parola viene codificato nello stesso embedding. Questo può causare problemi nei modelli a valle, poiché molte parole, come la parola 'play', hanno significati diversi a seconda del contesto in cui vengono utilizzate.\n",
    "\n",
    "Ad esempio, la parola 'play' in queste due frasi ha significati piuttosto diversi:\n",
    "- Sono andato a vedere una **play** a teatro.\n",
    "- John vuole **play** con i suoi amici.\n",
    "\n",
    "Gli embedding pre-addestrati sopra rappresentano entrambi i significati della parola 'play' nello stesso embedding. Per superare questa limitazione, dobbiamo costruire embedding basati sul **modello linguistico**, che è addestrato su un ampio corpus di testo e *sa* come le parole possono essere combinate in contesti diversi. Discutere degli embedding contestuali esula dall'ambito di questo tutorial, ma torneremo sull'argomento quando parleremo dei modelli linguistici nella prossima unità.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-28T14:30:39+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}