{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meccanismi di attenzione e trasformatori\n",
    "\n",
    "Uno dei principali svantaggi delle reti ricorrenti è che tutte le parole in una sequenza hanno lo stesso impatto sul risultato. Questo causa prestazioni subottimali con i modelli standard encoder-decoder basati su LSTM per compiti di sequenza a sequenza, come il riconoscimento di entità nominate e la traduzione automatica. In realtà, specifiche parole nella sequenza di input spesso hanno un impatto maggiore sugli output sequenziali rispetto ad altre.\n",
    "\n",
    "Consideriamo un modello di sequenza a sequenza, come la traduzione automatica. È implementato da due reti ricorrenti, dove una rete (**encoder**) comprime la sequenza di input in uno stato nascosto, e un'altra rete, **decoder**, espande questo stato nascosto nel risultato tradotto. Il problema con questo approccio è che lo stato finale della rete ha difficoltà a ricordare l'inizio di una frase, causando una scarsa qualità del modello per frasi lunghe.\n",
    "\n",
    "**I meccanismi di attenzione** forniscono un mezzo per pesare l'impatto contestuale di ciascun vettore di input su ciascuna previsione di output dell'RNN. Il modo in cui viene implementato è creando scorciatoie tra gli stati intermedi dell'RNN di input e l'RNN di output. In questo modo, quando si genera il simbolo di output $y_t$, si prenderanno in considerazione tutti gli stati nascosti di input $h_i$, con diversi coefficienti di peso $\\alpha_{t,i}$.\n",
    "\n",
    "![Immagine che mostra un modello encoder/decoder con uno strato di attenzione additiva](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.it.png)\n",
    "*Il modello encoder-decoder con meccanismo di attenzione additiva in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citato da [questo post sul blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "La matrice di attenzione $\\{\\alpha_{i,j}\\}$ rappresenta il grado in cui certe parole di input influenzano la generazione di una determinata parola nella sequenza di output. Di seguito è riportato un esempio di tale matrice:\n",
    "\n",
    "![Immagine che mostra un allineamento campione trovato da RNNsearch-50, tratto da Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.it.png)\n",
    "\n",
    "*Figura tratta da [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "I meccanismi di attenzione sono responsabili di gran parte dello stato dell'arte attuale o quasi attuale nell'elaborazione del linguaggio naturale. Tuttavia, l'aggiunta di attenzione aumenta notevolmente il numero di parametri del modello, il che ha portato a problemi di scalabilità con gli RNN. Un vincolo chiave nella scalabilità degli RNN è che la natura ricorrente dei modelli rende difficile il batching e la parallelizzazione dell'addestramento. In un RNN, ogni elemento di una sequenza deve essere elaborato in ordine sequenziale, il che significa che non può essere facilmente parallelizzato.\n",
    "\n",
    "L'adozione dei meccanismi di attenzione combinata con questo vincolo ha portato alla creazione dei modelli Transformer, ora lo stato dell'arte, che conosciamo e utilizziamo oggi, da BERT a OpenGPT3.\n",
    "\n",
    "## Modelli Transformer\n",
    "\n",
    "Invece di trasmettere il contesto di ciascuna previsione precedente al passo di valutazione successivo, i **modelli Transformer** utilizzano **codifiche posizionali** e attenzione per catturare il contesto di un dato input all'interno di una finestra di testo fornita. L'immagine seguente mostra come le codifiche posizionali con attenzione possano catturare il contesto all'interno di una finestra data.\n",
    "\n",
    "![GIF animata che mostra come vengono effettuate le valutazioni nei modelli Transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Poiché ogni posizione di input viene mappata indipendentemente a ciascuna posizione di output, i Transformer possono parallelizzare meglio rispetto agli RNN, il che consente modelli linguistici molto più grandi e più espressivi. Ogni testa di attenzione può essere utilizzata per apprendere diverse relazioni tra parole, migliorando i compiti di elaborazione del linguaggio naturale a valle.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) è una rete Transformer multi-strato molto grande con 12 strati per *BERT-base* e 24 per *BERT-large*. Il modello viene prima pre-addestrato su un ampio corpus di dati testuali (Wikipedia + libri) utilizzando un addestramento non supervisionato (predizione di parole mascherate in una frase). Durante il pre-addestramento, il modello acquisisce un livello significativo di comprensione del linguaggio che può poi essere sfruttato con altri dataset tramite il fine-tuning. Questo processo è chiamato **transfer learning**.\n",
    "\n",
    "![Immagine da http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.it.png)\n",
    "\n",
    "Esistono molte varianti delle architetture Transformer, tra cui BERT, DistilBERT, BigBird, OpenGPT3 e altre, che possono essere ottimizzate. Il pacchetto [HuggingFace](https://github.com/huggingface/) fornisce un repository per l'addestramento di molte di queste architetture con PyTorch.\n",
    "\n",
    "## Utilizzo di BERT per la classificazione del testo\n",
    "\n",
    "Vediamo come possiamo utilizzare il modello BERT pre-addestrato per risolvere il nostro compito tradizionale: la classificazione di sequenze. Classificheremo il nostro dataset originale AG News.\n",
    "\n",
    "Per prima cosa, carichiamo la libreria HuggingFace e il nostro dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poiché utilizzeremo un modello BERT pre-addestrato, sarà necessario utilizzare un tokenizer specifico. Per prima cosa, caricheremo un tokenizer associato al modello BERT pre-addestrato.\n",
    "\n",
    "La libreria HuggingFace contiene un repository di modelli pre-addestrati, che puoi utilizzare semplicemente specificando i loro nomi come argomenti nelle funzioni `from_pretrained`. Tutti i file binari necessari per il modello verranno scaricati automaticamente.\n",
    "\n",
    "Tuttavia, in alcuni casi potrebbe essere necessario caricare i propri modelli. In tal caso, puoi specificare la directory che contiene tutti i file pertinenti, inclusi i parametri per il tokenizer, il file `config.json` con i parametri del modello, i pesi binari, ecc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'oggetto `tokenizer` contiene la funzione `encode` che può essere utilizzata direttamente per codificare il testo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poi, creiamo degli iteratori che useremo durante l'addestramento per accedere ai dati. Poiché BERT utilizza la propria funzione di codifica, dovremmo definire una funzione di padding simile a `padify` che abbiamo definito in precedenza:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel nostro caso, utilizzeremo il modello BERT pre-addestrato chiamato `bert-base-uncased`. Carichiamo il modello utilizzando il pacchetto `BertForSequenceClassification`. Questo garantisce che il nostro modello abbia già un'architettura necessaria per la classificazione, incluso il classificatore finale. Vedrai un messaggio di avviso che indica che i pesi del classificatore finale non sono inizializzati e che il modello richiederebbe un pre-addestramento - va benissimo, perché è esattamente ciò che stiamo per fare!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora siamo pronti per iniziare l'addestramento! Poiché BERT è già pre-addestrato, vogliamo partire con un tasso di apprendimento piuttosto basso per non compromettere i pesi iniziali.\n",
    "\n",
    "Tutto il lavoro principale viene svolto dal modello `BertForSequenceClassification`. Quando applichiamo il modello ai dati di addestramento, esso restituisce sia la perdita che l'output della rete per il minibatch di input. Utilizziamo la perdita per l'ottimizzazione dei parametri (`loss.backward()` esegue il passaggio all'indietro) e `out` per calcolare l'accuratezza dell'addestramento confrontando le etichette ottenute `labs` (calcolate usando `argmax`) con le etichette attese `labels`.\n",
    "\n",
    "Per controllare il processo, accumuliamo la perdita e l'accuratezza su diverse iterazioni e le stampiamo ogni `report_freq` cicli di addestramento.\n",
    "\n",
    "Questo addestramento probabilmente richiederà parecchio tempo, quindi limitiamo il numero di iterazioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puoi vedere (soprattutto se aumenti il numero di iterazioni e aspetti abbastanza a lungo) che la classificazione con BERT ci offre una precisione piuttosto buona! Questo perché BERT comprende già molto bene la struttura della lingua, e dobbiamo solo perfezionare il classificatore finale. Tuttavia, poiché BERT è un modello grande, l'intero processo di addestramento richiede molto tempo e necessita di una potenza computazionale significativa! (GPU, e preferibilmente più di una).\n",
    "\n",
    "> **Note:** Nel nostro esempio, abbiamo utilizzato uno dei modelli BERT pre-addestrati più piccoli. Esistono modelli più grandi che probabilmente offrono risultati migliori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutare le prestazioni del modello\n",
    "\n",
    "Ora possiamo valutare le prestazioni del nostro modello sul dataset di test. Il ciclo di valutazione è molto simile al ciclo di addestramento, ma non dobbiamo dimenticare di impostare il modello in modalità di valutazione chiamando `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "In questa unità, abbiamo visto quanto sia semplice prendere un modello linguistico pre-addestrato dalla libreria **transformers** e adattarlo al nostro compito di classificazione del testo. Allo stesso modo, i modelli BERT possono essere utilizzati per l'estrazione di entità, il question answering e altri compiti di NLP.\n",
    "\n",
    "I modelli transformer rappresentano lo stato dell'arte attuale nel campo del NLP e, nella maggior parte dei casi, dovrebbero essere la prima soluzione con cui iniziare a sperimentare quando si implementano soluzioni NLP personalizzate. Tuttavia, comprendere i principi di base delle reti neurali ricorrenti discussi in questo modulo è estremamente importante se si desidera costruire modelli neurali avanzati.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T14:12:22+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}