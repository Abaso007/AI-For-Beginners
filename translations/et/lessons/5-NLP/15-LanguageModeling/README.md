<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-10-11T11:42:54+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "et"
}
-->
# Keelemudelid

Semantilised vektorid, nagu Word2Vec ja GloVe, on tegelikult esimene samm **keelemudelite** suunas ‚Äì mudelite loomine, mis mingil moel *m√µistavad* (v√µi *esindavad*) keele olemust.

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Keelemudelite peamine idee seisneb nende treenimises m√§rgistamata andmekogumite p√µhjal juhendamata viisil. See on oluline, kuna meil on saadaval tohutul hulgal m√§rgistamata teksti, samas kui m√§rgistatud teksti hulk on alati piiratud ajaga, mida saame m√§rgistamisele kulutada. Enamasti saame luua keelemudeleid, mis suudavad **ennustada puuduvaid s√µnu** tekstis, kuna juhusliku s√µna maskeerimine tekstis ja selle kasutamine treeningn√§itena on lihtne.

## Vektorite treenimine

Eelnevates n√§idetes kasutasime eelnevalt treenitud semantilisi vektoreid, kuid huvitav on n√§ha, kuidas neid vektoreid saab treenida. Selleks on mitmeid v√µimalikke ideid:

* **N-grammi** keelemudel, kus ennustame tokenit, vaadates N eelnevat tokenit (N-gramm).
* **J√§rjepidev s√µnakott** (CBoW), kus ennustame keskmist tokenit $W_0$ tokenite j√§rjestuses $W_{-N}$, ..., $W_N$.
* **Skip-gramm**, kus ennustame keskmise tokeni $W_0$ p√µhjal naabertokenite komplekti {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$}.

![pilt artiklist, mis k√§sitleb s√µnade teisendamist vektoriteks](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.et.png)

> Pilt [sellest artiklist](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è N√§idisnotebookid: CBoW mudeli treenimine

J√§tka √µppimist j√§rgmistes notebookides:

* [CBoW Word2Vec treenimine TensorFlow'ga](CBoW-TF.ipynb)
* [CBoW Word2Vec treenimine PyTorch'iga](CBoW-PyTorch.ipynb)

## Kokkuv√µte

Eelmises tunnis n√§gime, et s√µnade vektorid toimivad nagu v√µluv√§el! N√º√ºd teame, et s√µnade vektorite treenimine ei ole v√§ga keeruline √ºlesanne ning vajadusel peaksime suutma treenida oma vektoreid spetsiifilise valdkonna tekstide jaoks.

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## √úlevaade ja iseseisev √µppimine

* [PyTorch'i ametlik keelemudelite √µpetus](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [TensorFlow ametlik √µpetus Word2Vec mudeli treenimisest](https://www.TensorFlow.org/tutorials/text/word2vec).
* **gensim** raamistikuga k√µige levinumate vektorite treenimine m√µne koodirea abil on kirjeldatud [selles dokumentatsioonis](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [√úlesanne: Treeni Skip-Gram mudel](lab/README.md)

Laboris kutsume sind √ºles muutma selle tunni koodi, et treenida CBoW asemel Skip-Gram mudelit. [Loe l√§hemalt](lab/README.md)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.