{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tähelepanumehhanismid ja transformerid\n",
    "\n",
    "Üks peamisi korduvvõrkude puudusi on see, et kõik sõnad järjestuses mõjutavad tulemust võrdselt. See põhjustab standardsete LSTM kodeerija-dekodeerija mudelite puhul alamoptimaalset jõudlust järjestuste vaheliste ülesannete, nagu nimede tuvastamine ja masintõlge, lahendamisel. Tegelikkuses on teatud sisendjärjestuse sõnadel sageli suurem mõju järjestikustele väljunditele kui teistel.\n",
    "\n",
    "Vaatleme järjestuse-järjestuse mudelit, näiteks masintõlget. See on rakendatud kahe korduvvõrgu abil, kus üks võrk (**kodeerija**) koondab sisendjärjestuse varjatud olekusse ja teine, **dekodeerija**, lahtirullib selle varjatud oleku tõlgitud tulemuseks. Selle lähenemise probleem seisneb selles, et võrgu lõplikul olekul on raske meeles pidada lause algust, mis põhjustab mudeli kehva kvaliteeti pikkade lausete puhul.\n",
    "\n",
    "**Tähelepanumehhanismid** pakuvad võimalust kaaluda iga sisendvektori kontekstuaalset mõju RNN-i iga väljundprognoosi puhul. Selle rakendamine toimub lühenduste loomisega sisend-RNN-i vaheolekute ja väljund-RNN-i vahel. Sel viisil, kui genereerime väljundisümbolit $y_t$, võtame arvesse kõiki sisendi varjatud olekuid $h_i$, erinevate kaalukoefitsientidega $\\alpha_{t,i}$.\n",
    "\n",
    "![Pilt, mis näitab kodeerija/dekodeerija mudelit koos aditiivse tähelepanukihiga](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.et.png)\n",
    "*Kodeerija-dekodeerija mudel koos aditiivse tähelepanumehhanismiga [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), viidatud [selles blogipostituses](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Tähelepanumaatriks $\\{\\alpha_{i,j}\\}$ esindab, millisel määral teatud sisendsõnad osalevad antud sõna genereerimisel väljundjärjestuses. Allpool on näide sellisest maatriksist:\n",
    "\n",
    "![Pilt, mis näitab näidisalini, mille leidis RNNsearch-50, võetud Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.et.png)\n",
    "\n",
    "*Joonis võetud [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Joonis 3)*\n",
    "\n",
    "Tähelepanumehhanismid vastutavad suure osa praeguse või peaaegu praeguse tipptaseme eest loomuliku keele töötlemises. Tähelepanu lisamine suurendab aga oluliselt mudeli parameetrite arvu, mis põhjustas RNN-ide skaleerimisprobleeme. RNN-ide skaleerimise peamine piirang on see, et mudelite korduv olemus muudab treeningu rühmitamise ja paralleelimise keeruliseks. RNN-is tuleb järjestuse iga element töödelda järjestikuses järjekorras, mis tähendab, et seda ei saa lihtsalt paralleelselt töödelda.\n",
    "\n",
    "Tähelepanumehhanismide kasutuselevõtt koos selle piiranguga viis tänapäeval tuntud ja kasutatavate tipptasemel transformerimudelite loomiseni, nagu BERT ja OpenGPT3.\n",
    "\n",
    "## Transformerimudelid\n",
    "\n",
    "Selle asemel, et edastada iga eelneva prognoosi konteksti järgmisse hindamissammu, kasutavad **transformerimudelid** **positsioonikodeeringuid** ja **tähelepanu**, et haarata antud sisendi konteksti etteantud tekstiaknas. Allolev pilt näitab, kuidas positsioonikodeeringud koos tähelepanuga suudavad konteksti haarata antud aknas.\n",
    "\n",
    "![Animeeritud GIF, mis näitab, kuidas hindamisi tehakse transformerimudelites.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Kuna iga sisendpositsioon kaardistatakse sõltumatult iga väljundpositsiooniga, suudavad transformerid paremini paralleelselt töötada kui RNN-id, mis võimaldab palju suuremaid ja väljendusrikkamaid keelemudeleid. Iga tähelepanukeskus saab kasutada erinevate sõnadevaheliste seoste õppimiseks, mis parandab loomuliku keele töötlemise ülesandeid.\n",
    "\n",
    "## Lihtsa transformerimudeli loomine\n",
    "\n",
    "Keras ei sisalda sisseehitatud transformeri kihti, kuid me saame selle ise ehitada. Nagu varem, keskendume AG News andmestiku tekstiklassifikatsioonile, kuid tasub mainida, et transformerimudelid näitavad parimaid tulemusi keerukamate NLP ülesannete puhul.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uued kihid Kerases peaksid pärinema `Layer` klassist ja rakendama `call` meetodit. Alustame **Positsioonilise Embeddingu** kihiga. Kasutame [mõningast koodi ametlikust Kerase dokumentatsioonist](https://keras.io/examples/nlp/text_classification_with_transformer/). Eeldame, et täidame kõik sisendjärjestused pikkuseni `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See kiht koosneb kahest `Embedding` kihist: üks on mõeldud tokenite sisestamiseks (nagu me varem arutasime) ja teine tokenite positsioonide jaoks. Tokenite positsioonid luuakse naturaalarvude järjestusena vahemikus 0 kuni `maxlen`, kasutades `tf.range`, ning seejärel edastatakse need sisestuskihile. Kaks saadud sisestusvektorit liidetakse, mille tulemusena saadakse positsiooniliselt sisestatud sisendi esitus kujuga `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "<img src=\"../../../../../translated_images/pos-embedding.e41ce9b6cf6078afd28da02f27e33ac7026ed4c156491df7ad9aa96be7c194bb.et.png\" width=\"40%\"/>\n",
    "\n",
    "Nüüd rakendame transformer ploki. See võtab sisendiks varem määratletud sisestuskihi väljundi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer rakendab `MultiHeadAttention` positsiooniliselt kodeeritud sisendile, et luua tähelepanuvektor mõõtmetega `maxlen`$\\times$`embed_dim`, mis seejärel segatakse sisendiga ja normaliseeritakse `LayerNormalization` abil.\n",
    "\n",
    "> **Note**: `LayerNormalization` on sarnane `BatchNormalization`-iga, mida käsitleti *Arvutinägemise* osas selle õppeprogrammi raames, kuid see normaliseerib eelmise kihi väljundid iga treeningnäidise jaoks eraldi, viies need vahemikku [-1..1].\n",
    "\n",
    "Selle kihi väljund edastatakse seejärel läbi `Dense` võrgu (meie puhul - kahekihi perceptron), ja tulemus lisatakse lõplikule väljundile (mis läbib uuesti normaliseerimise).\n",
    "\n",
    "<img src=\"../../../../../translated_images/transformer-layer.905e14747ca4e7d5cf1409e8bf8944c9b1d6e4f5ce3ab167918af65c4904d727.et.png\" width=\"30%\" />\n",
    "\n",
    "Nüüd oleme valmis defineerima täieliku transformer mudeli:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Mudelid\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) on väga suur mitmekihiline transformer-võrk, millel on 12 kihti *BERT-base* jaoks ja 24 kihti *BERT-large* jaoks. Mudel treenitakse esmalt suurtel tekstikorpustel (WikiPedia + raamatud) kasutades juhendamata treenimist (ennustades lauses maskeeritud sõnu). Eeltreeningu käigus omandab mudel märkimisväärse taseme keele mõistmist, mida saab seejärel kasutada koos teiste andmekogumitega peenhäälestuse abil. Seda protsessi nimetatakse **ülekandeõppeks**.\n",
    "\n",
    "![pilt aadressilt http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.et.png)\n",
    "\n",
    "Transformer-arhitektuuridel, sealhulgas BERT, DistilBERT, BigBird, OpenGPT3 ja teistel, on palju variatsioone, mida saab peenhäälestada.\n",
    "\n",
    "Vaatame, kuidas saame kasutada eeltreenitud BERT-mudelit, et lahendada meie traditsioonilist järjestuse klassifitseerimise probleemi. Laename idee ja osa koodist [ametlikust dokumentatsioonist](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Eeltreenitud mudelite laadimiseks kasutame **Tensorflow hub**-i. Kõigepealt laadime BERT-spetsiifilise vektoriseerija:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oluline on kasutada sama vektoriseerijat, millega algne võrk treeniti. Lisaks tagastab BERT vektoriseerija kolm komponenti:\n",
    "* `input_word_ids`, mis on sisendlause jaoks mõeldud tokenite numbrite järjestus\n",
    "* `input_mask`, mis näitab, milline osa järjestusest sisaldab tegelikku sisendit ja milline osa on täitmine. See on sarnane maskile, mida genereerib `Masking` kiht\n",
    "* `input_type_ids` kasutatakse keelemodelleerimise ülesannete jaoks ja võimaldab määrata kaks sisendlause ühes järjestuses.\n",
    "\n",
    "Seejärel saame instantsida BERT-i omaduste ekstraktori:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niisiis, BERT-i kiht tagastab mitmeid kasulikke tulemusi:\n",
    "* `pooled_output` on tulemuseks kõigi järjestuse tokenite keskmistamine. Seda võib vaadelda kui kogu võrgu intelligentset semantilist esindust. See on samaväärne meie eelmise mudeli `GlobalAveragePooling1D` kihi väljundiga.\n",
    "* `sequence_output` on viimase transformer-kihi väljund (vastab ülaltoodud mudeli `TransformerBlock` väljundile).\n",
    "* `encoder_outputs` on kõigi transformer-kihtide väljundid. Kuna oleme laadinud 4-kihilise BERT-mudeli (nagu võib arvata nimest, mis sisaldab `4_H`), on sellel 4 tensori. Viimane neist on sama, mis `sequence_output`.\n",
    "\n",
    "Nüüd määratleme otsast lõpuni klassifitseerimismudeli. Kasutame *funktsionaalset mudeli määratlust*, kus määratleme mudeli sisendi ja seejärel esitame rea avaldisi selle väljundi arvutamiseks. Samuti muudame BERT-mudeli kaalud mitte-treenitavaks ja treenime ainult lõplikku klassifikaatorit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kuigi treenitavaid parameetreid on vähe, on protsess üsna aeglane, sest BERT-i omaduste ekstraktor on arvutuslikult raske. Tundub, et me ei suutnud saavutada mõistlikku täpsust, kas treeningu puudumise või mudeli parameetrite vähesuse tõttu.\n",
    "\n",
    "Proovime BERT-i kaalu lahti lukustada ja seda samuti treenida. See nõuab väga väikest õppemäära ning ka hoolikamat treeningstrateegiat koos **soojendusega**, kasutades **AdamW** optimeerijat. Kasutame optimeerija loomiseks `tf-models-official` paketti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nagu näete, toimub treening üsna aeglaselt – kuid võite katsetada ja treenida mudelit mõne epohhi (5–10) jooksul, et näha, kas saate parema tulemuse võrreldes varasemate lähenemistega.\n",
    "\n",
    "## Huggingface Transformers'i teek\n",
    "\n",
    "Teine väga levinud (ja veidi lihtsam) viis Transformer-mudelite kasutamiseks on [HuggingFace pakett](https://github.com/huggingface/), mis pakub lihtsaid ehitusplokke erinevate NLP ülesannete jaoks. See on saadaval nii Tensorflow'le kui ka PyTorchile, mis on teine väga populaarne tehisnärvivõrkude raamistik.\n",
    "\n",
    "> **Note**: Kui te ei soovi näha, kuidas Transformers'i teek töötab, võite liikuda otse selle märkmiku lõppu, kuna te ei näe midagi oluliselt erinevat sellest, mida me eespool tegime. Me kordame samu BERT-mudeli treenimise samme, kasutades teistsugust teeki ja oluliselt suuremat mudelit. Seetõttu hõlmab protsess üsna pikka treeningut, nii et võite lihtsalt koodi läbi vaadata.\n",
    "\n",
    "Vaatame, kuidas meie probleemi saab lahendada, kasutades [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esimene asi, mida peame tegema, on valida mudel, mida hakkame kasutama. Lisaks mõnedele sisseehitatud mudelitele sisaldab Huggingface [veebipõhist mudelite repositooriumi](https://huggingface.co/models), kus kogukond on jaganud palju rohkem eeltreenitud mudeleid. Kõiki neid mudeleid saab laadida ja kasutada lihtsalt mudeli nime esitamisega. Kõik vajalikud binaarfailid mudeli jaoks laaditakse automaatselt alla.\n",
    "\n",
    "Mõnikord võib tekkida vajadus laadida oma mudelid, sel juhul saate määrata kataloogi, mis sisaldab kõiki asjakohaseid faile, sealhulgas tokeniseerija parameetreid, `config.json` faili mudeli parameetritega, binaarkaale jne.\n",
    "\n",
    "Mudeli nime põhjal saame luua nii mudeli kui ka tokeniseerija. Alustame tokeniseerijast:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` objekt sisaldab `encode` funktsiooni, mida saab otse kasutada teksti kodeerimiseks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me saame kasutada ka tokenizerit, et kodeerida järjestust viisil, mis sobib mudelile edastamiseks, st sisaldades `token_ids`, `input_mask` välju jne. Samuti saame määrata, et soovime Tensorflow tensoreid, lisades argumendi `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meie puhul kasutame eelnevalt treenitud BERT-mudelit nimega `bert-base-uncased`. *Uncased* tähendab, et mudel ei erista suuri ja väikeseid tähti.\n",
    "\n",
    "Mudeli treenimisel peame sisendiks andma tokeniseeritud järjestuse, seega loome andmetöötluse torujuhtme. Kuna `tokenizer.encode` on Pythoni funktsioon, kasutame sama lähenemist nagu eelmises osas, kutsudes seda `py_function` abil:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nüüd saame laadida tegeliku mudeli, kasutades `BertForSequenceClassification` paketti. See tagab, et meie mudelil on juba vajalik klassifikatsiooni arhitektuur, sealhulgas lõplik klassifikaator. Näete hoiatusteadet, mis ütleb, et lõpliku klassifikaatori kaalud ei ole algväärtustatud ja mudel vajab eelõpetamist - see on täiesti normaalne, sest just seda me kavatseme teha!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nagu näete `summary()`-st, sisaldab mudel peaaegu 110 miljonit parameetrit! Tõenäoliselt, kui soovime lihtsat klassifitseerimisülesannet suhteliselt väikese andmestikuga, ei soovi me treenida BERT-i põhikihti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nüüd oleme valmis alustama treenimist!\n",
    "\n",
    "> **Märkus**: Täismahus BERT-mudeli treenimine võib olla väga aeganõudev! Seetõttu treenime seda ainult esimese 32 partii jaoks. See on lihtsalt selleks, et näidata, kuidas mudeli treenimine on üles seatud. Kui soovite proovida täismahus treenimist - eemaldage lihtsalt `steps_per_epoch` ja `validation_steps` parameetrid ning olge valmis ootama!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kui suurendate iteratsioonide arvu, ootate piisavalt kaua ja treenite mitu epohhi, võite eeldada, et BERT klassifikatsioon annab parima täpsuse! Seda seetõttu, et BERT mõistab keele struktuuri juba üsna hästi ja meil on vaja ainult lõplikku klassifikaatorit peenhäälestada. Kuid kuna BERT on suur mudel, võtab kogu treenimisprotsess kaua aega ja nõuab tõsist arvutusvõimsust! (GPU ja eelistatavalt rohkem kui üks).\n",
    "\n",
    "> **Note:** Meie näites oleme kasutanud üht väikseimat eeltreenitud BERT mudelit. On olemas suuremaid mudeleid, mis tõenäoliselt annavad paremaid tulemusi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peamine mõte\n",
    "\n",
    "Selles peatükis tutvusime väga hiljutiste mudeliarhitektuuridega, mis põhinevad **transformeritel**. Kasutasime neid tekstiklassifikatsiooni ülesande jaoks, kuid samamoodi saab BERT-mudeleid kasutada üksuste tuvastamiseks, küsimustele vastamiseks ja muude NLP ülesannete lahendamiseks.\n",
    "\n",
    "Transformer-mudelid esindavad praegust tipptaset NLP-s ning enamasti peaksid need olema esimene lahendus, millega alustad katsetamist, kui rakendad kohandatud NLP-lahendusi. Siiski on äärmiselt oluline mõista korduvate närvivõrkude põhimõtteid, mida selles moodulis käsitleti, kui soovid luua keerukaid närvimudeleid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Lahtiütlus**:  \nSee dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-10-11T12:39:36+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "et"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}