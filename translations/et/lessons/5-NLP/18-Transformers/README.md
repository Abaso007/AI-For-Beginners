<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f335dfcb4a993920504c387973a36957",
  "translation_date": "2025-10-11T11:39:25+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "et"
}
-->
# T√§helepanu mehhanismid ja Transformerid

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/35)

√úks olulisemaid probleeme NLP valdkonnas on **masint√µlge**, mis on oluline √ºlesanne selliste t√∂√∂riistade nagu Google Translate aluseks. Selles osas keskendume masint√µlkele v√µi √ºldisemalt igale *j√§rjestusest-j√§rjestusse* √ºlesandele (mida nimetatakse ka **lause transduktsiooniks**).

RNN-idega rakendatakse j√§rjestusest-j√§rjestusse meetodit kahe korduva v√µrgu abil, kus √ºks v√µrk, **kodeerija**, koondab sisendj√§rjestuse varjatud olekusse, samal ajal kui teine v√µrk, **dekodeerija**, laotab selle varjatud oleku t√µlgitud tulemuseks. Selle l√§henemisega kaasnevad m√µned probleemid:

* Kodeerija v√µrgu l√µplik olek ei suuda h√§sti meeles pidada lause algust, mis p√µhjustab mudeli kehva kvaliteeti pikkade lausete puhul.
* K√µigil s√µnadel j√§rjestuses on sama m√µju tulemusele. Tegelikkuses on aga konkreetsetel s√µnadel sisendj√§rjestuses sageli suurem m√µju j√§rjestikustele v√§ljunditele kui teistel.

**T√§helepanu mehhanismid** pakuvad v√µimalust kaaluda iga sisendvektori kontekstuaalset m√µju RNN-i iga v√§ljundprognoosi puhul. Seda rakendatakse, luues otseteid sisend-RNN-i vaheolekute ja v√§ljund-RNN-i vahel. Sel viisil, kui genereerime v√§ljundis√ºmbolit y<sub>t</sub>, v√µtame arvesse k√µiki sisendvarjatud olekuid h<sub>i</sub>, erinevate kaalukoefitsientidega &alpha;<sub>t,i</sub>.

![Pilt, mis n√§itab kodeerija/dekodeerija mudelit koos aditiivse t√§helepanu kihiga](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.et.png)

> Kodeerija-dekodeerija mudel aditiivse t√§helepanu mehhanismiga [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), viidatud [sellest blogipostitusest](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

T√§helepanu maatriks {&alpha;<sub>i,j</sub>} esindab, millises ulatuses teatud sisends√µnad m√µjutavad antud s√µna genereerimist v√§ljundj√§rjestuses. Allpool on n√§ide sellisest maatriksist:

![Pilt, mis n√§itab n√§idisalini, mille leidis RNNsearch-50, v√µetud Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.et.png)

> Joonis [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Joonis 3)

T√§helepanu mehhanismid vastutavad suure osa praeguse v√µi peaaegu praeguse NLP tipptaseme eest. T√§helepanu lisamine suurendab aga oluliselt mudeli parameetrite arvu, mis p√µhjustas RNN-idega skaleerimisprobleeme. RNN-ide skaleerimise peamine piirang on see, et mudelite korduv olemus muudab treeningu r√ºhmitamise ja paralleelimise keeruliseks. RNN-is tuleb iga j√§rjestuse element t√∂√∂delda j√§rjestikuses j√§rjekorras, mis t√§hendab, et seda ei saa lihtsalt paralleelselt t√∂√∂delda.

![Kodeerija Dekodeerija koos T√§helepanuga](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Joonis [Google'i blogist](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

T√§helepanu mehhanismide kasutuselev√µtt koos selle piiranguga viis t√§nap√§eval tuntud ja kasutatavate tipptasemel Transformer mudelite loomiseni, nagu BERT ja Open-GPT3.

## Transformer mudelid

√úks peamisi ideid transformerite taga on v√§ltida RNN-ide j√§rjestikust olemust ja luua mudel, mis on treeningu ajal paralleelsem. See saavutatakse kahe idee rakendamisega:

* positsiooniline kodeerimine
* mustrite tuvastamine iset√§helepanu mehhanismi abil RNN-ide (v√µi CNN-ide) asemel (seet√µttu on transformerite tutvustav artikkel pealkirjaga *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Positsiooniline kodeerimine/Embedimine

Positsioonilise kodeerimise idee on j√§rgmine. 
1. RNN-ide kasutamisel esindab tokenite suhtelist positsiooni sammude arv, mist√µttu ei pea seda otseselt esindama. 
2. Kui aga l√ºlitume t√§helepanule, peame teadma tokenite suhtelisi positsioone j√§rjestuses. 
3. Positsioonilise kodeerimise saamiseks t√§iendame oma tokenite j√§rjestust j√§rjestuse tokenipositsioonide j√§rjestusega (st numbrite j√§rjestus 0,1, ...).
4. Seej√§rel segame tokeni positsiooni tokeni embedimise vektoriga. Positsiooni (t√§isarvu) vektoriks teisendamiseks saame kasutada erinevaid l√§henemisi:

* Treenitav embedimine, sarnane tokeni embedimisele. See on l√§henemine, mida siin kaalume. Rakendame embedimise kihid nii tokenitele kui ka nende positsioonidele, mille tulemuseks on sama m√µ√µtmetega embedimise vektorid, mille me seej√§rel kokku liidame.
* Fikseeritud positsioonilise kodeerimise funktsioon, nagu on v√§lja pakutud algses artiklis.

<img src="../../../../../translated_images/pos-embedding.e41ce9b6cf6078afd28da02f27e33ac7026ed4c156491df7ad9aa96be7c194bb.et.png" width="50%"/>

> Pilt autorilt

Positsioonilise embedimise tulemusena saame vektori, mis sisaldab nii algset tokenit kui ka selle positsiooni j√§rjestuses.

### Mitmepealine iset√§helepanu

J√§rgmine samm on mustrite tuvastamine j√§rjestuses. Selleks kasutavad transformerid **iset√§helepanu** mehhanismi, mis on sisuliselt t√§helepanu rakendamine samale j√§rjestusele sisendi ja v√§ljundina. Iset√§helepanu rakendamine v√µimaldab meil arvestada **konteksti** lauses ja n√§ha, millised s√µnad on omavahel seotud. N√§iteks v√µimaldab see meil n√§ha, millistele s√µnadele viitavad kooreferentsid, nagu *see*, ja arvestada konteksti:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.et.png)

> Pilt [Google'i blogist](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Transformerites kasutame **Mitmepealist T√§helepanu**, et anda v√µrgule v√µime tuvastada mitut erinevat t√º√ºpi s√µltuvusi, nt. pikaajalised vs. l√ºhiajalised s√µnade suhted, kooreferentsid vs. midagi muud jne.

[TensorFlow Notebook](TransformersTF.ipynb) sisaldab rohkem detaile transformerite kihtide rakendamise kohta.

### Kodeerija-Dekodeerija T√§helepanu

Transformerites kasutatakse t√§helepanu kahes kohas:

* Mustrite tuvastamiseks sisendteksti sees iset√§helepanu abil
* J√§rjestuse t√µlkimiseks - see on t√§helepanu kiht kodeerija ja dekodeerija vahel.

Kodeerija-dekodeerija t√§helepanu on v√§ga sarnane RNN-ides kasutatava t√§helepanu mehhanismiga, nagu kirjeldatud selle jaotise alguses. See animeeritud diagramm selgitab kodeerija-dekodeerija t√§helepanu rolli.

![Animeeritud GIF, mis n√§itab, kuidas hinnangud tehakse transformer mudelites.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Kuna iga sisendi positsioon kaardistatakse s√µltumatult iga v√§ljundi positsiooniga, saavad transformerid paremini paralleelselt t√∂√∂tada kui RNN-id, mis v√µimaldab palju suuremaid ja v√§ljendusrikkamaid keelemudeleid. Iga t√§helepanu pea saab kasutada erinevate s√µnadevaheliste suhete √µppimiseks, mis parandab NLP √ºlesandeid.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) on v√§ga suur mitmekihiline transformer v√µrk, millel on 12 kihti *BERT-base* jaoks ja 24 kihti *BERT-large* jaoks. Mudel treenitakse esmalt suure tekstikorpuse (Wikipedia + raamatud) peal kasutades juhendamata treeningut (ennustades maskeeritud s√µnu lauses). Treeningu k√§igus omandab mudel m√§rkimisv√§√§rsel tasemel keele m√µistmist, mida saab seej√§rel kasutada teiste andmekogumitega peenh√§√§lestamise abil. Seda protsessi nimetatakse **√ºlekande√µppeks**.

![pilt aadressilt http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.et.png)

> Pildi [allikas](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Harjutused: Transformerid

J√§tka √µppimist j√§rgmistes m√§rkmikes:

* [Transformerid PyTorchis](TransformersPyTorch.ipynb)
* [Transformerid TensorFlowis](TransformersTF.ipynb)

## Kokkuv√µte

Selles √µppetunnis √µppisite Transformerite ja T√§helepanu Mehhanismide kohta, mis on NLP t√∂√∂riistakasti olulised vahendid. Transformerite arhitektuuril on palju variatsioone, sealhulgas BERT, DistilBERT, BigBird, OpenGPT3 ja palju muud, mida saab peenh√§√§lestada. [HuggingFace pakett](https://github.com/huggingface/) pakub mitmete nende arhitektuuride treenimiseks nii PyTorchi kui ka TensorFlowi jaoks.

## üöÄ V√§ljakutse

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## √úlevaade ja iseseisev √µppimine

* [Blogipostitus](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), mis selgitab klassikalist [Attention is all you need](https://arxiv.org/abs/1706.03762) artiklit transformeritest.
* [Blogipostituste seeria](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) transformeritest, mis selgitab arhitektuuri √ºksikasjalikult.

## [√úlesanne](assignment.md)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.