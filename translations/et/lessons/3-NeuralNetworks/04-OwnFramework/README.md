<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "789d6c3fb6fc7948a470b33078a5983a",
  "translation_date": "2025-10-11T11:30:37+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "et"
}
-->
# Sissejuhatus tehisn√§rviv√µrkudesse. Mitmekihiline perceptron

Eelmises osas √µppisite tundma k√µige lihtsamat tehisn√§rviv√µrgu mudelit ‚Äì √ºhekihilist perceptronit, mis on lineaarne kahe klassi klassifitseerimise mudel.

Selles osas laiendame seda mudelit paindlikumaks raamistikuks, mis v√µimaldab:

* teha **mitme klassi klassifikatsiooni** lisaks kahe klassi klassifikatsioonile
* lahendada **regressiooniprobleeme** lisaks klassifikatsioonile
* eristada klasse, mis ei ole lineaarselt eristatavad

Samuti arendame v√§lja oma modulaarse raamistiku Pythonis, mis v√µimaldab meil luua erinevaid tehisn√§rviv√µrgu arhitektuure.

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Masin√µppe formaliseerimine

Alustame masin√µppe probleemi formaliseerimisest. Oletame, et meil on treeningandmestik **X** koos siltidega **Y**, ja peame looma mudeli *f*, mis teeb k√µige t√§psemaid ennustusi. Ennustuste kvaliteeti m√µ√µdetakse **kaofunktsiooni** &lagran; abil. Sageli kasutatakse j√§rgmisi kaofunktsioone:

* Regressiooniprobleemi puhul, kui peame ennustama arvu, saame kasutada **absoluutset viga** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| v√µi **ruutviga** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Klassifikatsiooni puhul kasutame **0-1 kaotust** (mis sisuliselt on sama mis mudeli **t√§psus**) v√µi **logistilist kaotust**.

√úhekihilise perceptroni puhul oli funktsioon *f* defineeritud kui lineaarne funktsioon *f(x)=wx+b* (kus *w* on kaalude maatriks, *x* on sisendite tunnuste vektor ja *b* on nihkevektor). Erinevate tehisn√§rviv√µrgu arhitektuuride puhul v√µib see funktsioon olla keerukama kujuga.

> Klassifikatsiooni puhul on sageli soovitav saada v√µrgu v√§ljundina klasside t√µen√§osused. Arvude teisendamiseks t√µen√§osusteks (nt v√§ljundi normaliseerimiseks) kasutame sageli **softmax** funktsiooni &sigma;, ja funktsioon *f* muutub *f(x)=&sigma;(wx+b)*

√úlaltoodud *f* definitsioonis nimetatakse *w* ja *b* **parameetriteks** &theta;=‚ü®*w,b*‚ü©. Arvestades andmestikku ‚ü®**X**,**Y**‚ü©, saame arvutada kogu andmestiku vea parameetrite &theta; funktsioonina.

> ‚úÖ **Tehisn√§rviv√µrgu treenimise eesm√§rk on minimeerida viga, muutes parameetreid &theta;**

## Gradientlanguse optimeerimine

On olemas tuntud funktsiooni optimeerimise meetod, mida nimetatakse **gradientlanguseks**. Idee seisneb selles, et saame arvutada kaofunktsiooni tuletise (mitmem√µ√µtmelisel juhul nimetatakse seda **gradientiks**) parameetrite suhtes ja muuta parameetreid nii, et viga v√§heneks. Seda saab formaliseerida j√§rgmiselt:

* Algv√§√§rtusta parameetrid juhuslike v√§√§rtustega w<sup>(0)</sup>, b<sup>(0)</sup>
* Korda j√§rgmist sammu mitu korda:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Treeningu ajal peaksid optimeerimissammud olema arvutatud kogu andmestikku arvestades (pidage meeles, et kaotus arvutatakse k√µigi treeningn√§idete summa p√µhjal). Kuid reaalses elus v√µtame andmestikust v√§ikeseid osi, mida nimetatakse **minipartiideks**, ja arvutame gradientid andmealamkogumi p√µhjal. Kuna alamkogum valitakse iga kord juhuslikult, nimetatakse sellist meetodit **stohhastiliseks gradientlanguseks** (SGD).

## Mitmekihilised perceptronid ja tagasilevik

√úhekihiline v√µrk, nagu me eespool n√§gime, suudab klassifitseerida lineaarselt eristatavaid klasse. Rikkama mudeli loomiseks saame kombineerida mitu v√µrgu kihti. Matemaatiliselt t√§hendaks see, et funktsioon *f* oleks keerukama kujuga ja arvutataks mitmes etapis:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Siin &alpha; on **mittelineaarne aktivatsioonifunktsioon**, &sigma; on softmax funktsioon ja parameetrid &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientlanguse algoritm j√§√§b samaks, kuid gradientide arvutamine muutub keerulisemaks. Arvestades ahel-diferentseerimise reeglit, saame tuletised arvutada j√§rgmiselt:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ‚úÖ Ahel-diferentseerimise reeglit kasutatakse kaofunktsiooni tuletiste arvutamiseks parameetrite suhtes.

Pange t√§hele, et k√µigi nende avaldiste vasakpoolne osa on sama, ja seega saame tuletised t√µhusalt arvutada, alustades kaofunktsioonist ja liikudes "tagasi" l√§bi arvutusgraafi. Seet√µttu nimetatakse mitmekihilise perceptroni treenimise meetodit **tagasilevikuks** ehk 'backprop'.

<img alt="arvutusgraaf" src="../../../../../translated_images/ComputeGraphGrad.4626252c0de035075e5cd2b7f71b776d5e3e8f64f2dc472b4420d3fdfaf53ba8.et.png"/>

> TODO: pildi viide

> ‚úÖ Katame tagasileviku palju detailsemalt meie n√§idisnotebookis.  

## Kokkuv√µte

Selles √µppetunnis ehitasime oma tehisn√§rviv√µrgu raamistiku ja kasutasime seda lihtsa kahem√µ√µtmelise klassifikatsiooniprobleemi lahendamiseks.

## üöÄ V√§ljakutse

Kaasaolevas notebookis rakendate oma raamistiku mitmekihiliste perceptronite loomiseks ja treenimiseks. N√§ete √ºksikasjalikult, kuidas kaasaegsed tehisn√§rviv√µrgud t√∂√∂tavad.

J√§tkake [OwnFramework](OwnFramework.ipynb) notebookiga ja t√∂√∂tage see l√§bi.

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## √úlevaade ja iseseisev √µppimine

Tagasilevik on tehisintellekti ja masin√µppe valdkonnas levinud algoritm, mida tasub [t√§psemalt uurida](https://wikipedia.org/wiki/Backpropagation).

## [√úlesanne](lab/README.md)

Selles praktikumis palutakse teil kasutada selles √µppetunnis loodud raamistikku MNIST k√§sitsi kirjutatud numbrite klassifikatsiooni lahendamiseks.

* [Juhised](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud, kasutades AI t√µlketeenust [Co-op Translator](https://github.com/Azure/co-op-translator). Kuigi p√º√ºame tagada t√§psust, palun arvestage, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algkeeles tuleks lugeda autoriteetseks allikaks. Olulise teabe puhul on soovitatav kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valede t√µlgenduste eest.