<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c34cbba802058b6fa267e1a294d4e510",
  "translation_date": "2025-10-11T11:31:34+00:00",
  "source_file": "lessons/3-NeuralNetworks/03-Perceptron/README.md",
  "language_code": "et"
}
-->
# Sissejuhatus tehisn√§rviv√µrkudesse: Perceptron

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/5)

√úks esimesi katseid luua midagi t√§nap√§evasele tehisn√§rviv√µrgule sarnast tegi Frank Rosenblatt Cornelli Aeronautika Laboratooriumist 1957. aastal. See oli riistvaraline lahendus nimega "Mark-1", mis oli loodud primitiivsete geomeetriliste kujundite, nagu kolmnurgad, ruudud ja ringid, √§ratundmiseks.

|      |      |
|--------------|-----------|
|<img src='../../../../../translated_images/Rosenblatt-wikipedia.294821b285ac796d2281a556fe3de9a7aa328e34cdd25f3ac9deefa0982ea2df.et.jpg' alt='Frank Rosenblatt'/> | <img src='../../../../../translated_images/Mark_I_perceptron_wikipedia.1f84eaa2d4b76ec9fb32bab6ae07e40faa202be1935c0fbd8c2bc2600a87bcb1.et.jpg' alt='Mark 1 Perceptron' />|

> Pildid [Wikipediast](https://en.wikipedia.org/wiki/Perceptron)

Sisendpilt esitati 20x20 fototundliku elemendi maatriksina, seega oli tehisn√§rviv√µrgul 400 sisendit ja √ºks binaarne v√§ljund. Lihtne v√µrk koosnes √ºhest neuronist, mida nimetatakse ka **l√§ve loogika √ºksuseks**. N√§rviv√µrgu kaaluparameetrid toimisid nagu potentsiomeetrid, mida tuli treeningfaasis k√§sitsi reguleerida.

> ‚úÖ Potentsiomeeter on seade, mis v√µimaldab kasutajal reguleerida vooluringi takistust.

> The New York Times kirjutas tol ajal perceptroni kohta: *elektroonilise arvuti embr√ºo, mis [merev√§e arvates] suudab k√µndida, r√§√§kida, n√§ha, kirjutada, end paljundada ja olla teadlik oma olemasolust.*

## Perceptroni mudel

Oletame, et meie mudelil on N tunnust, sel juhul on sisendvektor suurusega N. Perceptron on **binaarne klassifitseerimismudel**, st see suudab eristada kahte sisendandmete klassi. Eeldame, et iga sisendvektori x korral on meie perceptroni v√§ljund kas +1 v√µi -1, s√µltuvalt klassist. V√§ljund arvutatakse j√§rgmise valemi abil:

y(x) = f(w<sup>T</sup>x)

kus f on astmelise aktiveerimise funktsioon

<!-- img src="http://www.sciweavers.org/tex2img.php?eq=f%28x%29%20%3D%20%5Cbegin%7Bcases%7D%0A%20%20%20%20%20%20%20%20%20%2B1%20%26%20x%20%5Cgeq%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20-1%20%26%20x%20%3C%200%0A%20%20%20%20%20%20%20%5Cend%7Bcases%7D%20%5C%5C%0A&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0" align="center" border="0" alt="f(x) = \begin{cases} +1 & x \geq 0 \\ -1 & x < 0 \end{cases} \\" width="154" height="50" / -->
<img src="../../../../../translated_images/activation-func.b4924007c7ce77648d4b1dd3e81c689453204902c096c626735c7b53688cdc63.et.png"/>

## Perceptroni treenimine

Perceptroni treenimiseks peame leidma kaaluvektori w, mis klassifitseerib enamik v√§√§rtusi √µigesti, st mille tulemuseks on k√µige v√§iksem **viga**. See viga E on m√§√§ratletud **perceptroni kriteeriumi** j√§rgi j√§rgmiselt:

E(w) = -&sum;w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

kus:

* summa arvutatakse nende treeningandmepunktide i pealt, mis on valesti klassifitseeritud
* x<sub>i</sub> on sisendandmed ja t<sub>i</sub> on kas -1 v√µi +1 vastavalt negatiivsetele ja positiivsetele n√§idetele.

Seda kriteeriumi k√§sitletakse kaaluvektori w funktsioonina, mida tuleb minimeerida. Sageli kasutatakse meetodit nimega **gradientlangus**, kus alustatakse m√µnest algkaalust w<sup>(0)</sup> ja seej√§rel igal sammul uuendatakse kaale j√§rgmise valemi j√§rgi:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - &eta;&nabla;E(w)

Siin on &eta; nn **√µppem√§√§r** ja &nabla;E(w) t√§histab E **gradienti**. P√§rast gradiendi arvutamist saame tulemuseks:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + &sum;&eta;x<sub>i</sub>t<sub>i</sub>

Pythonis n√§eb algoritm v√§lja selline:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Kokkuv√µte

Selles √µppet√ºkis √µppisite perceptroni kohta, mis on binaarne klassifitseerimismudel, ja kuidas seda treenida kaaluvektori abil.

## üöÄ V√§ljakutse

Kui soovite proovida ise perceptroni luua, proovige [seda laborit Microsoft Learnis](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/two-class-averaged-perceptron?WT.mc_id=academic-77998-cacaste), mis kasutab [Azure ML disainerit](https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer?WT.mc_id=academic-77998-cacaste).

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/6)

## √úlevaade ja iseseisev √µppimine

Et n√§ha, kuidas perceptronit saab kasutada nii m√§nguliste kui ka p√§riselu probleemide lahendamiseks, ja √µppimist j√§tkata, minge [Perceptroni](Perceptron.ipynb) m√§rkmikusse.

Siin on ka huvitav [artikkel perceptronite kohta](https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590).

## [√úlesanne](lab/README.md)

Selles √µppet√ºkis rakendasime perceptroni binaarse klassifitseerimise √ºlesande jaoks ja kasutasime seda kahe k√§sitsi kirjutatud numbri eristamiseks. Selles laboris palutakse teil lahendada numbrite klassifitseerimise probleem t√§ielikult, st m√§√§rata, millisele numbrile antud pilt k√µige t√µen√§olisemalt vastab.

* [Juhised](lab/README.md)
* [M√§rkmik](lab/PerceptronMultiClass.ipynb)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.