<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1b8d9e1b3a6f1daa864b1ff3dfc3076d",
  "translation_date": "2025-10-11T11:24:53+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "et"
}
-->
# Autoenkoodrid

CNN-ide treenimisel on √ºheks probleemiks see, et vajame palju m√§rgistatud andmeid. N√§iteks pildiklassifikatsiooni puhul peame pildid jagama erinevatesse klassidesse, mis on k√§sitsi tehtav t√∂√∂.

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Siiski v√µime soovida kasutada toorandmeid (m√§rgistamata) CNN-i funktsioonide ekstraktorite treenimiseks, mida nimetatakse **iseseisvaks √µppimiseks**. Siltide asemel kasutame treeningpilte nii v√µrgu sisendina kui ka v√§ljundina. **Autoenkoodri** peamine idee seisneb selles, et meil on **enkoodriv√µrk**, mis teisendab sisendpildi mingisse **latentruumi** (tavaliselt on see lihtsalt v√§iksema suurusega vektor), ja **dekoodriv√µrk**, mille eesm√§rk on taastada algne pilt.

> ‚úÖ [Autoenkooder](https://wikipedia.org/wiki/Autoencoder) on "tehisn√§rviv√µrkude t√º√ºp, mida kasutatakse m√§rgistamata andmete t√µhusate kodeeringute √µppimiseks."

Kuna treenime autoenkoodrit, et haarata v√µimalikult palju teavet algsest pildist t√§pseks taastamiseks, p√º√ºab v√µrk leida parima **sisendpiltide representatsiooni**, et tabada nende t√§hendus.

![Autoenkoodri skeem](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.et.jpg)

> Pilt [Kerase blogist](https://blog.keras.io/building-autoencoders-in-keras.html)

## Autoenkoodrite kasutamise stsenaariumid

Kuigi algsete piltide taastamine ei pruugi iseenesest tunduda kasulik, on m√µned stsenaariumid, kus autoenkoodrid on eriti kasulikud:

* **Piltide dimensiooni v√§hendamine visualiseerimiseks** v√µi **piltide representatsioonide treenimine**. Tavaliselt annavad autoenkoodrid paremaid tulemusi kui PCA, kuna nad arvestavad piltide ruumilist olemust ja hierarhilisi omadusi.
* **M√ºrav√§hendus**, st m√ºra eemaldamine pildilt. Kuna m√ºra sisaldab palju kasutut teavet, ei suuda autoenkooder seda k√µike suhteliselt v√§ikesesse latentruumi mahutada ja seega haarab ainult olulise osa pildist. M√ºrav√§hendajate treenimisel alustame algsetest piltidest ja kasutame autoenkoodri sisendina pilte, millele on kunstlikult m√ºra lisatud.
* **Superresolutsioon**, pildi eraldusv√µime suurendamine. Alustame k√µrge eraldusv√µimega piltidest ja kasutame autoenkoodri sisendina madalama eraldusv√µimega pilti.
* **Generatiivsed mudelid**. Kui oleme autoenkoodri treeninud, saab dekoodri osa kasutada uute objektide loomiseks, alustades juhuslikest latentvektoritest.

## Variatsioonilised autoenkoodrid (VAE)

Traditsioonilised autoenkoodrid v√§hendavad sisendandmete dimensiooni, tuvastades sisendpiltide olulised omadused. Kuid latentvektorid ei pruugi sageli olla m√µistlikud. Teisis√µnu, kui v√µtta MNIST andmestik n√§iteks, siis pole lihtne aru saada, millised numbrid vastavad erinevatele latentvektoritele, kuna l√§hedased latentvektorid ei pruugi tingimata vastata samadele numbritele.

Generatiivsete mudelite treenimiseks on aga parem omada mingit arusaama latentruumist. See idee viib meid **variatsioonilise autoenkoodrini** (VAE).

VAE on autoenkooder, mis √µpib ennustama latentparameetrite *statistilist jaotust*, nn **latentjaotust**. N√§iteks v√µime soovida, et latentvektorid jaotuksid normaalselt mingi keskmise z<sub>mean</sub> ja standardh√§lbe z<sub>sigma</sub> j√§rgi (nii keskmine kui ka standardh√§lve on mingis dimensioonis d vektorid). VAE enkooder √µpib ennustama neid parameetreid ja dekooder v√µtab seej√§rel juhusliku vektori sellest jaotusest, et objekti taastada.

Kokkuv√µtteks:

 * Sisendvektorist ennustame `z_mean` ja `z_log_sigma` (standardh√§lbe asemel ennustame selle logaritmi)
 * Valime vektori `sample` jaotusest N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Dekooder p√º√ºab dekodeerida algset pilti, kasutades `sample` sisendvektorina

 <img src="../../../../../translated_images/vae.464c465a5b6a9e253be65a8cb3be1724832cbde57ece3912ddc962b199472a89.et.png" width="50%">

> Pilt [sellest blogipostitusest](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) autorilt Isaak Dykeman

Variatsioonilised autoenkoodrid kasutavad keerulist kaotusefunktsiooni, mis koosneb kahest osast:

* **Taastamiskaotus** on kaotusefunktsioon, mis n√§itab, kui l√§hedane taastatud pilt on sihtpildile (see v√µib olla keskmine ruutude summa ehk MSE). See on sama kaotusefunktsioon nagu tavalistes autoenkoodrites.
* **KL-kaotus**, mis tagab, et latentmuutuja jaotused j√§√§vad normaalse jaotuse l√§hedale. See p√µhineb [Kullback-Leibleri divergentsi](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) m√µistel - meetrikal, mis hindab, kui sarnased kaks statistilist jaotust on.

√úks oluline eelis VAE-de puhul on see, et need v√µimaldavad meil suhteliselt lihtsalt uusi pilte genereerida, kuna teame, millist jaotust latentvektorite valimiseks kasutada. N√§iteks kui treenime VAE-d 2D latentvektoriga MNIST andmestikul, saame seej√§rel muuta latentvektori komponente, et saada erinevaid numbreid:

<img alt="vaemnist" src="../../../../../translated_images/vaemnist.cab9e602dc08dc5066ce14e005889d6b53ca5bcaf16e35c28dbf8cd40c304de1.et.png" width="50%"/>

> Pilt autorilt [Dmitry Soshnikov](http://soshnikov.com)

Vaadake, kuidas pildid sulanduvad √ºksteisesse, kui hakkame saama latentvektoreid latentparameetrite ruumi erinevatest osadest. Samuti saame visualiseerida seda ruumi 2D-s:

<img alt="vaemnist cluster" src="../../../../../translated_images/vaemnist-diag.694315f775d5d666b02fb54f8fc7c64db65a9d126a16c2fdb8683cf9726f9ff5.et.png" width="50%"/> 

> Pilt autorilt [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Harjutused: Autoenkoodrid

Tutvuge autoenkoodritega nendes vastavates m√§rkmikes:

* [Autoenkoodrid TensorFlow's](AutoencodersTF.ipynb)
* [Autoenkoodrid PyTorchis](AutoEncodersPyTorch.ipynb)

## Autoenkoodrite omadused

* **Andmespetsiifilised** - need t√∂√∂tavad h√§sti ainult selliste piltidega, millel nad on treenitud. N√§iteks kui treenime superresolutsiooniv√µrku lillede peal, ei t√∂√∂ta see h√§sti portreede puhul. See on tingitud sellest, et v√µrk suudab luua k√µrgema eraldusv√µimega pilte, kasutades peeneid detaile, mis on treeningandmestikust √µpitud.
* **Kaotusega** - taastatud pilt ei ole sama mis algne pilt. Kaotuse olemus m√§√§ratakse treeningu ajal kasutatud *kaotusefunktsiooni* j√§rgi.
* T√∂√∂tab **m√§rgistamata andmetega**

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Kokkuv√µte

Selles √µppet√ºkis √µppisite erinevat t√º√ºpi autoenkoodrite kohta, mis on AI teadlasele k√§ttesaadavad. √ïppisite, kuidas neid ehitada ja kasutada piltide taastamiseks. Samuti √µppisite VAE kohta ja kuidas seda kasutada uute piltide genereerimiseks.

## üöÄ V√§ljakutse

Selles √µppet√ºkis √µppisite autoenkoodrite kasutamist piltide jaoks. Kuid neid saab kasutada ka muusika jaoks! Vaadake Magenta projekti [MusicVAE](https://magenta.tensorflow.org/music-vae) projekti, mis kasutab autoenkoodreid muusika taastamise √µppimiseks. Tehke [katseid](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) selle teegiga, et n√§ha, mida suudate luua.

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## √úlevaade ja iseseisev √µppimine

Lisateabe saamiseks autoenkoodrite kohta lugege neid ressursse:

* [Autoenkoodrite ehitamine Kerasis](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Blogipostitus NeuroHive'is](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Variatsioonilised autoenkoodrid selgitatud](https://kvfrans.com/variational-autoencoders-explained/)
* [Tingimuslikud variatsioonilised autoenkoodrid](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## √úlesanne

[TensorFlow m√§rkmiku](AutoencodersTF.ipynb) l√µpus leiate "√ºlesande" - kasutage seda oma kodut√∂√∂na.

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.