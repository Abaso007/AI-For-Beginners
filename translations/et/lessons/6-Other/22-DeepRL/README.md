<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-10-11T11:47:44+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "et"
}
-->
# S√ºgav Tugevdus√µpe

Tugevdus√µpe (RL) on √ºks p√µhilisi masin√µppe paradigmasid, k√µrvuti juhendatud ja juhendamata √µppega. Kui juhendatud √µppes tugineb √µpe teadaolevate tulemustega andmekogule, siis RL p√µhineb **√µppimisel l√§bi tegutsemise**. N√§iteks, kui me esimest korda n√§eme arvutim√§ngu, hakkame seda m√§ngima, isegi kui me reegleid ei tea, ja peagi suudame oma oskusi parandada lihtsalt m√§ngimise ja k√§itumise kohandamise kaudu.

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL-i teostamiseks on vaja:

* **Keskkonda** v√µi **simulaatorit**, mis m√§√§rab m√§ngu reeglid. Me peaksime saama simulaatoris katseid l√§bi viia ja tulemusi j√§lgida.
* **Tasu funktsiooni**, mis n√§itab, kui edukas meie katse oli. Arvutim√§ngu m√§ngimise √µppimise puhul oleks tasu meie l√µplik punktisumma.

Tasu funktsiooni p√µhjal peaksime suutma oma k√§itumist kohandada ja oskusi parandada, et j√§rgmisel korral paremini m√§ngida. Peamine erinevus teiste masin√µppe t√º√ºpide ja RL-i vahel on see, et RL-is me tavaliselt ei tea, kas v√µidame v√µi kaotame enne, kui m√§ng on l√µppenud. Seega ei saa me √∂elda, kas teatud k√§ik iseenesest on hea v√µi mitte - tasu saame alles m√§ngu l√µpus.

RL-i k√§igus teeme tavaliselt palju katseid. Iga katse ajal peame tasakaalustama seni √µpitud optimaalse strateegia j√§rgimise (**kasutamine**) ja uute v√µimalike seisundite uurimise (**uurimine**).

## OpenAI Gym

Suurep√§rane t√∂√∂riist RL-i jaoks on [OpenAI Gym](https://gym.openai.com/) - **simulatsioonikeskkond**, mis suudab simuleerida mitmesuguseid keskkondi, alates Atari m√§ngudest kuni f√º√ºsikani, mis on seotud posti tasakaalustamisega. See on √ºks populaarsemaid simulatsioonikeskkondi tugevdus√µppe algoritmide treenimiseks ja seda haldab [OpenAI](https://openai.com/).

> **Note**: K√µiki OpenAI Gym-i keskkondi saab vaadata [siin](https://gym.openai.com/envs/#classic_control).

## CartPole tasakaalustamine

T√µen√§oliselt olete n√§inud kaasaegseid tasakaalustusseadmeid, nagu *Segway* v√µi *g√ºroskootrid*. Need suudavad automaatselt tasakaalu hoida, kohandades oma rattaid vastavalt signaalile, mis tuleb kiirendusm√µ√µturilt v√µi g√ºroskoobilt. Selles jaotises √µpime lahendama sarnast probleemi - posti tasakaalustamist. See on sarnane olukorrale, kus tsirkuseartist peab tasakaalustama posti oma k√§el - kuid see tasakaalustamine toimub ainult √ºhes dimensioonis.

Lihtsustatud versioon tasakaalustamisest on tuntud kui **CartPole** probleem. CartPole maailmas on meil horisontaalne liugur, mis saab liikuda vasakule v√µi paremale, ja eesm√§rk on tasakaalustada vertikaalne post liuguri peal, kui see liigub.

<img alt="cartpole" src="../../../../../translated_images/cartpole.f52a67f27e058170c25efc1bca8375b60906570ea757fe8d7ef04ae8e53df29d.et.png" width="200"/>

Selle keskkonna loomiseks ja kasutamiseks on vaja paar rida Python koodi:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Iga keskkonda saab kasutada t√§pselt samamoodi:
* `env.reset` alustab uut katset
* `env.step` teostab simulatsiooni sammu. See v√µtab vastu **tegevuse** **tegevusruumist** ja tagastab **vaatluse** (vaatluste ruumist), samuti tasu ja l√µpetamise lipu.

√úlaltoodud n√§ites teeme igal sammul juhusliku tegevuse, mist√µttu katse eluiga on v√§ga l√ºhike:

![tasakaalustamata cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL algoritmi eesm√§rk on treenida mudelit - nn **poliitikat** &pi; -, mis tagastab tegevuse vastuseks antud seisundile. Samuti v√µime pidada poliitikat t√µen√§osuslikuks, st iga seisundi *s* ja tegevuse *a* puhul tagastab see t√µen√§osuse &pi;(*a*|*s*), et peaksime seisundis *s* tegema tegevuse *a*.

## Poliitika gradientide algoritm

K√µige ilmsem viis poliitika modelleerimiseks on luua n√§rviv√µrk, mis v√µtab sisendiks seisundid ja tagastab vastavad tegevused (v√µi pigem k√µigi tegevuste t√µen√§osused). Teatud m√µttes oleks see sarnane tavalise klassifikatsiooniprobleemiga, kuid peamine erinevus seisneb selles, et me ei tea ette, milliseid tegevusi peaksime igal sammul tegema.

Idee seisneb siin t√µen√§osuste hindamises. Loome **kumulatiivsete tasude** vektori, mis n√§itab meie kogutasu igal katse sammul. Samuti rakendame **tasu diskonteerimist**, korrutades varasemad tasud koefitsiendiga &gamma;=0.99, et v√§hendada varasemate tasude rolli. Seej√§rel tugevdame neid samme katse teekonnal, mis annavad suuremaid tasusid.

> Lisateavet poliitika gradientide algoritmi kohta ja selle rakendust n√§ete [n√§idisp√§evikus](CartPole-RL-TF.ipynb).

## N√§itleja-kriitik algoritm

Poliitika gradientide l√§henemise t√§iustatud versiooni nimetatakse **n√§itleja-kriitikuks**. Selle peamine idee seisneb selles, et n√§rviv√µrk treenitakse tagastama kahte asja:

* Poliitika, mis m√§√§rab, millist tegevust teha. Seda osa nimetatakse **n√§itlejaks**.
* Hinnang kogutasule, mida me v√µime selles seisundis saada - seda osa nimetatakse **kriitikuks**.

Teatud m√µttes sarnaneb see arhitektuur [GAN-iga](../../4-ComputerVision/10-GANs/README.md), kus meil on kaks v√µrku, mis treenitakse √ºksteise vastu. N√§itleja-kriitiku mudelis pakub n√§itleja v√§lja tegevuse, mida peame tegema, ja kriitik p√º√ºab olla kriitiline ning hinnata tulemust. Kuid meie eesm√§rk on treenida neid v√µrke √ºhtselt.

Kuna me teame nii tegelikke kumulatiivseid tasusid kui ka kriitiku poolt katse ajal tagastatud tulemusi, on suhteliselt lihtne luua kaotuse funktsioon, mis minimeerib nendevahelist erinevust. See annaks meile **kriitiku kaotuse**. **N√§itleja kaotuse** saame arvutada, kasutades sama l√§henemist nagu poliitika gradientide algoritmis.

P√§rast √ºhe neist algoritmidest k√§ivitamist v√µime oodata, et meie CartPole k√§itub j√§rgmiselt:

![tasakaalustav cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Harjutused: Poliitika gradientid ja n√§itleja-kriitik RL

J√§tka √µppimist j√§rgmistes p√§evikutes:

* [RL TensorFlow-s](CartPole-RL-TF.ipynb)
* [RL PyTorch-is](CartPole-RL-PyTorch.ipynb)

## Muud RL √ºlesanded

Tugevdus√µpe on t√§nap√§eval kiiresti kasvav uurimisvaldkond. M√µned huvitavad tugevdus√µppe n√§ited on:

* Arvuti √µpetamine m√§ngima **Atari m√§nge**. Selle probleemi keeruline osa on see, et meil ei ole lihtsat seisundit, mis oleks esitatud vektorina, vaid pigem ekraanipilt - ja peame kasutama CNN-i, et muuta see ekraanipilt tunnuste vektoriks v√µi eraldada tasu teave. Atari m√§ngud on saadaval Gym-is.
* Arvuti √µpetamine m√§ngima lauam√§nge, nagu male ja Go. Hiljuti treeniti tipptasemel programme, nagu **Alpha Zero**, nullist, kus kaks agenti m√§ngisid √ºksteise vastu ja parandasid end igal sammul.
* T√∂√∂stuses kasutatakse RL-i juhtimiss√ºsteemide loomiseks simulatsioonist. Teenus nimega [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) on spetsiaalselt selleks loodud.

## Kokkuv√µte

Oleme n√º√ºd √µppinud, kuidas treenida agente saavutama h√§id tulemusi lihtsalt tasu funktsiooni pakkumisega, mis m√§√§ratleb m√§ngu soovitud seisundi, ja andes neile v√µimaluse intelligentselt otsinguruumi uurida. Oleme edukalt proovinud kahte algoritmi ja saavutanud hea tulemuse suhteliselt l√ºhikese aja jooksul. Kuid see on alles teie RL-i teekonna algus ja peaksite kindlasti kaaluma eraldi kursuse v√µtmist, kui soovite s√ºgavamale minna.

## üöÄ V√§ljakutse

Uurige rakendusi, mis on loetletud jaotises "Muud RL √ºlesanded", ja proovige √ºhte neist rakendada!

## [J√§relloengu viktoriin](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## √úlevaade ja iseseisev √µppimine

Lisateavet klassikalise tugevdus√µppe kohta leiate meie [Masin√µppe algajatele √µppekavast](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Vaadake [seda suurep√§rast videot](https://www.youtube.com/watch?v=qv6UVOQ0F44), mis r√§√§gib, kuidas arvuti saab √µppida m√§ngima Super Mariot.

## √úlesanne: [Treeni Mountain Car](lab/README.md)

Teie eesm√§rk selle √ºlesande k√§igus oleks treenida teistsugust Gym keskkonda - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.