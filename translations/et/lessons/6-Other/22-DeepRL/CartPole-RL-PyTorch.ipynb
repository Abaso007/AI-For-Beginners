{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL-i treenimine Cartpole tasakaalustamiseks\n",
    "\n",
    "See märkmik on osa [AI algajatele mõeldud õppekavast](http://aka.ms/ai-beginners). See on inspireeritud [ametlikust PyTorch juhendist](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) ja [sellest Cartpole PyTorch implementatsioonist](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Selles näites kasutame RL-i, et treenida mudelit tasakaalustama posti kärul, mis saab liikuda vasakule ja paremale horisontaalsel skaalal. Simulatsiooni jaoks kasutame [OpenAI Gym](https://www.gymlibrary.ml/) keskkonda.\n",
    "\n",
    "> **Märkus**: Selle õppetunni koodi saab käivitada lokaalselt (nt Visual Studio Code'is), sel juhul avaneb simulatsioon uues aknas. Kui koodi käitatakse veebis, võib olla vaja teha koodis mõningaid muudatusi, nagu kirjeldatud [siin](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Alustame veendumaks, et Gym on installitud:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nüüd loome CartPole'i keskkonna ja vaatame, kuidas sellega töötada. Keskkonnal on järgmised omadused:\n",
    "\n",
    "* **Tegevusruum** on võimalik tegevuste kogum, mida saame simuleerimise igal sammul teha\n",
    "* **Vaatlusruum** on vaatluste ruum, mida saame teha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaatame, kuidas simulatsioon töötab. Järgmine tsükkel käivitab simulatsiooni, kuni `env.step` ei tagasta lõpetamise lippu `done`. Me valime juhuslikult tegevusi, kasutades `env.action_space.sample()`, mis tähendab, et eksperiment ebaõnnestub tõenäoliselt väga kiiresti (CartPole keskkond lõpetab, kui CartPole kiirus, asukoht või nurk ületavad teatud piirid).\n",
    "\n",
    "> Simulatsioon avaneb uues aknas. Koodi saab mitu korda käivitada ja vaadata, kuidas see käitub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa märkad, et vaatlused sisaldavad 4 numbrit. Need on:\n",
    "- Käru asukoht\n",
    "- Käru kiirus\n",
    "- Posti nurk\n",
    "- Posti pöörlemiskiirus\n",
    "\n",
    "`rew` on tasu, mida saame igal sammul. CartPole'i keskkonnas antakse iga simulatsioonisammu eest 1 punkt ja eesmärk on maksimeerida kogutasu, st aeg, mille jooksul CartPole suudab tasakaalu hoida ilma ümber kukkumata.\n",
    "\n",
    "Tugevdusõppe käigus on meie eesmärk treenida **poliitikat** $\\pi$, mis iga oleku $s$ puhul ütleb, millise tegevuse $a$ peaksime valima, seega sisuliselt $a = \\pi(s)$.\n",
    "\n",
    "Kui soovid tõenäosuslikku lahendust, võid mõelda poliitikast kui tegevuste tõenäosuste komplektist, st $\\pi(a|s)$ tähendaks tõenäosust, et peaksime olekus $s$ valima tegevuse $a$.\n",
    "\n",
    "## Poliitika gradientmeetod\n",
    "\n",
    "Lihtsaimas RL algoritmis, mida nimetatakse **poliitika gradientiks**, treenime närvivõrku ennustama järgmist tegevust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me treenime võrku, tehes palju katseid ja uuendades oma võrku pärast iga katset. Määratleme funktsiooni, mis viib läbi katse ja tagastab tulemused (nn **jälje**) - kõik olekud, tegevused (ja nende soovitatud tõenäosused) ning tasud:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saad käivitada ühe episoodi treenimata võrguga ja jälgida, et kogutasu (ehk episoodi pikkus) on väga madal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poliitikagradientide algoritmi üks keerulisi aspekte on **diskonteeritud tasude** kasutamine. Idee seisneb selles, et arvutame mängu iga sammu kogutasude vektori ja selle protsessi käigus diskonteerime varased tasud koefitsiendiga $gamma$. Samuti normaliseerime saadud vektori, kuna kasutame seda kaalu mõjutamiseks meie treeningus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nüüd alustame tegelikku treeningut! Me viime läbi 300 episoodi ja igas episoodis teeme järgmist:\n",
    "\n",
    "1. Käivitame eksperimendi ja kogume jälje\n",
    "1. Arvutame erinevuse (`gradients`) võetud tegevuste ja ennustatud tõenäosuste vahel. Mida väiksem on erinevus, seda kindlamad oleme, et oleme teinud õige tegevuse.\n",
    "1. Arvutame diskonteeritud tasud ja korrutame gradientid diskonteeritud tasudega - see tagab, et sammud, millel on suuremad tasud, avaldavad lõpptulemusele suuremat mõju kui madalama tasuga sammud.\n",
    "1. Meie närvivõrgu oodatavad sihttegevused võetakse osaliselt jooksu ajal ennustatud tõenäosustest ja osaliselt arvutatud gradientidest. Kasutame `alpha` parameetrit, et määrata, millises ulatuses gradientid ja tasud arvesse võetakse - seda nimetatakse *õppemääraks* tugevdamise algoritmis.\n",
    "1. Lõpuks treenime oma võrku olekute ja oodatavate tegevuste põhjal ning kordame protsessi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nüüd käivitame episoodi koos renderdamisega, et tulemust näha:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loodetavasti näete, et post suudab nüüd üsna hästi tasakaalu hoida!\n",
    "\n",
    "## Näitleja-Kriitik mudel\n",
    "\n",
    "Näitleja-Kriitik mudel on poliitika gradientide edasiarendus, kus me loome tehisnärvivõrgu, et õppida nii poliitikat kui ka hinnangulisi tasusid. Võrgul on kaks väljundit (või võite seda vaadelda kui kahte eraldi võrku):\n",
    "* **Näitleja** soovitab tegevuse, mida ette võtta, andes meile oleku tõenäosusjaotuse, nagu poliitika gradientide mudelis.\n",
    "* **Kriitik** hindab, milline tasu võiks nendest tegevustest tulla. See tagastab tuleviku koguhinnangulised tasud antud olekus.\n",
    "\n",
    "Määratleme sellise mudeli:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me peaksime veidi muutma oma `discounted_rewards` ja `run_episode` funktsioone:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nüüd käivitame peamise treeningtsükli. Kasutame käsitsi võrgu treenimise protsessi, arvutades sobivad kaotusfunktsioonid ja uuendades võrgu parameetreid:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lõpuks sulgeme keskkonna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peamine mõte\n",
    "\n",
    "Selles demos tutvusime kahe tugeva õppimise algoritmiga: lihtsa poliitika gradienti ja keerukama näitleja-kriitikuga. Nagu näha, töötavad need algoritmid abstraktsete oleku, tegevuse ja tasu mõistetega – seega saab neid rakendada väga erinevates keskkondades.\n",
    "\n",
    "Tugevdatud õppimine võimaldab meil õppida parimat strateegiat probleemi lahendamiseks, tuginedes ainult lõplikule tasule. See, et me ei vaja märgistatud andmekogumeid, võimaldab meil simuleerida olukordi korduvalt, et oma mudeleid optimeerida. Siiski on RL-is endiselt palju väljakutseid, mida saate avastada, kui otsustate süveneda sellesse põnevasse tehisintellekti valdkonda.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Lahtiütlus**:  \nSee dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-10-11T13:00:45+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "et"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}