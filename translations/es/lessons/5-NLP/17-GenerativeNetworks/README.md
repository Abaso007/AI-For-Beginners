<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-24T09:12:44+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "es"
}
-->
# Redes generativas

## [Cuestionario previo a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Las Redes Neuronales Recurrentes (RNNs) y sus variantes con celdas de compuerta, como las Celdas de Memoria a Largo Plazo (LSTMs) y las Unidades Recurrentes de Compuerta (GRUs), proporcionan un mecanismo para modelar el lenguaje, ya que pueden aprender el orden de las palabras y ofrecer predicciones para la siguiente palabra en una secuencia. Esto nos permite usar RNNs para **tareas generativas**, como la generaci√≥n de texto ordinario, la traducci√≥n autom√°tica e incluso la generaci√≥n de subt√≠tulos para im√°genes.

> ‚úÖ Piensa en todas las veces que te has beneficiado de tareas generativas, como la autocompletaci√≥n de texto mientras escribes. Investiga tus aplicaciones favoritas para ver si han utilizado RNNs.

En la arquitectura de RNN que discutimos en la unidad anterior, cada unidad RNN produc√≠a el siguiente estado oculto como salida. Sin embargo, tambi√©n podemos agregar otra salida a cada unidad recurrente, lo que nos permitir√≠a generar una **secuencia** (que tiene la misma longitud que la secuencia original). Adem√°s, podemos usar unidades RNN que no aceptan una entrada en cada paso, y simplemente toman un vector de estado inicial y luego producen una secuencia de salidas.

Esto permite diferentes arquitecturas neuronales, como se muestra en la imagen a continuaci√≥n:

![Imagen que muestra patrones comunes de redes neuronales recurrentes.](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/unreasonable-effectiveness-of-rnn.jpg)

> Imagen del art√≠culo [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) por [Andrej Karpaty](http://karpathy.github.io/)

* **Uno a uno** es una red neuronal tradicional con una entrada y una salida.
* **Uno a muchos** es una arquitectura generativa que acepta un valor de entrada y genera una secuencia de valores de salida. Por ejemplo, si queremos entrenar una red de **generaci√≥n de subt√≠tulos para im√°genes** que produzca una descripci√≥n textual de una imagen, podemos tomar una imagen como entrada, pasarla por una CNN para obtener su estado oculto y luego usar una cadena recurrente para generar los subt√≠tulos palabra por palabra.
* **Muchos a uno** corresponde a las arquitecturas RNN que describimos en la unidad anterior, como la clasificaci√≥n de texto.
* **Muchos a muchos**, o **secuencia a secuencia**, corresponde a tareas como la **traducci√≥n autom√°tica**, donde primero una RNN recopila toda la informaci√≥n de la secuencia de entrada en el estado oculto, y otra cadena RNN despliega este estado en la secuencia de salida.

En esta unidad, nos enfocaremos en modelos generativos simples que nos ayuden a generar texto. Para simplificar, utilizaremos tokenizaci√≥n a nivel de caracteres.

Entrenaremos esta RNN para generar texto paso a paso. En cada paso, tomaremos una secuencia de caracteres de longitud `nchars` y pediremos a la red que genere el siguiente car√°cter de salida para cada car√°cter de entrada:

![Imagen que muestra un ejemplo de generaci√≥n de la palabra 'HELLO' con una RNN.](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate.png)

Cuando generamos texto (durante la inferencia), comenzamos con un **indicador inicial**, que se pasa por las celdas RNN para generar su estado intermedio, y luego desde este estado comienza la generaci√≥n. Generamos un car√°cter a la vez y pasamos el estado y el car√°cter generado a otra celda RNN para generar el siguiente, hasta que generamos suficientes caracteres.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Imagen del autor

## ‚úçÔ∏è Ejercicios: Redes generativas

Contin√∫a tu aprendizaje en los siguientes cuadernos:

* [Redes generativas con PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Redes generativas con TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Generaci√≥n de texto suave y temperatura

La salida de cada celda RNN es una distribuci√≥n de probabilidad de caracteres. Si siempre tomamos el car√°cter con la mayor probabilidad como el siguiente car√°cter en el texto generado, el texto puede volverse "c√≠clico", repitiendo las mismas secuencias de caracteres una y otra vez, como en este ejemplo:

```
today of the second the company and a second the company ...
```

Sin embargo, si observamos la distribuci√≥n de probabilidad para el siguiente car√°cter, podr√≠a ser que la diferencia entre algunas de las probabilidades m√°s altas no sea muy grande, por ejemplo, un car√°cter puede tener una probabilidad de 0.2 y otro de 0.19, etc. Por ejemplo, al buscar el siguiente car√°cter en la secuencia '*play*', el siguiente car√°cter podr√≠a ser igualmente un espacio o **e** (como en la palabra *player*).

Esto nos lleva a la conclusi√≥n de que no siempre es "justo" seleccionar el car√°cter con mayor probabilidad, ya que elegir el segundo m√°s alto a√∫n podr√≠a llevarnos a texto significativo. Es m√°s sabio **muestrear** caracteres de la distribuci√≥n de probabilidad dada por la salida de la red. Tambi√©n podemos usar un par√°metro, **temperatura**, que suaviza la distribuci√≥n de probabilidad si queremos agregar m√°s aleatoriedad, o la hace m√°s pronunciada si queremos ce√±irnos m√°s a los caracteres con mayor probabilidad.

Explora c√≥mo se implementa esta generaci√≥n de texto suave en los cuadernos vinculados anteriormente.

## Conclusi√≥n

Aunque la generaci√≥n de texto puede ser √∫til por s√≠ misma, los mayores beneficios provienen de la capacidad de generar texto utilizando RNNs a partir de alg√∫n vector de caracter√≠sticas inicial. Por ejemplo, la generaci√≥n de texto se utiliza como parte de la traducci√≥n autom√°tica (secuencia a secuencia, en este caso el vector de estado del *codificador* se utiliza para generar o *decodificar* el mensaje traducido), o para generar descripciones textuales de una imagen (en cuyo caso el vector de caracter√≠sticas provendr√≠a de un extractor CNN).

## üöÄ Desaf√≠o

Toma algunas lecciones en Microsoft Learn sobre este tema:

* Generaci√≥n de texto con [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Revisi√≥n y autoestudio

Aqu√≠ tienes algunos art√≠culos para ampliar tu conocimiento:

* Diferentes enfoques para la generaci√≥n de texto con Markov Chain, LSTM y GPT-2: [art√≠culo](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Ejemplo de generaci√≥n de texto en la [documentaci√≥n de Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Tarea](lab/README.md)

Hemos visto c√≥mo generar texto car√°cter por car√°cter. En el laboratorio, explorar√°s la generaci√≥n de texto a nivel de palabras.

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.