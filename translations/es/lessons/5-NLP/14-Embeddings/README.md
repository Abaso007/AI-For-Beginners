<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b708c9b85b833864c73c6281f1e6b96e",
  "translation_date": "2025-09-23T12:14:36+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "es"
}
-->
# Embeddings

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Al entrenar clasificadores basados en BoW o TF/IDF, trabaj√°bamos con vectores de bolsa de palabras de alta dimensi√≥n con longitud `vocab_size`, y convert√≠amos expl√≠citamente de vectores de representaci√≥n posicional de baja dimensi√≥n a representaciones dispersas de una sola posici√≥n activa (one-hot). Sin embargo, esta representaci√≥n one-hot no es eficiente en t√©rminos de memoria. Adem√°s, cada palabra se trata de manera independiente, es decir, los vectores codificados en one-hot no expresan ninguna similitud sem√°ntica entre palabras.

La idea de **embedding** es representar las palabras mediante vectores densos de menor dimensi√≥n, que de alguna manera reflejen el significado sem√°ntico de una palabra. M√°s adelante discutiremos c√≥mo construir embeddings de palabras significativos, pero por ahora pensemos en los embeddings como una forma de reducir la dimensionalidad de un vector de palabras.

Entonces, la capa de embedding tomar√≠a una palabra como entrada y producir√≠a un vector de salida con un tama√±o especificado `embedding_size`. En cierto sentido, es muy similar a una capa `Linear`, pero en lugar de tomar un vector codificado en one-hot, podr√° tomar un n√∫mero de palabra como entrada, permiti√©ndonos evitar la creaci√≥n de grandes vectores codificados en one-hot.

Al usar una capa de embedding como la primera capa en nuestra red clasificadora, podemos cambiar de un modelo de bolsa de palabras a un modelo de **embedding bag**, donde primero convertimos cada palabra en nuestro texto en su correspondiente embedding, y luego calculamos alguna funci√≥n de agregaci√≥n sobre todos esos embeddings, como `sum`, `average` o `max`.  

![Imagen que muestra un clasificador con embeddings para cinco palabras de una secuencia.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.es.png)

> Imagen por el autor

## ‚úçÔ∏è Ejercicios: Embeddings

Contin√∫a tu aprendizaje en los siguientes notebooks:
* [Embeddings con PyTorch](EmbeddingsPyTorch.ipynb)
* [Embeddings con TensorFlow](EmbeddingsTF.ipynb)

## Embeddings Sem√°nticos: Word2Vec

Aunque la capa de embedding aprendi√≥ a mapear palabras a representaciones vectoriales, esta representaci√≥n no necesariamente tiene mucho significado sem√°ntico. Ser√≠a ideal aprender una representaci√≥n vectorial tal que palabras similares o sin√≥nimos correspondan a vectores cercanos entre s√≠ en t√©rminos de alguna distancia vectorial (por ejemplo, distancia euclidiana).

Para lograr esto, necesitamos preentrenar nuestro modelo de embedding en una gran colecci√≥n de texto de una manera espec√≠fica. Una forma de entrenar embeddings sem√°nticos se llama [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Se basa en dos arquitecturas principales que se utilizan para producir una representaci√≥n distribuida de palabras:

 - **Continuous bag-of-words** (CBoW) ‚Äî en esta arquitectura, entrenamos el modelo para predecir una palabra a partir del contexto circundante. Dado el ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, el objetivo del modelo es predecir $W_0$ a partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** es opuesto a CBoW. El modelo utiliza una ventana de palabras de contexto circundante para predecir la palabra actual.

CBoW es m√°s r√°pido, mientras que skip-gram es m√°s lento, pero hace un mejor trabajo representando palabras poco frecuentes.

![Imagen que muestra los algoritmos CBoW y Skip-Gram para convertir palabras en vectores.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.es.png)

> Imagen tomada de [este art√≠culo](https://arxiv.org/pdf/1301.3781.pdf)

Los embeddings preentrenados de Word2Vec (as√≠ como otros modelos similares, como GloVe) tambi√©n pueden usarse en lugar de la capa de embedding en redes neuronales. Sin embargo, debemos lidiar con los vocabularios, porque el vocabulario utilizado para preentrenar Word2Vec/GloVe probablemente ser√° diferente del vocabulario en nuestro corpus de texto. Revisa los notebooks mencionados anteriormente para ver c√≥mo se puede resolver este problema.

## Embeddings Contextuales

Una limitaci√≥n clave de las representaciones tradicionales de embeddings preentrenados como Word2Vec es el problema de la desambiguaci√≥n del sentido de las palabras. Aunque los embeddings preentrenados pueden capturar parte del significado de las palabras en contexto, todos los posibles significados de una palabra se codifican en el mismo embedding. Esto puede causar problemas en los modelos posteriores, ya que muchas palabras, como la palabra 'play', tienen diferentes significados dependiendo del contexto en el que se usan.

Por ejemplo, la palabra 'play' en estas dos oraciones tiene significados bastante diferentes:

- Fui a una **obra** en el teatro.
- John quiere **jugar** con sus amigos.

Los embeddings preentrenados mencionados anteriormente representan ambos significados de la palabra 'play' en el mismo embedding. Para superar esta limitaci√≥n, necesitamos construir embeddings basados en el **modelo de lenguaje**, que se entrena en un gran corpus de texto y *sabe* c√≥mo las palabras pueden combinarse en diferentes contextos. Discutir embeddings contextuales est√° fuera del alcance de este tutorial, pero volveremos a ellos cuando hablemos de modelos de lenguaje m√°s adelante en el curso.

## Conclusi√≥n

En esta lecci√≥n, descubriste c√≥mo construir y usar capas de embedding en TensorFlow y PyTorch para reflejar mejor los significados sem√°nticos de las palabras.

## üöÄ Desaf√≠o

Word2Vec se ha utilizado para algunas aplicaciones interesantes, incluyendo la generaci√≥n de letras de canciones y poes√≠a. Echa un vistazo a [este art√≠culo](https://www.politetype.com/blog/word2vec-color-poems) que explica c√≥mo el autor utiliz√≥ Word2Vec para generar poes√≠a. Mira tambi√©n [este video de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) para descubrir una explicaci√≥n diferente de esta t√©cnica. Luego intenta aplicar estas t√©cnicas a tu propio corpus de texto, quiz√°s obtenido de Kaggle.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Revisi√≥n y Autoestudio

Lee este art√≠culo sobre Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Asignaci√≥n: Notebooks](assignment.md)

---

