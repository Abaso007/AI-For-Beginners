<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-24T09:12:21+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "es"
}
-->
# Embeddings

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Cuando entrenamos clasificadores basados en BoW o TF/IDF, trabajamos con vectores de bolsa de palabras de alta dimensi√≥n con una longitud de `vocab_size`, y est√°bamos convirtiendo expl√≠citamente de vectores de representaci√≥n posicional de baja dimensi√≥n a representaciones dispersas de una sola posici√≥n activa (one-hot). Sin embargo, esta representaci√≥n de una sola posici√≥n activa no es eficiente en t√©rminos de memoria. Adem√°s, cada palabra se trata de manera independiente, es decir, los vectores codificados en una sola posici√≥n activa no expresan ninguna similitud sem√°ntica entre palabras.

La idea de **embedding** es representar las palabras mediante vectores densos de menor dimensi√≥n, que de alguna manera reflejen el significado sem√°ntico de una palabra. M√°s adelante discutiremos c√≥mo construir embeddings de palabras significativos, pero por ahora pensemos en los embeddings como una forma de reducir la dimensionalidad de un vector de palabras.

Entonces, la capa de embedding tomar√≠a una palabra como entrada y producir√≠a un vector de salida con un tama√±o especificado por `embedding_size`. En cierto sentido, es muy similar a una capa `Linear`, pero en lugar de tomar un vector codificado en una sola posici√≥n activa, podr√° tomar un n√∫mero de palabra como entrada, permiti√©ndonos evitar la creaci√≥n de grandes vectores codificados en una sola posici√≥n activa.

Al usar una capa de embedding como la primera capa en nuestra red de clasificaci√≥n, podemos cambiar de un modelo de bolsa de palabras a un modelo de **embedding bag**, donde primero convertimos cada palabra en nuestro texto en su embedding correspondiente, y luego calculamos alguna funci√≥n de agregaci√≥n sobre todos esos embeddings, como `sum`, `average` o `max`.  

![Imagen que muestra un clasificador con embeddings para cinco palabras en una secuencia.](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)

> Imagen por el autor

## ‚úçÔ∏è Ejercicios: Embeddings

Contin√∫a tu aprendizaje en los siguientes notebooks:
* [Embeddings con PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Embeddings con TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Embeddings Sem√°nticos: Word2Vec

Aunque la capa de embedding aprendi√≥ a mapear palabras a representaciones vectoriales, esta representaci√≥n no necesariamente tiene mucho significado sem√°ntico. Ser√≠a ideal aprender una representaci√≥n vectorial tal que palabras similares o sin√≥nimos correspondan a vectores que est√©n cerca unos de otros en t√©rminos de alguna distancia vectorial (por ejemplo, distancia euclidiana).

Para lograr esto, necesitamos preentrenar nuestro modelo de embedding en una gran colecci√≥n de texto de una manera espec√≠fica. Una forma de entrenar embeddings sem√°nticos se llama [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Se basa en dos arquitecturas principales que se utilizan para producir una representaci√≥n distribuida de palabras:

 - **Bolsa de palabras continua** (CBoW) ‚Äî en esta arquitectura, entrenamos el modelo para predecir una palabra a partir del contexto circundante. Dado el ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, el objetivo del modelo es predecir $W_0$ a partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Skip-gram continuo** es opuesto a CBoW. El modelo utiliza una ventana de palabras de contexto circundantes para predecir la palabra actual.

CBoW es m√°s r√°pido, mientras que skip-gram es m√°s lento, pero hace un mejor trabajo representando palabras poco frecuentes.

![Imagen que muestra los algoritmos CBoW y Skip-Gram para convertir palabras en vectores.](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> Imagen de [este art√≠culo](https://arxiv.org/pdf/1301.3781.pdf)

Los embeddings preentrenados de Word2Vec (as√≠ como otros modelos similares, como GloVe) tambi√©n pueden ser utilizados en lugar de la capa de embedding en redes neuronales. Sin embargo, necesitamos manejar los vocabularios, porque el vocabulario utilizado para preentrenar Word2Vec/GloVe probablemente difiera del vocabulario en nuestro corpus de texto. Revisa los notebooks mencionados anteriormente para ver c√≥mo se puede resolver este problema.

## Embeddings Contextuales

Una limitaci√≥n clave de las representaciones tradicionales de embeddings preentrenados como Word2Vec es el problema de la desambiguaci√≥n del sentido de las palabras. Aunque los embeddings preentrenados pueden capturar parte del significado de las palabras en contexto, cada posible significado de una palabra se codifica en el mismo embedding. Esto puede causar problemas en los modelos posteriores, ya que muchas palabras, como la palabra 'play', tienen diferentes significados dependiendo del contexto en el que se utilizan.

Por ejemplo, la palabra 'play' en estas dos oraciones tiene significados bastante diferentes:

- Fui a una **obra** en el teatro.
- John quiere **jugar** con sus amigos.

Los embeddings preentrenados mencionados anteriormente representan ambos significados de la palabra 'play' en el mismo embedding. Para superar esta limitaci√≥n, necesitamos construir embeddings basados en el **modelo de lenguaje**, que se entrena en un gran corpus de texto y *sabe* c√≥mo las palabras pueden combinarse en diferentes contextos. Discutir embeddings contextuales est√° fuera del alcance de este tutorial, pero volveremos a ellos cuando hablemos de modelos de lenguaje m√°s adelante en el curso.

## Conclusi√≥n

En esta lecci√≥n, descubriste c√≥mo construir y usar capas de embedding en TensorFlow y PyTorch para reflejar mejor los significados sem√°nticos de las palabras.

## üöÄ Desaf√≠o

Word2Vec se ha utilizado para algunas aplicaciones interesantes, incluyendo la generaci√≥n de letras de canciones y poes√≠a. Echa un vistazo a [este art√≠culo](https://www.politetype.com/blog/word2vec-color-poems) que explica c√≥mo el autor utiliz√≥ Word2Vec para generar poes√≠a. Mira [este video de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) tambi√©n para descubrir una explicaci√≥n diferente de esta t√©cnica. Luego intenta aplicar estas t√©cnicas a tu propio corpus de texto, quiz√°s obtenido de Kaggle.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Revisi√≥n y Autoestudio

Lee este art√≠culo sobre Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Asignaci√≥n: Notebooks](assignment.md)

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.