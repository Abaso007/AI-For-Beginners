<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-24T09:15:35+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "es"
}
-->
# Autoencoders

Al entrenar redes neuronales convolucionales (CNNs), uno de los problemas es que necesitamos una gran cantidad de datos etiquetados. En el caso de la clasificaci√≥n de im√°genes, necesitamos separar las im√°genes en diferentes clases, lo cual requiere un esfuerzo manual.

## [Cuestionario previo a la lecci√≥n](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/109)

Sin embargo, podr√≠amos querer usar datos sin procesar (no etiquetados) para entrenar extractores de caracter√≠sticas de CNN, lo que se conoce como **aprendizaje auto-supervisado**. En lugar de etiquetas, utilizaremos las im√°genes de entrenamiento tanto como entrada como salida de la red. La idea principal de un **autoencoder** es que tendremos una **red codificadora** que convierte la imagen de entrada en alg√∫n **espacio latente** (normalmente es solo un vector de menor tama√±o), y luego una **red decodificadora**, cuyo objetivo ser√° reconstruir la imagen original.

> ‚úÖ Un [autoencoder](https://wikipedia.org/wiki/Autoencoder) es "un tipo de red neuronal artificial utilizada para aprender codificaciones eficientes de datos no etiquetados."

Dado que estamos entrenando un autoencoder para capturar la mayor cantidad de informaci√≥n posible de la imagen original para una reconstrucci√≥n precisa, la red intenta encontrar la mejor **representaci√≥n** de las im√°genes de entrada para capturar su significado.

![Diagrama de Autoencoder](../../../../../lessons/4-ComputerVision/09-Autoencoders/images/autoencoder_schema.jpg)

> Imagen del [blog de Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Escenarios para usar Autoencoders

Aunque reconstruir im√°genes originales no parece √∫til por s√≠ mismo, hay algunos escenarios donde los autoencoders son especialmente √∫tiles:

* **Reducir la dimensi√≥n de las im√°genes para visualizaci√≥n** o **entrenar representaciones de im√°genes**. Generalmente, los autoencoders ofrecen mejores resultados que PCA, porque tienen en cuenta la naturaleza espacial de las im√°genes y las caracter√≠sticas jer√°rquicas.
* **Eliminaci√≥n de ruido**, es decir, eliminar el ruido de la imagen. Como el ruido contiene mucha informaci√≥n in√∫til, el autoencoder no puede ajustarlo todo en un espacio latente relativamente peque√±o, y por lo tanto captura solo la parte importante de la imagen. Al entrenar eliminadores de ruido, comenzamos con im√°genes originales y usamos im√°genes con ruido artificialmente a√±adido como entrada para el autoencoder.
* **Superresoluci√≥n**, aumentar la resoluci√≥n de las im√°genes. Comenzamos con im√°genes de alta resoluci√≥n y usamos la imagen con menor resoluci√≥n como entrada del autoencoder.
* **Modelos generativos**. Una vez que entrenamos el autoencoder, la parte decodificadora puede usarse para crear nuevos objetos a partir de vectores latentes aleatorios.

## Autoencoders Variacionales (VAE)

Los autoencoders tradicionales reducen la dimensi√≥n de los datos de entrada de alguna manera, identificando las caracter√≠sticas importantes de las im√°genes de entrada. Sin embargo, los vectores latentes a menudo no tienen mucho sentido. En otras palabras, tomando como ejemplo el conjunto de datos MNIST, identificar qu√© d√≠gitos corresponden a diferentes vectores latentes no es una tarea sencilla, porque vectores latentes cercanos no necesariamente corresponden a los mismos d√≠gitos.

Por otro lado, para entrenar modelos *generativos* es mejor tener cierta comprensi√≥n del espacio latente. Esta idea nos lleva al **autoencoder variacional** (VAE).

El VAE es un autoencoder que aprende a predecir una *distribuci√≥n estad√≠stica* de los par√°metros latentes, llamada **distribuci√≥n latente**. Por ejemplo, podr√≠amos querer que los vectores latentes se distribuyan normalmente con una media z<sub>mean</sub> y una desviaci√≥n est√°ndar z<sub>sigma</sub> (tanto la media como la desviaci√≥n est√°ndar son vectores de alguna dimensionalidad d). El codificador en el VAE aprende a predecir esos par√°metros, y luego el decodificador toma un vector aleatorio de esta distribuci√≥n para reconstruir el objeto.

En resumen:

 * A partir del vector de entrada, predecimos `z_mean` y `z_log_sigma` (en lugar de predecir directamente la desviaci√≥n est√°ndar, predecimos su logaritmo).
 * Muestreamos un vector `sample` de la distribuci√≥n N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>)).
 * El decodificador intenta decodificar la imagen original usando `sample` como vector de entrada.

 <img src="images/vae.png" width="50%">

> Imagen de [este art√≠culo](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) por Isaak Dykeman

Los autoencoders variacionales utilizan una funci√≥n de p√©rdida compleja que consta de dos partes:

* **P√©rdida de reconstrucci√≥n**, que es la funci√≥n de p√©rdida que muestra qu√© tan cerca est√° una imagen reconstruida del objetivo (puede ser el Error Cuadr√°tico Medio, o MSE). Es la misma funci√≥n de p√©rdida que en los autoencoders normales.
* **P√©rdida KL**, que asegura que las distribuciones de las variables latentes se mantengan cercanas a una distribuci√≥n normal. Se basa en la noci√≥n de [divergencia de Kullback-Leibler](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained), una m√©trica para estimar qu√© tan similares son dos distribuciones estad√≠sticas.

Una ventaja importante de los VAEs es que nos permiten generar nuevas im√°genes con relativa facilidad, porque sabemos de qu√© distribuci√≥n muestrear los vectores latentes. Por ejemplo, si entrenamos un VAE con un vector latente 2D en MNIST, podemos variar los componentes del vector latente para obtener diferentes d√≠gitos:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Imagen por [Dmitry Soshnikov](http://soshnikov.com)

Observa c√≥mo las im√°genes se mezclan entre s√≠, a medida que comenzamos a obtener vectores latentes de diferentes porciones del espacio de par√°metros latentes. Tambi√©n podemos visualizar este espacio en 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Imagen por [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Ejercicios: Autoencoders

Aprende m√°s sobre autoencoders en estos notebooks correspondientes:

* [Autoencoders en TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [Autoencoders en PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Propiedades de los Autoencoders

* **Espec√≠ficos de los datos** - solo funcionan bien con el tipo de im√°genes con las que han sido entrenados. Por ejemplo, si entrenamos una red de superresoluci√≥n en flores, no funcionar√° bien en retratos. Esto se debe a que la red puede producir im√°genes de mayor resoluci√≥n tomando detalles finos de las caracter√≠sticas aprendidas del conjunto de datos de entrenamiento.
* **Con p√©rdida** - la imagen reconstruida no es igual a la imagen original. La naturaleza de la p√©rdida est√° definida por la *funci√≥n de p√©rdida* utilizada durante el entrenamiento.
* Funciona con **datos no etiquetados**.

## [Cuestionario posterior a la lecci√≥n](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/209)

## Conclusi√≥n

En esta lecci√≥n, aprendiste sobre los diversos tipos de autoencoders disponibles para el cient√≠fico de IA. Aprendiste c√≥mo construirlos y c√≥mo usarlos para reconstruir im√°genes. Tambi√©n aprendiste sobre el VAE y c√≥mo usarlo para generar nuevas im√°genes.

## üöÄ Desaf√≠o

En esta lecci√≥n, aprendiste sobre el uso de autoencoders para im√°genes. ¬°Pero tambi√©n pueden usarse para m√∫sica! Explora el proyecto [MusicVAE](https://magenta.tensorflow.org/music-vae) del proyecto Magenta, que utiliza autoencoders para aprender a reconstruir m√∫sica. Realiza algunos [experimentos](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) con esta biblioteca para ver qu√© puedes crear.

## [Cuestionario posterior a la lecci√≥n](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## Revisi√≥n y Autoestudio

Para referencia, lee m√°s sobre autoencoders en estos recursos:

* [Construyendo Autoencoders en Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Art√≠culo en NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Autoencoders Variacionales Explicados](https://kvfrans.com/variational-autoencoders-explained/)
* [Autoencoders Variacionales Condicionales](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Tarea

Al final de [este notebook usando TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb), encontrar√°s una 'tarea': √∫sala como tu asignaci√≥n.

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.