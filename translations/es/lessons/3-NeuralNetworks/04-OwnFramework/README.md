<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-24T09:22:01+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "es"
}
-->
# Introducci√≥n a Redes Neuronales. Perceptr√≥n Multicapa

En la secci√≥n anterior, aprendiste sobre el modelo m√°s simple de red neuronal: el perceptr√≥n de una sola capa, un modelo lineal de clasificaci√≥n binaria.

En esta secci√≥n, extenderemos este modelo a un marco m√°s flexible, permiti√©ndonos:

* realizar **clasificaci√≥n multiclase** adem√°s de clasificaci√≥n binaria  
* resolver **problemas de regresi√≥n** adem√°s de clasificaci√≥n  
* separar clases que no son linealmente separables  

Tambi√©n desarrollaremos nuestro propio marco modular en Python que nos permitir√° construir diferentes arquitecturas de redes neuronales.

## [Cuestionario previo a la lecci√≥n](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalizaci√≥n del Aprendizaje Autom√°tico

Comencemos formalizando el problema de Aprendizaje Autom√°tico. Supongamos que tenemos un conjunto de datos de entrenamiento **X** con etiquetas **Y**, y necesitamos construir un modelo *f* que haga predicciones lo m√°s precisas posible. La calidad de las predicciones se mide mediante la **funci√≥n de p√©rdida** ‚Ñí. Las siguientes funciones de p√©rdida se utilizan con frecuencia:

* Para problemas de regresi√≥n, cuando necesitamos predecir un n√∫mero, podemos usar el **error absoluto** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, o el **error cuadr√°tico** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>  
* Para clasificaci√≥n, usamos la **p√©rdida 0-1** (que es esencialmente lo mismo que la **precisi√≥n** del modelo) o la **p√©rdida log√≠stica**.

Para un perceptr√≥n de una sola capa, la funci√≥n *f* se defin√≠a como una funci√≥n lineal *f(x)=wx+b* (donde *w* es la matriz de pesos, *x* es el vector de caracter√≠sticas de entrada, y *b* es el vector de sesgo). Para diferentes arquitecturas de redes neuronales, esta funci√≥n puede tomar una forma m√°s compleja.

> En el caso de clasificaci√≥n, a menudo es deseable obtener probabilidades de las clases correspondientes como salida de la red. Para convertir n√∫meros arbitrarios en probabilidades (por ejemplo, para normalizar la salida), a menudo usamos la funci√≥n **softmax** œÉ, y la funci√≥n *f* se convierte en *f(x)=œÉ(wx+b)*.

En la definici√≥n de *f* anterior, *w* y *b* se denominan **par√°metros** Œ∏=‚ü®*w,b*‚ü©. Dado el conjunto de datos ‚ü®**X**,**Y**‚ü©, podemos calcular un error general en todo el conjunto de datos como una funci√≥n de los par√°metros Œ∏.

> ‚úÖ **El objetivo del entrenamiento de redes neuronales es minimizar el error variando los par√°metros Œ∏**

## Optimizaci√≥n por Descenso de Gradiente

Existe un m√©todo bien conocido de optimizaci√≥n de funciones llamado **descenso de gradiente**. La idea es que podemos calcular una derivada (en el caso multidimensional llamada **gradiente**) de la funci√≥n de p√©rdida con respecto a los par√°metros, y variar los par√°metros de tal manera que el error disminuya. Esto se puede formalizar de la siguiente manera:

* Inicializar los par√°metros con algunos valores aleatorios w<sup>(0)</sup>, b<sup>(0)</sup>  
* Repetir el siguiente paso muchas veces:  
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw  
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb  

Durante el entrenamiento, se supone que los pasos de optimizaci√≥n se calculan considerando todo el conjunto de datos (recuerda que la p√©rdida se calcula como una suma a trav√©s de todas las muestras de entrenamiento). Sin embargo, en la pr√°ctica tomamos peque√±as porciones del conjunto de datos llamadas **minilotes**, y calculamos los gradientes bas√°ndonos en un subconjunto de datos. Debido a que el subconjunto se toma aleatoriamente cada vez, este m√©todo se llama **descenso de gradiente estoc√°stico** (SGD).

## Perceptrones Multicapa y Retropropagaci√≥n

Una red de una sola capa, como hemos visto anteriormente, es capaz de clasificar clases linealmente separables. Para construir un modelo m√°s rico, podemos combinar varias capas de la red. Matem√°ticamente, esto significar√≠a que la funci√≥n *f* tendr√≠a una forma m√°s compleja y se calcular√≠a en varios pasos:  
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>  
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>  
* f = œÉ(z<sub>2</sub>)  

Aqu√≠, Œ± es una **funci√≥n de activaci√≥n no lineal**, œÉ es una funci√≥n softmax, y los par√°metros Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

El algoritmo de descenso de gradiente permanecer√≠a igual, pero ser√≠a m√°s dif√≠cil calcular los gradientes. Dado el principio de la regla de la cadena, podemos calcular las derivadas como:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)  
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)  

> ‚úÖ La regla de la cadena se utiliza para calcular las derivadas de la funci√≥n de p√©rdida con respecto a los par√°metros.

Nota que la parte m√°s a la izquierda de todas estas expresiones es la misma, y por lo tanto podemos calcular las derivadas de manera eficiente comenzando desde la funci√≥n de p√©rdida y yendo "hacia atr√°s" a trav√©s del grafo computacional. Por lo tanto, el m√©todo de entrenamiento de un perceptr√≥n multicapa se llama **retropropagaci√≥n**, o 'backprop'.

<img alt="grafo computacional" src="images/ComputeGraphGrad.png"/>

> TODO: cita de la imagen

> ‚úÖ Cubriremos la retropropagaci√≥n con mucho m√°s detalle en nuestro ejemplo en el notebook.

## Conclusi√≥n

En esta lecci√≥n, hemos construido nuestra propia biblioteca de redes neuronales y la hemos utilizado para una tarea simple de clasificaci√≥n bidimensional.

## üöÄ Desaf√≠o

En el notebook adjunto, implementar√°s tu propio marco para construir y entrenar perceptrones multicapa. Podr√°s ver en detalle c√≥mo operan las redes neuronales modernas.

Contin√∫a con el notebook [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) y trabaja en √©l.

## [Cuestionario posterior a la lecci√≥n](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Revisi√≥n y Autoestudio

La retropropagaci√≥n es un algoritmo com√∫n en IA y ML, vale la pena estudiarlo [en m√°s detalle](https://wikipedia.org/wiki/Backpropagation).

## [Tarea](lab/README.md)

En este laboratorio, se te pide que uses el marco que construiste en esta lecci√≥n para resolver la clasificaci√≥n de d√≠gitos escritos a mano de MNIST.

* [Instrucciones](lab/README.md)  
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)  

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.