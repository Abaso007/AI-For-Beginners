{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando RL para equilibrar el Cartpole\n",
    "\n",
    "Este cuaderno forma parte del [Currículo de IA para Principiantes](http://aka.ms/ai-beginners). Se ha inspirado en el [tutorial oficial de PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) y en [esta implementación de Cartpole en PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "En este ejemplo, utilizaremos RL para entrenar un modelo que pueda equilibrar un poste sobre un carrito que puede moverse hacia la izquierda y la derecha en una escala horizontal. Usaremos el entorno de [OpenAI Gym](https://www.gymlibrary.ml/) para simular el poste.\n",
    "\n",
    "> **Nota**: Puedes ejecutar el código de esta lección localmente (por ejemplo, desde Visual Studio Code), en cuyo caso la simulación se abrirá en una nueva ventana. Si ejecutas el código en línea, es posible que necesites hacer algunos ajustes al código, como se describe [aquí](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Comenzaremos asegurándonos de que Gym esté instalado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a crear el entorno CartPole y ver cómo interactuar con él. Un entorno tiene las siguientes propiedades:\n",
    "\n",
    "* **Action space** es el conjunto de acciones posibles que podemos realizar en cada paso de la simulación.\n",
    "* **Observation space** es el espacio de observaciones que podemos realizar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo funciona la simulación. El siguiente bucle ejecuta la simulación hasta que `env.step` no devuelva el indicador de terminación `done`. Elegiremos acciones de forma aleatoria utilizando `env.action_space.sample()`, lo que significa que el experimento probablemente fallará muy rápido (el entorno CartPole termina cuando la velocidad del CartPole, su posición o su ángulo están fuera de ciertos límites).\n",
    "\n",
    "> La simulación se abrirá en una nueva ventana. Puedes ejecutar el código varias veces y observar cómo se comporta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes notar que las observaciones contienen 4 números. Estos son:\n",
    "- Posición del carrito\n",
    "- Velocidad del carrito\n",
    "- Ángulo del poste\n",
    "- Tasa de rotación del poste\n",
    "\n",
    "`rew` es la recompensa que recibimos en cada paso. Puedes ver que en el entorno de CartPole se otorga 1 punto por cada paso de simulación, y el objetivo es maximizar la recompensa total, es decir, el tiempo que CartPole puede mantenerse equilibrado sin caerse.\n",
    "\n",
    "Durante el aprendizaje por refuerzo, nuestro objetivo es entrenar una **política** $\\pi$, que para cada estado $s$ nos indicará qué acción $a$ tomar, esencialmente $a = \\pi(s)$.\n",
    "\n",
    "Si deseas una solución probabilística, puedes pensar en la política como un conjunto de probabilidades para cada acción, es decir, $\\pi(a|s)$ representaría la probabilidad de que debamos tomar la acción $a$ en el estado $s$.\n",
    "\n",
    "## Método de Gradiente de Política\n",
    "\n",
    "En el algoritmo más simple de aprendizaje por refuerzo, llamado **Gradiente de Política**, entrenaremos una red neuronal para predecir la próxima acción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenaremos la red realizando muchos experimentos y actualizando nuestra red después de cada ejecución. Definamos una función que ejecutará el experimento y devolverá los resultados (el llamado **rastro**) - todos los estados, acciones (y sus probabilidades recomendadas) y recompensas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ejecutar un episodio con una red no entrenada y observar que la recompensa total (también conocida como la duración del episodio) es muy baja:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los aspectos complicados del algoritmo de gradiente de política es usar **recompensas descontadas**. La idea es que calculamos el vector de recompensas totales en cada paso del juego, y durante este proceso descontamos las recompensas iniciales utilizando algún coeficiente $gamma$. También normalizamos el vector resultante, porque lo usaremos como peso para afectar nuestro entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ahora vamos a entrenar! Ejecutaremos 300 episodios, y en cada episodio haremos lo siguiente:\n",
    "\n",
    "1. Ejecutar el experimento y recopilar el rastro.\n",
    "1. Calcular la diferencia (`gradients`) entre las acciones realizadas y las probabilidades predichas. Cuanto menor sea la diferencia, más seguros estaremos de haber tomado la acción correcta.\n",
    "1. Calcular las recompensas descontadas y multiplicar los gradientes por las recompensas descontadas. Esto asegurará que los pasos con mayores recompensas tengan un mayor impacto en el resultado final que aquellos con recompensas más bajas.\n",
    "1. Las acciones objetivo esperadas para nuestra red neuronal se tomarán en parte de las probabilidades predichas durante la ejecución y en parte de los gradientes calculados. Usaremos el parámetro `alpha` para determinar en qué medida se tienen en cuenta los gradientes y las recompensas; esto se conoce como *tasa de aprendizaje* del algoritmo de refuerzo.\n",
    "1. Finalmente, entrenamos nuestra red con los estados y las acciones esperadas, y repetimos el proceso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutemos el episodio con renderizado para ver el resultado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Esperemos que puedas ver que el poste ahora puede equilibrarse bastante bien!\n",
    "\n",
    "## Modelo Actor-Crítico\n",
    "\n",
    "El modelo Actor-Crítico es un desarrollo adicional de los gradientes de política, en el cual construimos una red neuronal para aprender tanto la política como las recompensas estimadas. La red tendrá dos salidas (o puedes verlo como dos redes separadas):\n",
    "* **Actor** recomendará la acción a tomar proporcionándonos la distribución de probabilidad del estado, como en el modelo de gradiente de política.\n",
    "* **Crítico** estimará cuál sería la recompensa de esas acciones. Devuelve las recompensas totales estimadas en el futuro en el estado dado.\n",
    "\n",
    "Definamos un modelo de este tipo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitaríamos modificar ligeramente nuestras funciones `discounted_rewards` y `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutaremos el bucle principal de entrenamiento. Utilizaremos un proceso de entrenamiento manual de la red calculando las funciones de pérdida adecuadas y actualizando los parámetros de la red:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Hemos visto dos algoritmos de aprendizaje por refuerzo en esta demostración: el gradiente de política simple y el actor-crítico más sofisticado. Puedes observar que estos algoritmos operan con nociones abstractas de estado, acción y recompensa, lo que les permite aplicarse a entornos muy diferentes.\n",
    "\n",
    "El aprendizaje por refuerzo nos permite aprender la mejor estrategia para resolver un problema simplemente observando la recompensa final. El hecho de que no necesitemos conjuntos de datos etiquetados nos permite repetir simulaciones muchas veces para optimizar nuestros modelos. Sin embargo, todavía existen muchos desafíos en el aprendizaje por refuerzo, los cuales podrías explorar si decides profundizar en esta interesante área de la inteligencia artificial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Descargo de responsabilidad**:  \nEste documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-31T15:45:14+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "es"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}