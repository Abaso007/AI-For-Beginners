<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-24T20:45:41+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "fr"
}
-->
# R√©seaux Neuronaux R√©currents

## [Quiz avant le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

Dans les sections pr√©c√©dentes, nous avons utilis√© des repr√©sentations s√©mantiques riches de texte et un simple classificateur lin√©aire au-dessus des embeddings. Cette architecture capture le sens global des mots dans une phrase, mais elle ne prend pas en compte l'**ordre** des mots, car l'op√©ration d'agr√©gation sur les embeddings supprime cette information du texte original. √âtant donn√© que ces mod√®les ne peuvent pas mod√©liser l'ordre des mots, ils ne peuvent pas r√©soudre des t√¢ches plus complexes ou ambigu√´s comme la g√©n√©ration de texte ou la r√©ponse √† des questions.

Pour capturer le sens d'une s√©quence de texte, nous devons utiliser une autre architecture de r√©seau neuronal, appel√©e **r√©seau neuronal r√©current**, ou RNN. Dans un RNN, nous faisons passer notre phrase √† travers le r√©seau un symbole √† la fois, et le r√©seau produit un certain **√©tat**, que nous transmettons ensuite au r√©seau avec le symbole suivant.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.fr.png)

> Image par l'auteur

√âtant donn√© la s√©quence d'entr√©e de tokens X<sub>0</sub>,...,X<sub>n</sub>, le RNN cr√©e une s√©quence de blocs de r√©seau neuronal et entra√Æne cette s√©quence de bout en bout en utilisant la r√©tropropagation. Chaque bloc de r√©seau prend une paire (X<sub>i</sub>,S<sub>i</sub>) comme entr√©e et produit S<sub>i+1</sub> en r√©sultat. L'√©tat final S<sub>n</sub> ou (la sortie Y<sub>n</sub>) est ensuite transmis √† un classificateur lin√©aire pour produire le r√©sultat. Tous les blocs de r√©seau partagent les m√™mes poids et sont entra√Æn√©s de bout en bout en une seule passe de r√©tropropagation.

Comme les vecteurs d'√©tat S<sub>0</sub>,...,S<sub>n</sub> sont transmis √† travers le r√©seau, celui-ci est capable d'apprendre les d√©pendances s√©quentielles entre les mots. Par exemple, lorsque le mot *pas* appara√Æt quelque part dans la s√©quence, le r√©seau peut apprendre √† n√©gativer certains √©l√©ments dans le vecteur d'√©tat, ce qui entra√Æne une n√©gation.

> ‚úÖ √âtant donn√© que les poids de tous les blocs RNN sur l'image ci-dessus sont partag√©s, la m√™me image peut √™tre repr√©sent√©e comme un seul bloc (√† droite) avec une boucle de r√©troaction r√©currente, qui transmet l'√©tat de sortie du r√©seau √† l'entr√©e.

## Anatomie d'une Cellule RNN

Voyons comment une cellule RNN simple est organis√©e. Elle accepte l'√©tat pr√©c√©dent S<sub>i-1</sub> et le symbole actuel X<sub>i</sub> comme entr√©es, et doit produire l'√©tat de sortie S<sub>i</sub> (et, parfois, nous nous int√©ressons √©galement √† une autre sortie Y<sub>i</sub>, comme dans le cas des r√©seaux g√©n√©ratifs).

Une cellule RNN simple contient deux matrices de poids internes : l'une transforme un symbole d'entr√©e (appelons-la W), et l'autre transforme un √©tat d'entr√©e (H). Dans ce cas, la sortie du r√©seau est calcul√©e comme œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b), o√π œÉ est la fonction d'activation et b est un biais suppl√©mentaire.

<img alt="Anatomie d'une cellule RNN" src="images/rnn-anatomy.png" width="50%"/>

> Image par l'auteur

Dans de nombreux cas, les tokens d'entr√©e passent par une couche d'embedding avant d'entrer dans le RNN pour r√©duire la dimensionnalit√©. Dans ce cas, si la dimension des vecteurs d'entr√©e est *emb_size*, et que le vecteur d'√©tat est *hid_size* - la taille de W est *emb_size*√ó*hid_size*, et la taille de H est *hid_size*√ó*hid_size*.

## Long Short Term Memory (LSTM)

L'un des principaux probl√®mes des RNN classiques est le probl√®me dit de **l'att√©nuation des gradients**. Comme les RNN sont entra√Æn√©s de bout en bout en une seule passe de r√©tropropagation, il est difficile de propager l'erreur jusqu'aux premi√®res couches du r√©seau, ce qui emp√™che le r√©seau d'apprendre les relations entre des tokens √©loign√©s. L'une des fa√ßons d'√©viter ce probl√®me est d'introduire une **gestion explicite de l'√©tat** en utilisant ce qu'on appelle des **portes**. Il existe deux architectures bien connues de ce type : **Long Short Term Memory** (LSTM) et **Gated Relay Unit** (GRU).

![Image montrant un exemple de cellule LSTM](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Source de l'image √† d√©terminer

Le r√©seau LSTM est organis√© de mani√®re similaire au RNN, mais il y a deux √©tats qui sont transmis d'une couche √† l'autre : l'√©tat r√©el C et le vecteur cach√© H. √Ä chaque unit√©, le vecteur cach√© H<sub>i</sub> est concat√©n√© avec l'entr√©e X<sub>i</sub>, et ils contr√¥lent ce qui arrive √† l'√©tat C via des **portes**. Chaque porte est un r√©seau neuronal avec une activation sigmo√Øde (sortie dans la plage [0,1]), qui peut √™tre consid√©r√©e comme un masque binaire lorsqu'elle est multipli√©e par le vecteur d'√©tat. Les portes suivantes existent (de gauche √† droite sur l'image ci-dessus) :

* La **porte d'oubli** prend un vecteur cach√© et d√©termine quelles composantes du vecteur C nous devons oublier et lesquelles transmettre.
* La **porte d'entr√©e** prend certaines informations des vecteurs d'entr√©e et cach√©s et les ins√®re dans l'√©tat.
* La **porte de sortie** transforme l'√©tat via une couche lin√©aire avec une activation *tanh*, puis s√©lectionne certaines de ses composantes √† l'aide d'un vecteur cach√© H<sub>i</sub> pour produire un nouvel √©tat C<sub>i+1</sub>.

Les composantes de l'√©tat C peuvent √™tre consid√©r√©es comme des indicateurs qui peuvent √™tre activ√©s ou d√©sactiv√©s. Par exemple, lorsque nous rencontrons un nom comme *Alice* dans la s√©quence, nous pouvons supposer qu'il s'agit d'un personnage f√©minin et activer l'indicateur dans l'√©tat indiquant que nous avons un nom f√©minin dans la phrase. Lorsque nous rencontrons ensuite des phrases comme *et Tom*, nous activerons l'indicateur indiquant que nous avons un nom au pluriel. Ainsi, en manipulant l'√©tat, nous pouvons th√©oriquement suivre les propri√©t√©s grammaticales des parties de la phrase.

> ‚úÖ Une excellente ressource pour comprendre les m√©canismes internes des LSTM est cet excellent article [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## RNN Bidirectionnels et Multicouches

Nous avons discut√© des r√©seaux r√©currents qui fonctionnent dans une seule direction, du d√©but d'une s√©quence √† la fin. Cela semble naturel, car cela ressemble √† la fa√ßon dont nous lisons et √©coutons un discours. Cependant, dans de nombreux cas pratiques, nous avons un acc√®s al√©atoire √† la s√©quence d'entr√©e, il peut donc √™tre judicieux d'ex√©cuter le calcul r√©current dans les deux directions. Ces r√©seaux sont appel√©s **RNN bidirectionnels**. Lorsqu'on travaille avec un r√©seau bidirectionnel, nous aurons besoin de deux vecteurs d'√©tat cach√©s, un pour chaque direction.

Un r√©seau r√©current, qu'il soit unidirectionnel ou bidirectionnel, capture certains motifs au sein d'une s√©quence et peut les stocker dans un vecteur d'√©tat ou les transmettre en sortie. Comme pour les r√©seaux convolutionnels, nous pouvons construire une autre couche r√©currente au-dessus de la premi√®re pour capturer des motifs de niveau sup√©rieur et construire √† partir des motifs de bas niveau extraits par la premi√®re couche. Cela nous am√®ne √† la notion de **RNN multicouche**, qui se compose de deux r√©seaux r√©currents ou plus, o√π la sortie de la couche pr√©c√©dente est transmise √† la couche suivante comme entr√©e.

![Image montrant un RNN multicouche avec LSTM](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.fr.jpg)

*Image tir√©e de [cet excellent article](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) de Fernando L√≥pez*

## ‚úçÔ∏è Exercices : Embeddings

Poursuivez votre apprentissage dans les notebooks suivants :

* [RNNs avec PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs avec TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Conclusion

Dans cette unit√©, nous avons vu que les RNN peuvent √™tre utilis√©s pour la classification de s√©quences, mais en r√©alit√©, ils peuvent g√©rer de nombreuses autres t√¢ches, telles que la g√©n√©ration de texte, la traduction automatique, et bien plus encore. Nous examinerons ces t√¢ches dans l'unit√© suivante.

## üöÄ D√©fi

Lisez quelques articles sur les LSTM et r√©fl√©chissez √† leurs applications :

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz apr√®s le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## R√©vision & Auto-apprentissage

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) de Christopher Olah.

## [Devoir : Notebooks](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.