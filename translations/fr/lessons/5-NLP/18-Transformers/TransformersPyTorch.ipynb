{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mécanismes d'attention et transformateurs\n",
    "\n",
    "Un inconvénient majeur des réseaux récurrents est que tous les mots d'une séquence ont le même impact sur le résultat. Cela entraîne des performances sous-optimales avec les modèles standard encodeur-décodeur LSTM pour les tâches de séquence à séquence, telles que la reconnaissance d'entités nommées et la traduction automatique. En réalité, certains mots spécifiques de la séquence d'entrée ont souvent plus d'impact sur les sorties séquentielles que d'autres.\n",
    "\n",
    "Prenons un modèle de séquence à séquence, comme la traduction automatique. Il est implémenté par deux réseaux récurrents, où un réseau (**encodeur**) compresse la séquence d'entrée dans un état caché, et un autre (**décodeur**) déploie cet état caché pour produire le résultat traduit. Le problème avec cette approche est que l'état final du réseau a du mal à se souvenir du début de la phrase, ce qui entraîne une qualité médiocre du modèle pour les phrases longues.\n",
    "\n",
    "Les **mécanismes d'attention** offrent un moyen de pondérer l'impact contextuel de chaque vecteur d'entrée sur chaque prédiction de sortie du RNN. Cela est réalisé en créant des raccourcis entre les états intermédiaires du RNN d'entrée et du RNN de sortie. Ainsi, lors de la génération du symbole de sortie $y_t$, nous prenons en compte tous les états cachés d'entrée $h_i$, avec différents coefficients de pondération $\\alpha_{t,i}$.\n",
    "\n",
    "![Image montrant un modèle encodeur/décodeur avec une couche d'attention additive](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)\n",
    "*Le modèle encodeur-décodeur avec mécanisme d'attention additive dans [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cité de [ce billet de blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "La matrice d'attention $\\{\\alpha_{i,j}\\}$ représente le degré auquel certains mots d'entrée influencent la génération d'un mot donné dans la séquence de sortie. Voici un exemple de cette matrice :\n",
    "\n",
    "![Image montrant un alignement trouvé par RNNsearch-50, tiré de Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)\n",
    "\n",
    "*Figure tirée de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
    "\n",
    "Les mécanismes d'attention sont responsables de la plupart des avancées actuelles ou proches de l'état de l'art en traitement du langage naturel. Cependant, l'ajout d'attention augmente considérablement le nombre de paramètres du modèle, ce qui a entraîné des problèmes de mise à l'échelle avec les RNN. Une contrainte clé pour la mise à l'échelle des RNN est que la nature récurrente des modèles rend difficile le traitement par lots et la parallélisation de l'entraînement. Dans un RNN, chaque élément d'une séquence doit être traité dans un ordre séquentiel, ce qui signifie qu'il ne peut pas être facilement parallélisé.\n",
    "\n",
    "L'adoption des mécanismes d'attention combinée à cette contrainte a conduit à la création des modèles transformateurs, désormais à l'état de l'art, que nous connaissons et utilisons aujourd'hui, de BERT à OpenGPT3.\n",
    "\n",
    "## Modèles transformateurs\n",
    "\n",
    "Au lieu de transmettre le contexte de chaque prédiction précédente à l'étape d'évaluation suivante, les **modèles transformateurs** utilisent des **encodages positionnels** et l'attention pour capturer le contexte d'une entrée donnée dans une fenêtre de texte fournie. L'image ci-dessous montre comment les encodages positionnels avec attention peuvent capturer le contexte dans une fenêtre donnée.\n",
    "\n",
    "![GIF animé montrant comment les évaluations sont effectuées dans les modèles transformateurs.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Étant donné que chaque position d'entrée est mappée indépendamment à chaque position de sortie, les transformateurs peuvent mieux paralléliser que les RNN, ce qui permet des modèles de langage beaucoup plus grands et plus expressifs. Chaque tête d'attention peut être utilisée pour apprendre différentes relations entre les mots, ce qui améliore les tâches de traitement du langage naturel en aval.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) est un réseau transformateur multi-couches très large avec 12 couches pour *BERT-base* et 24 pour *BERT-large*. Le modèle est d'abord pré-entraîné sur un corpus de texte volumineux (WikiPedia + livres) en utilisant un entraînement non supervisé (prédiction des mots masqués dans une phrase). Pendant le pré-entraînement, le modèle acquiert un niveau significatif de compréhension du langage qui peut ensuite être exploité avec d'autres ensembles de données via un ajustement fin. Ce processus est appelé **apprentissage par transfert**.\n",
    "\n",
    "![Image tirée de http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)\n",
    "\n",
    "Il existe de nombreuses variantes des architectures de transformateurs, notamment BERT, DistilBERT, BigBird, OpenGPT3 et bien d'autres, qui peuvent être ajustées. Le [package HuggingFace](https://github.com/huggingface/) fournit un dépôt pour entraîner plusieurs de ces architectures avec PyTorch.\n",
    "\n",
    "## Utilisation de BERT pour la classification de texte\n",
    "\n",
    "Voyons comment utiliser un modèle BERT pré-entraîné pour résoudre notre tâche traditionnelle : la classification de séquences. Nous allons classifier notre ensemble de données AG News original.\n",
    "\n",
    "Tout d'abord, chargeons la bibliothèque HuggingFace et notre ensemble de données :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parce que nous allons utiliser un modèle BERT pré-entraîné, nous devrons utiliser un tokenizer spécifique. Tout d'abord, nous allons charger un tokenizer associé au modèle BERT pré-entraîné.\n",
    "\n",
    "La bibliothèque HuggingFace contient un dépôt de modèles pré-entraînés, que vous pouvez utiliser simplement en spécifiant leurs noms comme arguments dans les fonctions `from_pretrained`. Tous les fichiers binaires nécessaires pour le modèle seront automatiquement téléchargés.\n",
    "\n",
    "Cependant, dans certains cas, vous devrez charger vos propres modèles. Dans ce cas, vous pouvez spécifier le répertoire contenant tous les fichiers pertinents, y compris les paramètres pour le tokenizer, le fichier `config.json` avec les paramètres du modèle, les poids binaires, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet `tokenizer` contient la fonction `encode` qui peut être utilisée directement pour encoder du texte :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, créons des itérateurs que nous utiliserons pendant l'entraînement pour accéder aux données. Étant donné que BERT utilise sa propre fonction d'encodage, nous devrons définir une fonction de remplissage similaire à `padify` que nous avons définie auparavant :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas, nous utiliserons un modèle BERT pré-entraîné appelé `bert-base-uncased`. Chargeons le modèle en utilisant le package `BertForSequenceClassification`. Cela garantit que notre modèle dispose déjà de l'architecture requise pour la classification, y compris le classificateur final. Vous verrez un message d'avertissement indiquant que les poids du classificateur final ne sont pas initialisés, et que le modèle nécessiterait un pré-entraînement - c'est tout à fait normal, car c'est exactement ce que nous allons faire !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sommes maintenant prêts à commencer l'entraînement ! Comme BERT est déjà pré-entraîné, nous souhaitons utiliser un taux d'apprentissage relativement faible afin de ne pas altérer les poids initiaux.\n",
    "\n",
    "Tout le travail important est effectué par le modèle `BertForSequenceClassification`. Lorsque nous appelons le modèle sur les données d'entraînement, il renvoie à la fois la perte (loss) et la sortie du réseau pour le minibatch d'entrée. Nous utilisons la perte pour l'optimisation des paramètres (`loss.backward()` effectue la rétropropagation), et `out` pour calculer la précision de l'entraînement en comparant les étiquettes obtenues `labs` (calculées avec `argmax`) avec les étiquettes attendues `labels`.\n",
    "\n",
    "Pour contrôler le processus, nous accumulons la perte et la précision sur plusieurs itérations, et nous les affichons tous les `report_freq` cycles d'entraînement.\n",
    "\n",
    "Cet entraînement prendra probablement beaucoup de temps, donc nous limitons le nombre d'itérations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez constater (surtout si vous augmentez le nombre d'itérations et attendez suffisamment longtemps) que la classification avec BERT nous donne une précision assez bonne ! Cela s'explique par le fait que BERT comprend déjà très bien la structure de la langue, et que nous n'avons qu'à ajuster le classificateur final. Cependant, comme BERT est un modèle volumineux, tout le processus d'entraînement prend beaucoup de temps et nécessite une puissance de calcul importante ! (GPU, et de préférence plus d'un).\n",
    "\n",
    "> **Note :** Dans notre exemple, nous utilisons l'un des plus petits modèles BERT pré-entraînés. Il existe des modèles plus grands qui sont susceptibles de donner de meilleurs résultats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation des performances du modèle\n",
    "\n",
    "Nous pouvons maintenant évaluer les performances de notre modèle sur le jeu de données de test. La boucle d'évaluation est assez similaire à la boucle d'entraînement, mais il ne faut pas oublier de passer le modèle en mode évaluation en appelant `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## À retenir\n",
    "\n",
    "Dans cette unité, nous avons vu à quel point il est facile de prendre un modèle de langage pré-entraîné de la bibliothèque **transformers** et de l'adapter à notre tâche de classification de texte. De la même manière, les modèles BERT peuvent être utilisés pour l'extraction d'entités, les questions-réponses et d'autres tâches de NLP.\n",
    "\n",
    "Les modèles de type Transformer représentent l'état de l'art actuel en NLP, et dans la plupart des cas, ils devraient être la première solution avec laquelle vous commencez à expérimenter lorsque vous mettez en œuvre des solutions NLP personnalisées. Cependant, comprendre les principes de base des réseaux neuronaux récurrents discutés dans ce module est extrêmement important si vous souhaitez construire des modèles neuronaux avancés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction professionnelle réalisée par un humain. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-31T15:16:28+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}