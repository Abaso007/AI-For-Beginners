<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-24T20:46:41+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "fr"
}
-->
# R√©seaux g√©n√©ratifs

## [Quiz avant le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Les r√©seaux neuronaux r√©currents (RNN) et leurs variantes √† cellules √† portes, comme les cellules Long Short Term Memory (LSTM) et les Gated Recurrent Units (GRU), offrent un m√©canisme pour la mod√©lisation du langage, car ils peuvent apprendre l'ordre des mots et pr√©dire le mot suivant dans une s√©quence. Cela nous permet d'utiliser les RNN pour des **t√¢ches g√©n√©ratives**, telles que la g√©n√©ration de texte ordinaire, la traduction automatique et m√™me la g√©n√©ration de l√©gendes pour des images.

> ‚úÖ Pensez √† toutes les fois o√π vous avez b√©n√©fici√© de t√¢ches g√©n√©ratives, comme la compl√©tion de texte pendant que vous tapez. Faites des recherches sur vos applications pr√©f√©r√©es pour voir si elles utilisent des RNN.

Dans l'architecture RNN que nous avons abord√©e dans l'unit√© pr√©c√©dente, chaque unit√© RNN produisait l'√©tat cach√© suivant comme sortie. Cependant, nous pouvons √©galement ajouter une autre sortie √† chaque unit√© r√©currente, ce qui nous permettrait de produire une **s√©quence** (de m√™me longueur que la s√©quence d'origine). De plus, nous pouvons utiliser des unit√©s RNN qui n'acceptent pas d'entr√©e √† chaque √©tape, mais prennent simplement un vecteur d'√©tat initial, puis produisent une s√©quence de sorties.

Cela permet diff√©rentes architectures neuronales, comme illustr√© dans l'image ci-dessous :

![Image montrant des mod√®les courants de r√©seaux neuronaux r√©currents.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.fr.jpg)

> Image tir√©e de l'article de blog [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) par [Andrej Karpaty](http://karpathy.github.io/)

* **Un-√†-un** est un r√©seau neuronal traditionnel avec une entr√©e et une sortie.
* **Un-√†-plusieurs** est une architecture g√©n√©rative qui accepte une valeur d'entr√©e et g√©n√®re une s√©quence de valeurs de sortie. Par exemple, si nous voulons entra√Æner un r√©seau de **g√©n√©ration de l√©gendes d'images** qui produirait une description textuelle d'une image, nous pouvons prendre une image en entr√©e, la passer √† travers un CNN pour obtenir son √©tat cach√©, puis utiliser une cha√Æne r√©currente pour g√©n√©rer la l√©gende mot par mot.
* **Plusieurs-√†-un** correspond aux architectures RNN que nous avons d√©crites dans l'unit√© pr√©c√©dente, comme la classification de texte.
* **Plusieurs-√†-plusieurs**, ou **s√©quence-√†-s√©quence**, correspond √† des t√¢ches telles que la **traduction automatique**, o√π un premier RNN collecte toutes les informations de la s√©quence d'entr√©e dans l'√©tat cach√©, et une autre cha√Æne RNN d√©roule cet √©tat en une s√©quence de sortie.

Dans cette unit√©, nous nous concentrerons sur des mod√®les g√©n√©ratifs simples qui nous aident √† g√©n√©rer du texte. Pour simplifier, nous utiliserons une tokenisation au niveau des caract√®res.

Nous entra√Ænerons ce RNN √† g√©n√©rer du texte √©tape par √©tape. √Ä chaque √©tape, nous prendrons une s√©quence de caract√®res de longueur `nchars` et demanderons au r√©seau de g√©n√©rer le caract√®re suivant pour chaque caract√®re d'entr√©e :

![Image montrant un exemple de g√©n√©ration RNN du mot 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.fr.png)

Lors de la g√©n√©ration de texte (pendant l'inf√©rence), nous commen√ßons par une **invite** qui est pass√©e √† travers les cellules RNN pour g√©n√©rer son √©tat interm√©diaire, puis la g√©n√©ration commence √† partir de cet √©tat. Nous g√©n√©rons un caract√®re √† la fois, et passons l'√©tat et le caract√®re g√©n√©r√© √† une autre cellule RNN pour g√©n√©rer le suivant, jusqu'√† ce que nous ayons g√©n√©r√© suffisamment de caract√®res.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Image par l'auteur

## ‚úçÔ∏è Exercices : R√©seaux g√©n√©ratifs

Poursuivez votre apprentissage dans les notebooks suivants :

* [R√©seaux g√©n√©ratifs avec PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [R√©seaux g√©n√©ratifs avec TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## G√©n√©ration de texte souple et temp√©rature

La sortie de chaque cellule RNN est une distribution de probabilit√© des caract√®res. Si nous prenons toujours le caract√®re avec la probabilit√© la plus √©lev√©e comme caract√®re suivant dans le texte g√©n√©r√©, le texte peut souvent devenir "cyclique", r√©p√©tant les m√™mes s√©quences de caract√®res encore et encore, comme dans cet exemple :

```
today of the second the company and a second the company ...
```

Cependant, si nous examinons la distribution de probabilit√© pour le caract√®re suivant, il se peut que la diff√©rence entre les quelques probabilit√©s les plus √©lev√©es ne soit pas √©norme, par exemple un caract√®re peut avoir une probabilit√© de 0,2, un autre de 0,19, etc. Par exemple, lorsqu'on cherche le caract√®re suivant dans la s√©quence '*play*', le caract√®re suivant pourrait tout aussi bien √™tre un espace ou **e** (comme dans le mot *player*).

Cela nous am√®ne √† la conclusion qu'il n'est pas toujours "juste" de s√©lectionner le caract√®re avec la probabilit√© la plus √©lev√©e, car choisir le deuxi√®me plus probable peut √©galement conduire √† un texte significatif. Il est plus judicieux de **pr√©lever** des caract√®res √† partir de la distribution de probabilit√© donn√©e par la sortie du r√©seau. Nous pouvons √©galement utiliser un param√®tre, la **temp√©rature**, qui aplatira la distribution de probabilit√© si nous voulons ajouter plus d'al√©atoire, ou la rendra plus raide si nous voulons nous en tenir davantage aux caract√®res les plus probables.

Explorez comment cette g√©n√©ration de texte souple est impl√©ment√©e dans les notebooks mentionn√©s ci-dessus.

## Conclusion

Bien que la g√©n√©ration de texte puisse √™tre utile en soi, les principaux avantages proviennent de la capacit√© √† g√©n√©rer du texte √† l'aide de RNN √† partir d'un vecteur de caract√©ristiques initial. Par exemple, la g√©n√©ration de texte est utilis√©e dans la traduction automatique (s√©quence-√†-s√©quence, dans ce cas le vecteur d'√©tat de l'*encodeur* est utilis√© pour g√©n√©rer ou *d√©coder* le message traduit), ou pour g√©n√©rer une description textuelle d'une image (dans ce cas, le vecteur de caract√©ristiques proviendrait d'un extracteur CNN).

## üöÄ D√©fi

Suivez des le√ßons sur Microsoft Learn sur ce sujet :

* G√©n√©ration de texte avec [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz apr√®s le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## R√©vision et auto-apprentissage

Voici quelques articles pour approfondir vos connaissances :

* Diff√©rentes approches de g√©n√©ration de texte avec Markov Chain, LSTM et GPT-2 : [article de blog](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exemple de g√©n√©ration de texte dans la [documentation Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Devoir](lab/README.md)

Nous avons vu comment g√©n√©rer du texte caract√®re par caract√®re. Dans le laboratoire, vous explorerez la g√©n√©ration de texte au niveau des mots.

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.