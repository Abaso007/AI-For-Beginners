{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intégrations\n",
    "\n",
    "Dans notre exemple précédent, nous avons travaillé avec des vecteurs bag-of-words de haute dimension de longueur `vocab_size`, et nous avons explicitement converti des vecteurs de représentation positionnelle de basse dimension en une représentation clairsemée à un seul bit actif. Cette représentation à un seul bit actif n'est pas efficace en termes de mémoire. De plus, chaque mot est traité indépendamment des autres, ce qui fait que les vecteurs encodés de cette manière ne reflètent pas les similitudes sémantiques entre les mots.\n",
    "\n",
    "Dans cette unité, nous continuerons à explorer le dataset **News AG**. Pour commencer, chargeons les données et récupérons quelques définitions de l'unité précédente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'un embedding ?\n",
    "\n",
    "L'idée d'un **embedding** est de représenter les mots à l'aide de vecteurs denses de dimension inférieure qui reflètent le sens sémantique du mot. Nous verrons plus tard comment construire des embeddings de mots significatifs, mais pour l'instant, considérons simplement les embeddings comme un moyen de réduire la dimensionnalité d'un vecteur de mots.\n",
    "\n",
    "Ainsi, une couche d'embedding prend un mot en entrée et produit un vecteur de sortie de taille `embedding_size`. En un sens, cela ressemble beaucoup à une couche `Dense`, mais au lieu de prendre un vecteur one-hot encodé en entrée, elle peut prendre un numéro de mot.\n",
    "\n",
    "En utilisant une couche d'embedding comme première couche de notre réseau, nous pouvons passer d'un modèle bag-of-words à un modèle **embedding bag**, où nous convertissons d'abord chaque mot de notre texte en l'embedding correspondant, puis calculons une fonction d'agrégation sur tous ces embeddings, comme `sum`, `average` ou `max`.\n",
    "\n",
    "![Image montrant un classificateur avec embedding pour cinq mots de séquence.](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)\n",
    "\n",
    "Notre réseau de neurones classificateur se compose des couches suivantes :\n",
    "\n",
    "* Une couche `TextVectorization`, qui prend une chaîne de caractères en entrée et produit un tenseur de numéros de tokens. Nous spécifierons une taille de vocabulaire raisonnable `vocab_size` et ignorerons les mots moins fréquemment utilisés. La forme d'entrée sera 1, et la forme de sortie sera $n$, car nous obtiendrons $n$ tokens en résultat, chacun contenant des numéros allant de 0 à `vocab_size`.\n",
    "* Une couche `Embedding`, qui prend $n$ numéros et réduit chaque numéro à un vecteur dense d'une longueur donnée (100 dans notre exemple). Ainsi, le tenseur d'entrée de forme $n$ sera transformé en un tenseur de forme $n\\times 100$.\n",
    "* Une couche d'agrégation, qui calcule la moyenne de ce tenseur le long du premier axe, c'est-à-dire qu'elle calculera la moyenne de tous les $n$ tenseurs d'entrée correspondant à différents mots. Pour implémenter cette couche, nous utiliserons une couche `Lambda` et lui passerons la fonction pour calculer la moyenne. La sortie aura une forme de 100, et ce sera la représentation numérique de toute la séquence d'entrée.\n",
    "* Enfin, un classificateur linéaire `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le résumé, dans la colonne **forme de sortie**, la première dimension du tenseur `None` correspond à la taille du lot (minibatch), et la seconde correspond à la longueur de la séquence de tokens. Toutes les séquences de tokens dans le lot ont des longueurs différentes. Nous verrons comment gérer cela dans la section suivante.\n",
    "\n",
    "Passons maintenant à l'entraînement du réseau :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Note** que nous construisons un vectoriseur basé sur un sous-ensemble des données. Cela est fait afin d'accélérer le processus, et cela pourrait entraîner une situation où tous les tokens de notre texte ne sont pas présents dans le vocabulaire. Dans ce cas, ces tokens seraient ignorés, ce qui pourrait entraîner une précision légèrement inférieure. Cependant, dans la réalité, un sous-ensemble de texte donne souvent une bonne estimation du vocabulaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des tailles de séquences de variables\n",
    "\n",
    "Comprenons comment l'entraînement se déroule dans les mini-lots. Dans l'exemple ci-dessus, le tenseur d'entrée a une dimension de 1, et nous utilisons des mini-lots de taille 128, ce qui donne une taille réelle du tenseur de $128 \\times 1$. Cependant, le nombre de tokens dans chaque phrase est différent. Si nous appliquons la couche `TextVectorization` à une seule entrée, le nombre de tokens retournés varie en fonction de la manière dont le texte est tokenisé :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, lorsque nous appliquons le vectoriseur à plusieurs séquences, il doit produire un tenseur de forme rectangulaire, donc il remplit les éléments inutilisés avec le jeton PAD (qui dans notre cas est zéro) :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les incorporations :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque** : Pour minimiser la quantité de remplissage, il peut être judicieux dans certains cas de trier toutes les séquences du jeu de données par ordre croissant de longueur (ou, plus précisément, par nombre de tokens). Cela garantira que chaque minibatch contient des séquences de longueur similaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incrustations sémantiques : Word2Vec\n",
    "\n",
    "Dans notre exemple précédent, la couche d'incrustation a appris à mapper des mots à des représentations vectorielles, mais ces représentations n'avaient pas de signification sémantique. Il serait intéressant d'apprendre une représentation vectorielle où des mots similaires ou des synonymes correspondent à des vecteurs proches les uns des autres selon une certaine distance vectorielle (par exemple, la distance euclidienne).\n",
    "\n",
    "Pour cela, nous devons préentraîner notre modèle d'incrustation sur une grande collection de textes en utilisant une technique telle que [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Cette méthode repose sur deux architectures principales utilisées pour produire une représentation distribuée des mots :\n",
    "\n",
    " - **Sac de mots continu** (CBoW), où l'on entraîne le modèle à prédire un mot à partir du contexte environnant. Étant donné le ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, l'objectif du modèle est de prédire $W_0$ à partir de $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Skip-gram continu**, qui est l'opposé du CBoW. Le modèle utilise la fenêtre de mots du contexte environnant pour prédire le mot actuel.\n",
    "\n",
    "CBoW est plus rapide, tandis que skip-gram, bien que plus lent, représente mieux les mots peu fréquents.\n",
    "\n",
    "![Image montrant les algorithmes CBoW et Skip-Gram pour convertir des mots en vecteurs.](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)\n",
    "\n",
    "Pour expérimenter avec l'incrustation Word2Vec préentraînée sur le dataset Google News, nous pouvons utiliser la bibliothèque **gensim**. Ci-dessous, nous trouvons les mots les plus similaires à 'neural'.\n",
    "\n",
    "> **Note:** Lorsque vous créez des vecteurs de mots pour la première fois, leur téléchargement peut prendre un certain temps !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également extraire l'incorporation vectorielle du mot, à utiliser dans l'entraînement du modèle de classification. L'incorporation comporte 300 composantes, mais ici nous montrons seulement les 20 premières composantes du vecteur pour plus de clarté :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La grande particularité des embeddings sémantiques est que vous pouvez manipuler l'encodage vectoriel en fonction des sémantiques. Par exemple, nous pouvons demander de trouver un mot dont la représentation vectorielle est aussi proche que possible des mots *roi* et *femme*, et aussi éloignée que possible du mot *homme* :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Un exemple ci-dessus utilise une certaine magie interne de GenSym, mais la logique sous-jacente est en réalité assez simple. Une chose intéressante à propos des embeddings est que vous pouvez effectuer des opérations vectorielles normales sur les vecteurs d'embedding, et cela refléterait des opérations sur les **significations** des mots. L'exemple ci-dessus peut être exprimé en termes d'opérations vectorielles : nous calculons le vecteur correspondant à **ROI-HOMME+FEMME** (les opérations `+` et `-` sont effectuées sur les représentations vectorielles des mots correspondants), puis nous trouvons le mot le plus proche dans le dictionnaire de ce vecteur :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE** : Nous avons dû ajouter de petits coefficients aux vecteurs *homme* et *femme* - essayez de les supprimer pour voir ce qui se passe.\n",
    "\n",
    "Pour trouver le vecteur le plus proche, nous utilisons les outils de TensorFlow pour calculer un vecteur de distances entre notre vecteur et tous les vecteurs du vocabulaire, puis nous trouvons l'index du mot minimal en utilisant `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que Word2Vec semble être un excellent moyen d'exprimer la sémantique des mots, il présente de nombreux inconvénients, notamment les suivants :\n",
    "\n",
    "* Les modèles CBoW et skip-gram sont des **représentations prédictives**, et ils ne prennent en compte que le contexte local. Word2Vec ne profite pas du contexte global.\n",
    "* Word2Vec ne prend pas en compte la **morphologie** des mots, c'est-à-dire le fait que le sens d'un mot peut dépendre de différentes parties du mot, comme la racine.\n",
    "\n",
    "**FastText** tente de surmonter cette deuxième limitation et s'appuie sur Word2Vec en apprenant des représentations vectorielles pour chaque mot ainsi que pour les n-grammes de caractères trouvés dans chaque mot. Les valeurs des représentations sont ensuite moyennées en un seul vecteur à chaque étape d'entraînement. Bien que cela ajoute beaucoup de calculs supplémentaires lors de la pré-formation, cela permet aux représentations vectorielles d'intégrer des informations sur les sous-mots.\n",
    "\n",
    "Une autre méthode, **GloVe**, utilise une approche différente pour les représentations vectorielles, basée sur la factorisation de la matrice mot-contexte. Tout d'abord, elle construit une grande matrice qui compte le nombre d'occurrences des mots dans différents contextes, puis elle tente de représenter cette matrice dans des dimensions inférieures de manière à minimiser la perte de reconstruction.\n",
    "\n",
    "La bibliothèque gensim prend en charge ces représentations vectorielles, et vous pouvez les expérimenter en modifiant le code de chargement du modèle ci-dessus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser des embeddings préentraînés dans Keras\n",
    "\n",
    "Nous pouvons modifier l'exemple ci-dessus pour préremplir la matrice de notre couche d'embedding avec des embeddings sémantiques, tels que Word2Vec. Les vocabulaires de l'embedding préentraîné et du corpus de texte ne correspondront probablement pas, donc nous devons en choisir un. Ici, nous explorons les deux options possibles : utiliser le vocabulaire du tokenizer et utiliser le vocabulaire des embeddings Word2Vec.\n",
    "\n",
    "### Utiliser le vocabulaire du tokenizer\n",
    "\n",
    "En utilisant le vocabulaire du tokenizer, certains mots du vocabulaire auront des embeddings Word2Vec correspondants, tandis que d'autres seront absents. Étant donné que la taille de notre vocabulaire est `vocab_size`, et que la longueur du vecteur d'embedding Word2Vec est `embed_size`, la couche d'embedding sera représentée par une matrice de poids de forme `vocab_size`$\\times$`embed_size`. Nous remplirons cette matrice en parcourant le vocabulaire :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les mots qui ne sont pas présents dans le vocabulaire de Word2Vec, nous pouvons soit les laisser comme des zéros, soit générer un vecteur aléatoire.\n",
    "\n",
    "Nous pouvons maintenant définir une couche d'embedding avec des poids préentraînés :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque** : Notez que nous avons défini `trainable=False` lors de la création de `Embedding`, ce qui signifie que nous ne réentraînons pas la couche Embedding. Cela peut entraîner une légère baisse de précision, mais cela accélère l'entraînement.\n",
    "\n",
    "### Utilisation du vocabulaire d'embedding\n",
    "\n",
    "Un problème avec l'approche précédente est que les vocabulaires utilisés dans TextVectorization et Embedding sont différents. Pour résoudre ce problème, nous pouvons utiliser l'une des solutions suivantes :\n",
    "* Réentraîner le modèle Word2Vec sur notre vocabulaire.\n",
    "* Charger notre jeu de données avec le vocabulaire du modèle Word2Vec préentraîné. Les vocabulaires utilisés pour charger le jeu de données peuvent être spécifiés lors du chargement.\n",
    "\n",
    "La deuxième approche semble plus simple, alors mettons-la en œuvre. Tout d'abord, nous allons créer une couche `TextVectorization` avec le vocabulaire spécifié, tiré des embeddings Word2Vec :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La bibliothèque d'embeddings de mots gensim contient une fonction pratique, `get_keras_embeddings`, qui créera automatiquement la couche d'embeddings Keras correspondante pour vous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une des raisons pour lesquelles nous n'observons pas une précision plus élevée est que certains mots de notre ensemble de données sont absents du vocabulaire préentraîné de GloVe, et sont donc essentiellement ignorés. Pour surmonter cela, nous pouvons entraîner nos propres embeddings basés sur notre ensemble de données.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les embeddings contextuels\n",
    "\n",
    "Une des principales limites des représentations d'embeddings préentraînés traditionnels comme Word2Vec est qu'ils ne peuvent pas différencier les différents sens d'un mot, même s'ils peuvent en capturer une partie de la signification. Cela peut poser des problèmes dans les modèles en aval.\n",
    "\n",
    "Par exemple, le mot \"play\" a des significations différentes dans ces deux phrases :\n",
    "- Je suis allé voir une **pièce** au théâtre.\n",
    "- John veut **jouer** avec ses amis.\n",
    "\n",
    "Les embeddings préentraînés dont nous avons parlé représentent les deux sens du mot \"play\" dans le même embedding. Pour surmonter cette limitation, nous devons construire des embeddings basés sur le **modèle de langage**, qui est entraîné sur un large corpus de texte et *sait* comment les mots peuvent être assemblés dans différents contextes. Discuter des embeddings contextuels dépasse le cadre de ce tutoriel, mais nous y reviendrons en parlant des modèles de langage dans la prochaine unité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction professionnelle réalisée par un humain. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-31T15:25:15+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}