<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-24T20:46:15+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "fr"
}
-->
# Int√©grations

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Lors de l'entra√Ænement de classificateurs bas√©s sur BoW ou TF/IDF, nous travaillions avec des vecteurs bag-of-words de haute dimension de longueur `vocab_size`, et nous convertissions explicitement des vecteurs de repr√©sentation positionnelle de basse dimension en une repr√©sentation sparse one-hot. Cependant, cette repr√©sentation one-hot n'est pas efficace en termes de m√©moire. De plus, chaque mot est trait√© ind√©pendamment des autres, c'est-√†-dire que les vecteurs encod√©s en one-hot n'expriment aucune similarit√© s√©mantique entre les mots.

L'id√©e des **int√©grations** est de repr√©senter les mots par des vecteurs denses de plus faible dimension, qui refl√®tent d'une certaine mani√®re le sens s√©mantique d'un mot. Nous discuterons plus tard de la mani√®re de construire des int√©grations de mots significatives, mais pour l'instant, consid√©rons simplement les int√©grations comme un moyen de r√©duire la dimensionnalit√© d'un vecteur de mots.

Ainsi, la couche d'int√©gration prendrait un mot en entr√©e et produirait un vecteur de sortie de taille `embedding_size` sp√©cifi√©e. En un sens, cela ressemble beaucoup √† une couche `Linear`, mais au lieu de prendre un vecteur encod√© en one-hot, elle pourra prendre un num√©ro de mot en entr√©e, nous permettant d'√©viter de cr√©er de grands vecteurs encod√©s en one-hot.

En utilisant une couche d'int√©gration comme premi√®re couche dans notre r√©seau de classification, nous pouvons passer d'un mod√®le bag-of-words √† un mod√®le **embedding bag**, o√π nous convertissons d'abord chaque mot de notre texte en l'int√©gration correspondante, puis calculons une fonction d'agr√©gation sur toutes ces int√©grations, comme `sum`, `average` ou `max`.  

![Image montrant un classificateur bas√© sur des int√©grations pour cinq mots d'une s√©quence.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.fr.png)

> Image de l'auteur

## ‚úçÔ∏è Exercices : Int√©grations

Poursuivez votre apprentissage dans les notebooks suivants :
* [Int√©grations avec PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Int√©grations avec TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Int√©grations s√©mantiques : Word2Vec

Bien que la couche d'int√©gration apprenne √† mapper les mots √† une repr√©sentation vectorielle, cette repr√©sentation n'a pas n√©cessairement beaucoup de sens s√©mantique. Il serait int√©ressant d'apprendre une repr√©sentation vectorielle telle que des mots similaires ou des synonymes correspondent √† des vecteurs proches les uns des autres en termes de distance vectorielle (par exemple, distance euclidienne).

Pour cela, nous devons pr√©-entra√Æner notre mod√®le d'int√©gration sur une grande collection de textes d'une mani√®re sp√©cifique. Une m√©thode pour entra√Æner des int√©grations s√©mantiques s'appelle [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Elle repose sur deux principales architectures utilis√©es pour produire une repr√©sentation distribu√©e des mots :

 - **Continuous bag-of-words** (CBoW) ‚Äî dans cette architecture, nous entra√Ænons le mod√®le √† pr√©dire un mot √† partir du contexte environnant. √âtant donn√© le ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, l'objectif du mod√®le est de pr√©dire $W_0$ √† partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** est l'oppos√© de CBoW. Le mod√®le utilise une fen√™tre de mots de contexte environnants pour pr√©dire le mot actuel.

CBoW est plus rapide, tandis que skip-gram est plus lent, mais repr√©sente mieux les mots peu fr√©quents.

![Image montrant les algorithmes CBoW et Skip-Gram pour convertir des mots en vecteurs.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fr.png)

> Image tir√©e de [cet article](https://arxiv.org/pdf/1301.3781.pdf)

Les int√©grations pr√©-entra√Æn√©es Word2Vec (ainsi que d'autres mod√®les similaires, comme GloVe) peuvent √©galement √™tre utilis√©es √† la place de la couche d'int√©gration dans les r√©seaux neuronaux. Cependant, nous devons g√©rer les vocabulaires, car le vocabulaire utilis√© pour pr√©-entra√Æner Word2Vec/GloVe est probablement diff√©rent du vocabulaire de notre corpus de texte. Consultez les notebooks ci-dessus pour voir comment r√©soudre ce probl√®me.

## Int√©grations contextuelles

Une limitation cl√© des repr√©sentations d'int√©grations pr√©-entra√Æn√©es traditionnelles comme Word2Vec est le probl√®me de d√©sambigu√Øsation des sens des mots. Bien que les int√©grations pr√©-entra√Æn√©es puissent capturer une partie du sens des mots dans leur contexte, chaque sens possible d'un mot est encod√© dans la m√™me int√©gration. Cela peut poser des probl√®mes dans les mod√®les en aval, car de nombreux mots, comme le mot "play", ont des significations diff√©rentes selon le contexte dans lequel ils sont utilis√©s.

Par exemple, le mot "play" dans ces deux phrases a des significations tr√®s diff√©rentes :

- Je suis all√© voir une **pi√®ce** au th√©√¢tre.
- John veut **jouer** avec ses amis.

Les int√©grations pr√©-entra√Æn√©es ci-dessus repr√©sentent ces deux significations du mot "play" dans la m√™me int√©gration. Pour surmonter cette limitation, nous devons construire des int√©grations bas√©es sur le **mod√®le de langage**, qui est entra√Æn√© sur un large corpus de texte et *sait* comment les mots peuvent √™tre assembl√©s dans diff√©rents contextes. La discussion sur les int√©grations contextuelles d√©passe le cadre de ce tutoriel, mais nous y reviendrons lorsque nous parlerons des mod√®les de langage plus tard dans le cours.

## Conclusion

Dans cette le√ßon, vous avez d√©couvert comment construire et utiliser des couches d'int√©gration dans TensorFlow et PyTorch pour mieux refl√©ter les significations s√©mantiques des mots.

## üöÄ D√©fi

Word2Vec a √©t√© utilis√© pour des applications int√©ressantes, notamment la g√©n√©ration de paroles de chansons et de po√©sie. Consultez [cet article](https://www.politetype.com/blog/word2vec-color-poems) qui explique comment l'auteur a utilis√© Word2Vec pour g√©n√©rer de la po√©sie. Regardez √©galement [cette vid√©o de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) pour d√©couvrir une autre explication de cette technique. Ensuite, essayez d'appliquer ces techniques √† votre propre corpus de texte, peut-√™tre issu de Kaggle.

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## R√©vision & Auto-apprentissage

Lisez cet article sur Word2Vec : [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Devoir : Notebooks](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.