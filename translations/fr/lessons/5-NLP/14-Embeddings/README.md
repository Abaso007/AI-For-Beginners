<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b708c9b85b833864c73c6281f1e6b96e",
  "translation_date": "2025-09-23T12:04:57+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "fr"
}
-->
# Embeddings

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Lors de l'entra√Ænement de classificateurs bas√©s sur BoW ou TF/IDF, nous travaillions avec des vecteurs bag-of-words de haute dimension, de longueur `vocab_size`, et nous convertissions explicitement des vecteurs de repr√©sentation positionnelle de faible dimension en une repr√©sentation sparse one-hot. Cependant, cette repr√©sentation one-hot n'est pas efficace en termes de m√©moire. De plus, chaque mot est trait√© ind√©pendamment des autres, c'est-√†-dire que les vecteurs encod√©s en one-hot n'expriment aucune similarit√© s√©mantique entre les mots.

L'id√©e des **embeddings** est de repr√©senter les mots par des vecteurs denses de plus faible dimension, qui refl√®tent d'une certaine mani√®re le sens s√©mantique d'un mot. Nous discuterons plus tard de la mani√®re de construire des embeddings de mots significatifs, mais pour l'instant, consid√©rons simplement les embeddings comme un moyen de r√©duire la dimensionnalit√© d'un vecteur de mots.

Ainsi, la couche d'embedding prendrait un mot en entr√©e et produirait un vecteur de sortie de taille sp√©cifi√©e `embedding_size`. En un sens, cela ressemble beaucoup √† une couche `Linear`, mais au lieu de prendre un vecteur encod√© en one-hot, elle peut prendre un num√©ro de mot en entr√©e, nous permettant ainsi d'√©viter de cr√©er de grands vecteurs encod√©s en one-hot.

En utilisant une couche d'embedding comme premi√®re couche dans notre r√©seau de classification, nous pouvons passer d'un mod√®le bag-of-words √† un mod√®le **embedding bag**, o√π nous convertissons d'abord chaque mot de notre texte en son embedding correspondant, puis calculons une fonction d'agr√©gation sur tous ces embeddings, comme `sum`, `average` ou `max`.

![Image montrant un classificateur bas√© sur des embeddings pour une s√©quence de cinq mots.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.fr.png)

> Image par l'auteur

## ‚úçÔ∏è Exercices : Embeddings

Poursuivez votre apprentissage dans les notebooks suivants :
* [Embeddings avec PyTorch](EmbeddingsPyTorch.ipynb)
* [Embeddings avec TensorFlow](EmbeddingsTF.ipynb)

## Embeddings S√©mantiques : Word2Vec

Bien que la couche d'embedding apprenne √† mapper les mots √† une repr√©sentation vectorielle, cette repr√©sentation n'a pas n√©cessairement une signification s√©mantique forte. Il serait int√©ressant d'apprendre une repr√©sentation vectorielle telle que des mots similaires ou des synonymes correspondent √† des vecteurs proches les uns des autres en termes de distance vectorielle (par exemple, distance euclidienne).

Pour cela, nous devons pr√©entra√Æner notre mod√®le d'embedding sur une grande collection de textes d'une mani√®re sp√©cifique. Une m√©thode pour entra√Æner des embeddings s√©mantiques est appel√©e [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Elle repose sur deux principales architectures utilis√©es pour produire une repr√©sentation distribu√©e des mots :

 - **Continuous bag-of-words** (CBoW) ‚Äî dans cette architecture, nous entra√Ænons le mod√®le √† pr√©dire un mot √† partir du contexte environnant. √âtant donn√© le ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, l'objectif du mod√®le est de pr√©dire $W_0$ √† partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** ‚Äî √† l'inverse de CBoW, ce mod√®le utilise une fen√™tre de mots de contexte environnants pour pr√©dire le mot actuel.

CBoW est plus rapide, tandis que skip-gram est plus lent, mais repr√©sente mieux les mots peu fr√©quents.

![Image montrant les algorithmes CBoW et Skip-Gram pour convertir des mots en vecteurs.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fr.png)

> Image tir√©e de [cet article](https://arxiv.org/pdf/1301.3781.pdf)

Les embeddings pr√©entra√Æn√©s Word2Vec (ainsi que d'autres mod√®les similaires, comme GloVe) peuvent √©galement √™tre utilis√©s √† la place de la couche d'embedding dans les r√©seaux neuronaux. Cependant, il faut g√©rer les vocabulaires, car le vocabulaire utilis√© pour pr√©entra√Æner Word2Vec/GloVe est probablement diff√©rent de celui de notre corpus de texte. Consultez les notebooks ci-dessus pour voir comment r√©soudre ce probl√®me.

## Embeddings Contextuels

Une limitation cl√© des repr√©sentations d'embeddings pr√©entra√Æn√©es traditionnelles comme Word2Vec est le probl√®me de la d√©sambigu√Øsation des sens des mots. Bien que ces embeddings puissent capturer une partie du sens des mots dans leur contexte, chaque sens possible d'un mot est encod√© dans le m√™me embedding. Cela peut poser des probl√®mes dans les mod√®les en aval, car de nombreux mots, comme le mot "play", ont des significations diff√©rentes selon le contexte dans lequel ils sont utilis√©s.

Par exemple, le mot "play" dans ces deux phrases a des significations tr√®s diff√©rentes :

- Je suis all√© voir une **pi√®ce** au th√©√¢tre.
- John veut **jouer** avec ses amis.

Les embeddings pr√©entra√Æn√©s mentionn√©s ci-dessus repr√©sentent ces deux significations du mot "play" dans le m√™me embedding. Pour surmonter cette limitation, nous devons construire des embeddings bas√©s sur un **mod√®le de langage**, qui est entra√Æn√© sur un large corpus de texte et *sait* comment les mots peuvent √™tre assembl√©s dans diff√©rents contextes. La discussion sur les embeddings contextuels d√©passe le cadre de ce tutoriel, mais nous y reviendrons lorsque nous parlerons des mod√®les de langage plus tard dans le cours.

## Conclusion

Dans cette le√ßon, vous avez d√©couvert comment construire et utiliser des couches d'embedding dans TensorFlow et PyTorch pour mieux refl√©ter les significations s√©mantiques des mots.

## üöÄ D√©fi

Word2Vec a √©t√© utilis√© pour des applications int√©ressantes, comme la g√©n√©ration de paroles de chansons et de po√®mes. Consultez [cet article](https://www.politetype.com/blog/word2vec-color-poems) qui explique comment l'auteur a utilis√© Word2Vec pour g√©n√©rer de la po√©sie. Regardez √©galement [cette vid√©o de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) pour d√©couvrir une autre explication de cette technique. Essayez ensuite d'appliquer ces techniques √† votre propre corpus de texte, peut-√™tre issu de Kaggle.

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## R√©vision & √âtude personnelle

Lisez cet article sur Word2Vec : [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Devoir : Notebooks](assignment.md)

---

