<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-24T20:47:39+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "fr"
}
-->
# Mod√©lisation du langage

Les embeddings s√©mantiques, tels que Word2Vec et GloVe, sont en r√©alit√© une premi√®re √©tape vers la **mod√©lisation du langage** - cr√©er des mod√®les qui *comprennent* (ou *repr√©sentent*) d'une certaine mani√®re la nature du langage.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/29)

L'id√©e principale derri√®re la mod√©lisation du langage est de les entra√Æner sur des ensembles de donn√©es non √©tiquet√©s de mani√®re non supervis√©e. Cela est important car nous disposons de grandes quantit√©s de texte non √©tiquet√©, tandis que la quantit√© de texte √©tiquet√© sera toujours limit√©e par l'effort n√©cessaire pour l'√©tiquetage. Le plus souvent, nous pouvons construire des mod√®les de langage capables de **pr√©dire les mots manquants** dans le texte, car il est facile de masquer un mot al√©atoire dans le texte et de l'utiliser comme √©chantillon d'entra√Ænement.

## Entra√Ænement des embeddings

Dans nos exemples pr√©c√©dents, nous avons utilis√© des embeddings s√©mantiques pr√©-entra√Æn√©s, mais il est int√©ressant de voir comment ces embeddings peuvent √™tre entra√Æn√©s. Plusieurs id√©es peuvent √™tre utilis√©es :

* **Mod√©lisation de langage N-Gram**, o√π l'on pr√©dit un token en se basant sur les N tokens pr√©c√©dents (N-gram).
* **Continuous Bag-of-Words** (CBoW), o√π l'on pr√©dit le token central $W_0$ dans une s√©quence de tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, o√π l'on pr√©dit un ensemble de tokens voisins {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} √† partir du token central $W_0$.

![image tir√©e d'un article sur la conversion des mots en vecteurs](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fr.png)

> Image tir√©e de [cet article](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Notebooks d'exemple : Entra√Ænement du mod√®le CBoW

Poursuivez votre apprentissage avec les notebooks suivants :

* [Entra√Ænement de CBoW Word2Vec avec TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Entra√Ænement de CBoW Word2Vec avec PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Conclusion

Dans la le√ßon pr√©c√©dente, nous avons vu que les embeddings de mots fonctionnent comme par magie ! Nous savons maintenant que l'entra√Ænement des embeddings de mots n'est pas une t√¢che tr√®s complexe, et nous devrions √™tre capables d'entra√Æner nos propres embeddings de mots pour du texte sp√©cifique √† un domaine si n√©cessaire.

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## R√©vision et auto-apprentissage

* [Tutoriel officiel PyTorch sur la mod√©lisation du langage](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutoriel officiel TensorFlow sur l'entra√Ænement du mod√®le Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* L'utilisation du framework **gensim** pour entra√Æner les embeddings les plus couramment utilis√©s en quelques lignes de code est d√©crite [dans cette documentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Exercice : Entra√Æner un mod√®le Skip-Gram](lab/README.md)

Dans le laboratoire, nous vous mettons au d√©fi de modifier le code de cette le√ßon pour entra√Æner un mod√®le Skip-Gram au lieu de CBoW. [Lisez les d√©tails](lab/README.md).

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.