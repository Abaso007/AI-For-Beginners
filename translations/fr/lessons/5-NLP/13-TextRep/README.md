<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-24T20:47:11+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "fr"
}
-->
# Repr√©senter le texte sous forme de tenseurs

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Classification de texte

Dans la premi√®re partie de cette section, nous nous concentrerons sur la t√¢che de **classification de texte**. Nous utiliserons le jeu de donn√©es [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), qui contient des articles de presse comme celui-ci :

* Cat√©gorie : Sci/Tech  
* Titre : Ky. Company Wins Grant to Study Peptides (AP)  
* Corps : AP - Une entreprise fond√©e par un chercheur en chimie de l'Universit√© de Louisville a obtenu une subvention pour d√©velopper...

Notre objectif sera de classer l'article dans l'une des cat√©gories en fonction du texte.

## Repr√©sentation du texte

Si nous voulons r√©soudre des t√¢ches de traitement du langage naturel (NLP) avec des r√©seaux neuronaux, nous devons trouver un moyen de repr√©senter le texte sous forme de tenseurs. Les ordinateurs repr√©sentent d√©j√† les caract√®res textuels sous forme de nombres qui correspondent aux polices affich√©es sur votre √©cran, en utilisant des encodages tels que ASCII ou UTF-8.

<img alt="Image montrant un diagramme mappant un caract√®re √† une repr√©sentation ASCII et binaire" src="images/ascii-character-map.png" width="50%"/>

> [Source de l'image](https://www.seobility.net/en/wiki/ASCII)

En tant qu'humains, nous comprenons ce que chaque lettre **repr√©sente** et comment tous les caract√®res se combinent pour former les mots d'une phrase. Cependant, les ordinateurs, par eux-m√™mes, n'ont pas cette compr√©hension, et le r√©seau neuronal doit apprendre la signification pendant l'entra√Ænement.

Ainsi, nous pouvons utiliser diff√©rentes approches pour repr√©senter le texte :

* **Repr√©sentation au niveau des caract√®res**, o√π nous repr√©sentons le texte en traitant chaque caract√®re comme un nombre. √âtant donn√© que nous avons *C* caract√®res diff√©rents dans notre corpus textuel, le mot *Hello* serait repr√©sent√© par un tenseur de 5x*C*. Chaque lettre correspondrait √† une colonne du tenseur en encodage one-hot.  
* **Repr√©sentation au niveau des mots**, o√π nous cr√©ons un **vocabulaire** de tous les mots dans notre texte, puis repr√©sentons les mots en utilisant un encodage one-hot. Cette approche est en quelque sorte meilleure, car chaque lettre prise isol√©ment n'a pas beaucoup de sens. En utilisant des concepts s√©mantiques de plus haut niveau - les mots - nous simplifions la t√¢che pour le r√©seau neuronal. Cependant, √©tant donn√© la taille importante du dictionnaire, nous devons g√©rer des tenseurs creux de haute dimension.

Quelle que soit la repr√©sentation, nous devons d'abord convertir le texte en une s√©quence de **tokens**, un token √©tant soit un caract√®re, un mot, ou parfois m√™me une partie d'un mot. Ensuite, nous convertissons le token en un nombre, g√©n√©ralement en utilisant un **vocabulaire**, et ce nombre peut √™tre introduit dans un r√©seau neuronal en utilisant un encodage one-hot.

## N-grammes

Dans le langage naturel, le sens pr√©cis des mots ne peut √™tre d√©termin√© qu'en contexte. Par exemple, les significations de *r√©seau neuronal* et *r√©seau de p√™che* sont compl√®tement diff√©rentes. Une des fa√ßons de prendre cela en compte est de construire notre mod√®le sur des paires de mots, en consid√©rant les paires de mots comme des tokens de vocabulaire distincts. Ainsi, la phrase *J'aime aller p√™cher* sera repr√©sent√©e par la s√©quence de tokens suivante : *J'aime*, *aime aller*, *aller p√™cher*. Le probl√®me avec cette approche est que la taille du dictionnaire augmente consid√©rablement, et des combinaisons comme *aller p√™cher* et *aller faire du shopping* sont repr√©sent√©es par des tokens diff√©rents, qui ne partagent aucune similarit√© s√©mantique malgr√© le m√™me verbe.

Dans certains cas, nous pouvons envisager d'utiliser des trigrammes - des combinaisons de trois mots - √©galement. Ainsi, cette approche est souvent appel√©e **n-grammes**. Il est √©galement pertinent d'utiliser des n-grammes avec une repr√©sentation au niveau des caract√®res, auquel cas les n-grammes correspondent approximativement √† diff√©rentes syllabes.

## Sac de mots et TF/IDF

Lorsque nous r√©solvons des t√¢ches comme la classification de texte, nous devons √™tre capables de repr√©senter le texte par un vecteur de taille fixe, que nous utiliserons comme entr√©e pour le classificateur dense final. Une des fa√ßons les plus simples de le faire est de combiner toutes les repr√©sentations individuelles des mots, par exemple en les additionnant. Si nous additionnons les encodages one-hot de chaque mot, nous obtiendrons un vecteur de fr√©quences, montrant combien de fois chaque mot appara√Æt dans le texte. Une telle repr√©sentation du texte est appel√©e **sac de mots** (BoW).

<img src="images/bow.png" width="90%"/>

> Image par l'auteur

Un BoW repr√©sente essentiellement quels mots apparaissent dans le texte et en quelles quantit√©s, ce qui peut effectivement √™tre une bonne indication de ce dont parle le texte. Par exemple, un article de presse sur la politique est susceptible de contenir des mots tels que *pr√©sident* et *pays*, tandis qu'une publication scientifique pourrait inclure des termes comme *collisionneur*, *d√©couvert*, etc. Ainsi, les fr√©quences des mots peuvent souvent √™tre un bon indicateur du contenu du texte.

Le probl√®me avec BoW est que certains mots communs, tels que *et*, *est*, etc., apparaissent dans la plupart des textes et ont les fr√©quences les plus √©lev√©es, masquant les mots qui sont r√©ellement importants. Nous pouvons r√©duire l'importance de ces mots en tenant compte de la fr√©quence √† laquelle ils apparaissent dans l'ensemble de la collection de documents. C'est l'id√©e principale derri√®re l'approche TF/IDF, qui est expliqu√©e plus en d√©tail dans les notebooks joints √† cette le√ßon.

Cependant, aucune de ces approches ne peut pleinement prendre en compte la **s√©mantique** du texte. Nous avons besoin de mod√®les de r√©seaux neuronaux plus puissants pour cela, que nous aborderons plus tard dans cette section.

## ‚úçÔ∏è Exercices : Repr√©sentation du texte

Poursuivez votre apprentissage dans les notebooks suivants :

* [Repr√©sentation du texte avec PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Repr√©sentation du texte avec TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## Conclusion

Jusqu'√† pr√©sent, nous avons √©tudi√© des techniques qui peuvent ajouter un poids de fr√©quence √† diff√©rents mots. Cependant, elles ne sont pas capables de repr√©senter le sens ou l'ordre. Comme l'a dit le c√©l√®bre linguiste J. R. Firth en 1935 : "Le sens complet d'un mot est toujours contextuel, et aucune √©tude de sens en dehors du contexte ne peut √™tre prise au s√©rieux." Nous apprendrons plus tard dans le cours comment capturer les informations contextuelles √† partir du texte en utilisant la mod√©lisation du langage.

## üöÄ D√©fi

Essayez d'autres exercices en utilisant le sac de mots et diff√©rents mod√®les de donn√©es. Vous pourriez √™tre inspir√© par cette [comp√©tition sur Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## R√©vision et auto-apprentissage

Exercez vos comp√©tences avec les techniques d'embedding de texte et de sac de mots sur [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste).

## [Devoir : Notebooks](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.