<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bd10f434e444bce61b7f97eeb1ff6a55",
  "translation_date": "2025-08-24T20:48:04+00:00",
  "source_file": "lessons/5-NLP/19-NER/README.md",
  "language_code": "fr"
}
-->
# Reconnaissance d'entit√©s nomm√©es

Jusqu'√† pr√©sent, nous nous sommes principalement concentr√©s sur une t√¢che de traitement du langage naturel (NLP) : la classification. Cependant, il existe d'autres t√¢ches NLP qui peuvent √™tre accomplies avec des r√©seaux neuronaux. L'une de ces t√¢ches est la **[reconnaissance d'entit√©s nomm√©es](https://wikipedia.org/wiki/Named-entity_recognition)** (NER), qui consiste √† identifier des entit√©s sp√©cifiques dans un texte, comme des lieux, des noms de personnes, des intervalles de temps, des formules chimiques, etc.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/37)

## Exemple d'utilisation de la NER

Supposons que vous souhaitiez d√©velopper un chatbot en langage naturel, similaire √† Amazon Alexa ou Google Assistant. Le fonctionnement des chatbots intelligents repose sur la *compr√©hension* de ce que l'utilisateur veut en effectuant une classification de texte sur la phrase d'entr√©e. Le r√©sultat de cette classification est ce qu'on appelle une **intention**, qui d√©termine ce que le chatbot doit faire.

<img alt="Bot NER" src="images/bot-ner.png" width="50%"/>

> Image de l'auteur

Cependant, un utilisateur peut fournir certains param√®tres dans sa phrase. Par exemple, en demandant la m√©t√©o, il peut sp√©cifier un lieu ou une date. Un bot doit √™tre capable de comprendre ces entit√©s et de remplir les param√®tres correspondants avant d'ex√©cuter l'action. C'est pr√©cis√©ment l√† que la NER intervient.

> ‚úÖ Un autre exemple serait [l'analyse d'articles scientifiques m√©dicaux](https://soshnikov.com/science/analyzing-medical-papers-with-azure-and-text-analytics-for-health/). L'un des principaux objectifs est d'identifier des termes m√©dicaux sp√©cifiques, comme des maladies et des substances m√©dicales. Alors qu'un petit nombre de maladies peut probablement √™tre extrait par recherche de sous-cha√Ænes, des entit√©s plus complexes, comme des compos√©s chimiques et des noms de m√©dicaments, n√©cessitent une approche plus sophistiqu√©e.

## La NER comme classification de tokens

Les mod√®les NER sont essentiellement des **mod√®les de classification de tokens**, car pour chaque token d'entr√©e, nous devons d√©cider s'il appartient √† une entit√© ou non, et si oui, √† quelle classe d'entit√©.

Prenons le titre d'article suivant :

**R√©gurgitation de la valve tricuspide** et **carbonate de lithium** **toxicit√©** chez un nouveau-n√©.

Les entit√©s ici sont :

* R√©gurgitation de la valve tricuspide est une maladie (`DIS`)
* Carbonate de lithium est une substance chimique (`CHEM`)
* Toxicit√© est √©galement une maladie (`DIS`)

Remarquez qu'une entit√© peut s'√©tendre sur plusieurs tokens. Et, comme dans ce cas, nous devons distinguer deux entit√©s cons√©cutives. Ainsi, il est courant d'utiliser deux classes pour chaque entit√© : une pour sp√©cifier le premier token de l'entit√© (souvent avec le pr√©fixe `B-` pour **d√©but**) et une autre pour la continuation de l'entit√© (`I-`, pour **int√©rieur**). Nous utilisons √©galement `O` comme classe pour repr√©senter tous les **autres** tokens. Ce type d'√©tiquetage des tokens est appel√© [√©tiquetage BIO](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) (ou IOB). Une fois √©tiquet√©, notre titre ressemblera √† ceci :

Token | Tag
------|-----
R√©gurgitation | B-DIS
de | I-DIS
la | I-DIS
valve | I-DIS
tricuspide | I-DIS
et | O
carbonate | B-CHEM
de | I-CHEM
lithium | I-CHEM
toxicit√© | B-DIS
chez | O
un | O
nouveau-n√© | O
. | O

Puisque nous devons √©tablir une correspondance un-√†-un entre les tokens et les classes, nous pouvons entra√Æner un mod√®le neuronal **many-to-many** (plusieurs-√†-plusieurs) bas√© sur cette image :

![Image montrant des mod√®les r√©currents de r√©seaux neuronaux courants.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.fr.jpg)

> *Image tir√©e de [cet article de blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) par [Andrej Karpathy](http://karpathy.github.io/). Les mod√®les de classification de tokens NER correspondent √† l'architecture de r√©seau situ√©e √† l'extr√™me droite de cette image.*

## Entra√Ænement des mod√®les NER

√âtant donn√© qu'un mod√®le NER est essentiellement un mod√®le de classification de tokens, nous pouvons utiliser des RNN, que nous connaissons d√©j√†, pour cette t√¢che. Dans ce cas, chaque bloc du r√©seau r√©current renverra l'ID du token. L'exemple de notebook suivant montre comment entra√Æner un LSTM pour la classification de tokens.

## ‚úçÔ∏è Notebooks d'exemple : NER

Poursuivez votre apprentissage avec le notebook suivant :

* [NER avec TensorFlow](../../../../../lessons/5-NLP/19-NER/NER-TF.ipynb)

## Conclusion

Un mod√®le NER est un **mod√®le de classification de tokens**, ce qui signifie qu'il peut √™tre utilis√© pour effectuer une classification de tokens. C'est une t√¢che tr√®s courante en NLP, permettant de reconna√Ætre des entit√©s sp√©cifiques dans un texte, y compris des lieux, des noms, des dates, et bien plus encore.

## üöÄ D√©fi

R√©alisez l'exercice ci-dessous pour entra√Æner un mod√®le de reconnaissance d'entit√©s nomm√©es pour des termes m√©dicaux, puis testez-le sur un autre jeu de donn√©es.

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/38)

## R√©vision et auto-apprentissage

Lisez l'article de blog [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) et suivez la section "Lectures compl√©mentaires" de cet article pour approfondir vos connaissances.

## [Exercice](lab/README.md)

Dans l'exercice de cette le√ßon, vous devrez entra√Æner un mod√®le de reconnaissance d'entit√©s m√©dicales. Vous pouvez commencer par entra√Æner un mod√®le LSTM comme d√©crit dans cette le√ßon, puis passer √† l'utilisation du mod√®le transformateur BERT. Lisez [les instructions](lab/README.md) pour obtenir tous les d√©tails.

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.