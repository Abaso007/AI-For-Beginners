<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T11:55:29+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "fr"
}
-->
# Apprentissage par renforcement profond

L'apprentissage par renforcement (RL) est consid√©r√© comme l'un des paradigmes fondamentaux de l'apprentissage automatique, aux c√¥t√©s de l'apprentissage supervis√© et non supervis√©. Alors que l'apprentissage supervis√© repose sur un ensemble de donn√©es avec des r√©sultats connus, le RL est bas√© sur **l'apprentissage par l'exp√©rience**. Par exemple, lorsque nous d√©couvrons un jeu vid√©o pour la premi√®re fois, nous commen√ßons √† jouer, m√™me sans conna√Ætre les r√®gles, et nous am√©liorons rapidement nos comp√©tences simplement en jouant et en ajustant notre comportement.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Pour effectuer du RL, nous avons besoin de :

* Un **environnement** ou **simulateur** qui d√©finit les r√®gles du jeu. Nous devons pouvoir ex√©cuter des exp√©riences dans le simulateur et observer les r√©sultats.
* Une **fonction de r√©compense**, qui indique √† quel point notre exp√©rience a √©t√© r√©ussie. Dans le cas de l'apprentissage d'un jeu vid√©o, la r√©compense serait notre score final.

En fonction de la fonction de r√©compense, nous devrions √™tre capables d'ajuster notre comportement et d'am√©liorer nos comp√©tences, afin de mieux jouer la prochaine fois. La principale diff√©rence entre les autres types d'apprentissage automatique et le RL est qu'en RL, nous ne savons g√©n√©ralement pas si nous gagnons ou perdons avant la fin du jeu. Ainsi, nous ne pouvons pas dire si un mouvement particulier est bon ou non - nous ne recevons une r√©compense qu'√† la fin du jeu.

Pendant le RL, nous effectuons g√©n√©ralement de nombreuses exp√©riences. Lors de chaque exp√©rience, nous devons trouver un √©quilibre entre suivre la strat√©gie optimale que nous avons apprise jusqu'√† pr√©sent (**exploitation**) et explorer de nouveaux √©tats possibles (**exploration**).

## OpenAI Gym

Un excellent outil pour le RL est le [OpenAI Gym](https://gym.openai.com/) - un **environnement de simulation**, capable de simuler de nombreux environnements diff√©rents, allant des jeux Atari √† la physique derri√®re l'√©quilibre d'un poteau. C'est l'un des environnements de simulation les plus populaires pour entra√Æner des algorithmes d'apprentissage par renforcement, et il est maintenu par [OpenAI](https://openai.com/).

> **Note** : Vous pouvez voir tous les environnements disponibles dans OpenAI Gym [ici](https://gym.openai.com/envs/#classic_control).

## √âquilibrage du CartPole

Vous avez probablement d√©j√† vu des dispositifs modernes d'√©quilibrage tels que le *Segway* ou les *gyroscooters*. Ils sont capables de s'√©quilibrer automatiquement en ajustant leurs roues en r√©ponse √† un signal provenant d'un acc√©l√©rom√®tre ou d'un gyroscope. Dans cette section, nous allons apprendre √† r√©soudre un probl√®me similaire : √©quilibrer un poteau. Cela ressemble √† la situation o√π un artiste de cirque doit √©quilibrer un poteau sur sa main - mais cet √©quilibrage ne se produit qu'en 1D.

Une version simplifi√©e de l'√©quilibrage est connue sous le nom de probl√®me **CartPole**. Dans le monde du CartPole, nous avons un curseur horizontal qui peut se d√©placer √† gauche ou √† droite, et l'objectif est d'√©quilibrer un poteau vertical au-dessus du curseur pendant qu'il se d√©place.

<img alt="un CartPole" src="images/cartpole.png" width="200"/>

Pour cr√©er et utiliser cet environnement, nous avons besoin de quelques lignes de code Python :

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Chaque environnement peut √™tre utilis√© de la m√™me mani√®re :
* `env.reset` d√©marre une nouvelle exp√©rience
* `env.step` effectue une √©tape de simulation. Il re√ßoit une **action** de l'**espace d'action**, et retourne une **observation** (de l'espace d'observation), ainsi qu'une r√©compense et un indicateur de fin.

Dans l'exemple ci-dessus, nous effectuons une action al√©atoire √† chaque √©tape, ce qui explique pourquoi la dur√©e de vie de l'exp√©rience est tr√®s courte :

![CartPole sans √©quilibre](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

L'objectif d'un algorithme de RL est d'entra√Æner un mod√®le - la **politique** &pi; - qui retournera l'action en r√©ponse √† un √©tat donn√©. Nous pouvons √©galement consid√©rer la politique comme probabiliste, c'est-√†-dire que pour tout √©tat *s* et action *a*, elle retournera la probabilit√© &pi;(*a*|*s*) que nous devrions prendre *a* dans l'√©tat *s*.

## Algorithme des gradients de politique

La mani√®re la plus √©vidente de mod√©liser une politique est de cr√©er un r√©seau neuronal qui prendra les √©tats en entr√©e et retournera les actions correspondantes (ou plut√¥t les probabilit√©s de toutes les actions). En un sens, cela serait similaire √† une t√¢che de classification normale, avec une diff√©rence majeure : nous ne savons pas √† l'avance quelles actions nous devrions prendre √† chaque √©tape.

L'id√©e ici est d'estimer ces probabilit√©s. Nous construisons un vecteur de **r√©compenses cumul√©es** qui montre notre r√©compense totale √† chaque √©tape de l'exp√©rience. Nous appliquons √©galement un **rabais sur les r√©compenses** en multipliant les r√©compenses ant√©rieures par un coefficient &gamma;=0.99, afin de diminuer l'importance des r√©compenses ant√©rieures. Ensuite, nous renfor√ßons les √©tapes de l'exp√©rience qui g√©n√®rent des r√©compenses plus importantes.

> Apprenez-en davantage sur l'algorithme des gradients de politique et voyez-le en action dans le [notebook d'exemple](CartPole-RL-TF.ipynb).

## Algorithme Actor-Critic

Une version am√©lior√©e de l'approche des gradients de politique est appel√©e **Actor-Critic**. L'id√©e principale est que le r√©seau neuronal serait entra√Æn√© pour retourner deux choses :

* La politique, qui d√©termine quelle action entreprendre. Cette partie est appel√©e **acteur**.
* L'estimation de la r√©compense totale que nous pouvons esp√©rer obtenir dans cet √©tat - cette partie est appel√©e **critique**.

En un sens, cette architecture ressemble √† un [GAN](../../4-ComputerVision/10-GANs/README.md), o√π nous avons deux r√©seaux qui s'entra√Ænent l'un contre l'autre. Dans le mod√®le Actor-Critic, l'acteur propose l'action √† entreprendre, et le critique essaie d'√™tre critique et d'estimer le r√©sultat. Cependant, notre objectif est d'entra√Æner ces r√©seaux en harmonie.

Parce que nous connaissons √† la fois les r√©compenses cumul√©es r√©elles et les r√©sultats retourn√©s par le critique pendant l'exp√©rience, il est relativement facile de construire une fonction de perte qui minimisera la diff√©rence entre eux. Cela nous donnerait la **perte du critique**. Nous pouvons calculer la **perte de l'acteur** en utilisant la m√™me approche que dans l'algorithme des gradients de politique.

Apr√®s avoir ex√©cut√© l'un de ces algorithmes, nous pouvons nous attendre √† ce que notre CartPole se comporte comme ceci :

![CartPole √©quilibr√©](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Exercices : Gradients de politique et RL Actor-Critic

Poursuivez votre apprentissage dans les notebooks suivants :

* [RL avec TensorFlow](CartPole-RL-TF.ipynb)
* [RL avec PyTorch](CartPole-RL-PyTorch.ipynb)

## Autres t√¢ches de RL

L'apprentissage par renforcement est aujourd'hui un domaine de recherche en pleine croissance. Voici quelques exemples int√©ressants d'apprentissage par renforcement :

* Apprendre √† un ordinateur √† jouer √† des **jeux Atari**. La difficult√© de ce probl√®me r√©side dans le fait que nous n'avons pas un √©tat simple repr√©sent√© sous forme de vecteur, mais plut√¥t une capture d'√©cran - et nous devons utiliser un CNN pour convertir cette image en vecteur de caract√©ristiques ou pour extraire des informations de r√©compense. Les jeux Atari sont disponibles dans le Gym.
* Apprendre √† un ordinateur √† jouer √† des jeux de plateau, comme les √©checs et le Go. R√©cemment, des programmes de pointe comme **Alpha Zero** ont √©t√© entra√Æn√©s √† partir de z√©ro par deux agents jouant l'un contre l'autre et s'am√©liorant √† chaque √©tape.
* Dans l'industrie, le RL est utilis√© pour cr√©er des syst√®mes de contr√¥le √† partir de simulations. Un service appel√© [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) est sp√©cialement con√ßu pour cela.

## Conclusion

Nous avons maintenant appris √† entra√Æner des agents pour obtenir de bons r√©sultats simplement en leur fournissant une fonction de r√©compense qui d√©finit l'√©tat souhait√© du jeu, et en leur donnant l'opportunit√© d'explorer intelligemment l'espace de recherche. Nous avons essay√© avec succ√®s deux algorithmes et obtenu de bons r√©sultats en relativement peu de temps. Cependant, ce n'est que le d√©but de votre voyage dans le RL, et vous devriez envisager de suivre un cours d√©di√© si vous souhaitez approfondir vos connaissances.

## üöÄ D√©fi

Explorez les applications list√©es dans la section "Autres t√¢ches de RL" et essayez d'en impl√©menter une !

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## R√©vision et auto-apprentissage

Apprenez-en davantage sur l'apprentissage par renforcement classique dans notre [programme d'apprentissage automatique pour d√©butants](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Regardez [cette excellente vid√©o](https://www.youtube.com/watch?v=qv6UVOQ0F44) qui explique comment un ordinateur peut apprendre √† jouer √† Super Mario.

## Devoir : [Entra√Æner une voiture de montagne](lab/README.md)

Votre objectif pour ce devoir sera d'entra√Æner un environnement Gym diff√©rent - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

