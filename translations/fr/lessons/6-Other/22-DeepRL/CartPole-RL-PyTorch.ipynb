{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraîner un RL à équilibrer un Cartpole\n",
    "\n",
    "Ce notebook fait partie du [programme AI for Beginners](http://aka.ms/ai-beginners). Il s'inspire du [tutoriel officiel de PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) et de [cette implémentation Cartpole avec PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Dans cet exemple, nous utiliserons le RL pour entraîner un modèle à équilibrer une barre sur un chariot qui peut se déplacer à gauche et à droite sur une échelle horizontale. Nous utiliserons l'environnement [OpenAI Gym](https://www.gymlibrary.ml/) pour simuler la barre.\n",
    "\n",
    "> **Note** : Vous pouvez exécuter le code de cette leçon localement (par exemple, depuis Visual Studio Code), auquel cas la simulation s'ouvrira dans une nouvelle fenêtre. Lorsque vous exécutez le code en ligne, il peut être nécessaire d'apporter quelques ajustements au code, comme décrit [ici](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Nous commencerons par nous assurer que Gym est installé :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons maintenant l'environnement CartPole et voyons comment l'utiliser. Un environnement possède les propriétés suivantes :\n",
    "\n",
    "* **Action space** est l'ensemble des actions possibles que nous pouvons effectuer à chaque étape de la simulation  \n",
    "* **Observation space** est l'ensemble des observations que nous pouvons réaliser  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons comment fonctionne la simulation. La boucle suivante exécute la simulation jusqu'à ce que `env.step` ne renvoie plus le drapeau de terminaison `done`. Nous choisirons des actions de manière aléatoire en utilisant `env.action_space.sample()`, ce qui signifie que l'expérience échouera probablement très rapidement (l'environnement CartPole se termine lorsque la vitesse du CartPole, sa position ou son angle dépassent certaines limites).\n",
    "\n",
    "> La simulation s'ouvrira dans une nouvelle fenêtre. Vous pouvez exécuter le code plusieurs fois et observer son comportement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez remarquer que les observations contiennent 4 nombres. Ce sont :\n",
    "- Position du chariot\n",
    "- Vitesse du chariot\n",
    "- Angle de la tige\n",
    "- Taux de rotation de la tige\n",
    "\n",
    "`rew` est la récompense que nous recevons à chaque étape. Vous pouvez constater que dans l'environnement CartPole, vous recevez 1 point de récompense pour chaque étape de simulation, et l'objectif est de maximiser la récompense totale, c'est-à-dire le temps pendant lequel le CartPole peut rester en équilibre sans tomber.\n",
    "\n",
    "Pendant l'apprentissage par renforcement, notre objectif est d'entraîner une **politique** $\\pi$, qui pour chaque état $s$ nous indiquera quelle action $a$ entreprendre, donc essentiellement $a = \\pi(s)$.\n",
    "\n",
    "Si vous souhaitez une solution probabiliste, vous pouvez considérer la politique comme renvoyant un ensemble de probabilités pour chaque action, c'est-à-dire que $\\pi(a|s)$ représenterait la probabilité que nous devrions entreprendre l'action $a$ dans l'état $s$.\n",
    "\n",
    "## Méthode du Gradient de Politique\n",
    "\n",
    "Dans l'algorithme d'apprentissage par renforcement le plus simple, appelé **Gradient de Politique**, nous allons entraîner un réseau de neurones à prédire la prochaine action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons entraîner le réseau en réalisant de nombreuses expériences et en mettant à jour notre réseau après chaque exécution. Définissons une fonction qui exécutera l'expérience et renverra les résultats (le **trace** ainsi nommé) - tous les états, actions (et leurs probabilités recommandées), et récompenses :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez exécuter un épisode avec un réseau non entraîné et observer que la récompense totale (AKA durée de l'épisode) est très faible :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'un des aspects délicats de l'algorithme de gradient de politique est d'utiliser **les récompenses actualisées**. L'idée est que nous calculons le vecteur des récompenses totales à chaque étape du jeu, et pendant ce processus, nous actualisons les premières récompenses en utilisant un coefficient $gamma$. Nous normalisons également le vecteur résultant, car nous l'utiliserons comme poids pour influencer notre entraînement :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant à l'entraînement proprement dit ! Nous allons exécuter 300 épisodes, et à chaque épisode, nous effectuerons les étapes suivantes :\n",
    "\n",
    "1. Exécuter l'expérience et collecter la trace.\n",
    "2. Calculer la différence (`gradients`) entre les actions effectuées et les probabilités prédites. Plus cette différence est faible, plus nous sommes certains d'avoir pris la bonne décision.\n",
    "3. Calculer les récompenses actualisées et multiplier les gradients par ces récompenses actualisées - cela garantit que les étapes avec des récompenses plus élevées auront un impact plus important sur le résultat final que celles avec des récompenses plus faibles.\n",
    "4. Les actions cibles attendues pour notre réseau neuronal seront en partie issues des probabilités prédites pendant l'exécution, et en partie des gradients calculés. Nous utiliserons le paramètre `alpha` pour déterminer dans quelle mesure les gradients et les récompenses sont pris en compte - c'est ce qu'on appelle le *taux d'apprentissage* de l'algorithme de renforcement.\n",
    "5. Enfin, nous entraînons notre réseau sur les états et les actions attendues, puis nous répétons le processus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, lançons l'épisode avec le rendu pour voir le résultat :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espérons que vous pouvez constater que la tige peut maintenant s'équilibrer assez bien !\n",
    "\n",
    "## Modèle Acteur-Critique\n",
    "\n",
    "Le modèle Acteur-Critique est une évolution des gradients de politique, dans lequel nous construisons un réseau neuronal pour apprendre à la fois la politique et les récompenses estimées. Le réseau aura deux sorties (ou vous pouvez le voir comme deux réseaux distincts) :\n",
    "* **Acteur** recommandera l'action à entreprendre en nous donnant la distribution de probabilité des états, comme dans le modèle de gradient de politique.\n",
    "* **Critique** estimera quelle serait la récompense issue de ces actions. Il renvoie les récompenses totales estimées dans le futur pour l'état donné.\n",
    "\n",
    "Définissons un tel modèle :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devrions légèrement modifier nos fonctions `discounted_rewards` et `run_episode` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons exécuter la boucle principale d'entraînement. Nous utiliserons un processus d'entraînement manuel du réseau en calculant les fonctions de perte appropriées et en mettant à jour les paramètres du réseau :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points clés\n",
    "\n",
    "Nous avons vu deux algorithmes de RL dans cette démonstration : le gradient de politique simple et l'acteur-critique plus sophistiqué. Vous pouvez constater que ces algorithmes fonctionnent avec des notions abstraites d'état, d'action et de récompense - ce qui leur permet d'être appliqués à des environnements très différents.\n",
    "\n",
    "L'apprentissage par renforcement nous permet d'apprendre la meilleure stratégie pour résoudre un problème simplement en observant la récompense finale. Le fait de ne pas avoir besoin de jeux de données étiquetés nous permet de répéter les simulations plusieurs fois afin d'optimiser nos modèles. Cependant, il reste encore de nombreux défis dans le domaine du RL, que vous pourrez découvrir si vous décidez de vous concentrer davantage sur cette branche fascinante de l'IA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-31T14:25:30+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}