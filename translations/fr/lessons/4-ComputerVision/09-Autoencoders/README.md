<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-24T20:49:30+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "fr"
}
-->
# Autoencodeurs

Lors de l'entra√Ænement des CNN, l'un des probl√®mes est que nous avons besoin de beaucoup de donn√©es √©tiquet√©es. Dans le cas de la classification d'images, nous devons s√©parer les images en diff√©rentes classes, ce qui demande un effort manuel.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Cependant, nous pourrions vouloir utiliser des donn√©es brutes (non √©tiquet√©es) pour entra√Æner des extracteurs de caract√©ristiques CNN, ce que l'on appelle **l'apprentissage auto-supervis√©**. Au lieu d'utiliser des √©tiquettes, nous utilisons les images d'entra√Ænement √† la fois comme entr√©e et sortie du r√©seau. L'id√©e principale de l'**autoencodeur** est d'avoir un **r√©seau encodeur** qui convertit l'image d'entr√©e en un certain **espace latent** (g√©n√©ralement un vecteur de taille r√©duite), puis un **r√©seau d√©codeur**, dont l'objectif est de reconstruire l'image originale.

> ‚úÖ Un [autoencodeur](https://wikipedia.org/wiki/Autoencoder) est "un type de r√©seau de neurones artificiels utilis√© pour apprendre des codages efficaces de donn√©es non √©tiquet√©es."

Comme nous entra√Ænons un autoencodeur pour capturer autant d'informations que possible de l'image originale afin de la reconstruire avec pr√©cision, le r√©seau essaie de trouver la meilleure **repr√©sentation** des images d'entr√©e pour en capturer le sens.

![Sch√©ma Autoencodeur](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.fr.jpg)

> Image tir√©e du [blog Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Sc√©narios d'utilisation des autoencodeurs

Bien que reconstruire des images originales ne semble pas utile en soi, il existe quelques sc√©narios o√π les autoencodeurs sont particuli√®rement utiles :

* **R√©duction de la dimension des images pour la visualisation** ou **entra√Ænement des repr√©sentations d'images**. Les autoencodeurs donnent g√©n√©ralement de meilleurs r√©sultats que l'ACP, car ils prennent en compte la nature spatiale des images et les caract√©ristiques hi√©rarchiques.
* **D√©noiser**, c'est-√†-dire supprimer le bruit de l'image. Comme le bruit contient beaucoup d'informations inutiles, l'autoencodeur ne peut pas tout int√©grer dans un espace latent relativement petit, et capture donc uniquement la partie importante de l'image. Lors de l'entra√Ænement des d√©bruiteurs, nous partons d'images originales et utilisons des images avec du bruit artificiellement ajout√© comme entr√©e pour l'autoencodeur.
* **Super-r√©solution**, augmentation de la r√©solution des images. Nous partons d'images haute r√©solution et utilisons l'image avec une r√©solution inf√©rieure comme entr√©e de l'autoencodeur.
* **Mod√®les g√©n√©ratifs**. Une fois l'autoencodeur entra√Æn√©, la partie d√©codeur peut √™tre utilis√©e pour cr√©er de nouveaux objets √† partir de vecteurs latents al√©atoires.

## Autoencodeurs Variationnels (VAE)

Les autoencodeurs traditionnels r√©duisent la dimension des donn√©es d'entr√©e d'une certaine mani√®re, en identifiant les caract√©ristiques importantes des images d'entr√©e. Cependant, les vecteurs latents n'ont souvent pas beaucoup de sens. En d'autres termes, en prenant l'exemple du jeu de donn√©es MNIST, il n'est pas facile de d√©terminer quels chiffres correspondent √† diff√©rents vecteurs latents, car des vecteurs latents proches ne correspondent pas n√©cessairement aux m√™mes chiffres.

En revanche, pour entra√Æner des mod√®les *g√©n√©ratifs*, il est pr√©f√©rable de comprendre l'espace latent. Cette id√©e nous conduit √† l'**autoencodeur variationnel** (VAE).

Le VAE est un autoencodeur qui apprend √† pr√©dire une *distribution statistique* des param√®tres latents, appel√©e **distribution latente**. Par exemple, nous pouvons vouloir que les vecteurs latents soient distribu√©s normalement avec une certaine moyenne z<sub>mean</sub> et un √©cart-type z<sub>sigma</sub> (la moyenne et l'√©cart-type √©tant tous deux des vecteurs de dimensionnalit√© d). L'encodeur dans le VAE apprend √† pr√©dire ces param√®tres, puis le d√©codeur prend un vecteur al√©atoire de cette distribution pour reconstruire l'objet.

Pour r√©sumer :

 * √Ä partir du vecteur d'entr√©e, nous pr√©disons `z_mean` et `z_log_sigma` (au lieu de pr√©dire directement l'√©cart-type, nous pr√©disons son logarithme)
 * Nous √©chantillonnons un vecteur `sample` √† partir de la distribution N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Le d√©codeur essaie de d√©coder l'image originale en utilisant `sample` comme vecteur d'entr√©e

 <img src="images/vae.png" width="50%">

> Image tir√©e de [cet article de blog](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) par Isaak Dykeman

Les autoencodeurs variationnels utilisent une fonction de perte complexe compos√©e de deux parties :

* **Perte de reconstruction**, qui est la fonction de perte indiquant √† quel point une image reconstruite est proche de la cible (cela peut √™tre l'erreur quadratique moyenne, ou MSE). C'est la m√™me fonction de perte que dans les autoencodeurs normaux.
* **Perte KL**, qui garantit que les distributions des variables latentes restent proches d'une distribution normale. Elle est bas√©e sur la notion de [divergence de Kullback-Leibler](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - une m√©trique pour estimer la similarit√© entre deux distributions statistiques.

Un avantage important des VAE est qu'ils permettent de g√©n√©rer de nouvelles images relativement facilement, car nous savons de quelle distribution √©chantillonner les vecteurs latents. Par exemple, si nous entra√Ænons un VAE avec un vecteur latent 2D sur MNIST, nous pouvons ensuite faire varier les composantes du vecteur latent pour obtenir diff√©rents chiffres :

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Image par [Dmitry Soshnikov](http://soshnikov.com)

Observez comment les images se fondent les unes dans les autres, √† mesure que nous obtenons des vecteurs latents provenant de diff√©rentes parties de l'espace des param√®tres latents. Nous pouvons √©galement visualiser cet espace en 2D :

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Image par [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Exercices : Autoencodeurs

Apprenez-en davantage sur les autoencodeurs dans ces notebooks correspondants :

* [Autoencodeurs avec TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [Autoencodeurs avec PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Propri√©t√©s des Autoencodeurs

* **Sp√©cifiques aux donn√©es** - ils ne fonctionnent bien qu'avec le type d'images sur lequel ils ont √©t√© entra√Æn√©s. Par exemple, si nous entra√Ænons un r√©seau de super-r√©solution sur des fleurs, il ne fonctionnera pas bien sur des portraits. Cela s'explique par le fait que le r√©seau peut produire une image de r√©solution sup√©rieure en utilisant les d√©tails fins appris √† partir du jeu de donn√©es d'entra√Ænement.
* **Avec perte** - l'image reconstruite n'est pas identique √† l'image originale. La nature de la perte est d√©finie par la *fonction de perte* utilis√©e pendant l'entra√Ænement.
* Fonctionne sur des donn√©es **non √©tiquet√©es**

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Conclusion

Dans cette le√ßon, vous avez appris les diff√©rents types d'autoencodeurs disponibles pour le scientifique en IA. Vous avez appris √† les construire et √† les utiliser pour reconstruire des images. Vous avez √©galement d√©couvert le VAE et comment l'utiliser pour g√©n√©rer de nouvelles images.

## üöÄ D√©fi

Dans cette le√ßon, vous avez appris √† utiliser les autoencodeurs pour les images. Mais ils peuvent √©galement √™tre utilis√©s pour la musique ! Consultez le projet [MusicVAE](https://magenta.tensorflow.org/music-vae) du projet Magenta, qui utilise des autoencodeurs pour apprendre √† reconstruire de la musique. Faites quelques [exp√©riences](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) avec cette biblioth√®que pour voir ce que vous pouvez cr√©er.

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## R√©vision & Auto-apprentissage

Pour r√©f√©rence, lisez-en davantage sur les autoencodeurs dans ces ressources :

* [Construire des Autoencodeurs avec Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Article de blog sur NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Explication des Autoencodeurs Variationnels](https://kvfrans.com/variational-autoencoders-explained/)
* [Autoencodeurs Variationnels Conditionnels](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Devoir

√Ä la fin de [ce notebook utilisant TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb), vous trouverez une "t√¢che" - utilisez-la comme devoir.

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.