<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f7b97b375358cb51a1e098df306bf73",
  "translation_date": "2025-08-24T20:51:44+00:00",
  "source_file": "lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md",
  "language_code": "fr"
}
-->
# Architectures CNN Bien Connues

### VGG-16

VGG-16 est un r√©seau qui a atteint une pr√©cision de 92,7 % dans la classification top-5 d'ImageNet en 2014. Il poss√®de la structure de couches suivante :

![Couches ImageNet](../../../../../translated_images/vgg-16-arch1.d901a5583b3a51baeaab3e768567d921e5d54befa46e1e642616c5458c934028.fr.jpg)

Comme vous pouvez le voir, VGG suit une architecture pyramidale traditionnelle, qui est une s√©quence de couches de convolution et de pooling.

![Pyramide ImageNet](../../../../../translated_images/vgg-16-arch.64ff2137f50dd49fdaa786e3f3a975b3f22615efd13efb19c5d22f12e01451a1.fr.jpg)

> Image tir√©e de [Researchgate](https://www.researchgate.net/figure/Vgg16-model-structure-To-get-the-VGG-NIN-model-we-replace-the-2-nd-4-th-6-th-7-th_fig2_335194493)

### ResNet

ResNet est une famille de mod√®les propos√©e par Microsoft Research en 2015. L'id√©e principale de ResNet est d'utiliser des **blocs r√©siduels** :

<img src="images/resnet-block.png" width="300"/>

> Image tir√©e de [cet article](https://arxiv.org/pdf/1512.03385.pdf)

La raison d'utiliser un passage identitaire est de permettre √† notre couche de pr√©dire **la diff√©rence** entre le r√©sultat d'une couche pr√©c√©dente et la sortie du bloc r√©siduel - d'o√π le nom *r√©siduel*. Ces blocs sont beaucoup plus faciles √† entra√Æner, et il est possible de construire des r√©seaux avec plusieurs centaines de ces blocs (les variantes les plus courantes sont ResNet-52, ResNet-101 et ResNet-152).

Vous pouvez √©galement consid√©rer ce r√©seau comme capable d'ajuster sa complexit√© au jeu de donn√©es. Au d√©but, lorsque vous commencez √† entra√Æner le r√©seau, les valeurs des poids sont faibles, et la plupart du signal passe par les couches identitaires. Au fur et √† mesure de l'entra√Ænement et de l'augmentation des poids, l'importance des param√®tres du r√©seau grandit, et le r√©seau s'ajuste pour fournir la puissance expressive n√©cessaire √† la classification correcte des images d'entra√Ænement.

### Google Inception

L'architecture Google Inception pousse cette id√©e encore plus loin et construit chaque couche du r√©seau comme une combinaison de plusieurs chemins diff√©rents :

<img src="images/inception.png" width="400"/>

> Image tir√©e de [Researchgate](https://www.researchgate.net/figure/Inception-module-with-dimension-reductions-left-and-schema-for-Inception-ResNet-v1_fig2_355547454)

Ici, il est important de souligner le r√¥le des convolutions 1x1, car au premier abord, elles peuvent sembler inutiles. Pourquoi utiliser un filtre 1x1 sur une image ? Cependant, il faut se rappeler que les filtres de convolution travaillent √©galement avec plusieurs canaux de profondeur (initialement - les couleurs RGB, dans les couches suivantes - des canaux pour diff√©rents filtres), et la convolution 1x1 est utilis√©e pour m√©langer ces canaux d'entr√©e √† l'aide de poids entra√Ænables diff√©rents. Elle peut √©galement √™tre vue comme un √©chantillonnage (pooling) sur la dimension des canaux.

Voici [un excellent article de blog](https://medium.com/analytics-vidhya/talented-mr-1x1-comprehensive-look-at-1x1-convolution-in-deep-learning-f6b355825578) sur le sujet, ainsi que [l'article original](https://arxiv.org/pdf/1312.4400.pdf).

### MobileNet

MobileNet est une famille de mod√®les de taille r√©duite, adapt√©e aux appareils mobiles. Utilisez-les si vous disposez de ressources limit√©es et que vous pouvez sacrifier un peu de pr√©cision. L'id√©e principale derri√®re ces mod√®les est la **convolution s√©par√©e en profondeur**, qui permet de repr√©senter les filtres de convolution par une composition de convolutions spatiales et de convolutions 1x1 sur les canaux de profondeur. Cela r√©duit consid√©rablement le nombre de param√®tres, rendant le r√©seau plus petit et √©galement plus facile √† entra√Æner avec moins de donn√©es.

Voici [un excellent article de blog sur MobileNet](https://medium.com/analytics-vidhya/image-classification-with-mobilenet-cc6fbb2cd470).

## Conclusion

Dans cette unit√©, vous avez appris le concept principal derri√®re les r√©seaux neuronaux de vision par ordinateur - les r√©seaux convolutifs. Les architectures r√©elles qui alimentent la classification d'images, la d√©tection d'objets et m√™me les r√©seaux de g√©n√©ration d'images sont toutes bas√©es sur les CNN, avec simplement plus de couches et quelques astuces d'entra√Ænement suppl√©mentaires.

## üöÄ D√©fi

Dans les notebooks associ√©s, il y a des notes en bas sur la fa√ßon d'obtenir une meilleure pr√©cision. Faites quelques exp√©riences pour voir si vous pouvez atteindre une pr√©cision plus √©lev√©e.

## [Quiz post-cours](https://ff-quizzes.netlify.app/en/ai/quiz/14)

## R√©vision & Auto-apprentissage

Bien que les CNN soient le plus souvent utilis√©s pour les t√¢ches de vision par ordinateur, ils sont g√©n√©ralement efficaces pour extraire des motifs de taille fixe. Par exemple, si nous travaillons avec des sons, nous pourrions √©galement vouloir utiliser des CNN pour rechercher certains motifs sp√©cifiques dans le signal audio - dans ce cas, les filtres seraient unidimensionnels (et ce CNN serait appel√© 1D-CNN). Parfois, un 3D-CNN est √©galement utilis√© pour extraire des caract√©ristiques dans un espace multidimensionnel, comme certains √©v√©nements se produisant dans une vid√©o - le CNN peut capturer certains motifs de changement de caract√©ristiques au fil du temps. Faites des recherches et de l'auto-apprentissage sur d'autres t√¢ches pouvant √™tre r√©alis√©es avec des CNN.

## [Devoir](lab/README.md)

Dans ce laboratoire, vous devez classifier diff√©rentes races de chats et de chiens. Ces images sont plus complexes que le jeu de donn√©es MNIST, ont des dimensions plus √©lev√©es, et il y a plus de 10 classes.

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.