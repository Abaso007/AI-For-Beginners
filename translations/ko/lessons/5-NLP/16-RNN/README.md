<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-24T21:23:48+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "ko"
}
-->
# 순환 신경망 (Recurrent Neural Networks)

## [강의 전 퀴즈](https://ff-quizzes.netlify.app/en/ai/quiz/31)

이전 섹션에서는 텍스트의 풍부한 의미 표현과 임베딩 위에 간단한 선형 분류기를 사용했습니다. 이 아키텍처는 문장에서 단어의 집합적인 의미를 포착하지만, 임베딩 위의 집계 작업이 원본 텍스트에서 이 정보를 제거했기 때문에 단어의 **순서**를 고려하지 않습니다. 이러한 모델은 단어 순서를 모델링할 수 없기 때문에 텍스트 생성이나 질문 응답과 같은 더 복잡하거나 모호한 작업을 해결할 수 없습니다.

텍스트 시퀀스의 의미를 포착하려면 **순환 신경망**(Recurrent Neural Network, RNN)이라는 다른 신경망 아키텍처를 사용해야 합니다. RNN에서는 문장을 네트워크에 한 번에 하나의 기호씩 전달하며, 네트워크는 **상태**를 생성하고 이를 다음 기호와 함께 다시 네트워크에 전달합니다.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.ko.png)

> 작성자 제공 이미지

토큰 입력 시퀀스 X<sub>0</sub>,...,X<sub>n</sub>이 주어지면, RNN은 신경망 블록의 시퀀스를 생성하고 역전파를 사용하여 이 시퀀스를 끝까지 훈련합니다. 각 네트워크 블록은 (X<sub>i</sub>,S<sub>i</sub>) 쌍을 입력으로 받아들이고 결과로 S<sub>i+1</sub>을 생성합니다. 최종 상태 S<sub>n</sub> 또는 (출력 Y<sub>n</sub>)은 선형 분류기로 전달되어 결과를 생성합니다. 모든 네트워크 블록은 동일한 가중치를 공유하며, 하나의 역전파 패스를 통해 끝까지 훈련됩니다.

상태 벡터 S<sub>0</sub>,...,S<sub>n</sub>이 네트워크를 통해 전달되기 때문에 단어 간의 순차적 의존성을 학습할 수 있습니다. 예를 들어, 시퀀스 어딘가에 *not*이라는 단어가 나타나면, 상태 벡터 내 특정 요소를 부정하도록 학습하여 부정을 나타낼 수 있습니다.

> ✅ 위 그림에서 모든 RNN 블록의 가중치가 공유되므로, 동일한 그림을 하나의 블록(오른쪽)으로 표현할 수 있으며, 순환 피드백 루프를 통해 네트워크의 출력 상태를 다시 입력으로 전달합니다.

## RNN 셀의 구조

간단한 RNN 셀이 어떻게 구성되는지 살펴보겠습니다. 이전 상태 S<sub>i-1</sub>과 현재 기호 X<sub>i</sub>를 입력으로 받아들이며, 출력 상태 S<sub>i</sub>를 생성해야 합니다(때로는 생성 네트워크의 경우처럼 다른 출력 Y<sub>i</sub>에도 관심이 있습니다).

간단한 RNN 셀에는 내부에 두 개의 가중치 행렬이 있습니다. 하나는 입력 기호를 변환(W라고 부르겠습니다)하고, 다른 하나는 입력 상태를 변환(H)합니다. 이 경우 네트워크의 출력은 σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b)로 계산되며, 여기서 σ는 활성화 함수이고 b는 추가적인 바이어스입니다.

<img alt="RNN 셀 구조" src="images/rnn-anatomy.png" width="50%"/>

> 작성자 제공 이미지

많은 경우, 입력 토큰은 RNN에 들어가기 전에 임베딩 레이어를 통해 전달되어 차원을 낮춥니다. 이 경우, 입력 벡터의 차원이 *emb_size*이고 상태 벡터가 *hid_size*라면, W의 크기는 *emb_size*×*hid_size*이고 H의 크기는 *hid_size*×*hid_size*입니다.

## 장단기 메모리 (LSTM)

고전적인 RNN의 주요 문제 중 하나는 **기울기 소실** 문제입니다. RNN은 하나의 역전파 패스를 통해 끝까지 훈련되기 때문에 네트워크의 첫 번째 레이어로 오류를 전달하기가 어렵고, 따라서 먼 토큰 간의 관계를 학습할 수 없습니다. 이 문제를 피하기 위한 방법 중 하나는 **게이트**를 사용하여 **명시적인 상태 관리**를 도입하는 것입니다. 이러한 종류의 잘 알려진 아키텍처로는 **장단기 메모리**(Long Short Term Memory, LSTM)와 **게이트 순환 유닛**(Gated Relay Unit, GRU)이 있습니다.

![장단기 메모리 셀 예시를 보여주는 이미지](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> 이미지 출처 TBD

LSTM 네트워크는 RNN과 유사한 방식으로 구성되지만, 레이어 간에 전달되는 두 가지 상태가 있습니다: 실제 상태 C와 숨겨진 벡터 H. 각 유닛에서 숨겨진 벡터 H<sub>i</sub>는 입력 X<sub>i</sub>와 연결되며, **게이트**를 통해 상태 C에서 어떤 일이 발생할지를 제어합니다. 각 게이트는 시그모이드 활성화(출력 범위 [0,1])를 가진 신경망으로, 상태 벡터와 곱해질 때 비트 마스크로 생각할 수 있습니다. 아래 그림에서 왼쪽에서 오른쪽으로 다음과 같은 게이트가 있습니다:

* **Forget 게이트**는 숨겨진 벡터를 사용하여 벡터 C의 어떤 구성 요소를 잊어야 할지, 어떤 것을 통과시켜야 할지를 결정합니다.
* **Input 게이트**는 입력 및 숨겨진 벡터에서 일부 정보를 가져와 상태에 삽입합니다.
* **Output 게이트**는 상태를 *tanh* 활성화가 있는 선형 레이어를 통해 변환한 다음, 숨겨진 벡터 H<sub>i</sub>를 사용하여 상태 C<sub>i+1</sub>의 새로운 구성 요소를 선택합니다.

상태 C의 구성 요소는 켜고 끌 수 있는 플래그로 생각할 수 있습니다. 예를 들어, 시퀀스에서 *Alice*라는 이름을 만나면 여성 캐릭터를 나타낸다고 가정하고 상태에서 여성 명사가 있다는 플래그를 올릴 수 있습니다. 이후 *and Tom*이라는 구절을 만나면 복수 명사가 있다는 플래그를 올릴 수 있습니다. 따라서 상태를 조작함으로써 문장 부분의 문법적 속성을 추적할 수 있습니다.

> ✅ LSTM의 내부를 이해하는 데 훌륭한 자료는 Christopher Olah의 [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)입니다.

## 양방향 및 다층 RNN

우리는 시퀀스의 시작부터 끝까지 한 방향으로 작동하는 순환 네트워크에 대해 논의했습니다. 이는 우리가 읽거나 말을 듣는 방식과 유사하기 때문에 자연스럽게 보입니다. 그러나 많은 실용적인 경우에 입력 시퀀스에 무작위로 접근할 수 있으므로 순환 계산을 양방향으로 실행하는 것이 합리적일 수 있습니다. 이러한 네트워크를 **양방향** RNN이라고 합니다. 양방향 네트워크를 다룰 때는 각 방향에 대해 하나씩 두 개의 숨겨진 상태 벡터가 필요합니다.

순환 네트워크는 한 방향이든 양방향이든 시퀀스 내 특정 패턴을 캡처하고 이를 상태 벡터에 저장하거나 출력으로 전달할 수 있습니다. 합성곱 네트워크와 마찬가지로 첫 번째 레이어에서 추출한 저수준 패턴을 기반으로 더 높은 수준의 패턴을 캡처하기 위해 첫 번째 레이어 위에 또 다른 순환 레이어를 구축할 수 있습니다. 이는 **다층 RNN**의 개념으로 이어지며, 두 개 이상의 순환 네트워크로 구성되며 이전 레이어의 출력이 다음 레이어의 입력으로 전달됩니다.

![다층 장단기 메모리 RNN을 보여주는 이미지](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.ko.jpg)

*Fernando López의 [이 훌륭한 글](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3)에서 가져온 이미지*

## ✍️ 연습: 임베딩

다음 노트북에서 학습을 계속하세요:

* [PyTorch를 사용한 RNN](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow를 사용한 RNN](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## 결론

이 단원에서는 RNN이 시퀀스 분류에 사용될 수 있음을 보았지만, 실제로는 텍스트 생성, 기계 번역 등 더 많은 작업을 처리할 수 있습니다. 다음 단원에서 이러한 작업을 고려할 것입니다.

## 🚀 도전 과제

LSTM에 대한 문헌을 읽고 그 응용 사례를 고려해보세요:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [강의 후 퀴즈](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## 복습 및 자기 학습

- Christopher Olah의 [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## [과제: 노트북](assignment.md)

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.