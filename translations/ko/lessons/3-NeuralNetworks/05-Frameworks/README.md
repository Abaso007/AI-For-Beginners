<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-24T21:34:28+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "ko"
}
-->
# 신경망 프레임워크

이미 배운 것처럼, 신경망을 효율적으로 학습시키기 위해서는 두 가지를 수행해야 합니다:

* 텐서를 다루는 작업, 예를 들어 곱셈, 덧셈, 그리고 sigmoid나 softmax 같은 함수 계산
* 모든 표현식의 기울기를 계산하여 경사 하강법 최적화를 수행

## [강의 전 퀴즈](https://ff-quizzes.netlify.app/en/ai/quiz/9)

`numpy` 라이브러리가 첫 번째 작업을 수행할 수 있지만, 기울기를 계산할 메커니즘이 필요합니다. 이전 섹션에서 개발한 [우리의 프레임워크](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb)에서는 `backward` 메서드 내부에서 모든 미분 함수를 수동으로 프로그래밍해야 했습니다. 이 메서드는 역전파를 수행합니다. 이상적으로는 프레임워크가 우리가 정의할 수 있는 *어떤 표현식*의 기울기를 계산할 수 있는 기능을 제공해야 합니다.

또 다른 중요한 점은 GPU나 [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)와 같은 특수 계산 장치에서 계산을 수행할 수 있는 능력입니다. 딥 신경망 학습은 *매우 많은* 계산을 요구하며, 이러한 계산을 GPU에서 병렬화하는 것이 매우 중요합니다.

> ✅ '병렬화'라는 용어는 계산을 여러 장치에 분배하는 것을 의미합니다.

현재 가장 인기 있는 신경망 프레임워크는 [TensorFlow](http://TensorFlow.org)와 [PyTorch](https://pytorch.org/)입니다. 두 프레임워크 모두 CPU와 GPU에서 텐서를 다룰 수 있는 저수준 API를 제공합니다. 저수준 API 외에도 [Keras](https://keras.io/)와 [PyTorch Lightning](https://pytorchlightning.ai/)라는 고수준 API도 있습니다.

저수준 API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
고수준 API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**저수준 API**는 **계산 그래프**를 구축할 수 있게 해줍니다. 이 그래프는 주어진 입력 매개변수로 출력(일반적으로 손실 함수)을 계산하는 방법을 정의하며, GPU가 사용 가능한 경우 GPU에서 계산을 수행할 수 있습니다. 이 계산 그래프를 미분하고 기울기를 계산하는 함수가 있으며, 이를 통해 모델 매개변수를 최적화할 수 있습니다.

**고수준 API**는 신경망을 **레이어의 순서**로 간주하며, 대부분의 신경망을 쉽게 구성할 수 있게 해줍니다. 모델 학습은 일반적으로 데이터를 준비한 후 `fit` 함수를 호출하여 수행됩니다.

고수준 API는 일반적인 신경망을 매우 빠르게 구성할 수 있도록 하며, 많은 세부 사항에 대해 걱정할 필요가 없습니다. 반면, 저수준 API는 학습 과정에 대한 더 많은 제어를 제공하므로 새로운 신경망 아키텍처를 다룰 때 연구에서 많이 사용됩니다.

또한 두 API를 함께 사용할 수 있다는 점을 이해하는 것이 중요합니다. 예를 들어, 저수준 API를 사용하여 자체 네트워크 레이어 아키텍처를 개발한 후, 이를 고수준 API로 구성된 더 큰 네트워크 내에서 사용할 수 있습니다. 또는 레이어의 순서로 고수준 API를 사용하여 네트워크를 정의한 후, 자체 저수준 학습 루프를 사용하여 최적화를 수행할 수 있습니다. 두 API는 동일한 기본 개념을 사용하며, 서로 잘 작동하도록 설계되었습니다.

## 학습

이 과정에서는 PyTorch와 TensorFlow 모두에 대한 콘텐츠를 제공합니다. 선호하는 프레임워크를 선택하고 해당 노트북만 학습하면 됩니다. 어떤 프레임워크를 선택해야 할지 모르겠다면, **PyTorch vs. TensorFlow**에 대한 인터넷 논의를 읽어보세요. 두 프레임워크를 살펴보며 더 나은 이해를 얻을 수도 있습니다.

가능한 경우, 간단함을 위해 고수준 API를 사용할 것입니다. 하지만 신경망이 기초부터 어떻게 작동하는지 이해하는 것이 중요하다고 믿기 때문에, 처음에는 저수준 API와 텐서를 다루는 작업부터 시작합니다. 그러나 빠르게 시작하고 이러한 세부 사항을 배우는 데 많은 시간을 들이고 싶지 않다면, 이를 건너뛰고 고수준 API 노트북으로 바로 넘어갈 수 있습니다.

## ✍️ 연습: 프레임워크

다음 노트북에서 학습을 이어가세요:

저수준 API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
고수준 API | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

프레임워크를 숙달한 후, 과적합 개념을 다시 살펴보겠습니다.

# 과적합

과적합은 머신러닝에서 매우 중요한 개념이며, 이를 올바르게 이해하는 것이 매우 중요합니다!

다음 문제를 고려해보세요. 5개의 점(`x`로 표시된 그래프)을 근사화하는 문제입니다:

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.ko.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.ko.jpg)
-------------------------|--------------------------
**선형 모델, 2개의 매개변수** | **비선형 모델, 7개의 매개변수**
훈련 오류 = 5.3 | 훈련 오류 = 0
검증 오류 = 5.1 | 검증 오류 = 20

* 왼쪽에서는 적절한 직선 근사가 보입니다. 매개변수의 수가 적절하기 때문에 모델이 점 분포의 패턴을 올바르게 이해합니다.
* 오른쪽에서는 모델이 너무 강력합니다. 점이 5개뿐이고 모델에 7개의 매개변수가 있기 때문에 모든 점을 통과하도록 조정할 수 있습니다. 이로 인해 훈련 오류는 0이 되지만, 데이터의 올바른 패턴을 이해하지 못하게 되어 검증 오류가 매우 높아집니다.

모델의 풍부함(매개변수 수)과 훈련 샘플 수 사이의 적절한 균형을 맞추는 것이 매우 중요합니다.

## 과적합이 발생하는 이유

  * 훈련 데이터 부족
  * 너무 강력한 모델
  * 입력 데이터에 너무 많은 노이즈

## 과적합을 감지하는 방법

위 그래프에서 볼 수 있듯이, 과적합은 매우 낮은 훈련 오류와 높은 검증 오류로 감지할 수 있습니다. 일반적으로 훈련 중에는 훈련 오류와 검증 오류가 모두 감소하다가, 어느 시점에서 검증 오류가 감소를 멈추고 증가하기 시작할 수 있습니다. 이는 과적합의 신호이며, 이 시점에서 훈련을 중단하거나 모델의 스냅샷을 저장해야 할 가능성이 있습니다.

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.ko.png)

## 과적합을 방지하는 방법

과적합이 발생하는 것을 확인하면 다음 중 하나를 수행할 수 있습니다:

 * 훈련 데이터 양을 늘리기
 * 모델의 복잡성을 줄이기
 * [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout)과 같은 [정규화 기법](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md)을 사용하기 (이후에 다룰 예정)

## 과적합과 편향-분산 트레이드오프

과적합은 통계에서 [편향-분산 트레이드오프](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)라는 더 일반적인 문제의 사례입니다. 모델의 오류 원인을 고려하면 두 가지 유형의 오류를 볼 수 있습니다:

* **편향 오류**는 알고리즘이 훈련 데이터 간의 관계를 올바르게 캡처하지 못해 발생합니다. 이는 모델이 충분히 강력하지 않은 경우(**과소적합**)에 발생할 수 있습니다.
* **분산 오류**는 모델이 입력 데이터의 노이즈를 의미 있는 관계 대신 근사화하려고 할 때 발생합니다(**과적합**).

훈련 중에는 편향 오류가 감소하고(모델이 데이터를 근사화하는 법을 배우면서), 분산 오류가 증가합니다. 과적합을 방지하기 위해 훈련을 중단해야 합니다 - 수동으로(과적합을 감지했을 때) 또는 자동으로(정규화를 도입하여).

## 결론

이 강의에서는 TensorFlow와 PyTorch라는 두 가지 인기 있는 AI 프레임워크의 다양한 API 차이점을 배웠습니다. 또한 과적합이라는 매우 중요한 주제에 대해 배웠습니다.

## 🚀 도전 과제

첨부된 노트북에서 '작업' 섹션을 찾을 수 있습니다. 노트북을 학습하고 작업을 완료하세요.

## [강의 후 퀴즈](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## 복습 및 자기 학습

다음 주제에 대해 연구해보세요:

- TensorFlow
- PyTorch
- 과적합

다음 질문을 스스로에게 던져보세요:

- TensorFlow와 PyTorch의 차이점은 무엇인가요?
- 과적합과 과소적합의 차이점은 무엇인가요?

## [과제](lab/README.md)

이 실습에서는 PyTorch 또는 TensorFlow를 사용하여 단일 및 다층 완전 연결 네트워크를 통해 두 가지 분류 문제를 해결해야 합니다.

* [지침](lab/README.md)
* [노트북](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.