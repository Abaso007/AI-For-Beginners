<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "789d6c3fb6fc7948a470b33078a5983a",
  "translation_date": "2025-09-23T13:22:36+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "ko"
}
-->
# 신경망 소개: 다층 퍼셉트론

이전 섹션에서는 가장 간단한 신경망 모델인 단일 계층 퍼셉트론, 즉 선형 이진 분류 모델에 대해 배웠습니다.

이번 섹션에서는 이 모델을 확장하여 더 유연한 프레임워크를 만들어 보겠습니다. 이를 통해 다음을 수행할 수 있습니다:

* 이진 분류 외에 **다중 클래스 분류** 수행
* 분류 외에 **회귀 문제** 해결
* 선형적으로 분리되지 않는 클래스 구분

또한, 다양한 신경망 아키텍처를 구성할 수 있는 자체 모듈식 프레임워크를 Python으로 개발할 것입니다.

## [강의 전 퀴즈](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## 머신러닝의 공식화

먼저 머신러닝 문제를 공식화해 봅시다. 훈련 데이터셋 **X**와 레이블 **Y**가 주어졌다고 가정하고, 가장 정확한 예측을 수행할 모델 *f*를 만들어야 합니다. 예측의 품질은 **손실 함수** &lagran;로 측정됩니다. 다음과 같은 손실 함수가 자주 사용됩니다:

* 숫자를 예측해야 하는 회귀 문제의 경우, **절대 오차** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| 또는 **제곱 오차** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>를 사용할 수 있습니다.
* 분류 문제의 경우, **0-1 손실**(모델의 **정확도**와 본질적으로 동일) 또는 **로지스틱 손실**을 사용합니다.

단일 계층 퍼셉트론의 경우, 함수 *f*는 선형 함수 *f(x)=wx+b*로 정의되었습니다. 여기서 *w*는 가중치 행렬, *x*는 입력 특징 벡터, *b*는 편향 벡터입니다. 다른 신경망 아키텍처에서는 이 함수가 더 복잡한 형태를 가질 수 있습니다.

> 분류의 경우, 네트워크 출력으로 해당 클래스의 확률을 얻는 것이 바람직할 때가 많습니다. 임의의 숫자를 확률로 변환(즉, 출력을 정규화)하기 위해 **소프트맥스** 함수 &sigma;를 자주 사용하며, 함수 *f*는 *f(x)=&sigma;(wx+b)*가 됩니다.

위 정의에서 *w*와 *b*는 **파라미터** &theta;=⟨*w,b*⟩라고 불립니다. 데이터셋 ⟨**X**,**Y**⟩가 주어지면, 전체 데이터셋에 대한 전체 오류를 파라미터 &theta;의 함수로 계산할 수 있습니다.

> ✅ **신경망 훈련의 목표는 파라미터 &theta;를 조정하여 오류를 최소화하는 것입니다.**

## 경사 하강법 최적화

**경사 하강법**이라는 함수 최적화의 잘 알려진 방법이 있습니다. 이 방법은 손실 함수의 파라미터에 대한 도함수(다차원에서는 **기울기**라고 함)를 계산하고, 오류가 감소하도록 파라미터를 조정하는 아이디어입니다. 이를 다음과 같이 공식화할 수 있습니다:

* 파라미터를 임의의 값으로 초기화: w<sup>(0)</sup>, b<sup>(0)</sup>
* 다음 단계를 여러 번 반복:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

훈련 중 최적화 단계는 전체 데이터셋을 고려하여 계산되어야 합니다(손실은 모든 훈련 샘플을 통해 합산하여 계산됨을 기억하세요). 그러나 실제로는 데이터셋의 작은 부분인 **미니배치**를 사용하여 기울기를 계산합니다. 매번 무작위로 부분 집합을 선택하기 때문에, 이러한 방법을 **확률적 경사 하강법**(SGD)이라고 합니다.

## 다층 퍼셉트론과 역전파

앞서 본 단일 계층 네트워크는 선형적으로 분리 가능한 클래스를 분류할 수 있습니다. 더 풍부한 모델을 구축하려면 네트워크의 여러 계층을 결합할 수 있습니다. 수학적으로 이는 함수 *f*가 더 복잡한 형태를 가지며, 여러 단계로 계산됨을 의미합니다:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

여기서 &alpha;는 **비선형 활성화 함수**, &sigma;는 소프트맥스 함수이며, 파라미터는 &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>입니다.

경사 하강법 알고리즘은 동일하게 유지되지만, 기울기를 계산하는 것이 더 어려워집니다. 연쇄 미분 법칙에 따라, 도함수를 다음과 같이 계산할 수 있습니다:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ✅ 연쇄 미분 법칙은 손실 함수의 파라미터에 대한 도함수를 계산하는 데 사용됩니다.

위 표현식의 가장 왼쪽 부분은 동일하므로, 손실 함수에서 시작하여 계산 그래프를 "역방향"으로 따라가며 도함수를 효과적으로 계산할 수 있습니다. 따라서 다층 퍼셉트론을 훈련하는 방법을 **역전파**(backpropagation) 또는 '백프롭'이라고 합니다.

<img alt="계산 그래프" src="images/ComputeGraphGrad.png"/>

> TODO: 이미지 출처

> ✅ 역전파에 대해서는 노트북 예제에서 훨씬 더 자세히 다룰 것입니다.

## 결론

이번 강의에서는 자체 신경망 라이브러리를 구축하고, 이를 사용하여 간단한 2차원 분류 작업을 수행했습니다.

## 🚀 도전 과제

첨부된 노트북에서 다층 퍼셉트론을 구축하고 훈련하는 자체 프레임워크를 구현해 보세요. 이를 통해 현대 신경망이 어떻게 작동하는지 자세히 살펴볼 수 있습니다.

[OwnFramework](OwnFramework.ipynb) 노트북으로 이동하여 작업을 진행하세요.

## [강의 후 퀴즈](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## 복습 및 자기 학습

역전파는 AI와 ML에서 널리 사용되는 알고리즘으로, [더 자세히 공부할 가치가 있습니다](https://wikipedia.org/wiki/Backpropagation).

## [과제](lab/README.md)

이번 강의에서 구축한 프레임워크를 사용하여 MNIST 손글씨 숫자 분류 문제를 해결하세요.

* [지침](lab/README.md)
* [노트북](lab/MyFW_MNIST.ipynb)

---

