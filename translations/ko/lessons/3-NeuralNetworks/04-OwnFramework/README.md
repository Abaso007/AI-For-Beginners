<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-24T21:34:02+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "ko"
}
-->
# 신경망 소개: 다층 퍼셉트론

이전 섹션에서는 가장 간단한 신경망 모델인 단층 퍼셉트론, 즉 선형 이진 분류 모델에 대해 배웠습니다.

이번 섹션에서는 이 모델을 확장하여 더 유연한 프레임워크를 구축할 것입니다. 이를 통해 다음을 수행할 수 있습니다:

* **다중 클래스 분류**를 이진 분류 외에도 수행
* 분류 외에도 **회귀 문제** 해결
* 선형적으로 분리되지 않는 클래스 분리

또한, 다양한 신경망 아키텍처를 구성할 수 있는 Python 기반의 모듈형 프레임워크를 개발할 것입니다.

## [강의 전 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## 머신러닝의 공식화

먼저 머신러닝 문제를 공식화해 봅시다. 훈련 데이터셋 **X**와 레이블 **Y**가 주어졌다고 가정하고, 가장 정확한 예측을 수행할 모델 *f*를 구축해야 합니다. 예측의 품질은 **손실 함수** ℒ로 측정됩니다. 다음과 같은 손실 함수가 자주 사용됩니다:

* 숫자를 예측해야 하는 회귀 문제에서는 **절대 오차** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| 또는 **제곱 오차** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>를 사용할 수 있습니다.
* 분류 문제에서는 **0-1 손실**(모델의 **정확도**와 사실상 동일) 또는 **로지스틱 손실**을 사용합니다.

단층 퍼셉트론의 경우, 함수 *f*는 선형 함수 *f(x)=wx+b*로 정의되었습니다(여기서 *w*는 가중치 행렬, *x*는 입력 특징 벡터, *b*는 바이어스 벡터). 다양한 신경망 아키텍처에서는 이 함수가 더 복잡한 형태를 가질 수 있습니다.

> 분류의 경우, 네트워크 출력으로 해당 클래스의 확률을 얻는 것이 바람직할 때가 많습니다. 임의의 숫자를 확률로 변환하기 위해(예: 출력을 정규화하기 위해) **소프트맥스** 함수 σ를 자주 사용하며, 함수 *f*는 *f(x)=σ(wx+b)*가 됩니다.

위의 *f* 정의에서, *w*와 *b*는 **파라미터** θ=⟨*w,b*⟩라고 불립니다. 데이터셋 ⟨**X**,**Y**⟩가 주어지면, 전체 데이터셋에 대한 전체 오류를 파라미터 θ의 함수로 계산할 수 있습니다.

> ✅ **신경망 훈련의 목표는 파라미터 θ를 조정하여 오류를 최소화하는 것입니다**

## 경사 하강법 최적화

함수 최적화의 잘 알려진 방법 중 하나는 **경사 하강법**입니다. 이 방법은 손실 함수의 파라미터에 대한 미분(다차원 경우에는 **그래디언트**라고 함)을 계산하고, 오류가 감소하도록 파라미터를 조정하는 아이디어를 기반으로 합니다. 이를 다음과 같이 공식화할 수 있습니다:

* 파라미터를 임의의 값으로 초기화 w<sup>(0)</sup>, b<sup>(0)</sup>
* 다음 단계를 여러 번 반복:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

훈련 중에는 전체 데이터셋을 고려하여 최적화 단계를 계산해야 합니다(손실은 모든 훈련 샘플을 통해 합산하여 계산됨을 기억하세요). 하지만 실제로는 데이터셋의 작은 부분인 **미니배치**를 사용하여 데이터를 기반으로 그래디언트를 계산합니다. 매번 무작위로 부분 집합을 선택하기 때문에 이러한 방법을 **확률적 경사 하강법**(SGD)이라고 합니다.

## 다층 퍼셉트론과 역전파

위에서 본 단층 네트워크는 선형적으로 분리 가능한 클래스를 분류할 수 있습니다. 더 풍부한 모델을 구축하기 위해 네트워크의 여러 층을 결합할 수 있습니다. 수학적으로 이는 함수 *f*가 더 복잡한 형태를 가지며 여러 단계로 계산된다는 것을 의미합니다:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

여기서 α는 **비선형 활성화 함수**, σ는 소프트맥스 함수, 파라미터는 θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>입니다.

경사 하강법 알고리즘은 동일하게 유지되지만, 그래디언트를 계산하는 것이 더 어려워집니다. 체인 미분 법칙을 사용하면 다음과 같이 미분을 계산할 수 있습니다:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ 체인 미분 법칙은 손실 함수의 파라미터에 대한 미분을 계산하는 데 사용됩니다.

위 표현에서 가장 왼쪽 부분은 동일하므로, 손실 함수에서 시작하여 계산 그래프를 "역방향"으로 따라가며 미분을 효과적으로 계산할 수 있습니다. 따라서 다층 퍼셉트론을 훈련하는 방법을 **역전파** 또는 '백프롭'이라고 합니다.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: 이미지 출처

> ✅ 역전파는 노트북 예제에서 훨씬 더 자세히 다룰 것입니다.  

## 결론

이번 강의에서는 자체 신경망 라이브러리를 구축하고, 이를 사용하여 간단한 이차원 분류 작업을 수행했습니다.

## 🚀 도전 과제

첨부된 노트북에서 다층 퍼셉트론을 구축하고 훈련하는 자체 프레임워크를 구현할 것입니다. 이를 통해 현대 신경망이 어떻게 작동하는지 자세히 확인할 수 있습니다.

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) 노트북으로 이동하여 작업을 진행하세요.

## [강의 후 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## 복습 및 자기 학습

역전파는 AI와 ML에서 자주 사용되는 알고리즘으로, [더 자세히](https://wikipedia.org/wiki/Backpropagation) 공부할 가치가 있습니다.

## [과제](lab/README.md)

이번 실습에서는 이번 강의에서 구축한 프레임워크를 사용하여 MNIST 손글씨 숫자 분류 문제를 해결해야 합니다.

* [지침](lab/README.md)
* [노트북](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.