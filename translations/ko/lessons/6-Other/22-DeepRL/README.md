<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-24T21:33:00+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ko"
}
-->
# 딥 강화 학습

강화 학습(RL)은 지도 학습과 비지도 학습과 함께 기본적인 머신 러닝 패러다임 중 하나로 간주됩니다. 지도 학습에서는 결과가 알려진 데이터셋에 의존하는 반면, RL은 **직접 행동하며 배우는 것**에 기반합니다. 예를 들어, 처음 컴퓨터 게임을 접했을 때 규칙을 몰라도 게임을 시작하고, 게임을 하면서 행동을 조정하는 과정을 통해 점차 실력을 향상시킬 수 있습니다.

## [강의 전 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RL을 수행하기 위해 필요한 요소는 다음과 같습니다:

* **환경** 또는 **시뮬레이터**: 게임의 규칙을 설정하는 역할을 합니다. 시뮬레이터에서 실험을 실행하고 결과를 관찰할 수 있어야 합니다.
* **보상 함수**: 실험이 얼마나 성공적이었는지를 나타냅니다. 컴퓨터 게임을 배우는 경우, 보상은 최종 점수가 될 것입니다.

보상 함수에 따라 행동을 조정하고 실력을 향상시켜 다음 번에 더 나은 결과를 얻을 수 있습니다. RL과 다른 머신 러닝 유형의 주요 차이점은 RL에서는 게임이 끝날 때까지 승패를 알 수 없다는 점입니다. 따라서 특정 행동이 단독으로 좋은지 나쁜지 판단할 수 없으며, 게임이 끝난 후에야 보상을 받게 됩니다.

RL 과정에서는 일반적으로 많은 실험을 수행합니다. 각 실험에서는 지금까지 학습한 최적의 전략을 따르는 것(**활용**)과 새로운 상태를 탐색하는 것(**탐험**) 사이에서 균형을 맞춰야 합니다.

## OpenAI Gym

RL을 위한 훌륭한 도구 중 하나는 [OpenAI Gym](https://gym.openai.com/)입니다. 이는 다양한 환경을 시뮬레이션할 수 있는 **시뮬레이션 환경**으로, 아타리 게임부터 막대 균형 물리학까지 다양한 환경을 제공합니다. OpenAI에서 유지 관리하며, 강화 학습 알고리즘을 훈련시키는 데 가장 인기 있는 시뮬레이션 환경 중 하나입니다.

> **Note**: OpenAI Gym에서 제공하는 모든 환경은 [여기](https://gym.openai.com/envs/#classic_control)에서 확인할 수 있습니다.

## CartPole 균형 잡기

여러분은 아마도 *세그웨이*나 *자이로스쿠터*와 같은 현대적인 균형 장치를 본 적이 있을 것입니다. 이 장치들은 가속도계나 자이로스코프의 신호에 따라 바퀴를 조정하여 자동으로 균형을 잡습니다. 이 섹션에서는 비슷한 문제인 막대 균형 잡기를 해결하는 방법을 배워보겠습니다. 이는 서커스 공연자가 손 위에 막대를 균형 잡는 상황과 비슷하지만, 이 문제는 1차원에서만 발생합니다.

간소화된 균형 문제는 **CartPole** 문제로 알려져 있습니다. CartPole 세계에서는 좌우로 움직일 수 있는 수평 슬라이더가 있으며, 목표는 슬라이더 위에 세워진 수직 막대를 균형 잡는 것입니다.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

이 환경을 생성하고 사용하는 데 필요한 Python 코드 몇 줄은 다음과 같습니다:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

각 환경은 동일한 방식으로 접근할 수 있습니다:
* `env.reset`: 새로운 실험을 시작합니다.
* `env.step`: 시뮬레이션 단계를 수행합니다. **액션 공간**에서 **액션**을 받아들이고, **관찰**(관찰 공간에서), 보상 및 종료 플래그를 반환합니다.

위 예제에서는 각 단계에서 랜덤 액션을 수행하기 때문에 실험의 지속 시간이 매우 짧습니다:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL 알고리즘의 목표는 모델, 즉 **정책** π를 훈련시키는 것입니다. 이 정책은 주어진 상태에 대한 응답으로 액션을 반환합니다. 정책은 확률적으로도 고려할 수 있으며, 상태 *s*와 액션 *a*에 대해 π(*a*|*s*)는 상태 *s*에서 *a*를 선택해야 할 확률을 반환합니다.

## 정책 경사 알고리즘

정책을 모델링하는 가장 명확한 방법은 상태를 입력으로 받아 액션(혹은 모든 액션의 확률)을 반환하는 신경망을 만드는 것입니다. 이는 일반적인 분류 작업과 비슷하지만 주요 차이점은 각 단계에서 어떤 액션을 취해야 할지 미리 알 수 없다는 점입니다.

여기서의 아이디어는 이러한 확률을 추정하는 것입니다. 실험의 각 단계에서 총 보상을 보여주는 **누적 보상 벡터**를 생성합니다. 또한 초기 보상의 역할을 줄이기 위해 γ=0.99와 같은 계수를 곱하여 **보상 할인**을 적용합니다. 그런 다음 더 큰 보상을 가져오는 실험 경로의 단계를 강화합니다.

> 정책 경사 알고리즘에 대해 더 배우고 실제로 작동하는 모습을 보려면 [예제 노트북](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)을 확인하세요.

## 액터-크리틱 알고리즘

정책 경사 접근법의 개선된 버전은 **액터-크리틱**이라고 불립니다. 이 접근법의 주요 아이디어는 신경망이 두 가지를 반환하도록 훈련된다는 것입니다:

* 어떤 액션을 취할지 결정하는 **정책**: 이 부분은 **액터**라고 합니다.
* 해당 상태에서 얻을 수 있는 총 보상을 추정하는 부분: 이 부분은 **크리틱**이라고 합니다.

이 구조는 [GAN](../../4-ComputerVision/10-GANs/README.md)과 유사합니다. GAN에서는 두 개의 네트워크가 서로 경쟁하며 훈련됩니다. 액터-크리틱 모델에서는 액터가 취해야 할 액션을 제안하고, 크리틱이 비판적으로 결과를 추정합니다. 하지만 우리의 목표는 이 두 네트워크를 조화롭게 훈련시키는 것입니다.

실험 중에 실제 누적 보상과 크리틱이 반환한 결과를 모두 알고 있기 때문에, 이들 간의 차이를 최소화하는 손실 함수를 만드는 것이 비교적 쉽습니다. 이를 **크리틱 손실**이라고 합니다. **액터 손실**은 정책 경사 알고리즘과 동일한 접근법을 사용하여 계산할 수 있습니다.

이 알고리즘 중 하나를 실행한 후, 우리의 CartPole은 다음과 같이 행동할 것으로 기대됩니다:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ 연습: 정책 경사와 액터-크리틱 RL

다음 노트북에서 학습을 이어가세요:

* [TensorFlow에서 RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch에서 RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## 기타 RL 작업

오늘날 강화 학습은 빠르게 성장하는 연구 분야입니다. 강화 학습의 흥미로운 예는 다음과 같습니다:

* **아타리 게임**을 컴퓨터가 배우도록 가르치기. 이 문제의 도전 과제는 간단한 상태가 벡터로 표현되지 않고 스크린샷으로 제공된다는 점입니다. 따라서 CNN을 사용하여 화면 이미지를 특징 벡터로 변환하거나 보상 정보를 추출해야 합니다. 아타리 게임은 Gym에서 사용할 수 있습니다.
* 체스와 바둑 같은 보드 게임을 컴퓨터가 배우도록 가르치기. 최근에는 **Alpha Zero**와 같은 최첨단 프로그램이 두 에이전트가 서로 대결하며 점진적으로 향상되는 방식으로 처음부터 훈련되었습니다.
* 산업에서는 RL을 사용하여 시뮬레이션에서 제어 시스템을 생성합니다. [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste)라는 서비스는 이를 위해 특별히 설계되었습니다.

## 결론

우리는 이제 게임의 원하는 상태를 정의하는 보상 함수를 제공하고, 지능적으로 탐색 공간을 탐험할 기회를 제공함으로써 에이전트를 훈련시켜 좋은 결과를 얻는 방법을 배웠습니다. 두 가지 알고리즘을 성공적으로 시도했으며 비교적 짧은 시간 안에 좋은 결과를 얻었습니다. 하지만 이는 RL에 대한 여정의 시작일 뿐이며, 더 깊이 탐구하고 싶다면 별도의 과정을 고려해야 합니다.

## 🚀 도전 과제

'기타 RL 작업' 섹션에 나열된 응용 프로그램을 탐색하고 하나를 구현해보세요!

## [강의 후 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## 복습 및 자기 학습

[초보자를 위한 머신 러닝 커리큘럼](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)에서 고전적인 강화 학습에 대해 더 알아보세요.

컴퓨터가 슈퍼 마리오를 배우는 방법에 대해 이야기하는 [이 훌륭한 영상](https://www.youtube.com/watch?v=qv6UVOQ0F44)을 시청하세요.

## 과제: [Mountain Car 훈련하기](lab/README.md)

이 과제에서 여러분의 목표는 다른 Gym 환경인 [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)를 훈련시키는 것입니다.

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.