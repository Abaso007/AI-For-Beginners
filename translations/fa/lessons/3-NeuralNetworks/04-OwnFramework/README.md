<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-24T10:40:02+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "fa"
}
-->
# مقدمه‌ای بر شبکه‌های عصبی: پرسپترون چندلایه

در بخش قبلی، با ساده‌ترین مدل شبکه عصبی آشنا شدید - پرسپترون تک‌لایه، که یک مدل خطی برای طبقه‌بندی دوکلاسه است.

در این بخش، این مدل را به یک چارچوب انعطاف‌پذیرتر گسترش می‌دهیم که به ما امکان می‌دهد:

* علاوه بر طبقه‌بندی دوکلاسه، **طبقه‌بندی چندکلاسه** انجام دهیم  
* علاوه بر طبقه‌بندی، **مسائل رگرسیون** را حل کنیم  
* کلاس‌هایی را که به صورت خطی قابل جداسازی نیستند، جدا کنیم  

همچنین چارچوب ماژولاری در پایتون توسعه خواهیم داد که به ما امکان می‌دهد معماری‌های مختلف شبکه عصبی را بسازیم.

## [پیش‌آزمون](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## فرمالیزه کردن یادگیری ماشین

بیایید با فرمالیزه کردن مسئله یادگیری ماشین شروع کنیم. فرض کنید یک مجموعه داده آموزشی **X** با برچسب‌های **Y** داریم و باید مدلی *f* بسازیم که پیش‌بینی‌های دقیقی انجام دهد. کیفیت پیش‌بینی‌ها با **تابع خطا** ℒ اندازه‌گیری می‌شود. توابع خطای زیر معمولاً استفاده می‌شوند:

* برای مسائل رگرسیون، زمانی که باید یک عدد پیش‌بینی کنیم، می‌توانیم از **خطای مطلق** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| یا **خطای مربعی** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> استفاده کنیم.  
* برای طبقه‌بندی، از **خطای ۰-۱** (که اساساً همان **دقت مدل** است) یا **خطای لجستیک** استفاده می‌کنیم.  

برای پرسپترون تک‌لایه، تابع *f* به صورت یک تابع خطی تعریف می‌شد: *f(x)=wx+b* (در اینجا *w* ماتریس وزن، *x* بردار ویژگی‌های ورودی، و *b* بردار بایاس است). برای معماری‌های مختلف شبکه عصبی، این تابع می‌تواند شکل پیچیده‌تری به خود بگیرد.

> در مسائل طبقه‌بندی، اغلب مطلوب است که خروجی شبکه به صورت احتمالات کلاس‌های مربوطه باشد. برای تبدیل اعداد دلخواه به احتمالات (مثلاً برای نرمال‌سازی خروجی)، اغلب از تابع **softmax** σ استفاده می‌کنیم و تابع *f* به صورت *f(x)=σ(wx+b)* در می‌آید.

در تعریف *f* بالا، *w* و *b* به عنوان **پارامترها** θ=⟨*w,b*⟩ شناخته می‌شوند. با داشتن مجموعه داده ⟨**X**,**Y**⟩، می‌توانیم خطای کلی روی کل مجموعه داده را به عنوان تابعی از پارامترهای θ محاسبه کنیم.

> ✅ **هدف از آموزش شبکه عصبی، کمینه کردن خطا با تغییر پارامترهای θ است.**

## بهینه‌سازی با گرادیان نزولی

یک روش شناخته‌شده برای بهینه‌سازی توابع، **گرادیان نزولی** است. ایده این است که می‌توان مشتق (در حالت چندبعدی به آن **گرادیان** می‌گویند) تابع خطا را نسبت به پارامترها محاسبه کرد و پارامترها را به گونه‌ای تغییر داد که خطا کاهش یابد. این روش به صورت زیر فرمالیزه می‌شود:

* پارامترها را با مقادیر تصادفی اولیه‌سازی کنید: w<sup>(0)</sup>, b<sup>(0)</sup>  
* مراحل زیر را چندین بار تکرار کنید:  
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w  
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b  

در طول آموزش، مراحل بهینه‌سازی باید با در نظر گرفتن کل مجموعه داده محاسبه شوند (به یاد داشته باشید که خطا به صورت مجموعی از تمام نمونه‌های آموزشی محاسبه می‌شود). اما در عمل، ما از بخش‌های کوچکی از مجموعه داده به نام **مینی‌بچ‌ها** استفاده می‌کنیم و گرادیان‌ها را بر اساس زیرمجموعه‌ای از داده‌ها محاسبه می‌کنیم. از آنجا که هر بار زیرمجموعه به صورت تصادفی انتخاب می‌شود، این روش **گرادیان نزولی تصادفی** (SGD) نامیده می‌شود.

## پرسپترون چندلایه و پس‌انتشار

شبکه تک‌لایه، همان‌طور که دیدیم، قادر به طبقه‌بندی کلاس‌های خطی قابل جداسازی است. برای ساخت مدلی غنی‌تر، می‌توانیم چندین لایه از شبکه را ترکیب کنیم. به صورت ریاضی، این به معنای آن است که تابع *f* شکل پیچیده‌تری به خود می‌گیرد و در چندین مرحله محاسبه می‌شود:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>  
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>  
* f = σ(z<sub>2</sub>)  

در اینجا، α یک **تابع فعال‌سازی غیرخطی** است، σ تابع softmax است، و پارامترها θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> هستند.

الگوریتم گرادیان نزولی همان‌طور باقی می‌ماند، اما محاسبه گرادیان‌ها دشوارتر می‌شود. با استفاده از قانون زنجیره‌ای مشتق‌گیری، می‌توان مشتقات را به صورت زیر محاسبه کرد:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)  
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)  

> ✅ قانون زنجیره‌ای مشتق‌گیری برای محاسبه مشتقات تابع خطا نسبت به پارامترها استفاده می‌شود.

توجه داشته باشید که بخش سمت چپ تمام این عبارات یکسان است، بنابراین می‌توان مشتقات را به طور مؤثر از تابع خطا شروع کرده و به صورت "معکوس" از طریق گراف محاسباتی محاسبه کرد. به همین دلیل، روش آموزش پرسپترون چندلایه **پس‌انتشار** یا 'backprop' نامیده می‌شود.

<img alt="گراف محاسباتی" src="images/ComputeGraphGrad.png"/>

> TODO: ذکر منبع تصویر

> ✅ ما در مثال نوت‌بوک خود، پس‌انتشار را با جزئیات بیشتری پوشش خواهیم داد.

## نتیجه‌گیری

در این درس، کتابخانه شبکه عصبی خود را ساختیم و از آن برای یک مسئله طبقه‌بندی دوبعدی ساده استفاده کردیم.

## 🚀 چالش

در نوت‌بوک همراه این درس، چارچوب خود را برای ساخت و آموزش پرسپترون‌های چندلایه پیاده‌سازی خواهید کرد. شما می‌توانید به طور دقیق ببینید که شبکه‌های عصبی مدرن چگونه کار می‌کنند.

به نوت‌بوک [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) بروید و آن را بررسی کنید.

## [پس‌آزمون](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## مرور و مطالعه شخصی

پس‌انتشار یک الگوریتم رایج در هوش مصنوعی و یادگیری ماشین است که ارزش مطالعه [بیشتر](https://wikipedia.org/wiki/Backpropagation) را دارد.

## [تکلیف](lab/README.md)

در این آزمایشگاه، از چارچوبی که در این درس ساختید برای حل مسئله طبقه‌بندی ارقام دست‌نویس MNIST استفاده خواهید کرد.

* [دستورالعمل‌ها](lab/README.md)  
* [نوت‌بوک](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)  

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادقتی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.