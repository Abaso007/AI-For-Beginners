<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-24T10:25:07+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "fa"
}
-->
# خودرمزگذارها

هنگام آموزش شبکه‌های عصبی کانولوشنی (CNN)، یکی از مشکلات این است که به داده‌های برچسب‌دار زیادی نیاز داریم. در مورد دسته‌بندی تصاویر، باید تصاویر را به کلاس‌های مختلف تقسیم کنیم که این کار به صورت دستی انجام می‌شود.

## [پیش‌آزمون](https://ff-quizzes.netlify.app/en/ai/quiz/17)

با این حال، ممکن است بخواهیم از داده‌های خام (بدون برچسب) برای آموزش استخراج‌کننده‌های ویژگی CNN استفاده کنیم، که به آن **یادگیری خودنظارتی** گفته می‌شود. به جای برچسب‌ها، از تصاویر آموزشی به عنوان ورودی و خروجی شبکه استفاده خواهیم کرد. ایده اصلی **خودرمزگذار** این است که یک **شبکه رمزگذار** داشته باشیم که تصویر ورودی را به یک **فضای نهان** تبدیل کند (معمولاً فقط یک بردار با اندازه کوچکتر است)، سپس یک **شبکه رمزگشا** که هدف آن بازسازی تصویر اصلی باشد.

> ✅ یک [خودرمزگذار](https://wikipedia.org/wiki/Autoencoder) نوعی شبکه عصبی مصنوعی است که برای یادگیری کدگذاری‌های کارآمد از داده‌های بدون برچسب استفاده می‌شود.

از آنجا که ما خودرمزگذار را برای گرفتن اطلاعات بیشتر از تصویر اصلی به منظور بازسازی دقیق آموزش می‌دهیم، شبکه تلاش می‌کند بهترین **تعبیر** از تصاویر ورودی را برای درک معنا پیدا کند.

![نمودار خودرمزگذار](../../../../../lessons/4-ComputerVision/09-Autoencoders/images/autoencoder_schema.jpg)

> تصویر از [وبلاگ Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## سناریوهای استفاده از خودرمزگذارها

در حالی که بازسازی تصاویر اصلی به خودی خود ممکن است مفید به نظر نرسد، چند سناریو وجود دارد که خودرمزگذارها به طور خاص مفید هستند:

* **کاهش ابعاد تصاویر برای مصورسازی** یا **آموزش تعبیه‌های تصویری**. معمولاً خودرمزگذارها نتایج بهتری نسبت به PCA ارائه می‌دهند، زیرا ماهیت فضایی تصاویر و ویژگی‌های سلسله‌مراتبی را در نظر می‌گیرند.
* **حذف نویز**، یعنی حذف نویز از تصویر. از آنجا که نویز اطلاعات بی‌فایده زیادی را حمل می‌کند، خودرمزگذار نمی‌تواند همه آن را در فضای نهان نسبتاً کوچک جای دهد و بنابراین فقط بخش مهم تصویر را ثبت می‌کند. هنگام آموزش حذف‌کننده‌های نویز، با تصاویر اصلی شروع می‌کنیم و از تصاویر با نویز مصنوعی اضافه شده به عنوان ورودی برای خودرمزگذار استفاده می‌کنیم.
* **افزایش وضوح تصویر**، افزایش وضوح تصویر. با تصاویر با وضوح بالا شروع می‌کنیم و از تصویر با وضوح پایین‌تر به عنوان ورودی خودرمزگذار استفاده می‌کنیم.
* **مدل‌های مولد**. پس از آموزش خودرمزگذار، بخش رمزگشا می‌تواند برای ایجاد اشیاء جدید از بردارهای نهان تصادفی استفاده شود.

## خودرمزگذارهای تنوعی (VAE)

خودرمزگذارهای سنتی به نوعی ابعاد داده‌های ورودی را کاهش می‌دهند و ویژگی‌های مهم تصاویر ورودی را شناسایی می‌کنند. با این حال، بردارهای نهان اغلب معنای خاصی ندارند. به عبارت دیگر، اگر مجموعه داده MNIST را به عنوان مثال در نظر بگیریم، شناسایی اینکه کدام ارقام به بردارهای نهان مختلف مربوط می‌شوند کار آسانی نیست، زیرا بردارهای نهان نزدیک لزوماً به همان ارقام مربوط نمی‌شوند.

از سوی دیگر، برای آموزش مدل‌های *مولد* بهتر است درک بیشتری از فضای نهان داشته باشیم. این ایده ما را به **خودرمزگذار تنوعی** (VAE) هدایت می‌کند.

VAE نوعی خودرمزگذار است که یاد می‌گیرد *توزیع آماری* پارامترهای نهان، به اصطلاح **توزیع نهان** را پیش‌بینی کند. به عنوان مثال، ممکن است بخواهیم بردارهای نهان به طور نرمال با میانگین z<sub>mean</sub> و انحراف معیار z<sub>sigma</sub> توزیع شوند (هر دو میانگین و انحراف معیار بردارهایی با ابعاد مشخص هستند). رمزگذار در VAE یاد می‌گیرد این پارامترها را پیش‌بینی کند و سپس رمزگشا یک بردار تصادفی از این توزیع را برای بازسازی شیء می‌گیرد.

به طور خلاصه:

* از بردار ورودی، `z_mean` و `z_log_sigma` را پیش‌بینی می‌کنیم (به جای پیش‌بینی انحراف معیار، لگاریتم آن را پیش‌بینی می‌کنیم)
* یک بردار `sample` از توزیع N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>)) نمونه‌برداری می‌کنیم
* رمزگشا تلاش می‌کند تصویر اصلی را با استفاده از `sample` به عنوان بردار ورودی رمزگشایی کند

<img src="images/vae.png" width="50%">

> تصویر از [این پست وبلاگ](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) توسط Isaak Dykeman

خودرمزگذارهای تنوعی از یک تابع هزینه پیچیده استفاده می‌کنند که شامل دو بخش است:

* **هزینه بازسازی**، تابع هزینه‌ای که نشان می‌دهد تصویر بازسازی شده چقدر به هدف نزدیک است (می‌تواند خطای میانگین مربعات یا MSE باشد). این همان تابع هزینه‌ای است که در خودرمزگذارهای معمولی استفاده می‌شود.
* **هزینه KL**، که تضمین می‌کند توزیع متغیرهای نهان به توزیع نرمال نزدیک بماند. این هزینه بر اساس مفهوم [واگرایی کولبک-لیبلر](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) است - معیاری برای تخمین شباهت دو توزیع آماری.

یکی از مزایای مهم VAE این است که به ما امکان می‌دهد تصاویر جدید را نسبتاً آسان تولید کنیم، زیرا می‌دانیم از کدام توزیع باید بردارهای نهان نمونه‌برداری کنیم. به عنوان مثال، اگر VAE را با بردار نهان دو‌بعدی روی MNIST آموزش دهیم، می‌توانیم سپس اجزای بردار نهان را تغییر دهیم تا ارقام مختلفی به دست آوریم:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> تصویر توسط [Dmitry Soshnikov](http://soshnikov.com)

مشاهده کنید که چگونه تصاویر به یکدیگر تبدیل می‌شوند، زیرا شروع به گرفتن بردارهای نهان از بخش‌های مختلف فضای پارامتر نهان می‌کنیم. همچنین می‌توانیم این فضا را در دو‌بعد مصورسازی کنیم:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> تصویر توسط [Dmitry Soshnikov](http://soshnikov.com)

## ✍️ تمرین‌ها: خودرمزگذارها

اطلاعات بیشتری درباره خودرمزگذارها در این نوت‌بوک‌ها بیاموزید:

* [خودرمزگذارها در TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [خودرمزگذارها در PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## ویژگی‌های خودرمزگذارها

* **داده‌محور** - آن‌ها فقط با نوع تصاویری که آموزش دیده‌اند خوب کار می‌کنند. به عنوان مثال، اگر یک شبکه افزایش وضوح تصویر را روی گل‌ها آموزش دهیم، روی پرتره‌ها خوب کار نخواهد کرد. این به این دلیل است که شبکه می‌تواند تصویر با وضوح بالاتر را با گرفتن جزئیات دقیق از ویژگی‌های یادگرفته شده از مجموعه داده آموزشی تولید کند.
* **دارای افت کیفیت** - تصویر بازسازی شده همان تصویر اصلی نیست. ماهیت افت کیفیت توسط *تابع هزینه* استفاده شده در طول آموزش تعریف می‌شود.
* کار با **داده‌های بدون برچسب**

## [پس‌آزمون](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## نتیجه‌گیری

در این درس، شما با انواع مختلف خودرمزگذارها که در دسترس دانشمند هوش مصنوعی هستند آشنا شدید. یاد گرفتید چگونه آن‌ها را بسازید و چگونه از آن‌ها برای بازسازی تصاویر استفاده کنید. همچنین با VAE آشنا شدید و یاد گرفتید چگونه از آن برای تولید تصاویر جدید استفاده کنید.

## 🚀 چالش

در این درس، شما درباره استفاده از خودرمزگذارها برای تصاویر یاد گرفتید. اما آن‌ها می‌توانند برای موسیقی نیز استفاده شوند! پروژه Magenta [MusicVAE](https://magenta.tensorflow.org/music-vae) از خودرمزگذارها برای یادگیری بازسازی موسیقی استفاده می‌کند. با این کتابخانه [آزمایش کنید](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) تا ببینید چه چیزی می‌توانید خلق کنید.

## [پس‌آزمون](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## مرور و مطالعه خودآموز

برای مرجع، اطلاعات بیشتری درباره خودرمزگذارها در این منابع بخوانید:

* [ساخت خودرمزگذارها در Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [پست وبلاگ در NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [توضیح خودرمزگذارهای تنوعی](https://kvfrans.com/variational-autoencoders-explained/)
* [خودرمزگذارهای تنوعی شرطی](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## تکلیف

در انتهای [این نوت‌بوک با استفاده از TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)، یک "وظیفه" پیدا خواهید کرد - از این به عنوان تکلیف خود استفاده کنید.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما هیچ مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.