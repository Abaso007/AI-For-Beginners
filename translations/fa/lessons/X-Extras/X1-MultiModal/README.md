<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9c592c26aca16ca085d268c732284187",
  "translation_date": "2025-08-24T10:38:40+00:00",
  "source_file": "lessons/X-Extras/X1-MultiModal/README.md",
  "language_code": "fa"
}
-->
# شبکه‌های چندوجهی

پس از موفقیت مدل‌های ترنسفورمر در حل وظایف پردازش زبان طبیعی (NLP)، معماری‌های مشابه برای وظایف بینایی کامپیوتر نیز به کار گرفته شدند. علاقه‌مندی به ساخت مدل‌هایی که قابلیت‌های بینایی و زبان طبیعی را *ترکیب* کنند، در حال افزایش است. یکی از این تلاش‌ها توسط OpenAI انجام شده و به نام CLIP و DALL.E شناخته می‌شود.

## پیش‌آموزش متضاد تصویر (CLIP)

ایده اصلی CLIP این است که بتوان متن‌های ورودی را با یک تصویر مقایسه کرده و تعیین کند که تصویر تا چه حد با متن مطابقت دارد.

![معماری CLIP](../../../../../lessons/X-Extras/X1-MultiModal/images/clip-arch.png)

> *تصویر از [این پست وبلاگ](https://openai.com/blog/clip/)*

این مدل بر روی تصاویری که از اینترنت به دست آمده‌اند و توضیحات آن‌ها آموزش داده شده است. برای هر دسته، ما N جفت (تصویر، متن) می‌گیریم و آن‌ها را به نمایش‌های برداری تبدیل می‌کنیم.

این نمایش‌ها سپس با یکدیگر تطبیق داده می‌شوند. تابع زیان به گونه‌ای تعریف شده است که شباهت کسینوسی بین بردارهای مربوط به یک جفت (مثلاً I و T) را حداکثر کند و شباهت کسینوسی بین تمام جفت‌های دیگر را حداقل کند. به همین دلیل این روش **متضاد** نامیده می‌شود.

مدل/کتابخانه CLIP از [گیت‌هاب OpenAI](https://github.com/openai/CLIP) در دسترس است. این روش در [این پست وبلاگ](https://openai.com/blog/clip/) توضیح داده شده و به طور مفصل‌تر در [این مقاله](https://arxiv.org/pdf/2103.00020.pdf) شرح داده شده است.

پس از پیش‌آموزش این مدل، می‌توانیم به آن یک دسته از تصاویر و یک دسته از متن‌های ورودی بدهیم و نتیجه یک تنسور با احتمالات خواهد بود. CLIP می‌تواند برای چندین وظیفه استفاده شود:

**طبقه‌بندی تصویر**

فرض کنید نیاز داریم تصاویر را بین گربه‌ها، سگ‌ها و انسان‌ها طبقه‌بندی کنیم. در این حالت، می‌توانیم به مدل یک تصویر و مجموعه‌ای از متن‌های ورودی بدهیم: "*تصویری از یک گربه*", "*تصویری از یک سگ*", "*تصویری از یک انسان*". در بردار احتمالات حاصل که شامل ۳ مقدار است، فقط باید شاخصی را انتخاب کنیم که بالاترین مقدار را دارد.

![CLIP برای طبقه‌بندی تصویر](../../../../../lessons/X-Extras/X1-MultiModal/images/clip-class.png)

> *تصویر از [این پست وبلاگ](https://openai.com/blog/clip/)*

**جستجوی تصویر بر اساس متن**

ما همچنین می‌توانیم برعکس عمل کنیم. اگر مجموعه‌ای از تصاویر داشته باشیم، می‌توانیم این مجموعه را به مدل بدهیم و یک متن ورودی ارائه کنیم - این کار تصویری را که بیشترین شباهت را به متن دارد، به ما می‌دهد.

## ✍️ مثال: [استفاده از CLIP برای طبقه‌بندی تصویر و جستجوی تصویر](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb)

دفترچه [Clip.ipynb](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb) را باز کنید تا CLIP را در عمل مشاهده کنید.

## تولید تصویر با VQGAN+CLIP

CLIP همچنین می‌تواند برای **تولید تصویر** از یک متن ورودی استفاده شود. برای این کار، به یک **مدل تولیدکننده** نیاز داریم که بتواند تصاویر را بر اساس یک ورودی برداری تولید کند. یکی از این مدل‌ها [VQGAN](https://compvis.github.io/taming-transformers/) (شبکه مولد متخاصم بردار-کوانتیزه) نام دارد.

ایده‌های اصلی VQGAN که آن را از [GAN](../../4-ComputerVision/10-GANs/README.md) معمولی متمایز می‌کند، عبارتند از:
* استفاده از معماری ترنسفورمر خودبازگشتی برای تولید دنباله‌ای از بخش‌های بصری غنی از زمینه که تصویر را تشکیل می‌دهند. این بخش‌های بصری به نوبه خود توسط [CNN](../../4-ComputerVision/07-ConvNets/README.md) یاد گرفته می‌شوند.
* استفاده از یک تفکیک‌کننده زیرتصویر که تشخیص می‌دهد آیا بخش‌های تصویر "واقعی" یا "جعلی" هستند (برخلاف رویکرد "همه یا هیچ" در GAN سنتی).

اطلاعات بیشتر درباره VQGAN را در وب‌سایت [Taming Transformers](https://compvis.github.io/taming-transformers/) بیابید.

یکی از تفاوت‌های مهم بین VQGAN و GAN سنتی این است که دومی می‌تواند از هر بردار ورودی یک تصویر مناسب تولید کند، در حالی که VQGAN احتمالاً تصویری تولید می‌کند که منسجم نباشد. بنابراین، باید فرآیند ایجاد تصویر را بیشتر هدایت کنیم و این کار می‌تواند با استفاده از CLIP انجام شود.

![معماری VQGAN+CLIP](../../../../../lessons/X-Extras/X1-MultiModal/images/vqgan.png)

برای تولید تصویری که با یک متن ورودی مطابقت داشته باشد، با یک بردار کدگذاری تصادفی شروع می‌کنیم که از طریق VQGAN عبور داده می‌شود تا یک تصویر تولید شود. سپس CLIP برای تولید یک تابع زیان استفاده می‌شود که نشان می‌دهد تصویر تا چه حد با متن مطابقت دارد. هدف این است که این زیان را با استفاده از پس‌انتشار برای تنظیم پارامترهای بردار ورودی به حداقل برسانیم.

یک کتابخانه عالی که VQGAN+CLIP را پیاده‌سازی می‌کند، [Pixray](http://github.com/pixray/pixray) است.

![تصویر تولید شده توسط Pixray](../../../../../lessons/X-Extras/X1-MultiModal/images/a_closeup_watercolor_portrait_of_young_male_teacher_of_literature_with_a_book.png) |  ![تصویر تولید شده توسط Pixray](../../../../../lessons/X-Extras/X1-MultiModal/images/a_closeup_oil_portrait_of_young_female_teacher_of_computer_science_with_a_computer.png) | ![تصویر تولید شده توسط Pixray](../../../../../lessons/X-Extras/X1-MultiModal/images/a_closeup_oil_portrait_of_old_male_teacher_of_math.png)
----|----|----
تصویر تولید شده از متن ورودی *یک پرتره آبرنگ نزدیک از معلم جوان ادبیات با یک کتاب* | تصویر تولید شده از متن ورودی *یک پرتره روغنی نزدیک از معلم جوان علوم کامپیوتر با یک کامپیوتر* | تصویر تولید شده از متن ورودی *یک پرتره روغنی نزدیک از معلم مسن ریاضیات در مقابل تخته سیاه*

> تصاویر از مجموعه **معلمان مصنوعی** توسط [دمیتری سوشنیکوف](http://soshnikov.com)

## DALL-E
### [DALL-E 1](https://openai.com/research/dall-e)
DALL-E نسخه‌ای از GPT-3 است که برای تولید تصاویر از متن‌های ورودی آموزش دیده است. این مدل با ۱۲ میلیارد پارامتر آموزش دیده است.

برخلاف CLIP، DALL-E متن و تصویر را به عنوان یک جریان واحد از توکن‌ها برای هر دو دریافت می‌کند. بنابراین، از چندین متن ورودی می‌توان تصاویر را بر اساس متن تولید کرد.

### [DALL-E 2](https://openai.com/dall-e-2)
تفاوت اصلی بین DALL-E 1 و 2 این است که نسخه دوم تصاویر و هنرهای واقعی‌تر تولید می‌کند.

نمونه‌هایی از تولید تصویر با DALL-E:
![تصویر تولید شده توسط Pixray](../../../../../lessons/X-Extras/X1-MultiModal/images/DALL·E%202023-06-20%2015.56.56%20-%20a%20closeup%20watercolor%20portrait%20of%20young%20male%20teacher%20of%20literature%20with%20a%20book.png) |  ![تصویر تولید شده توسط Pixray](../../../../../lessons/X-Extras/X1-MultiModal/images/DALL·E%202023-06-20%2015.57.43%20-%20a%20closeup%20oil%20portrait%20of%20young%20female%20teacher%20of%20computer%20science%20with%20a%20computer.png) | ![تصویر تولید شده توسط Pixray](../../../../../lessons/X-Extras/X1-MultiModal/images/DALL·E%202023-06-20%2015.58.42%20-%20%20a%20closeup%20oil%20portrait%20of%20old%20male%20teacher%20of%20mathematics%20in%20front%20of%20blackboard.png)
----|----|----
تصویر تولید شده از متن ورودی *یک پرتره آبرنگ نزدیک از معلم جوان ادبیات با یک کتاب* | تصویر تولید شده از متن ورودی *یک پرتره روغنی نزدیک از معلم جوان علوم کامپیوتر با یک کامپیوتر* | تصویر تولید شده از متن ورودی *یک پرتره روغنی نزدیک از معلم مسن ریاضیات در مقابل تخته سیاه*

## منابع

* مقاله VQGAN: [Taming Transformers for High-Resolution Image Synthesis](https://compvis.github.io/taming-transformers/paper/paper.pdf)
* مقاله CLIP: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادقتی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.