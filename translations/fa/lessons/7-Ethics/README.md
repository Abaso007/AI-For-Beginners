<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "437c988596e751072e41a5aad3fcc5d9",
  "translation_date": "2025-08-24T10:10:24+00:00",
  "source_file": "lessons/7-Ethics/README.md",
  "language_code": "fa"
}
-->
# هوش مصنوعی اخلاقی و مسئولانه

شما تقریباً این دوره را به پایان رسانده‌اید و امیدوارم تا این لحظه به وضوح متوجه شده باشید که هوش مصنوعی بر اساس تعدادی روش‌های ریاضی رسمی بنا شده است که به ما امکان می‌دهد روابط موجود در داده‌ها را پیدا کنیم و مدل‌هایی را آموزش دهیم تا برخی جنبه‌های رفتار انسانی را بازتولید کنند. در این نقطه از تاریخ، ما هوش مصنوعی را به عنوان ابزاری بسیار قدرتمند برای استخراج الگوها از داده‌ها و استفاده از این الگوها برای حل مشکلات جدید در نظر می‌گیریم.

## [آزمون پیش از درس](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5/)

با این حال، در داستان‌های علمی تخیلی اغلب داستان‌هایی می‌بینیم که در آن‌ها هوش مصنوعی خطری برای بشریت ایجاد می‌کند. معمولاً این داستان‌ها حول نوعی شورش هوش مصنوعی می‌چرخند، زمانی که هوش مصنوعی تصمیم می‌گیرد با انسان‌ها مقابله کند. این موضوع نشان می‌دهد که هوش مصنوعی نوعی احساس دارد یا می‌تواند تصمیماتی بگیرد که توسط توسعه‌دهندگان آن پیش‌بینی نشده است.

نوع هوش مصنوعی که در این دوره درباره آن یاد گرفتیم چیزی جز محاسبات ماتریسی بزرگ نیست. این یک ابزار بسیار قدرتمند برای کمک به حل مشکلات ما است و مانند هر ابزار قدرتمند دیگری - می‌تواند برای اهداف خوب یا بد استفاده شود. مهم‌تر از همه، ممکن است *سوءاستفاده* شود.

## اصول هوش مصنوعی مسئولانه

برای جلوگیری از سوءاستفاده تصادفی یا عمدی از هوش مصنوعی، مایکروسافت اصول مهم [هوش مصنوعی مسئولانه](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-77998-cacaste) را بیان می‌کند. مفاهیم زیر پایه‌های این اصول هستند:

* **عدالت** به مشکل مهم *تعصب مدل‌ها* مربوط می‌شود که ممکن است به دلیل استفاده از داده‌های متعصب برای آموزش ایجاد شود. به عنوان مثال، زمانی که سعی می‌کنیم احتمال دریافت شغل توسعه‌دهنده نرم‌افزار برای یک فرد را پیش‌بینی کنیم، مدل احتمالاً ترجیح بیشتری به مردان می‌دهد - فقط به این دلیل که مجموعه داده‌های آموزشی احتمالاً به سمت مخاطبان مرد متعصب بوده است. ما باید داده‌های آموزشی را به دقت متعادل کنیم و مدل را بررسی کنیم تا از تعصب‌ها جلوگیری کنیم و مطمئن شویم که مدل ویژگی‌های مرتبط‌تر را در نظر می‌گیرد.
* **قابلیت اطمینان و ایمنی**. به طور طبیعی، مدل‌های هوش مصنوعی ممکن است اشتباه کنند. یک شبکه عصبی احتمال‌ها را بازمی‌گرداند و ما باید این موضوع را هنگام تصمیم‌گیری در نظر بگیریم. هر مدل دارای دقت و بازخوانی خاصی است و ما باید این موضوع را درک کنیم تا از آسیب‌هایی که مشاوره اشتباه ممکن است ایجاد کند جلوگیری کنیم.
* **حریم خصوصی و امنیت** دارای برخی پیامدهای خاص هوش مصنوعی هستند. به عنوان مثال، زمانی که از برخی داده‌ها برای آموزش یک مدل استفاده می‌کنیم، این داده‌ها به نوعی در مدل "ادغام" می‌شوند. از یک طرف، این موضوع امنیت و حریم خصوصی را افزایش می‌دهد، از طرف دیگر - باید به یاد داشته باشیم که مدل با چه داده‌هایی آموزش داده شده است.
* **شمول** به این معناست که ما هوش مصنوعی را برای جایگزینی انسان‌ها نمی‌سازیم، بلکه برای تقویت انسان‌ها و خلاق‌تر کردن کارمان طراحی می‌کنیم. این موضوع همچنین به عدالت مربوط می‌شود، زیرا هنگام کار با جوامع کم‌نماینده، اکثر مجموعه داده‌هایی که جمع‌آوری می‌کنیم احتمالاً متعصب هستند و باید مطمئن شویم که این جوامع در نظر گرفته شده و به درستی توسط هوش مصنوعی مدیریت می‌شوند.
* **شفافیت**. این شامل اطمینان از این است که همیشه به وضوح مشخص کنیم که از هوش مصنوعی استفاده می‌شود. همچنین، هر جا که ممکن باشد، می‌خواهیم از سیستم‌های هوش مصنوعی استفاده کنیم که *قابل تفسیر* باشند.
* **پاسخگویی**. زمانی که مدل‌های هوش مصنوعی تصمیماتی می‌گیرند، همیشه مشخص نیست که چه کسی مسئول این تصمیمات است. ما باید مطمئن شویم که درک می‌کنیم مسئولیت تصمیمات هوش مصنوعی کجا قرار دارد. در بیشتر موارد، می‌خواهیم انسان‌ها را در فرآیند تصمیم‌گیری‌های مهم وارد کنیم تا افراد واقعی مسئولیت‌پذیر باشند.

## ابزارهای هوش مصنوعی مسئولانه

مایکروسافت [جعبه ابزار هوش مصنوعی مسئولانه](https://github.com/microsoft/responsible-ai-toolbox) را توسعه داده است که شامل مجموعه‌ای از ابزارها است:

* داشبورد تفسیرپذیری (InterpretML)
* داشبورد عدالت (FairLearn)
* داشبورد تحلیل خطا
* داشبورد هوش مصنوعی مسئولانه که شامل موارد زیر است:

   - EconML - ابزاری برای تحلیل علّی که بر سوالات "چه می‌شود اگر" تمرکز دارد
   - DiCE - ابزاری برای تحلیل متقابل که به شما امکان می‌دهد ببینید کدام ویژگی‌ها باید تغییر کنند تا تصمیم مدل تحت تأثیر قرار گیرد

برای اطلاعات بیشتر درباره اخلاق هوش مصنوعی، لطفاً [این درس](https://github.com/microsoft/ML-For-Beginners/tree/main/1-Introduction/3-fairness?WT.mc_id=academic-77998-cacaste) را در برنامه درسی یادگیری ماشین که شامل تکالیف است، مشاهده کنید.

## مرور و مطالعه شخصی

این [مسیر یادگیری](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77998-cacaste) را دنبال کنید تا اطلاعات بیشتری درباره هوش مصنوعی مسئولانه کسب کنید.

## [آزمون پس از درس](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6/)

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌هایی باشد. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.