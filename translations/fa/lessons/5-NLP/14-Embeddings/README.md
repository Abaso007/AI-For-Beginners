<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-24T10:13:17+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "fa"
}
-->
# تعبیه‌ها

## [آزمون پیش از درس](https://ff-quizzes.netlify.app/en/ai/quiz/27)

هنگام آموزش دسته‌بندها بر اساس BoW یا TF/IDF، ما با بردارهای کیسه کلمات با ابعاد بالا که طول آن‌ها `vocab_size` بود کار می‌کردیم و به طور صریح بردارهای نمایشی موقعیتی با ابعاد پایین را به نمایشی پراکنده و تک‌داغ تبدیل می‌کردیم. با این حال، این نمایشی تک‌داغ از نظر حافظه کارآمد نیست. علاوه بر این، هر کلمه به صورت مستقل از دیگر کلمات در نظر گرفته می‌شود، یعنی بردارهای تک‌داغ هیچ شباهت معنایی بین کلمات را بیان نمی‌کنند.

ایده‌ی **تعبیه** این است که کلمات را با بردارهای متراکم و با ابعاد پایین‌تر نمایش دهیم که به نوعی معنای کلمه را منعکس کنند. بعداً درباره‌ی چگونگی ساخت تعبیه‌های معنایی بحث خواهیم کرد، اما فعلاً تعبیه‌ها را به عنوان روشی برای کاهش ابعاد بردار کلمات در نظر بگیرید.

بنابراین، لایه‌ی تعبیه یک کلمه را به عنوان ورودی می‌گیرد و یک بردار خروجی با اندازه‌ی مشخص `embedding_size` تولید می‌کند. به نوعی، این لایه بسیار شبیه به یک لایه‌ی `Linear` است، اما به جای گرفتن یک بردار تک‌داغ، می‌تواند یک شماره‌ی کلمه را به عنوان ورودی بگیرد و ما را از ایجاد بردارهای بزرگ تک‌داغ بی‌نیاز کند.

با استفاده از یک لایه‌ی تعبیه به عنوان اولین لایه در شبکه‌ی دسته‌بند خود، می‌توانیم از مدل کیسه کلمات به مدل **کیسه تعبیه‌ها** تغییر دهیم، جایی که ابتدا هر کلمه در متن خود را به تعبیه‌ی مربوطه تبدیل می‌کنیم و سپس یک تابع تجمیع مانند `sum`، `average` یا `max` را بر روی تمام این تعبیه‌ها اعمال می‌کنیم.

![تصویری که یک دسته‌بند تعبیه برای پنج کلمه‌ی دنباله را نشان می‌دهد.](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)

> تصویر از نویسنده

## ✍️ تمرین‌ها: تعبیه‌ها

یادگیری خود را در نوت‌بوک‌های زیر ادامه دهید:
* [تعبیه‌ها با PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [تعبیه‌ها با TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## تعبیه‌های معنایی: Word2Vec

در حالی که لایه‌ی تعبیه یاد می‌گیرد که کلمات را به بردارهای نمایشی نگاشت کند، اما این نمایشی لزوماً معنای زیادی از نظر معنایی ندارد. بهتر است بردارهایی یاد بگیریم که کلمات مشابه یا مترادف‌ها به بردارهایی نزدیک به هم از نظر فاصله‌ی برداری (مثلاً فاصله‌ی اقلیدسی) نگاشت شوند.

برای این کار، باید مدل تعبیه‌ی خود را به روشی خاص بر روی مجموعه‌ی بزرگی از متن پیش‌آموزش دهیم. یکی از روش‌های آموزش تعبیه‌های معنایی [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) نام دارد. این روش بر اساس دو معماری اصلی است که برای تولید نمایشی توزیع‌شده از کلمات استفاده می‌شوند:

- **کیسه کلمات پیوسته** (CBoW) — در این معماری، مدل را برای پیش‌بینی یک کلمه از متن اطراف آن آموزش می‌دهیم. با توجه به ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$، هدف مدل پیش‌بینی $W_0$ از $(W_{-2},W_{-1},W_1,W_2)$ است.
- **اسکیپ‌گرام پیوسته** برعکس CBoW است. مدل از پنجره‌ی کلمات متنی اطراف برای پیش‌بینی کلمه‌ی فعلی استفاده می‌کند.

CBoW سریع‌تر است، در حالی که اسکیپ‌گرام کندتر است اما در نمایش کلمات نادر بهتر عمل می‌کند.

![تصویری که الگوریتم‌های CBoW و اسکیپ‌گرام را برای تبدیل کلمات به بردارها نشان می‌دهد.](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> تصویر از [این مقاله](https://arxiv.org/pdf/1301.3781.pdf)

تعبیه‌های پیش‌آموزش‌داده‌شده‌ی Word2Vec (و همچنین مدل‌های مشابه دیگر مانند GloVe) می‌توانند به جای لایه‌ی تعبیه در شبکه‌های عصبی استفاده شوند. با این حال، باید با واژگان‌ها کار کنیم، زیرا واژگانی که برای پیش‌آموزش Word2Vec/GloVe استفاده شده است احتمالاً با واژگان موجود در متن ما متفاوت است. به نوت‌بوک‌های بالا نگاهی بیندازید تا ببینید چگونه می‌توان این مشکل را حل کرد.

## تعبیه‌های متنی

یکی از محدودیت‌های کلیدی تعبیه‌های پیش‌آموزش‌داده‌شده‌ی سنتی مانند Word2Vec مشکل ابهام معنای کلمات است. در حالی که تعبیه‌های پیش‌آموزش‌داده‌شده می‌توانند بخشی از معنای کلمات در متن را درک کنند، هر معنای ممکن یک کلمه در یک تعبیه‌ی واحد کدگذاری می‌شود. این می‌تواند در مدل‌های پایین‌دستی مشکل ایجاد کند، زیرا بسیاری از کلمات مانند کلمه‌ی «play» بسته به متنی که در آن استفاده می‌شوند معانی متفاوتی دارند.

برای مثال، کلمه‌ی «play» در این دو جمله‌ی مختلف معانی کاملاً متفاوتی دارد:

- من به یک **نمایش** در تئاتر رفتم.
- جان می‌خواهد با دوستانش **بازی** کند.

تعبیه‌های پیش‌آموزش‌داده‌شده‌ی بالا هر دو معنای کلمه‌ی «play» را در یک تعبیه‌ی واحد نمایش می‌دهند. برای غلبه بر این محدودیت، باید تعبیه‌هایی بر اساس **مدل زبانی** بسازیم که بر روی مجموعه‌ی بزرگی از متن آموزش دیده و *می‌داند* چگونه کلمات می‌توانند در متن‌های مختلف کنار هم قرار گیرند. بحث درباره‌ی تعبیه‌های متنی خارج از محدوده‌ی این آموزش است، اما وقتی درباره‌ی مدل‌های زبانی صحبت کنیم، به آن‌ها بازخواهیم گشت.

## نتیجه‌گیری

در این درس، یاد گرفتید که چگونه لایه‌های تعبیه را در TensorFlow و PyTorch بسازید و استفاده کنید تا معنای کلمات را بهتر منعکس کنید.

## 🚀 چالش

Word2Vec برای برخی کاربردهای جالب مانند تولید اشعار و ترانه‌ها استفاده شده است. به [این مقاله](https://www.politetype.com/blog/word2vec-color-poems) نگاهی بیندازید که توضیح می‌دهد چگونه نویسنده از Word2Vec برای تولید شعر استفاده کرده است. همچنین [این ویدیو از دن شیفمن](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) را تماشا کنید تا توضیح متفاوتی از این تکنیک را کشف کنید. سپس سعی کنید این تکنیک‌ها را بر روی مجموعه‌ی متن خود، شاید از Kaggle، اعمال کنید.

## [آزمون پس از درس](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## مرور و مطالعه‌ی خودخوان

این مقاله درباره‌ی Word2Vec را بخوانید: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [تکلیف: نوت‌بوک‌ها](assignment.md)

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.