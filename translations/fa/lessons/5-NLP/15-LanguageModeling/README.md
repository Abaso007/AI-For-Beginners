<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-24T10:16:13+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "fa"
}
-->
# مدل‌سازی زبان

بردارهای معنایی، مانند Word2Vec و GloVe، در واقع اولین گام به سمت **مدل‌سازی زبان** هستند - ایجاد مدل‌هایی که به نوعی *ماهیت* (یا *نمایش*) زبان را درک می‌کنند.

## [پیش‌ آزمون](https://ff-quizzes.netlify.app/en/ai/quiz/29)

ایده اصلی مدل‌سازی زبان، آموزش آن‌ها بر روی داده‌های بدون برچسب به صورت نظارت‌نشده است. این موضوع اهمیت دارد زیرا حجم زیادی از متن‌های بدون برچسب در دسترس داریم، در حالی که مقدار متن‌های برچسب‌دار همیشه به میزان تلاشی که می‌توانیم برای برچسب‌گذاری صرف کنیم محدود خواهد بود. اغلب، می‌توانیم مدل‌های زبانی بسازیم که بتوانند **کلمات گمشده** در متن را پیش‌بینی کنند، زیرا حذف تصادفی یک کلمه از متن و استفاده از آن به عنوان نمونه آموزشی کار ساده‌ای است.

## آموزش بردارها

در مثال‌های قبلی، ما از بردارهای معنایی از پیش آموزش‌داده‌شده استفاده کردیم، اما جالب است ببینیم این بردارها چگونه آموزش داده می‌شوند. چندین ایده ممکن وجود دارد که می‌توان از آن‌ها استفاده کرد:

* **مدل‌سازی زبان N-گرم**، که در آن یک توکن را با نگاه به N توکن قبلی پیش‌بینی می‌کنیم (N-گرم).
* **کیسه کلمات پیوسته** (CBoW)، که در آن توکن میانی $W_0$ را در یک دنباله توکن $W_{-N}$، ...، $W_N$ پیش‌بینی می‌کنیم.
* **اسکیپ‌گرم**، که در آن مجموعه‌ای از توکن‌های همسایه {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} را از توکن میانی $W_0$ پیش‌بینی می‌کنیم.

![تصویری از مقاله‌ای درباره تبدیل کلمات به بردارها](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> تصویر از [این مقاله](https://arxiv.org/pdf/1301.3781.pdf)

## ✍️ مثال‌های عملی: آموزش مدل CBoW

یادگیری خود را در نوت‌بوک‌های زیر ادامه دهید:

* [آموزش Word2Vec با استفاده از TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [آموزش Word2Vec با استفاده از PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## نتیجه‌گیری

در درس قبلی دیدیم که بردارهای کلمات مانند جادو عمل می‌کنند! اکنون می‌دانیم که آموزش بردارهای کلمات کار پیچیده‌ای نیست و اگر نیاز باشد، می‌توانیم بردارهای کلمات خود را برای متون خاص یک حوزه آموزش دهیم.

## [پس‌ آزمون](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## مرور و مطالعه شخصی

* [آموزش رسمی PyTorch درباره مدل‌سازی زبان](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [آموزش رسمی TensorFlow درباره آموزش مدل Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* استفاده از فریم‌ورک **gensim** برای آموزش رایج‌ترین بردارها تنها در چند خط کد در [این مستندات](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) توضیح داده شده است.

## 🚀 [تمرین: آموزش مدل اسکیپ‌گرم](lab/README.md)

در آزمایشگاه، از شما می‌خواهیم کد این درس را تغییر دهید تا به جای CBoW، مدل اسکیپ‌گرم را آموزش دهید. [جزئیات را بخوانید](lab/README.md)

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.