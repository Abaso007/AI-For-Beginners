<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-24T10:12:16+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "fa"
}
-->
# شبکه‌های عصبی بازگشتی

## [پیش‌زمینه: آزمون کوتاه](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

در بخش‌های قبلی، ما از نمایش‌های معنایی غنی متن و یک طبقه‌بند خطی ساده بر روی تعبیه‌ها استفاده کرده‌ایم. این معماری معنای کلی کلمات در یک جمله را ثبت می‌کند، اما ترتیب کلمات را در نظر نمی‌گیرد، زیرا عملیات تجمیع بر روی تعبیه‌ها این اطلاعات را از متن اصلی حذف می‌کند. به دلیل اینکه این مدل‌ها قادر به مدل‌سازی ترتیب کلمات نیستند، نمی‌توانند وظایف پیچیده‌تر یا مبهم‌تر مانند تولید متن یا پاسخ به سوالات را حل کنند.

برای ثبت معنای دنباله متن، باید از معماری دیگری از شبکه‌های عصبی استفاده کنیم که به آن **شبکه عصبی بازگشتی** یا RNN گفته می‌شود. در RNN، جمله را به صورت نماد به نماد از طریق شبکه عبور می‌دهیم و شبکه یک **وضعیت** تولید می‌کند که سپس با نماد بعدی دوباره به شبکه داده می‌شود.

![RNN](../../../../../lessons/5-NLP/16-RNN/images/rnn.png)

> تصویر توسط نویسنده

با توجه به دنباله ورودی توکن‌ها X<sub>0</sub>,...,X<sub>n</sub>، RNN یک دنباله از بلوک‌های شبکه عصبی ایجاد می‌کند و این دنباله را به صورت انتها به انتها با استفاده از پس‌انتشار آموزش می‌دهد. هر بلوک شبکه یک جفت (X<sub>i</sub>,S<sub>i</sub>) را به عنوان ورودی می‌گیرد و S<sub>i+1</sub> را به عنوان نتیجه تولید می‌کند. وضعیت نهایی S<sub>n</sub> یا (خروجی Y<sub>n</sub>) به یک طبقه‌بند خطی داده می‌شود تا نتیجه تولید شود. تمام بلوک‌های شبکه وزن‌های یکسانی دارند و به صورت انتها به انتها با یک پاس پس‌انتشار آموزش داده می‌شوند.

به دلیل اینکه بردارهای وضعیت S<sub>0</sub>,...,S<sub>n</sub> از طریق شبکه عبور می‌کنند، شبکه قادر است وابستگی‌های دنباله‌ای بین کلمات را یاد بگیرد. برای مثال، وقتی کلمه *نه* در جایی از دنباله ظاهر می‌شود، می‌تواند یاد بگیرد که برخی عناصر درون بردار وضعیت را نفی کند و نتیجه نفی را ایجاد کند.

> ✅ از آنجا که وزن‌های تمام بلوک‌های RNN در تصویر بالا مشترک هستند، همان تصویر می‌تواند به صورت یک بلوک (در سمت راست) با یک حلقه بازگشتی نمایش داده شود که وضعیت خروجی شبکه را به ورودی بازمی‌گرداند.

## ساختار یک سلول RNN

بیایید ببینیم یک سلول ساده RNN چگونه سازماندهی شده است. این سلول وضعیت قبلی S<sub>i-1</sub> و نماد فعلی X<sub>i</sub> را به عنوان ورودی می‌پذیرد و باید وضعیت خروجی S<sub>i</sub> (و گاهی اوقات خروجی دیگری Y<sub>i</sub>، همانطور که در مورد شبکه‌های تولیدی وجود دارد) را تولید کند.

یک سلول ساده RNN دو ماتریس وزن داخلی دارد: یکی نماد ورودی را تبدیل می‌کند (آن را W می‌نامیم)، و دیگری وضعیت ورودی را تبدیل می‌کند (H). در این حالت خروجی شبکه به صورت σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b) محاسبه می‌شود، که در آن σ تابع فعال‌سازی و b بایاس اضافی است.

<img alt="ساختار سلول RNN" src="images/rnn-anatomy.png" width="50%"/>

> تصویر توسط نویسنده

در بسیاری از موارد، توکن‌های ورودی قبل از ورود به RNN از لایه تعبیه عبور می‌کنند تا ابعاد کاهش یابد. در این حالت، اگر بعد بردارهای ورودی *emb_size* باشد و بردار وضعیت *hid_size* باشد - اندازه W برابر *emb_size*×*hid_size* و اندازه H برابر *hid_size*×*hid_size* خواهد بود.

## حافظه بلندمدت کوتاه (LSTM)

یکی از مشکلات اصلی RNN‌های کلاسیک مشکل **گرادیان‌های ناپدیدشونده** است. به دلیل اینکه RNN‌ها به صورت انتها به انتها در یک پاس پس‌انتشار آموزش داده می‌شوند، انتقال خطا به لایه‌های اولیه شبکه دشوار است و بنابراین شبکه نمی‌تواند روابط بین توکن‌های دور را یاد بگیرد. یکی از راه‌های جلوگیری از این مشکل معرفی **مدیریت وضعیت صریح** با استفاده از **دروازه‌ها** است. دو معماری معروف از این نوع وجود دارد: **حافظه بلندمدت کوتاه** (LSTM) و **واحد انتقال دروازه‌ای** (GRU).

![تصویر نشان‌دهنده یک سلول حافظه بلندمدت کوتاه](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> منبع تصویر TBD

شبکه LSTM به شکلی مشابه RNN سازماندهی شده است، اما دو وضعیت از لایه‌ای به لایه دیگر منتقل می‌شوند: وضعیت واقعی C و بردار مخفی H. در هر واحد، بردار مخفی H<sub>i</sub> با ورودی X<sub>i</sub> ترکیب می‌شود و آنها کنترل می‌کنند که چه اتفاقی برای وضعیت C از طریق **دروازه‌ها** بیفتد. هر دروازه یک شبکه عصبی با فعال‌سازی سیگموئید (خروجی در محدوده [0,1]) است که می‌توان آن را به عنوان یک ماسک بیتی در نظر گرفت که وقتی با بردار وضعیت ضرب می‌شود، عمل می‌کند. دروازه‌های زیر وجود دارند (از چپ به راست در تصویر بالا):

* **دروازه فراموشی** بردار مخفی را می‌گیرد و تعیین می‌کند که کدام اجزای بردار C باید فراموش شوند و کدام باید عبور کنند.
* **دروازه ورودی** اطلاعاتی از ورودی و بردارهای مخفی می‌گیرد و آن را وارد وضعیت می‌کند.
* **دروازه خروجی** وضعیت را از طریق یک لایه خطی با فعال‌سازی *tanh* تبدیل می‌کند، سپس برخی از اجزای آن را با استفاده از بردار مخفی H<sub>i</sub> انتخاب می‌کند تا وضعیت جدید C<sub>i+1</sub> تولید شود.

اجزای وضعیت C را می‌توان به عنوان برخی پرچم‌ها در نظر گرفت که می‌توانند روشن و خاموش شوند. برای مثال، وقتی نام *آلیس* را در دنباله مشاهده می‌کنیم، ممکن است بخواهیم فرض کنیم که به یک شخصیت زن اشاره دارد و پرچم را در وضعیت بالا ببریم که یک اسم زنانه در جمله داریم. وقتی عبارت *و تام* را مشاهده می‌کنیم، پرچم را بالا می‌بریم که یک اسم جمع داریم. بنابراین با دستکاری وضعیت می‌توانیم به طور فرضی ویژگی‌های دستوری بخش‌های جمله را پیگیری کنیم.

> ✅ یک منبع عالی برای درک جزئیات داخلی LSTM این مقاله فوق‌العاده [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) نوشته کریستوفر اولا است.

## RNN‌های دوطرفه و چندلایه

ما شبکه‌های بازگشتی را که در یک جهت عمل می‌کنند، از ابتدای دنباله تا انتها، مورد بحث قرار داده‌ایم. این روش طبیعی به نظر می‌رسد، زیرا شبیه به نحوه خواندن و گوش دادن به گفتار است. با این حال، از آنجا که در بسیاری از موارد عملی به دنباله ورودی دسترسی تصادفی داریم، ممکن است منطقی باشد که محاسبات بازگشتی را در هر دو جهت اجرا کنیم. چنین شبکه‌هایی **RNN‌های دوطرفه** نامیده می‌شوند. هنگام کار با شبکه دوطرفه، به دو بردار وضعیت مخفی نیاز داریم، یکی برای هر جهت.

یک شبکه بازگشتی، چه یک‌طرفه و چه دوطرفه، الگوهای خاصی را در یک دنباله ثبت می‌کند و می‌تواند آنها را در یک بردار وضعیت ذخیره کند یا به خروجی منتقل کند. همانند شبکه‌های کانولوشنی، می‌توانیم یک لایه بازگشتی دیگر بر روی لایه اول بسازیم تا الگوهای سطح بالاتر را ثبت کنیم و از الگوهای سطح پایین استخراج‌شده توسط لایه اول بسازیم. این ما را به مفهوم **RNN چندلایه** می‌رساند که شامل دو یا چند شبکه بازگشتی است، جایی که خروجی لایه قبلی به عنوان ورودی به لایه بعدی منتقل می‌شود.

![تصویر نشان‌دهنده یک RNN چندلایه حافظه بلندمدت کوتاه](../../../../../lessons/5-NLP/16-RNN/images/multi-layer-lstm.jpg)

*تصویر از [این پست فوق‌العاده](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) نوشته فرناندو لوپز*

## ✍️ تمرین‌ها: تعبیه‌ها

یادگیری خود را در نوت‌بوک‌های زیر ادامه دهید:

* [RNN‌ها با PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNN‌ها با TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## نتیجه‌گیری

در این واحد، دیدیم که RNN‌ها می‌توانند برای طبقه‌بندی دنباله استفاده شوند، اما در واقع، آنها می‌توانند بسیاری از وظایف دیگر مانند تولید متن، ترجمه ماشینی و موارد دیگر را انجام دهند. ما این وظایف را در واحد بعدی بررسی خواهیم کرد.

## 🚀 چالش

برخی از مقالات درباره LSTM‌ها را مطالعه کنید و کاربردهای آنها را بررسی کنید:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [آزمون پس از درس](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## مرور و مطالعه خودآموز

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) نوشته کریستوفر اولا.

## [تکلیف: نوت‌بوک‌ها](assignment.md)

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.