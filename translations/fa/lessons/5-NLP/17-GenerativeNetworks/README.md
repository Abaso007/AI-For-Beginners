<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-24T10:14:18+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "fa"
}
-->
# شبکه‌های مولد

## [آزمون پیش از درس](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

شبکه‌های عصبی بازگشتی (RNNs) و انواع سلول‌های دروازه‌دار آن‌ها مانند سلول‌های حافظه بلندمدت (LSTMs) و واحدهای بازگشتی دروازه‌دار (GRUs) مکانیزمی برای مدل‌سازی زبان فراهم کردند که می‌توانند ترتیب کلمات را یاد بگیرند و پیش‌بینی‌هایی برای کلمه بعدی در یک دنباله ارائه دهند. این قابلیت به ما اجازه می‌دهد از RNNها برای **وظایف مولد** استفاده کنیم، مانند تولید متن معمولی، ترجمه ماشینی، و حتی توضیحات تصویری.

> ✅ به تمام مواقعی فکر کنید که از وظایف مولد مانند تکمیل متن هنگام تایپ بهره برده‌اید. درباره برنامه‌های مورد علاقه‌تان تحقیق کنید تا ببینید آیا از RNNها استفاده کرده‌اند.

در معماری RNN که در واحد قبلی بحث کردیم، هر واحد RNN حالت مخفی بعدی را به عنوان خروجی تولید می‌کرد. با این حال، می‌توانیم خروجی دیگری به هر واحد بازگشتی اضافه کنیم که به ما اجازه می‌دهد یک **دنباله** (که طول آن برابر با دنباله اصلی است) خروجی بدهیم. علاوه بر این، می‌توانیم از واحدهای RNN استفاده کنیم که در هر مرحله ورودی دریافت نمی‌کنند و فقط یک بردار حالت اولیه می‌گیرند و سپس یک دنباله خروجی تولید می‌کنند.

این قابلیت معماری‌های عصبی مختلفی را فراهم می‌کند که در تصویر زیر نشان داده شده‌اند:

![تصویری که الگوهای رایج شبکه‌های عصبی بازگشتی را نشان می‌دهد.](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/unreasonable-effectiveness-of-rnn.jpg)

> تصویر از پست وبلاگ [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) نوشته [آندری کارپاتی](http://karpathy.github.io/)

* **یک به یک** یک شبکه عصبی سنتی با یک ورودی و یک خروجی است.
* **یک به چند** یک معماری مولد است که یک مقدار ورودی دریافت می‌کند و یک دنباله از مقادیر خروجی تولید می‌کند. به عنوان مثال، اگر بخواهیم یک شبکه **توضیحات تصویری** آموزش دهیم که توضیحات متنی یک تصویر را تولید کند، می‌توانیم یک تصویر به عنوان ورودی بدهیم، آن را از طریق یک CNN عبور دهیم تا حالت مخفی آن را به دست آوریم، و سپس یک زنجیره بازگشتی کلمه به کلمه توضیحات را تولید کند.
* **چند به یک** مربوط به معماری‌های RNN است که در واحد قبلی توضیح دادیم، مانند طبقه‌بندی متن.
* **چند به چند** یا **دنباله به دنباله** مربوط به وظایفی مانند **ترجمه ماشینی** است، جایی که ابتدا RNN تمام اطلاعات دنباله ورودی را در حالت مخفی جمع‌آوری می‌کند و سپس یک زنجیره RNN دیگر این حالت را به دنباله خروجی باز می‌کند.

در این واحد، ما بر مدل‌های مولد ساده تمرکز خواهیم کرد که به ما کمک می‌کنند متن تولید کنیم. برای سادگی، از توکن‌سازی سطح کاراکتر استفاده خواهیم کرد.

ما این RNN را آموزش خواهیم داد تا متن را مرحله به مرحله تولید کند. در هر مرحله، یک دنباله از کاراکترها به طول `nchars` می‌گیریم و از شبکه می‌خواهیم کاراکتر خروجی بعدی را برای هر کاراکتر ورودی تولید کند:

![تصویری که یک مثال از تولید کلمه 'HELLO' توسط RNN را نشان می‌دهد.](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate.png)

هنگام تولید متن (در زمان استنتاج)، با یک **پیش‌زمینه** شروع می‌کنیم که از طریق سلول‌های RNN عبور داده می‌شود تا حالت میانی آن تولید شود، و سپس از این حالت تولید آغاز می‌شود. ما یک کاراکتر را در هر زمان تولید می‌کنیم و حالت و کاراکتر تولید شده را به یک سلول RNN دیگر می‌دهیم تا کاراکتر بعدی را تولید کند، تا زمانی که به اندازه کافی کاراکتر تولید کنیم.

<img src="images/rnn-generate-inf.png" width="60%"/>

> تصویر توسط نویسنده

## ✍️ تمرین‌ها: شبکه‌های مولد

یادگیری خود را در نوت‌بوک‌های زیر ادامه دهید:

* [شبکه‌های مولد با PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [شبکه‌های مولد با TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## تولید متن نرم و دما

خروجی هر سلول RNN یک توزیع احتمالی از کاراکترها است. اگر همیشه کاراکتری با بالاترین احتمال را به عنوان کاراکتر بعدی در متن تولید شده انتخاب کنیم، متن اغلب ممکن است بین دنباله‌های کاراکتری مشابه بارها و بارها "چرخه" شود، مانند این مثال:

```
today of the second the company and a second the company ...
```

با این حال، اگر به توزیع احتمالی برای کاراکتر بعدی نگاه کنیم، ممکن است تفاوت بین چند احتمال بالاتر زیاد نباشد، به عنوان مثال یک کاراکتر ممکن است احتمال 0.2 داشته باشد و دیگری 0.19. برای مثال، هنگام جستجوی کاراکتر بعدی در دنباله '*play*'، کاراکتر بعدی می‌تواند به همان اندازه فضای خالی یا **e** باشد (مانند کلمه *player*).

این ما را به این نتیجه می‌رساند که همیشه انتخاب کاراکتر با احتمال بالاتر "منصفانه" نیست، زیرا انتخاب دومین احتمال بالاتر همچنان می‌تواند به متن معنادار منجر شود. عاقلانه‌تر است که **نمونه‌گیری** کاراکترها را از توزیع احتمالی ارائه شده توسط خروجی شبکه انجام دهیم. همچنین می‌توانیم از یک پارامتر به نام **دما** استفاده کنیم که توزیع احتمالی را صاف‌تر کند، در صورتی که بخواهیم تصادفی بیشتری اضافه کنیم، یا آن را شیب‌دارتر کنیم، اگر بخواهیم بیشتر به کاراکترهای با احتمال بالاتر پایبند باشیم.

بررسی کنید که این تولید متن نرم چگونه در نوت‌بوک‌های لینک شده بالا پیاده‌سازی شده است.

## نتیجه‌گیری

در حالی که تولید متن ممکن است به خودی خود مفید باشد، مزایای اصلی از توانایی تولید متن با استفاده از RNNها از یک بردار ویژگی اولیه ناشی می‌شود. به عنوان مثال، تولید متن به عنوان بخشی از ترجمه ماشینی استفاده می‌شود (دنباله به دنباله، در این حالت بردار حالت از *رمزگذار* برای تولید یا *رمزگشایی* پیام ترجمه شده استفاده می‌شود)، یا تولید توضیحات متنی یک تصویر (در این حالت بردار ویژگی از استخراج‌کننده CNN می‌آید).

## 🚀 چالش

برخی درس‌ها را در Microsoft Learn در این موضوع بگذرانید:

* تولید متن با [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [آزمون پس از درس](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## مرور و مطالعه خودآموز

در اینجا برخی مقالات برای گسترش دانش شما آورده شده است:

* رویکردهای مختلف برای تولید متن با زنجیره مارکوف، LSTM و GPT-2: [پست وبلاگ](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* نمونه تولید متن در [مستندات Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [تکلیف](lab/README.md)

ما دیدیم که چگونه متن را کاراکتر به کاراکتر تولید کنیم. در آزمایشگاه، تولید متن در سطح کلمه را بررسی خواهید کرد.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.