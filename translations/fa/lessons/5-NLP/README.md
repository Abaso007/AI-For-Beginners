<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ef02a9318257ea140ed3ed74442096d",
  "translation_date": "2025-08-24T10:11:26+00:00",
  "source_file": "lessons/5-NLP/README.md",
  "language_code": "fa"
}
-->
# پردازش زبان طبیعی

![خلاصه‌ای از وظایف NLP در یک طرح](../../../../lessons/sketchnotes/ai-nlp.png)

در این بخش، ما بر استفاده از شبکه‌های عصبی برای انجام وظایف مرتبط با **پردازش زبان طبیعی (NLP)** تمرکز خواهیم کرد. مسائل زیادی در NLP وجود دارد که می‌خواهیم کامپیوترها قادر به حل آن‌ها باشند:

* **طبقه‌بندی متن** یک مسئله طبقه‌بندی معمولی مربوط به توالی‌های متنی است. مثال‌ها شامل طبقه‌بندی پیام‌های ایمیل به عنوان اسپم یا غیر اسپم، یا دسته‌بندی مقالات به موضوعاتی مانند ورزش، تجارت، سیاست و غیره است. همچنین، هنگام توسعه چت‌بات‌ها، اغلب نیاز داریم بفهمیم کاربر چه چیزی می‌خواهد بگوید -- در این حالت با **طبقه‌بندی نیت** سروکار داریم. در طبقه‌بندی نیت، اغلب باید با دسته‌بندی‌های زیادی کار کنیم.
* **تحلیل احساسات** یک مسئله رگرسیون معمولی است، جایی که باید عددی (یک احساس) را نسبت دهیم که نشان‌دهنده مثبت یا منفی بودن معنای یک جمله باشد. نسخه پیشرفته‌تر تحلیل احساسات، **تحلیل احساسات مبتنی بر جنبه** (ABSA) است، جایی که احساسات را نه به کل جمله، بلکه به بخش‌های مختلف آن (جنبه‌ها) نسبت می‌دهیم، مثلاً: *در این رستوران، من غذا را دوست داشتم، اما جو وحشتناک بود*.
* **شناسایی موجودیت‌های نام‌دار** (NER) به مسئله استخراج موجودیت‌های خاص از متن اشاره دارد. برای مثال، ممکن است نیاز داشته باشیم بفهمیم که در عبارت *من باید فردا به پاریس پرواز کنم* کلمه *فردا* به تاریخ اشاره دارد و *پاریس* یک مکان است.
* **استخراج کلمات کلیدی** مشابه NER است، اما باید کلمات مهم برای معنای جمله را به صورت خودکار استخراج کنیم، بدون آموزش قبلی برای انواع خاص موجودیت‌ها.
* **خوشه‌بندی متن** می‌تواند زمانی مفید باشد که بخواهیم جملات مشابه را گروه‌بندی کنیم، مثلاً درخواست‌های مشابه در مکالمات پشتیبانی فنی.
* **پاسخ به سوالات** به توانایی یک مدل برای پاسخ دادن به یک سوال خاص اشاره دارد. مدل یک متن و یک سوال را به عنوان ورودی دریافت می‌کند و باید مکانی در متن که پاسخ سوال در آن قرار دارد را ارائه دهد (یا گاهی اوقات، متن پاسخ را تولید کند).
* **تولید متن** توانایی یک مدل برای تولید متن جدید است. این می‌تواند به عنوان یک مسئله طبقه‌بندی در نظر گرفته شود که حرف/کلمه بعدی را بر اساس یک *متن اولیه* پیش‌بینی می‌کند. مدل‌های پیشرفته تولید متن، مانند GPT-3، قادر به حل وظایف دیگر NLP مانند طبقه‌بندی با استفاده از تکنیکی به نام [برنامه‌نویسی با پرامپت](https://towardsdatascience.com/software-3-0-how-prompting-will-change-the-rules-of-the-game-a982fbfe1e0) یا [مهندسی پرامپت](https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29) هستند.
* **خلاصه‌سازی متن** تکنیکی است که در آن می‌خواهیم کامپیوتر متن طولانی را "بخواند" و آن را در چند جمله خلاصه کند.
* **ترجمه ماشینی** می‌تواند به عنوان ترکیبی از درک متن در یک زبان و تولید متن در زبان دیگر در نظر گرفته شود.

در ابتدا، بیشتر وظایف NLP با استفاده از روش‌های سنتی مانند گرامرها حل می‌شدند. برای مثال، در ترجمه ماشینی از تجزیه‌کننده‌ها برای تبدیل جمله اولیه به یک درخت نحوی استفاده می‌شد، سپس ساختارهای معنایی سطح بالاتر استخراج می‌شدند تا معنای جمله را نشان دهند، و بر اساس این معنا و گرامر زبان هدف، نتیجه تولید می‌شد. امروزه، بسیاری از وظایف NLP به طور موثرتری با استفاده از شبکه‌های عصبی حل می‌شوند.

> بسیاری از روش‌های کلاسیک NLP در کتابخانه پایتون [Natural Language Processing Toolkit (NLTK)](https://www.nltk.org) پیاده‌سازی شده‌اند. یک [کتاب NLTK](https://www.nltk.org/book/) عالی به صورت آنلاین در دسترس است که توضیح می‌دهد چگونه وظایف مختلف NLP می‌توانند با استفاده از NLTK حل شوند.

در دوره ما، بیشتر بر استفاده از شبکه‌های عصبی برای NLP تمرکز خواهیم کرد و در صورت نیاز از NLTK استفاده خواهیم کرد.

ما قبلاً درباره استفاده از شبکه‌های عصبی برای کار با داده‌های جدولی و تصاویر یاد گرفته‌ایم. تفاوت اصلی بین این نوع داده‌ها و متن این است که متن یک توالی با طول متغیر است، در حالی که اندازه ورودی در مورد تصاویر از قبل مشخص است. در حالی که شبکه‌های کانولوشنی می‌توانند الگوها را از داده‌های ورودی استخراج کنند، الگوها در متن پیچیده‌تر هستند. به عنوان مثال، ممکن است نفی از موضوع با کلمات زیادی جدا شود (مثلاً *من پرتقال دوست ندارم* در مقابل *من آن پرتقال‌های بزرگ رنگارنگ خوشمزه را دوست ندارم*)، و این باید همچنان به عنوان یک الگو تفسیر شود. بنابراین، برای پردازش زبان نیاز داریم که انواع جدیدی از شبکه‌های عصبی، مانند *شبکه‌های بازگشتی* و *ترانسفورمرها* را معرفی کنیم.

## نصب کتابخانه‌ها

اگر از نصب محلی پایتون برای اجرای این دوره استفاده می‌کنید، ممکن است نیاز داشته باشید تمام کتابخانه‌های مورد نیاز برای NLP را با استفاده از دستورات زیر نصب کنید:

**برای PyTorch**  
```bash
pip install -r requirements-torch.txt
```  
**برای TensorFlow**  
```bash
pip install -r requirements-tf.txt
```  

> می‌توانید NLP با TensorFlow را در [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/?WT.mc_id=academic-77998-cacaste) امتحان کنید.

## هشدار GPU

در این بخش، در برخی از مثال‌ها مدل‌های نسبتاً بزرگی را آموزش خواهیم داد.
* **استفاده از کامپیوتر مجهز به GPU**: توصیه می‌شود نوت‌بوک‌های خود را روی کامپیوتری که از GPU پشتیبانی می‌کند اجرا کنید تا زمان انتظار هنگام کار با مدل‌های بزرگ کاهش یابد.
* **محدودیت‌های حافظه GPU**: اجرای روی GPU ممکن است منجر به موقعیت‌هایی شود که حافظه GPU شما تمام شود، به ویژه هنگام آموزش مدل‌های بزرگ.
* **مصرف حافظه GPU**: مقدار حافظه GPU مصرف‌شده در طول آموزش به عوامل مختلفی از جمله اندازه مینی‌بچ بستگی دارد.
* **کاهش اندازه مینی‌بچ**: اگر با مشکلات حافظه GPU مواجه شدید، کاهش اندازه مینی‌بچ در کد خود را به عنوان یک راه‌حل احتمالی در نظر بگیرید.
* **آزادسازی حافظه GPU در TensorFlow**: نسخه‌های قدیمی‌تر TensorFlow ممکن است حافظه GPU را به درستی آزاد نکنند، به ویژه هنگام آموزش چندین مدل در یک کرنل پایتون. برای مدیریت موثر استفاده از حافظه GPU، می‌توانید TensorFlow را طوری پیکربندی کنید که حافظه GPU را فقط در صورت نیاز تخصیص دهد.
* **اضافه کردن کد**: برای تنظیم TensorFlow به گونه‌ای که تخصیص حافظه GPU فقط در صورت نیاز انجام شود، کد زیر را در نوت‌بوک‌های خود قرار دهید:

```python
physical_devices = tf.config.list_physical_devices('GPU') 
if len(physical_devices)>0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True) 
```  

اگر علاقه‌مند به یادگیری NLP از دیدگاه یادگیری ماشین کلاسیک هستید، به [این مجموعه درس‌ها](https://github.com/microsoft/ML-For-Beginners/tree/main/6-NLP) مراجعه کنید.

## در این بخش
در این بخش درباره موضوعات زیر یاد خواهیم گرفت:

* [نمایش متن به صورت تنسورها](13-TextRep/README.md)  
* [تعبیه کلمات](14-Emdeddings/README.md)  
* [مدل‌سازی زبان](15-LanguageModeling/README.md)  
* [شبکه‌های عصبی بازگشتی](16-RNN/README.md)  
* [شبکه‌های مولد](17-GenerativeNetworks/README.md)  
* [ترانسفورمرها](18-Transformers/README.md)  

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.