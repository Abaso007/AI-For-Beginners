{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# مکانیزم‌های توجه و مدل‌های ترانسفورمر\n",
    "\n",
    "یکی از مشکلات اصلی شبکه‌های بازگشتی این است که تمام کلمات در یک دنباله تأثیر یکسانی بر نتیجه دارند. این موضوع باعث عملکرد زیر حد مطلوب در مدل‌های استاندارد LSTM برای وظایف دنباله به دنباله، مانند شناسایی موجودیت‌های نام‌دار و ترجمه ماشینی می‌شود. در واقعیت، برخی کلمات در دنباله ورودی تأثیر بیشتری بر خروجی‌های دنباله‌ای نسبت به دیگر کلمات دارند.\n",
    "\n",
    "به مدل دنباله به دنباله، مانند ترجمه ماشینی، توجه کنید. این مدل توسط دو شبکه بازگشتی پیاده‌سازی می‌شود، که در آن یک شبکه (**رمزگذار**) دنباله ورودی را به حالت پنهان تبدیل می‌کند و شبکه دیگر (**رمزگشا**) این حالت پنهان را به نتیجه ترجمه‌شده باز می‌گرداند. مشکل این روش این است که حالت نهایی شبکه به سختی می‌تواند ابتدای جمله را به خاطر بسپارد، که این موضوع باعث کاهش کیفیت مدل در جملات طولانی می‌شود.\n",
    "\n",
    "**مکانیزم‌های توجه** راهی برای وزن‌دهی تأثیر متنی هر بردار ورودی بر هر پیش‌بینی خروجی RNN فراهم می‌کنند. این مکانیزم با ایجاد میان‌برهایی بین حالت‌های میانی RNN ورودی و RNN خروجی پیاده‌سازی می‌شود. به این ترتیب، هنگام تولید نماد خروجی $y_t$، تمام حالت‌های پنهان ورودی $h_i$ را با ضرایب وزنی مختلف $\\alpha_{t,i}$ در نظر می‌گیریم.\n",
    "\n",
    "![تصویری که مدل رمزگذار/رمزگشا با لایه توجه افزایشی را نشان می‌دهد](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)\n",
    "*مدل رمزگذار-رمزگشا با مکانیزم توجه افزایشی در [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)، نقل شده از [این پست وبلاگ](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "ماتریس توجه $\\{\\alpha_{i,j}\\}$ نشان‌دهنده درجه‌ای است که کلمات خاص ورودی در تولید یک کلمه مشخص در دنباله خروجی نقش دارند. در زیر نمونه‌ای از چنین ماتریسی آورده شده است:\n",
    "\n",
    "![تصویری که یک نمونه هم‌ترازی پیدا شده توسط RNNsearch-50 را نشان می‌دهد، گرفته شده از Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)\n",
    "\n",
    "*تصویر گرفته شده از [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (شکل ۳)*\n",
    "\n",
    "مکانیزم‌های توجه مسئول بسیاری از دستاوردهای فعلی یا نزدیک به فعلی در پردازش زبان طبیعی هستند. با این حال، افزودن توجه به طور قابل توجهی تعداد پارامترهای مدل را افزایش می‌دهد که منجر به مشکلات مقیاس‌پذیری در RNN‌ها می‌شود. یکی از محدودیت‌های کلیدی در مقیاس‌پذیری RNN‌ها این است که ماهیت بازگشتی مدل‌ها باعث می‌شود آموزش دسته‌ای و موازی‌سازی چالش‌برانگیز شود. در RNN، هر عنصر از دنباله باید به ترتیب پردازش شود، که به این معناست که نمی‌توان آن را به راحتی موازی‌سازی کرد.\n",
    "\n",
    "پذیرش مکانیزم‌های توجه همراه با این محدودیت منجر به ایجاد مدل‌های ترانسفورمر شد که اکنون به عنوان پیشرفته‌ترین مدل‌ها شناخته می‌شوند و از BERT تا OpenGPT3 مورد استفاده قرار می‌گیرند.\n",
    "\n",
    "## مدل‌های ترانسفورمر\n",
    "\n",
    "به جای انتقال زمینه هر پیش‌بینی قبلی به مرحله ارزیابی بعدی، **مدل‌های ترانسفورمر** از **کدگذاری موقعیتی** و **توجه** برای درک زمینه ورودی در یک پنجره متن مشخص استفاده می‌کنند. تصویر زیر نشان می‌دهد که چگونه کدگذاری موقعیتی همراه با توجه می‌تواند زمینه را در یک پنجره مشخص درک کند.\n",
    "\n",
    "![GIF متحرک که نشان می‌دهد ارزیابی‌ها در مدل‌های ترانسفورمر چگونه انجام می‌شوند.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "از آنجا که هر موقعیت ورودی به طور مستقل به هر موقعیت خروجی نگاشت می‌شود، ترانسفورمرها بهتر از RNN‌ها موازی‌سازی می‌شوند، که این امکان را فراهم می‌کند تا مدل‌های زبانی بزرگ‌تر و بیان‌گرتر ایجاد شوند. هر سر توجه می‌تواند برای یادگیری روابط مختلف بین کلمات استفاده شود که وظایف پردازش زبان طبیعی را بهبود می‌بخشد.\n",
    "\n",
    "## ساخت مدل ترانسفورمر ساده\n",
    "\n",
    "Keras شامل لایه ترانسفورمر داخلی نیست، اما می‌توانیم خودمان آن را بسازیم. همانند قبل، ما بر طبقه‌بندی متن مجموعه داده AG News تمرکز خواهیم کرد، اما شایان ذکر است که مدل‌های ترانسفورمر بهترین نتایج را در وظایف دشوارتر NLP نشان می‌دهند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لایه‌های جدید در Keras باید کلاس `Layer` را زیرکلاس کنند و متد `call` را پیاده‌سازی کنند. بیایید با لایه **جایگذاری موقعیتی** شروع کنیم. ما از [کدی از مستندات رسمی Keras](https://keras.io/examples/nlp/text_classification_with_transformer/) استفاده خواهیم کرد. فرض می‌کنیم که تمام دنباله‌های ورودی را به طول `maxlen` پر می‌کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "این لایه شامل دو لایه `Embedding` است: یکی برای جاسازی توکن‌ها (به شیوه‌ای که قبلاً بحث کردیم) و دیگری برای موقعیت توکن‌ها. موقعیت توکن‌ها به صورت یک دنباله از اعداد طبیعی از 0 تا `maxlen` با استفاده از `tf.range` ایجاد می‌شوند و سپس از طریق لایه جاسازی عبور داده می‌شوند. دو بردار جاسازی حاصل با هم جمع می‌شوند و نمایشی جاسازی‌شده موقعیتی از ورودی با شکل `maxlen`$\\times$`embed_dim` تولید می‌کنند.\n",
    "\n",
    "حالا بیایید بلوک ترانسفورمر را پیاده‌سازی کنیم. این بلوک خروجی لایه جاسازی تعریف‌شده قبلی را دریافت خواهد کرد:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون آماده‌ایم مدل کامل ترانسفورمر را تعریف کنیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مدل‌های ترنسفورمر BERT\n",
    "\n",
    "**BERT** (نمایش‌های رمزگذار دوطرفه از ترنسفورمرها) یک شبکه ترنسفورمر چندلایه بسیار بزرگ است که برای *BERT-base* شامل ۱۲ لایه و برای *BERT-large* شامل ۲۴ لایه می‌باشد. این مدل ابتدا با استفاده از یک مجموعه داده متنی بزرگ (ویکی‌پدیا + کتاب‌ها) و با روش آموزش بدون نظارت (پیش‌بینی کلمات ماسک‌شده در یک جمله) پیش‌آموزش داده می‌شود. در طول پیش‌آموزش، مدل سطح قابل توجهی از درک زبان را جذب می‌کند که می‌توان از آن در ترکیب با مجموعه داده‌های دیگر از طریق تنظیم دقیق استفاده کرد. این فرآیند **یادگیری انتقالی** نامیده می‌شود.\n",
    "\n",
    "![تصویر از http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)\n",
    "\n",
    "معماری‌های مختلفی از ترنسفورمرها وجود دارند، از جمله BERT، DistilBERT، BigBird، OpenGPT3 و موارد دیگر که می‌توان آن‌ها را تنظیم دقیق کرد.\n",
    "\n",
    "بیایید ببینیم چگونه می‌توان از مدل پیش‌آموزش‌داده‌شده BERT برای حل مسئله سنتی طبقه‌بندی توالی استفاده کرد. ما ایده و بخشی از کد را از [مستندات رسمی](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) قرض می‌گیریم.\n",
    "\n",
    "برای بارگذاری مدل‌های پیش‌آموزش‌داده‌شده، از **Tensorflow hub** استفاده خواهیم کرد. ابتدا، بیایید وکتورایزر مخصوص BERT را بارگذاری کنیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مهم است که از همان وکتورایزر استفاده کنید که شبکه اصلی با آن آموزش داده شده است. همچنین، وکتورایزر BERT سه مؤلفه بازمی‌گرداند:\n",
    "* `input_word_ids`، که یک دنباله از شماره‌های توکن برای جمله ورودی است\n",
    "* `input_mask`، که نشان می‌دهد کدام بخش از دنباله شامل ورودی واقعی است و کدام بخش پدینگ است. این مشابه ماسکی است که توسط لایه `Masking` تولید می‌شود\n",
    "* `input_type_ids` برای وظایف مدل‌سازی زبان استفاده می‌شود و امکان مشخص کردن دو جمله ورودی در یک دنباله را فراهم می‌کند.\n",
    "\n",
    "سپس، می‌توانیم استخراج‌کننده ویژگی BERT را ایجاد کنیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لایه BERT چندین نتیجه مفید ارائه می‌دهد:\n",
    "* `pooled_output` نتیجه میانگین‌گیری تمام توکن‌ها در دنباله است. می‌توانید آن را به‌عنوان یک جاسازی معنایی هوشمند برای کل شبکه در نظر بگیرید. این معادل خروجی لایه `GlobalAveragePooling1D` در مدل قبلی ما است.\n",
    "* `sequence_output` خروجی آخرین لایه ترانسفورمر است (مطابق با خروجی `TransformerBlock` در مدل بالا).\n",
    "* `encoder_outputs` خروجی‌های تمام لایه‌های ترانسفورمر هستند. از آنجا که ما مدل BERT چهار لایه بارگذاری کرده‌ایم (همان‌طور که احتمالاً از نام آن که شامل `4_H` است حدس زده‌اید)، این مدل دارای چهار تنسور است. آخرین تنسور همان `sequence_output` است.\n",
    "\n",
    "اکنون مدل طبقه‌بندی انتها به انتها را تعریف خواهیم کرد. از *تعریف مدل تابعی* استفاده خواهیم کرد، جایی که ورودی مدل را تعریف می‌کنیم و سپس مجموعه‌ای از عبارات برای محاسبه خروجی آن ارائه می‌دهیم. همچنین وزن‌های مدل BERT را غیرقابل آموزش قرار می‌دهیم و فقط طبقه‌بند نهایی را آموزش می‌دهیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "با وجود تعداد کم پارامترهای قابل آموزش، فرآیند نسبتاً کند است، زیرا استخراج‌کننده ویژگی‌های BERT از نظر محاسباتی سنگین است. به نظر می‌رسد که نتوانستیم دقت قابل قبولی به دست آوریم، یا به دلیل کمبود آموزش، یا کمبود پارامترهای مدل.\n",
    "\n",
    "بیایید سعی کنیم وزن‌های BERT را از حالت فریز خارج کنیم و آن را نیز آموزش دهیم. این کار نیاز به نرخ یادگیری بسیار کوچک دارد و همچنین استراتژی آموزشی دقیق‌تری با **گرم کردن** و استفاده از بهینه‌ساز **AdamW** دارد. ما از بسته `tf-models-official` برای ایجاد بهینه‌ساز استفاده خواهیم کرد:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "همانطور که می‌بینید، فرآیند آموزش مدل به کندی پیش می‌رود - اما ممکن است بخواهید آزمایش کنید و مدل را برای چند دوره (۵-۱۰) آموزش دهید و ببینید آیا می‌توانید نتیجه بهتری نسبت به روش‌هایی که قبلاً استفاده کرده‌ایم، به دست آورید.\n",
    "\n",
    "## کتابخانه Huggingface Transformers\n",
    "\n",
    "یکی دیگر از روش‌های بسیار رایج (و کمی ساده‌تر) برای استفاده از مدل‌های Transformer، [بسته HuggingFace](https://github.com/huggingface/) است که بلوک‌های سازنده ساده‌ای برای وظایف مختلف پردازش زبان طبیعی (NLP) ارائه می‌دهد. این کتابخانه هم برای Tensorflow و هم برای PyTorch، که یکی دیگر از چارچوب‌های محبوب شبکه‌های عصبی است، در دسترس است.\n",
    "\n",
    "> **توجه**: اگر علاقه‌ای به دیدن نحوه عملکرد کتابخانه Transformers ندارید - می‌توانید به انتهای این دفترچه یادداشت بروید، زیرا چیز متفاوتی نسبت به آنچه قبلاً انجام داده‌ایم نخواهید دید. ما همان مراحل آموزش مدل BERT را با استفاده از کتابخانه‌ای متفاوت و مدلی به‌مراتب بزرگ‌تر تکرار خواهیم کرد. بنابراین، این فرآیند شامل آموزش‌های نسبتاً طولانی است، و ممکن است بخواهید فقط کدها را مرور کنید.\n",
    "\n",
    "بیایید ببینیم چگونه می‌توانیم مسئله خود را با استفاده از [Huggingface Transformers](http://huggingface.co) حل کنیم.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اولین کاری که باید انجام دهیم انتخاب مدلی است که قصد استفاده از آن را داریم. علاوه بر مدل‌های داخلی، Huggingface یک [مخزن آنلاین مدل‌ها](https://huggingface.co/models) دارد که در آن می‌توانید تعداد زیادی مدل از پیش آموزش‌دیده شده توسط جامعه را پیدا کنید. تمام این مدل‌ها تنها با ارائه نام مدل قابل بارگذاری و استفاده هستند. تمام فایل‌های باینری مورد نیاز برای مدل به صورت خودکار دانلود می‌شوند.\n",
    "\n",
    "در برخی مواقع ممکن است نیاز داشته باشید مدل‌های خودتان را بارگذاری کنید. در این صورت می‌توانید دایرکتوری‌ای را مشخص کنید که شامل تمام فایل‌های مرتبط باشد، از جمله پارامترهای مربوط به tokenizer، فایل `config.json` با پارامترهای مدل، وزن‌های باینری و غیره.\n",
    "\n",
    "از طریق نام مدل، می‌توانیم هم مدل و هم tokenizer را ایجاد کنیم. بیایید با یک tokenizer شروع کنیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "شیء `tokenizer` شامل تابع `encode` است که می‌توان آن را مستقیماً برای کدگذاری متن استفاده کرد:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما همچنین می‌توانیم از توکنایزر برای رمزگذاری یک دنباله به روشی مناسب برای ارسال به مدل استفاده کنیم، یعنی شامل فیلدهای `token_ids`، `input_mask` و غیره. ما همچنین می‌توانیم مشخص کنیم که می‌خواهیم تنسورهای Tensorflow را با ارائه آرگومان `return_tensors='tf'` دریافت کنیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در اینجا از مدل از پیش آموزش‌دیده‌ی BERT به نام `bert-base-uncased` استفاده خواهیم کرد. *Uncased* نشان‌دهنده‌ی این است که مدل نسبت به حروف بزرگ و کوچک حساس نیست.\n",
    "\n",
    "هنگام آموزش مدل، باید دنباله‌ی توکن‌شده را به عنوان ورودی ارائه دهیم، و به همین دلیل یک خط پردازش داده طراحی خواهیم کرد. از آنجا که `tokenizer.encode` یک تابع پایتون است، از همان رویکردی که در واحد قبلی استفاده کردیم بهره خواهیم برد و آن را با استفاده از `py_function` فراخوانی می‌کنیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون می‌توانیم مدل واقعی را با استفاده از بسته `BertForSequenceClassification` بارگذاری کنیم. این اطمینان حاصل می‌کند که مدل ما از قبل معماری مورد نیاز برای طبقه‌بندی، از جمله طبقه‌بند نهایی را دارد. شما پیام هشداری خواهید دید که بیان می‌کند وزن‌های طبقه‌بند نهایی مقداردهی اولیه نشده‌اند و مدل نیاز به پیش‌آموزش دارد - این کاملاً مشکلی ندارد، زیرا دقیقاً همان چیزی است که قصد انجام آن را داریم!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "همانطور که از `summary()` می‌بینید، مدل تقریباً ۱۱۰ میلیون پارامتر دارد! احتمالاً، اگر بخواهیم یک وظیفه طبقه‌بندی ساده روی مجموعه داده نسبتاً کوچک انجام دهیم، نمی‌خواهیم لایه پایه BERT را آموزش دهیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون آماده شروع آموزش هستیم!\n",
    "\n",
    "> **توجه**: آموزش مدل کامل BERT می‌تواند بسیار زمان‌بر باشد! بنابراین، ما فقط آن را برای ۳۲ دسته اول آموزش خواهیم داد. این فقط برای نشان دادن نحوه تنظیم آموزش مدل است. اگر علاقه‌مند به امتحان آموزش کامل هستید - فقط پارامترهای `steps_per_epoch` و `validation_steps` را حذف کنید و آماده صبر کردن باشید!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اگر تعداد تکرارها را افزایش دهید، زمان بیشتری منتظر بمانید و مدل را برای چندین دوره آموزش دهید، می‌توانید انتظار داشته باشید که طبقه‌بندی BERT بهترین دقت را به ما بدهد! دلیل این موضوع این است که BERT ساختار زبان را به‌خوبی درک کرده است و ما فقط نیاز داریم که طبقه‌بند نهایی را به‌صورت دقیق تنظیم کنیم. با این حال، چون BERT یک مدل بزرگ است، کل فرآیند آموزش زمان زیادی می‌برد و به قدرت محاسباتی قابل توجهی نیاز دارد! (GPU و ترجیحاً بیش از یک عدد).\n",
    "\n",
    "> **توجه:** در مثال ما، از یکی از کوچک‌ترین مدل‌های BERT از پیش آموزش‌دیده استفاده کرده‌ایم. مدل‌های بزرگ‌تری وجود دارند که احتمالاً نتایج بهتری ارائه می‌دهند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## نکات کلیدی\n",
    "\n",
    "در این بخش، ما با معماری‌های مدل بسیار جدید مبتنی بر **ترانسفورمرها** آشنا شدیم. این مدل‌ها را برای وظیفه طبقه‌بندی متن به کار بردیم، اما به همین شکل، مدل‌های BERT می‌توانند برای استخراج موجودیت‌ها، پاسخ به سوالات و سایر وظایف پردازش زبان طبیعی استفاده شوند.\n",
    "\n",
    "مدل‌های ترانسفورمر نمایانگر پیشرفته‌ترین روش‌ها در پردازش زبان طبیعی هستند و در بیشتر موارد باید اولین راه‌حلی باشند که هنگام پیاده‌سازی راه‌حل‌های سفارشی پردازش زبان طبیعی شروع به آزمایش با آن‌ها می‌کنید. با این حال، درک اصول پایه‌ای شبکه‌های عصبی بازگشتی که در این بخش مورد بحث قرار گرفتند، بسیار مهم است اگر قصد دارید مدل‌های عصبی پیشرفته بسازید.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-31T17:02:26+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}