<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-24T10:37:28+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "fa"
}
-->
# یادگیری تقویتی عمیق

یادگیری تقویتی (RL) به عنوان یکی از الگوهای اصلی یادگیری ماشین در کنار یادگیری نظارت‌شده و یادگیری بدون نظارت شناخته می‌شود. در حالی که در یادگیری نظارت‌شده به مجموعه داده‌هایی با نتایج مشخص تکیه می‌کنیم، یادگیری تقویتی بر اساس **یادگیری از طریق عمل** است. برای مثال، وقتی برای اولین بار یک بازی کامپیوتری را می‌بینیم، شروع به بازی می‌کنیم، حتی بدون دانستن قوانین، و به زودی تنها با فرآیند بازی کردن و تنظیم رفتار خود، مهارت‌هایمان را بهبود می‌بخشیم.

## [آزمون پیش از درس](https://ff-quizzes.netlify.app/en/ai/quiz/43)

برای انجام یادگیری تقویتی، به موارد زیر نیاز داریم:

* یک **محیط** یا **شبیه‌ساز** که قوانین بازی را تعیین می‌کند. باید بتوانیم آزمایش‌ها را در شبیه‌ساز اجرا کرده و نتایج را مشاهده کنیم.
* یک **تابع پاداش** که نشان می‌دهد آزمایش ما چقدر موفق بوده است. در مورد یادگیری بازی کامپیوتری، پاداش می‌تواند امتیاز نهایی ما باشد.

بر اساس تابع پاداش، باید بتوانیم رفتار خود را تنظیم کرده و مهارت‌هایمان را بهبود دهیم تا دفعه بعد بهتر بازی کنیم. تفاوت اصلی بین یادگیری تقویتی و سایر انواع یادگیری ماشین این است که در یادگیری تقویتی معمولاً تا پایان بازی نمی‌دانیم که برنده شده‌ایم یا باخته‌ایم. بنابراین نمی‌توانیم بگوییم که یک حرکت خاص به تنهایی خوب است یا نه - ما فقط در پایان بازی پاداش دریافت می‌کنیم.

در طول یادگیری تقویتی، معمولاً آزمایش‌های زیادی انجام می‌دهیم. در هر آزمایش، باید بین دنبال کردن استراتژی بهینه‌ای که تاکنون یاد گرفته‌ایم (**بهره‌برداری**) و کشف حالت‌های جدید ممکن (**اکتشاف**) تعادل برقرار کنیم.

## OpenAI Gym

یک ابزار عالی برای یادگیری تقویتی [OpenAI Gym](https://gym.openai.com/) است - یک **محیط شبیه‌سازی** که می‌تواند بسیاری از محیط‌های مختلف را شبیه‌سازی کند، از بازی‌های آتاری گرفته تا فیزیک پشت تعادل میله. این ابزار یکی از محبوب‌ترین محیط‌های شبیه‌سازی برای آموزش الگوریتم‌های یادگیری تقویتی است و توسط [OpenAI](https://openai.com/) نگهداری می‌شود.

> **Note**: می‌توانید تمام محیط‌های موجود در OpenAI Gym را [اینجا](https://gym.openai.com/envs/#classic_control) مشاهده کنید.

## تعادل میله (CartPole)

احتمالاً همه شما دستگاه‌های تعادل مدرن مانند *Segway* یا *Gyroscooters* را دیده‌اید. این دستگاه‌ها می‌توانند به طور خودکار با تنظیم چرخ‌های خود در پاسخ به سیگنال‌های شتاب‌سنج یا ژیروسکوپ تعادل برقرار کنند. در این بخش، یاد می‌گیریم که چگونه یک مسئله مشابه - تعادل یک میله - را حل کنیم. این مسئله شبیه به موقعیتی است که یک هنرمند سیرک باید یک میله را روی دست خود متعادل کند - اما این تعادل میله فقط در یک بعد (1D) اتفاق می‌افتد.

نسخه ساده‌شده‌ای از تعادل به عنوان مسئله **CartPole** شناخته می‌شود. در دنیای CartPole، ما یک لغزنده افقی داریم که می‌تواند به چپ یا راست حرکت کند، و هدف این است که یک میله عمودی را روی لغزنده در حال حرکت متعادل کنیم.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

برای ایجاد و استفاده از این محیط، به چند خط کد پایتون نیاز داریم:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

هر محیط دقیقاً به همین روش قابل دسترسی است:
* `env.reset` یک آزمایش جدید را شروع می‌کند.
* `env.step` یک گام شبیه‌سازی را انجام می‌دهد. این تابع یک **عمل** از فضای عمل دریافت می‌کند و یک **مشاهده** (از فضای مشاهده)، به همراه یک پاداش و یک پرچم پایان بازمی‌گرداند.

در مثال بالا، ما در هر گام یک عمل تصادفی انجام می‌دهیم، به همین دلیل طول عمر آزمایش بسیار کوتاه است:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

هدف یک الگوریتم یادگیری تقویتی این است که یک مدل - به اصطلاح **سیاست** π - را آموزش دهد که در پاسخ به یک حالت خاص، عمل مناسب را بازگرداند. همچنین می‌توان سیاست را احتمالی در نظر گرفت، به این معنا که برای هر حالت *s* و عمل *a*، احتمال π(*a*|*s*) را بازمی‌گرداند که باید عمل *a* را در حالت *s* انجام دهیم.

## الگوریتم گرادیان سیاست (Policy Gradients)

واضح‌ترین روش برای مدل‌سازی یک سیاست، ایجاد یک شبکه عصبی است که حالت‌ها را به عنوان ورودی دریافت کرده و اعمال مربوطه (یا احتمال تمام اعمال) را بازمی‌گرداند. به نوعی، این روش شبیه به یک مسئله طبقه‌بندی معمولی است، با این تفاوت عمده که ما از قبل نمی‌دانیم در هر گام کدام عمل را باید انجام دهیم.

ایده این است که این احتمالات را تخمین بزنیم. ما یک بردار از **پاداش‌های تجمعی** می‌سازیم که پاداش کل ما را در هر گام آزمایش نشان می‌دهد. همچنین با استفاده از **تخفیف پاداش**، پاداش‌های اولیه را با یک ضریب γ=0.99 ضرب می‌کنیم تا نقش پاداش‌های اولیه کاهش یابد. سپس، آن گام‌هایی از مسیر آزمایش را که پاداش بیشتری به همراه دارند، تقویت می‌کنیم.

> درباره الگوریتم گرادیان سیاست بیشتر بیاموزید و آن را در [دفترچه مثال](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) مشاهده کنید.

## الگوریتم Actor-Critic

نسخه بهبود یافته رویکرد گرادیان سیاست به نام **Actor-Critic** شناخته می‌شود. ایده اصلی این است که شبکه عصبی برای بازگرداندن دو چیز آموزش داده شود:

* سیاست، که تعیین می‌کند کدام عمل را انجام دهیم. این بخش **actor** نامیده می‌شود.
* تخمین پاداش کل که می‌توانیم در این حالت انتظار داشته باشیم - این بخش **critic** نامیده می‌شود.

به نوعی، این معماری شبیه به [GAN](../../4-ComputerVision/10-GANs/README.md) است، جایی که دو شبکه در برابر یکدیگر آموزش می‌بینند. در مدل Actor-Critic، actor عملی را که باید انجام دهیم پیشنهاد می‌دهد، و critic سعی می‌کند انتقادی باشد و نتیجه را تخمین بزند. با این حال، هدف ما این است که این شبکه‌ها را به صورت هماهنگ آموزش دهیم.

از آنجا که هم پاداش‌های تجمعی واقعی و هم نتایج بازگردانده شده توسط critic را در طول آزمایش می‌دانیم، ساخت یک تابع هزینه که تفاوت بین آن‌ها را به حداقل برساند نسبتاً آسان است. این کار به ما **هزینه critic** می‌دهد. می‌توانیم **هزینه actor** را با استفاده از همان رویکرد الگوریتم گرادیان سیاست محاسبه کنیم.

پس از اجرای یکی از این الگوریتم‌ها، می‌توانیم انتظار داشته باشیم که CartPole ما به این شکل رفتار کند:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ تمرین‌ها: گرادیان سیاست و یادگیری تقویتی Actor-Critic

یادگیری خود را در دفترچه‌های زیر ادامه دهید:

* [یادگیری تقویتی در TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [یادگیری تقویتی در PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## سایر وظایف یادگیری تقویتی

یادگیری تقویتی امروزه یک حوزه پژوهشی با رشد سریع است. برخی از مثال‌های جالب یادگیری تقویتی عبارتند از:

* آموزش کامپیوتر برای بازی کردن **بازی‌های آتاری**. چالش اصلی در این مسئله این است که ما حالت ساده‌ای به صورت یک بردار نداریم، بلکه یک تصویر از صفحه داریم - و باید از CNN برای تبدیل این تصویر به یک بردار ویژگی یا استخراج اطلاعات پاداش استفاده کنیم. بازی‌های آتاری در Gym موجود هستند.
* آموزش کامپیوتر برای بازی کردن بازی‌های تخته‌ای، مانند شطرنج و Go. اخیراً برنامه‌های پیشرفته‌ای مانند **Alpha Zero** از ابتدا با بازی دو عامل در برابر یکدیگر آموزش داده شدند و در هر گام بهبود یافتند.
* در صنعت، یادگیری تقویتی برای ایجاد سیستم‌های کنترلی از شبیه‌سازی استفاده می‌شود. سرویسی به نام [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) به طور خاص برای این منظور طراحی شده است.

## نتیجه‌گیری

ما اکنون یاد گرفته‌ایم که چگونه عوامل را آموزش دهیم تا تنها با ارائه یک تابع پاداش که حالت مطلوب بازی را تعریف می‌کند و با دادن فرصت برای کاوش هوشمندانه فضای جستجو، به نتایج خوبی دست یابند. ما دو الگوریتم را با موفقیت امتحان کردیم و در مدت زمان نسبتاً کوتاهی به نتیجه خوبی رسیدیم. با این حال، این تنها آغاز سفر شما به دنیای یادگیری تقویتی است، و اگر می‌خواهید عمیق‌تر به این موضوع بپردازید، حتماً باید یک دوره جداگانه را در نظر بگیرید.

## 🚀 چالش

برنامه‌های ذکر شده در بخش "سایر وظایف یادگیری تقویتی" را بررسی کنید و سعی کنید یکی از آن‌ها را پیاده‌سازی کنید!

## [آزمون پس از درس](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## مرور و مطالعه شخصی

درباره یادگیری تقویتی کلاسیک در [برنامه درسی یادگیری ماشین برای مبتدیان](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) بیشتر بیاموزید.

[این ویدیوی عالی](https://www.youtube.com/watch?v=qv6UVOQ0F44) را تماشا کنید که درباره نحوه یادگیری کامپیوتر برای بازی کردن Super Mario صحبت می‌کند.

## تکلیف: [آموزش یک ماشین کوهستانی](lab/README.md)

هدف شما در این تکلیف آموزش یک محیط Gym متفاوت - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) - خواهد بود.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.