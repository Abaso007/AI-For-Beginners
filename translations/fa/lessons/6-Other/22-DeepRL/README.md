<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T12:26:31+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "fa"
}
-->
# یادگیری تقویتی عمیق

یادگیری تقویتی (RL) یکی از پارادایم‌های اصلی یادگیری ماشین محسوب می‌شود، در کنار یادگیری نظارت‌شده و یادگیری بدون نظارت. در حالی که در یادگیری نظارت‌شده به مجموعه داده‌هایی با نتایج مشخص تکیه می‌کنیم، RL بر اساس **یادگیری از طریق انجام دادن** است. برای مثال، وقتی برای اولین بار یک بازی کامپیوتری را می‌بینیم، شروع به بازی می‌کنیم، حتی بدون دانستن قوانین، و به زودی فقط با فرآیند بازی کردن و تنظیم رفتارمان مهارت‌های خود را بهبود می‌دهیم.

## [پیش‌زمینه: آزمون قبل از درس](https://ff-quizzes.netlify.app/en/ai/quiz/43)

برای انجام RL، به موارد زیر نیاز داریم:

* یک **محیط** یا **شبیه‌ساز** که قوانین بازی را تعیین کند. باید بتوانیم آزمایش‌ها را در شبیه‌ساز اجرا کنیم و نتایج را مشاهده کنیم.
* یک **تابع پاداش** که نشان دهد آزمایش ما چقدر موفق بوده است. در مورد یادگیری بازی کامپیوتری، پاداش می‌تواند امتیاز نهایی ما باشد.

بر اساس تابع پاداش، باید بتوانیم رفتار خود را تنظیم کنیم و مهارت‌های خود را بهبود دهیم تا دفعه بعد بهتر بازی کنیم. تفاوت اصلی بین RL و سایر انواع یادگیری ماشین این است که در RL معمولاً نمی‌دانیم که برنده شده‌ایم یا باخته‌ایم تا زمانی که بازی تمام شود. بنابراین نمی‌توانیم بگوییم که یک حرکت خاص به تنهایی خوب است یا نه - فقط در پایان بازی پاداش دریافت می‌کنیم.

در طول RL، معمولاً آزمایش‌های زیادی انجام می‌دهیم. در هر آزمایش، باید بین دنبال کردن استراتژی بهینه‌ای که تاکنون یاد گرفته‌ایم (**بهره‌برداری**) و کشف حالت‌های جدید ممکن (**اکتشاف**) تعادل برقرار کنیم.

## OpenAI Gym

ابزاری عالی برای RL، [OpenAI Gym](https://gym.openai.com/) است - یک **محیط شبیه‌سازی** که می‌تواند بسیاری از محیط‌های مختلف را شبیه‌سازی کند، از بازی‌های آتاری گرفته تا فیزیک پشت تعادل میله. این یکی از محبوب‌ترین محیط‌های شبیه‌سازی برای آموزش الگوریتم‌های یادگیری تقویتی است و توسط [OpenAI](https://openai.com/) نگهداری می‌شود.

> **Note**: می‌توانید تمام محیط‌های موجود در OpenAI Gym را [اینجا](https://gym.openai.com/envs/#classic_control) مشاهده کنید.

## تعادل CartPole

احتمالاً همه شما دستگاه‌های مدرن تعادل مانند *Segway* یا *Gyroscooters* را دیده‌اید. این دستگاه‌ها قادرند به طور خودکار با تنظیم چرخ‌های خود در پاسخ به سیگنال‌های شتاب‌سنج یا ژیروسکوپ تعادل برقرار کنند. در این بخش، یاد می‌گیریم که چگونه یک مشکل مشابه - تعادل یک میله - را حل کنیم. این شبیه به موقعیتی است که یک هنرمند سیرک باید یک میله را روی دست خود متعادل کند - اما این تعادل میله فقط در یک بعد اتفاق می‌افتد.

نسخه ساده‌شده‌ای از تعادل به عنوان مشکل **CartPole** شناخته می‌شود. در دنیای CartPole، ما یک لغزنده افقی داریم که می‌تواند به چپ یا راست حرکت کند، و هدف این است که یک میله عمودی را روی لغزنده در حال حرکت متعادل کنیم.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

برای ایجاد و استفاده از این محیط، به چند خط کد پایتون نیاز داریم:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

هر محیط دقیقاً به همین روش قابل دسترسی است:
* `env.reset` یک آزمایش جدید را شروع می‌کند
* `env.step` یک مرحله شبیه‌سازی را انجام می‌دهد. این تابع یک **عمل** از **فضای عمل** دریافت می‌کند و یک **مشاهده** (از فضای مشاهده)، به همراه پاداش و یک پرچم پایان بازمی‌گرداند.

در مثال بالا، در هر مرحله یک عمل تصادفی انجام می‌دهیم، به همین دلیل عمر آزمایش بسیار کوتاه است:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

هدف یک الگوریتم RL آموزش یک مدل - به اصطلاح **سیاست** &pi; - است که در پاسخ به یک حالت خاص، عمل مناسب را بازگرداند. همچنین می‌توان سیاست را به صورت احتمالی در نظر گرفت، به این صورت که برای هر حالت *s* و عمل *a*، احتمال &pi;(*a*|*s*) را که باید عمل *a* را در حالت *s* انجام دهیم، بازگرداند.

## الگوریتم گرادیان سیاست

واضح‌ترین روش برای مدل‌سازی سیاست، ایجاد یک شبکه عصبی است که حالت‌ها را به عنوان ورودی دریافت کند و اعمال مربوطه (یا بهتر بگوییم، احتمالات تمام اعمال) را بازگرداند. به نوعی، این مشابه یک وظیفه طبقه‌بندی معمولی است، با یک تفاوت عمده - ما از قبل نمی‌دانیم که در هر مرحله چه اعمالی باید انجام دهیم.

ایده اینجا این است که این احتمالات را تخمین بزنیم. ما یک بردار از **پاداش‌های تجمعی** می‌سازیم که پاداش کل ما را در هر مرحله از آزمایش نشان می‌دهد. همچنین **تخفیف پاداش** را با ضرب پاداش‌های اولیه در یک ضریب &gamma;=0.99 اعمال می‌کنیم تا نقش پاداش‌های اولیه کاهش یابد. سپس، آن مراحل در مسیر آزمایش را که پاداش‌های بیشتری به همراه دارند، تقویت می‌کنیم.

> درباره الگوریتم گرادیان سیاست بیشتر بیاموزید و آن را در عمل در [دفترچه مثال](CartPole-RL-TF.ipynb) مشاهده کنید.

## الگوریتم Actor-Critic

نسخه بهبود یافته رویکرد گرادیان سیاست به نام **Actor-Critic** شناخته می‌شود. ایده اصلی پشت آن این است که شبکه عصبی برای بازگرداندن دو چیز آموزش داده شود:

* سیاست، که تعیین می‌کند کدام عمل را انجام دهیم. این بخش **actor** نامیده می‌شود.
* تخمین پاداش کل که می‌توانیم انتظار داشته باشیم در این حالت دریافت کنیم - این بخش **critic** نامیده می‌شود.

به نوعی، این معماری شبیه به [GAN](../../4-ComputerVision/10-GANs/README.md) است، جایی که دو شبکه داریم که در برابر یکدیگر آموزش داده می‌شوند. در مدل Actor-Critic، actor عمل پیشنهادی را ارائه می‌دهد که باید انجام دهیم، و critic تلاش می‌کند انتقادی باشد و نتیجه را تخمین بزند. با این حال، هدف ما این است که این شبکه‌ها را به صورت هماهنگ آموزش دهیم.

از آنجا که هم پاداش‌های تجمعی واقعی و هم نتایج بازگردانده شده توسط critic را در طول آزمایش می‌دانیم، ساخت یک تابع زیان که تفاوت بین آنها را به حداقل برساند نسبتاً آسان است. این به ما **زیان critic** می‌دهد. می‌توانیم **زیان actor** را با استفاده از همان رویکرد در الگوریتم گرادیان سیاست محاسبه کنیم.

پس از اجرای یکی از این الگوریتم‌ها، می‌توانیم انتظار داشته باشیم که CartPole ما به این شکل رفتار کند:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ تمرین‌ها: گرادیان سیاست و RL Actor-Critic

یادگیری خود را در دفترچه‌های زیر ادامه دهید:

* [RL در TensorFlow](CartPole-RL-TF.ipynb)
* [RL در PyTorch](CartPole-RL-PyTorch.ipynb)

## وظایف دیگر RL

یادگیری تقویتی امروزه یک حوزه تحقیقاتی در حال رشد سریع است. برخی از مثال‌های جالب یادگیری تقویتی عبارتند از:

* آموزش کامپیوتر برای بازی‌های **آتاری**. بخش چالش‌برانگیز در این مشکل این است که ما حالت ساده‌ای به صورت یک بردار نداریم، بلکه یک تصویر از صفحه داریم - و باید از CNN برای تبدیل این تصویر صفحه به یک بردار ویژگی یا استخراج اطلاعات پاداش استفاده کنیم. بازی‌های آتاری در Gym موجود هستند.
* آموزش کامپیوتر برای بازی‌های تخته‌ای، مانند شطرنج و Go. اخیراً برنامه‌های پیشرفته‌ای مانند **Alpha Zero** از ابتدا توسط دو عامل که در برابر یکدیگر بازی می‌کنند و در هر مرحله بهبود می‌یابند، آموزش داده شده‌اند.
* در صنعت، RL برای ایجاد سیستم‌های کنترلی از شبیه‌سازی استفاده می‌شود. سرویسی به نام [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) به طور خاص برای این منظور طراحی شده است.

## نتیجه‌گیری

اکنون یاد گرفته‌ایم که چگونه عوامل را آموزش دهیم تا فقط با ارائه یک تابع پاداش که حالت مطلوب بازی را تعریف می‌کند و با دادن فرصت برای کشف هوشمندانه فضای جستجو، به نتایج خوبی دست یابند. ما دو الگوریتم را با موفقیت امتحان کردیم و در مدت زمان نسبتاً کوتاهی به نتیجه خوبی دست یافتیم. با این حال، این فقط آغاز سفر شما به RL است، و باید حتماً یک دوره جداگانه را در نظر بگیرید اگر می‌خواهید عمیق‌تر به این موضوع بپردازید.

## 🚀 چالش

برنامه‌های ذکر شده در بخش 'وظایف دیگر RL' را بررسی کنید و سعی کنید یکی را پیاده‌سازی کنید!

## [آزمون بعد از درس](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## مرور و مطالعه خودآموز

درباره یادگیری تقویتی کلاسیک بیشتر در [برنامه درسی یادگیری ماشین برای مبتدیان](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) ما بیاموزید.

[این ویدئوی عالی](https://www.youtube.com/watch?v=qv6UVOQ0F44) را تماشا کنید که درباره چگونگی یادگیری کامپیوتر برای بازی Super Mario صحبت می‌کند.

## تکلیف: [آموزش یک ماشین کوهستانی](lab/README.md)

هدف شما در این تکلیف آموزش یک محیط Gym متفاوت - [ماشین کوهستانی](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) خواهد بود.

---

