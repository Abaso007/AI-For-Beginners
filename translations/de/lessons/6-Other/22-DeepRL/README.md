<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-24T09:38:46+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "de"
}
-->
# Deep Reinforcement Learning

Reinforcement Learning (RL) wird als eines der grundlegenden Paradigmen des maschinellen Lernens angesehen, neben √ºberwachten und un√ºberwachten Lernen. W√§hrend wir beim √ºberwachten Lernen auf Datens√§tze mit bekannten Ergebnissen angewiesen sind, basiert RL auf dem Prinzip des **Lernens durch Handeln**. Zum Beispiel: Wenn wir ein Computerspiel zum ersten Mal sehen, beginnen wir zu spielen, auch ohne die Regeln zu kennen, und verbessern unsere F√§higkeiten allein durch das Spielen und Anpassen unseres Verhaltens.

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Um RL durchzuf√ºhren, ben√∂tigen wir:

* Eine **Umgebung** oder einen **Simulator**, der die Regeln des Spiels festlegt. Wir sollten in der Lage sein, Experimente im Simulator durchzuf√ºhren und die Ergebnisse zu beobachten.
* Eine **Belohnungsfunktion**, die angibt, wie erfolgreich unser Experiment war. Im Fall des Lernens, ein Computerspiel zu spielen, w√§re die Belohnung unsere Endpunktzahl.

Basierend auf der Belohnungsfunktion sollten wir unser Verhalten anpassen und unsere F√§higkeiten verbessern, sodass wir beim n√§chsten Mal besser spielen. Der Hauptunterschied zwischen anderen Arten des maschinellen Lernens und RL besteht darin, dass wir bei RL normalerweise nicht wissen, ob wir gewinnen oder verlieren, bis das Spiel beendet ist. Daher k√∂nnen wir nicht sagen, ob ein bestimmter Zug allein gut oder schlecht ist ‚Äì wir erhalten die Belohnung erst am Ende des Spiels.

W√§hrend des RL f√ºhren wir typischerweise viele Experimente durch. Bei jedem Experiment m√ºssen wir zwischen der Anwendung der optimalen Strategie, die wir bisher gelernt haben (**Ausnutzung**), und der Erforschung neuer m√∂glicher Zust√§nde (**Erkundung**) abw√§gen.

## OpenAI Gym

Ein gro√üartiges Werkzeug f√ºr RL ist das [OpenAI Gym](https://gym.openai.com/) ‚Äì eine **Simulationsumgebung**, die viele verschiedene Umgebungen simulieren kann, von Atari-Spielen bis hin zur Physik des Balancierens von Stangen. Es ist eine der beliebtesten Simulationsumgebungen f√ºr das Training von Reinforcement-Learning-Algorithmen und wird von [OpenAI](https://openai.com/) gepflegt.

> **Hinweis**: Sie k√∂nnen alle verf√ºgbaren Umgebungen von OpenAI Gym [hier](https://gym.openai.com/envs/#classic_control) sehen.

## CartPole-Balancieren

Ihr kennt wahrscheinlich moderne Balancierger√§te wie den *Segway* oder *Gyroscooter*. Sie k√∂nnen sich automatisch ausbalancieren, indem sie ihre R√§der entsprechend einem Signal von einem Beschleunigungsmesser oder Gyroskop anpassen. In diesem Abschnitt lernen wir, wie man ein √§hnliches Problem l√∂st ‚Äì das Balancieren einer Stange. Es √§hnelt der Situation, in der ein Zirkusk√ºnstler eine Stange auf seiner Hand balancieren muss ‚Äì aber dieses Balancieren erfolgt nur in einer Dimension.

Eine vereinfachte Version des Balancierens ist als **CartPole-Problem** bekannt. In der CartPole-Welt haben wir einen horizontalen Schlitten, der sich nach links oder rechts bewegen kann, und das Ziel ist es, eine vertikale Stange oben auf dem Schlitten zu balancieren, w√§hrend er sich bewegt.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Um diese Umgebung zu erstellen und zu nutzen, ben√∂tigen wir ein paar Zeilen Python-Code:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Jede Umgebung kann auf genau dieselbe Weise angesprochen werden:
* `env.reset` startet ein neues Experiment
* `env.step` f√ºhrt einen Simulationsschritt aus. Es erh√§lt eine **Aktion** aus dem **Aktionsraum** und gibt eine **Beobachtung** (aus dem Beobachtungsraum) sowie eine Belohnung und ein Abbruchflag zur√ºck.

Im obigen Beispiel f√ºhren wir bei jedem Schritt eine zuf√§llige Aktion aus, weshalb die Lebensdauer des Experiments sehr kurz ist:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Das Ziel eines RL-Algorithmus ist es, ein Modell zu trainieren ‚Äì die sogenannte **Policy** œÄ ‚Äì, die die Aktion als Antwort auf einen gegebenen Zustand zur√ºckgibt. Wir k√∂nnen die Policy auch als probabilistisch betrachten, z. B. f√ºr jeden Zustand *s* und jede Aktion *a* gibt sie die Wahrscheinlichkeit œÄ(*a*|*s*) zur√ºck, dass wir *a* im Zustand *s* ausf√ºhren sollten.

## Policy-Gradient-Algorithmus

Die offensichtlichste M√∂glichkeit, eine Policy zu modellieren, besteht darin, ein neuronales Netzwerk zu erstellen, das Zust√§nde als Eingabe nimmt und entsprechende Aktionen (oder vielmehr die Wahrscheinlichkeiten aller Aktionen) zur√ºckgibt. In gewisser Weise w√§re es √§hnlich wie eine normale Klassifikationsaufgabe, mit einem gro√üen Unterschied ‚Äì wir wissen im Voraus nicht, welche Aktionen wir bei jedem Schritt ausf√ºhren sollten.

Die Idee hier ist, diese Wahrscheinlichkeiten zu sch√§tzen. Wir erstellen einen Vektor von **kumulierten Belohnungen**, der unsere Gesamtbelohnung bei jedem Schritt des Experiments zeigt. Wir wenden auch **Belohnungsdiskontierung** an, indem wir fr√ºhere Belohnungen mit einem Koeffizienten Œ≥=0.99 multiplizieren, um die Rolle fr√ºherer Belohnungen zu verringern. Dann verst√§rken wir die Schritte entlang des Experimentpfads, die gr√∂√üere Belohnungen bringen.

> Erfahren Sie mehr √ºber den Policy-Gradient-Algorithmus und sehen Sie ihn in Aktion im [Beispiel-Notebook](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Actor-Critic-Algorithmus

Eine verbesserte Version des Policy-Gradient-Ansatzes wird **Actor-Critic** genannt. Die Hauptidee dahinter ist, dass das neuronale Netzwerk so trainiert wird, dass es zwei Dinge zur√ºckgibt:

* Die Policy, die bestimmt, welche Aktion ausgef√ºhrt werden soll. Dieser Teil wird **Actor** genannt.
* Die Sch√§tzung der Gesamtbelohnung, die wir in diesem Zustand erwarten k√∂nnen ‚Äì dieser Teil wird **Critic** genannt.

In gewisser Weise √§hnelt diese Architektur einem [GAN](../../4-ComputerVision/10-GANs/README.md), bei dem wir zwei Netzwerke haben, die gegeneinander trainiert werden. Im Actor-Critic-Modell schl√§gt der Actor die Aktion vor, die wir ausf√ºhren m√ºssen, und der Critic versucht kritisch zu sein und das Ergebnis zu sch√§tzen. Unser Ziel ist es jedoch, diese Netzwerke im Einklang zu trainieren.

Da wir sowohl die tats√§chlichen kumulierten Belohnungen als auch die vom Critic w√§hrend des Experiments zur√ºckgegebenen Ergebnisse kennen, ist es relativ einfach, eine Verlustfunktion zu erstellen, die die Differenz zwischen ihnen minimiert. Das w√ºrde uns den **Critic-Loss** geben. Wir k√∂nnen den **Actor-Loss** mit demselben Ansatz wie im Policy-Gradient-Algorithmus berechnen.

Nach dem Ausf√ºhren eines dieser Algorithmen k√∂nnen wir erwarten, dass unser CartPole sich wie folgt verh√§lt:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è √úbungen: Policy-Gradient und Actor-Critic RL

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [RL in TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL in PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Andere RL-Aufgaben

Reinforcement Learning ist heutzutage ein schnell wachsendes Forschungsfeld. Einige interessante Beispiele f√ºr Reinforcement Learning sind:

* Einem Computer beibringen, **Atari-Spiele** zu spielen. Die Herausforderung bei diesem Problem besteht darin, dass wir keinen einfachen Zustand als Vektor haben, sondern einen Screenshot ‚Äì und wir m√ºssen ein CNN verwenden, um dieses Bildschirmbild in einen Feature-Vektor umzuwandeln oder Belohnungsinformationen zu extrahieren. Atari-Spiele sind im Gym verf√ºgbar.
* Einem Computer beibringen, Brettspiele wie Schach und Go zu spielen. K√ºrzlich wurden Programme wie **Alpha Zero** von Grund auf durch zwei Agenten trainiert, die gegeneinander spielen und sich bei jedem Schritt verbessern.
* In der Industrie wird RL verwendet, um Steuerungssysteme aus Simulationen zu erstellen. Ein Dienst namens [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) ist speziell daf√ºr konzipiert.

## Fazit

Wir haben nun gelernt, wie man Agenten trainiert, um gute Ergebnisse zu erzielen, indem man ihnen lediglich eine Belohnungsfunktion bereitstellt, die den gew√ºnschten Zustand des Spiels definiert, und ihnen die M√∂glichkeit gibt, den Suchraum intelligent zu erkunden. Wir haben erfolgreich zwei Algorithmen ausprobiert und in relativ kurzer Zeit ein gutes Ergebnis erzielt. Dies ist jedoch nur der Anfang Ihrer Reise in RL, und Sie sollten definitiv einen separaten Kurs in Betracht ziehen, wenn Sie tiefer eintauchen m√∂chten.

## üöÄ Herausforderung

Erkunden Sie die Anwendungen, die im Abschnitt "Andere RL-Aufgaben" aufgef√ºhrt sind, und versuchen Sie, eine davon umzusetzen!

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## √úberpr√ºfung & Selbststudium

Erfahren Sie mehr √ºber klassisches Reinforcement Learning in unserem [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Sehen Sie sich [dieses gro√üartige Video](https://www.youtube.com/watch?v=qv6UVOQ0F44) an, das zeigt, wie ein Computer lernen kann, Super Mario zu spielen.

## Aufgabe: [Trainieren Sie ein Mountain Car](lab/README.md)

Ihr Ziel bei dieser Aufgabe ist es, eine andere Gym-Umgebung zu trainieren ‚Äì [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.