<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T12:16:09+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "de"
}
-->
# Deep Reinforcement Learning

Reinforcement Learning (RL) wird als eines der grundlegenden Paradigmen des maschinellen Lernens angesehen, neben √ºberwachten und un√ºberwachten Lernen. W√§hrend wir beim √ºberwachten Lernen auf Datens√§tze mit bekannten Ergebnissen angewiesen sind, basiert RL auf dem Prinzip des **Lernens durch Handeln**. Zum Beispiel: Wenn wir ein Computerspiel zum ersten Mal sehen, beginnen wir zu spielen, auch ohne die Regeln zu kennen, und verbessern unsere F√§higkeiten allein durch das Spielen und Anpassen unseres Verhaltens.

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Um RL durchzuf√ºhren, ben√∂tigen wir:

* Eine **Umgebung** oder einen **Simulator**, der die Regeln des Spiels festlegt. Wir sollten in der Lage sein, Experimente im Simulator durchzuf√ºhren und die Ergebnisse zu beobachten.
* Eine **Belohnungsfunktion**, die anzeigt, wie erfolgreich unser Experiment war. Im Fall des Lernens, ein Computerspiel zu spielen, w√§re die Belohnung unsere Endpunktzahl.

Basierend auf der Belohnungsfunktion sollten wir unser Verhalten anpassen und unsere F√§higkeiten verbessern, sodass wir beim n√§chsten Mal besser spielen. Der Hauptunterschied zwischen anderen Arten des maschinellen Lernens und RL besteht darin, dass wir bei RL normalerweise erst am Ende des Spiels wissen, ob wir gewonnen oder verloren haben. Daher k√∂nnen wir nicht sagen, ob ein bestimmter Zug allein gut oder schlecht ist ‚Äì wir erhalten die Belohnung erst am Ende des Spiels.

W√§hrend des RL f√ºhren wir typischerweise viele Experimente durch. Bei jedem Experiment m√ºssen wir zwischen der Anwendung der optimalen Strategie, die wir bisher gelernt haben (**Exploitation**), und der Erkundung neuer m√∂glicher Zust√§nde (**Exploration**) abw√§gen.

## OpenAI Gym

Ein gro√üartiges Werkzeug f√ºr RL ist das [OpenAI Gym](https://gym.openai.com/) ‚Äì eine **Simulationsumgebung**, die viele verschiedene Umgebungen simulieren kann, von Atari-Spielen bis hin zur Physik des Stangenbalancierens. Es ist eine der beliebtesten Simulationsumgebungen f√ºr das Training von Reinforcement-Learning-Algorithmen und wird von [OpenAI](https://openai.com/) gepflegt.

> **Note**: Alle verf√ºgbaren Umgebungen von OpenAI Gym k√∂nnen [hier](https://gym.openai.com/envs/#classic_control) eingesehen werden.

## CartPole Balancing

Ihr habt wahrscheinlich alle moderne Balancierger√§te wie den *Segway* oder *Gyroscooter* gesehen. Sie k√∂nnen sich automatisch ausbalancieren, indem sie ihre R√§der entsprechend einem Signal von einem Beschleunigungsmesser oder Gyroskop anpassen. In diesem Abschnitt lernen wir, ein √§hnliches Problem zu l√∂sen ‚Äì das Balancieren einer Stange. Es ist vergleichbar mit der Situation, in der ein Zirkusk√ºnstler eine Stange auf seiner Hand balancieren muss ‚Äì aber dieses Balancieren erfolgt nur in einer Dimension.

Eine vereinfachte Version des Balancierens ist als **CartPole**-Problem bekannt. In der CartPole-Welt haben wir einen horizontalen Schlitten, der sich nach links oder rechts bewegen kann, und das Ziel ist es, eine vertikale Stange oben auf dem Schlitten zu balancieren, w√§hrend er sich bewegt.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Um diese Umgebung zu erstellen und zu nutzen, ben√∂tigen wir ein paar Zeilen Python-Code:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Jede Umgebung kann auf die gleiche Weise angesprochen werden:
* `env.reset` startet ein neues Experiment
* `env.step` f√ºhrt einen Simulationsschritt aus. Es erh√§lt eine **Aktion** aus dem **Aktionsraum** und gibt eine **Beobachtung** (aus dem Beobachtungsraum) sowie eine Belohnung und ein Abbruchflag zur√ºck.

Im obigen Beispiel f√ºhren wir bei jedem Schritt eine zuf√§llige Aktion aus, weshalb die Lebensdauer des Experiments sehr kurz ist:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Das Ziel eines RL-Algorithmus ist es, ein Modell zu trainieren ‚Äì die sogenannte **Policy** &pi; ‚Äì, die die Aktion als Antwort auf einen gegebenen Zustand zur√ºckgibt. Wir k√∂nnen die Policy auch als probabilistisch betrachten, z. B. f√ºr jeden Zustand *s* und jede Aktion *a* gibt sie die Wahrscheinlichkeit &pi;(*a*|*s*) zur√ºck, dass wir *a* im Zustand *s* ausf√ºhren sollten.

## Policy-Gradient-Algorithmus

Die offensichtlichste M√∂glichkeit, eine Policy zu modellieren, besteht darin, ein neuronales Netzwerk zu erstellen, das Zust√§nde als Eingabe nimmt und entsprechende Aktionen (oder vielmehr die Wahrscheinlichkeiten aller Aktionen) zur√ºckgibt. In gewisser Weise w√§re es √§hnlich wie eine normale Klassifikationsaufgabe, mit einem wesentlichen Unterschied ‚Äì wir wissen im Voraus nicht, welche Aktionen wir bei jedem Schritt ausf√ºhren sollten.

Die Idee ist hier, diese Wahrscheinlichkeiten zu sch√§tzen. Wir erstellen einen Vektor von **kumulierten Belohnungen**, der unsere Gesamtbelohnung bei jedem Schritt des Experiments zeigt. Wir wenden auch **Belohnungsdiskontierung** an, indem wir fr√ºhere Belohnungen mit einem Koeffizienten &gamma;=0.99 multiplizieren, um die Rolle fr√ºherer Belohnungen zu verringern. Dann verst√§rken wir die Schritte entlang des Experimentpfads, die gr√∂√üere Belohnungen bringen.

> Erfahren Sie mehr √ºber den Policy-Gradient-Algorithmus und sehen Sie ihn in Aktion im [Beispiel-Notebook](CartPole-RL-TF.ipynb).

## Actor-Critic-Algorithmus

Eine verbesserte Version des Policy-Gradient-Ansatzes wird **Actor-Critic** genannt. Die Hauptidee dahinter ist, dass das neuronale Netzwerk so trainiert wird, dass es zwei Dinge zur√ºckgibt:

* Die Policy, die bestimmt, welche Aktion ausgef√ºhrt werden soll. Dieser Teil wird **Actor** genannt.
* Die Sch√§tzung der Gesamtbelohnung, die wir in diesem Zustand erwarten k√∂nnen ‚Äì dieser Teil wird **Critic** genannt.

In gewisser Weise √§hnelt diese Architektur einem [GAN](../../4-ComputerVision/10-GANs/README.md), bei dem wir zwei Netzwerke haben, die gegeneinander trainiert werden. Im Actor-Critic-Modell schl√§gt der Actor die Aktion vor, die wir ausf√ºhren m√ºssen, und der Critic versucht kritisch zu sein und das Ergebnis zu sch√§tzen. Unser Ziel ist jedoch, diese Netzwerke im Einklang zu trainieren.

Da wir sowohl die tats√§chlichen kumulierten Belohnungen als auch die vom Critic w√§hrend des Experiments zur√ºckgegebenen Ergebnisse kennen, ist es relativ einfach, eine Verlustfunktion zu erstellen, die die Differenz zwischen ihnen minimiert. Das ergibt den **Critic-Loss**. Den **Actor-Loss** k√∂nnen wir mit dem gleichen Ansatz wie beim Policy-Gradient-Algorithmus berechnen.

Nach dem Ausf√ºhren eines dieser Algorithmen k√∂nnen wir erwarten, dass unser CartPole sich so verh√§lt:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è √úbungen: Policy-Gradient und Actor-Critic RL

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [RL in TensorFlow](CartPole-RL-TF.ipynb)
* [RL in PyTorch](CartPole-RL-PyTorch.ipynb)

## Andere RL-Aufgaben

Reinforcement Learning ist heutzutage ein schnell wachsendes Forschungsfeld. Einige interessante Beispiele f√ºr Reinforcement Learning sind:

* Das Lehren eines Computers, **Atari-Spiele** zu spielen. Die Herausforderung bei diesem Problem besteht darin, dass wir keinen einfachen Zustand als Vektor haben, sondern einen Screenshot ‚Äì und wir m√ºssen CNNs verwenden, um dieses Bildschirmbild in einen Feature-Vektor umzuwandeln oder Belohnungsinformationen zu extrahieren. Atari-Spiele sind im Gym verf√ºgbar.
* Das Lehren eines Computers, Brettspiele wie Schach und Go zu spielen. K√ºrzlich wurden Programme wie **Alpha Zero** von Grund auf trainiert, indem zwei Agenten gegeneinander spielten und sich bei jedem Schritt verbesserten.
* In der Industrie wird RL verwendet, um Steuerungssysteme aus Simulationen zu erstellen. Ein Dienst namens [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) ist speziell daf√ºr konzipiert.

## Fazit

Wir haben nun gelernt, wie man Agenten trainiert, um gute Ergebnisse zu erzielen, indem man ihnen lediglich eine Belohnungsfunktion gibt, die den gew√ºnschten Zustand des Spiels definiert, und ihnen die M√∂glichkeit gibt, den Suchraum intelligent zu erkunden. Wir haben erfolgreich zwei Algorithmen ausprobiert und in relativ kurzer Zeit ein gutes Ergebnis erzielt. Dies ist jedoch nur der Anfang Ihrer Reise in RL, und Sie sollten definitiv einen separaten Kurs in Betracht ziehen, wenn Sie tiefer eintauchen m√∂chten.

## üöÄ Herausforderung

Erkunden Sie die Anwendungen, die im Abschnitt "Andere RL-Aufgaben" aufgef√ºhrt sind, und versuchen Sie, eine davon umzusetzen!

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## √úberpr√ºfung & Selbststudium

Erfahren Sie mehr √ºber klassisches Reinforcement Learning in unserem [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Sehen Sie sich [dieses gro√üartige Video](https://www.youtube.com/watch?v=qv6UVOQ0F44) an, das zeigt, wie ein Computer lernen kann, Super Mario zu spielen.

## Aufgabe: [Trainieren Sie ein Mountain Car](lab/README.md)

Ihr Ziel bei dieser Aufgabe ist es, eine andere Gym-Umgebung zu trainieren ‚Äì [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

