{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RL zur Balancierung des Cartpole\n",
    "\n",
    "Dieses Notebook ist Teil des [AI for Beginners Curriculum](http://aka.ms/ai-beginners). Es wurde inspiriert von [offiziellem PyTorch-Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) und [dieser Cartpole-PyTorch-Implementierung](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "In diesem Beispiel werden wir RL verwenden, um ein Modell zu trainieren, das eine Stange auf einem Wagen balancieren kann, der sich auf einer horizontalen Skala nach links und rechts bewegen kann. Wir nutzen die [OpenAI Gym](https://www.gymlibrary.ml/)-Umgebung, um die Stange zu simulieren.\n",
    "\n",
    "> **Hinweis**: Du kannst den Code dieser Lektion lokal ausführen (z. B. mit Visual Studio Code), wobei die Simulation in einem neuen Fenster geöffnet wird. Wenn du den Code online ausführst, musst du möglicherweise einige Anpassungen vornehmen, wie [hier](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) beschrieben.\n",
    "\n",
    "Wir beginnen damit, sicherzustellen, dass Gym installiert ist:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erstellen wir die CartPole-Umgebung und sehen uns an, wie man damit arbeitet. Eine Umgebung hat die folgenden Eigenschaften:\n",
    "\n",
    "* **Action space** ist die Menge der möglichen Aktionen, die wir bei jedem Schritt der Simulation ausführen können.\n",
    "* **Observation space** ist der Raum der Beobachtungen, die wir machen können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns an, wie die Simulation funktioniert. Die folgende Schleife führt die Simulation aus, bis `env.step` das Abbruchsignal `done` zurückgibt. Wir werden Aktionen zufällig mit `env.action_space.sample()` auswählen, was bedeutet, dass das Experiment wahrscheinlich sehr schnell scheitern wird (die CartPole-Umgebung endet, wenn die Geschwindigkeit des CartPole, seine Position oder sein Winkel bestimmte Grenzen überschreiten).\n",
    "\n",
    "> Die Simulation wird in einem neuen Fenster geöffnet. Sie können den Code mehrmals ausführen und beobachten, wie er sich verhält.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kannst feststellen, dass die Beobachtungen aus 4 Zahlen bestehen. Diese sind:  \n",
    "- Position des Wagens  \n",
    "- Geschwindigkeit des Wagens  \n",
    "- Winkel der Stange  \n",
    "- Rotationsgeschwindigkeit der Stange  \n",
    "\n",
    "`rew` ist die Belohnung, die wir bei jedem Schritt erhalten. Im CartPole-Umfeld erhältst du für jeden Simulationsschritt 1 Punkt als Belohnung, und das Ziel ist es, die Gesamtbelohnung zu maximieren, d.h. die Zeit, in der CartPole das Gleichgewicht halten kann, ohne umzufallen.\n",
    "\n",
    "Während des Reinforcement Learnings ist es unser Ziel, eine **Policy** $\\pi$ zu trainieren, die uns für jeden Zustand $s$ sagt, welche Aktion $a$ wir ausführen sollen, also im Wesentlichen $a = \\pi(s)$.\n",
    "\n",
    "Wenn du eine probabilistische Lösung möchtest, kannst du dir die Policy so vorstellen, dass sie eine Menge von Wahrscheinlichkeiten für jede Aktion zurückgibt, d.h. $\\pi(a|s)$ würde die Wahrscheinlichkeit bedeuten, dass wir die Aktion $a$ im Zustand $s$ ausführen sollten.\n",
    "\n",
    "## Policy-Gradient-Methode\n",
    "\n",
    "Im einfachsten RL-Algorithmus, der **Policy Gradient** genannt wird, trainieren wir ein neuronales Netzwerk, um die nächste Aktion vorherzusagen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden das Netzwerk trainieren, indem wir viele Experimente durchführen und unser Netzwerk nach jedem Durchlauf aktualisieren. Lassen Sie uns eine Funktion definieren, die das Experiment ausführt und die Ergebnisse zurückgibt (sogenannte **Spur**) - alle Zustände, Aktionen (und ihre empfohlenen Wahrscheinlichkeiten) und Belohnungen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können eine Episode mit einem untrainierten Netzwerk ausführen und beobachten, dass die Gesamtbelohnung (auch bekannt als Episodenlänge) sehr niedrig ist:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einer der kniffligen Aspekte des Policy-Gradient-Algorithmus ist die Verwendung von **abgezinsten Belohnungen**. Die Idee ist, dass wir den Vektor der Gesamtbelohnungen bei jedem Schritt des Spiels berechnen und dabei die frühen Belohnungen mit einem Koeffizienten $gamma$ abdiskontieren. Wir normalisieren auch den resultierenden Vektor, da wir ihn als Gewicht verwenden werden, um unser Training zu beeinflussen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt geht's ans eigentliche Training! Wir werden 300 Episoden durchführen, und in jeder Episode werden wir Folgendes tun:\n",
    "\n",
    "1. Das Experiment ausführen und die Spur aufzeichnen.\n",
    "2. Die Differenz (`gradients`) zwischen den ausgeführten Aktionen und den vorhergesagten Wahrscheinlichkeiten berechnen. Je geringer die Differenz, desto sicherer können wir sein, dass wir die richtige Aktion gewählt haben.\n",
    "3. Diskontierte Belohnungen berechnen und die Gradienten mit den diskontierten Belohnungen multiplizieren – das stellt sicher, dass Schritte mit höheren Belohnungen einen größeren Einfluss auf das Endergebnis haben als solche mit niedrigeren Belohnungen.\n",
    "4. Die erwarteten Zielaktionen für unser neuronales Netzwerk werden teilweise aus den vorhergesagten Wahrscheinlichkeiten während des Laufs und teilweise aus den berechneten Gradienten abgeleitet. Wir verwenden den Parameter `alpha`, um zu bestimmen, in welchem Maße Gradienten und Belohnungen berücksichtigt werden – dies wird als *Lernrate* des Verstärkungsalgorithmus bezeichnet.\n",
    "5. Schließlich trainieren wir unser Netzwerk mit den Zuständen und den erwarteten Aktionen und wiederholen den Prozess.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lassen Sie uns nun die Episode mit Rendering ausführen, um das Ergebnis zu sehen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoffentlich kannst du sehen, dass der Stab jetzt ziemlich gut balancieren kann!\n",
    "\n",
    "## Actor-Critic-Modell\n",
    "\n",
    "Das Actor-Critic-Modell ist eine Weiterentwicklung der Policy-Gradient-Methoden, bei der wir ein neuronales Netzwerk erstellen, das sowohl die Policy als auch die geschätzten Belohnungen lernt. Das Netzwerk wird zwei Ausgaben haben (oder man kann es als zwei separate Netzwerke betrachten):\n",
    "* **Actor** wird die Aktion empfehlen, die ausgeführt werden soll, indem es uns die Zustandswahrscheinlichkeitsverteilung gibt, wie im Policy-Gradient-Modell.\n",
    "* **Critic** würde schätzen, wie die Belohnung aus diesen Aktionen aussehen könnte. Es gibt die insgesamt geschätzten zukünftigen Belohnungen im gegebenen Zustand zurück.\n",
    "\n",
    "Lass uns ein solches Modell definieren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir müssten unsere Funktionen `discounted_rewards` und `run_episode` leicht modifizieren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir die Haupttrainingsschleife ausführen. Wir verwenden den manuellen Netzwerk-Trainingsprozess, indem wir geeignete Verlustfunktionen berechnen und Netzwerkparameter aktualisieren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erkenntnisse\n",
    "\n",
    "Wir haben in dieser Demo zwei RL-Algorithmen kennengelernt: den einfachen Policy-Gradient-Algorithmus und den anspruchsvolleren Actor-Critic-Algorithmus. Sie können sehen, dass diese Algorithmen mit abstrakten Konzepten wie Zustand, Aktion und Belohnung arbeiten – daher können sie auf sehr unterschiedliche Umgebungen angewendet werden.\n",
    "\n",
    "Reinforcement Learning ermöglicht es uns, die beste Strategie zur Lösung eines Problems allein durch die Betrachtung der endgültigen Belohnung zu erlernen. Die Tatsache, dass wir keine gelabelten Datensätze benötigen, erlaubt es uns, Simulationen viele Male zu wiederholen, um unsere Modelle zu optimieren. Dennoch gibt es immer noch viele Herausforderungen im Bereich RL, die Sie kennenlernen können, wenn Sie sich entscheiden, sich intensiver mit diesem faszinierenden Bereich der KI zu beschäftigen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-31T15:44:40+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}