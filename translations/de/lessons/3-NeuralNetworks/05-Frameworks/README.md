<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-24T09:40:17+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "de"
}
-->
# Neuronale Netzwerk-Frameworks

Wie wir bereits gelernt haben, m√ºssen wir zwei Dinge tun, um neuronale Netzwerke effizient trainieren zu k√∂nnen:

* Mit Tensoren arbeiten, z. B. multiplizieren, addieren und Funktionen wie Sigmoid oder Softmax berechnen
* Gradienten aller Ausdr√ºcke berechnen, um die Gradientenabstiegsoptimierung durchzuf√ºhren

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/9)

W√§hrend die Bibliothek `numpy` den ersten Teil √ºbernehmen kann, ben√∂tigen wir einen Mechanismus, um Gradienten zu berechnen. In [unserem Framework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), das wir im vorherigen Abschnitt entwickelt haben, mussten wir alle Ableitungsfunktionen manuell im `backward`-Methodenprogrammieren, die die R√ºckw√§rtspropagation durchf√ºhrt. Idealerweise sollte ein Framework uns die M√∂glichkeit geben, Gradienten f√ºr *jeden Ausdruck* zu berechnen, den wir definieren k√∂nnen.

Ein weiterer wichtiger Punkt ist die M√∂glichkeit, Berechnungen auf der GPU oder anderen spezialisierten Recheneinheiten wie [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) durchzuf√ºhren. Das Training tiefer neuronaler Netzwerke erfordert *sehr viele* Berechnungen, und die M√∂glichkeit, diese Berechnungen auf GPUs zu parallelisieren, ist von gro√üer Bedeutung.

> ‚úÖ Der Begriff 'parallelisieren' bedeutet, die Berechnungen auf mehrere Ger√§te zu verteilen.

Derzeit sind die beiden beliebtesten neuronalen Frameworks: [TensorFlow](http://TensorFlow.org) und [PyTorch](https://pytorch.org/). Beide bieten eine Low-Level-API, um mit Tensoren sowohl auf der CPU als auch auf der GPU zu arbeiten. Zus√§tzlich zur Low-Level-API gibt es auch eine High-Level-API, die entsprechend [Keras](https://keras.io/) und [PyTorch Lightning](https://pytorchlightning.ai/) genannt wird.

Low-Level API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
High-Level API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Low-Level-APIs** in beiden Frameworks erm√∂glichen es, sogenannte **Rechengraphen** zu erstellen. Dieser Graph definiert, wie die Ausgabe (normalerweise die Verlustfunktion) mit gegebenen Eingabeparametern berechnet wird, und kann zur Berechnung auf die GPU √ºbertragen werden, falls verf√ºgbar. Es gibt Funktionen, um diesen Rechengraphen zu differenzieren und Gradienten zu berechnen, die dann zur Optimierung der Modellparameter verwendet werden k√∂nnen.

**High-Level-APIs** betrachten neuronale Netzwerke im Wesentlichen als eine **Abfolge von Schichten** und erleichtern den Aufbau der meisten neuronalen Netzwerke erheblich. Das Training des Modells erfordert in der Regel die Vorbereitung der Daten und dann das Aufrufen einer `fit`-Funktion, um die Arbeit zu erledigen.

Die High-Level-API erm√∂glicht es, typische neuronale Netzwerke sehr schnell zu erstellen, ohne sich um viele Details k√ºmmern zu m√ºssen. Gleichzeitig bieten Low-Level-APIs viel mehr Kontrolle √ºber den Trainingsprozess und werden daher h√§ufig in der Forschung verwendet, wenn man mit neuen Architekturen neuronaler Netzwerke arbeitet.

Es ist auch wichtig zu verstehen, dass beide APIs zusammen verwendet werden k√∂nnen, z. B. kann man eine eigene Netzwerkschichtarchitektur mit der Low-Level-API entwickeln und diese dann in einem gr√∂√üeren Netzwerk verwenden, das mit der High-Level-API erstellt und trainiert wurde. Oder man definiert ein Netzwerk mit der High-Level-API als Abfolge von Schichten und verwendet dann eine eigene Low-Level-Trainingsschleife zur Optimierung. Beide APIs basieren auf denselben grundlegenden Konzepten und sind so konzipiert, dass sie gut zusammenarbeiten.

## Lernen

In diesem Kurs bieten wir die meisten Inhalte sowohl f√ºr PyTorch als auch f√ºr TensorFlow an. Sie k√∂nnen Ihr bevorzugtes Framework ausw√§hlen und nur die entsprechenden Notebooks durchgehen. Wenn Sie sich nicht sicher sind, welches Framework Sie w√§hlen sollen, lesen Sie einige Diskussionen im Internet √ºber **PyTorch vs. TensorFlow**. Sie k√∂nnen sich auch beide Frameworks ansehen, um ein besseres Verst√§ndnis zu bekommen.

Wo m√∂glich, verwenden wir High-Level-APIs der Einfachheit halber. Wir glauben jedoch, dass es wichtig ist, zu verstehen, wie neuronale Netzwerke von Grund auf funktionieren. Daher beginnen wir zun√§chst mit der Arbeit mit Low-Level-APIs und Tensoren. Wenn Sie jedoch schnell loslegen m√∂chten und nicht viel Zeit mit diesen Details verbringen m√∂chten, k√∂nnen Sie diese √ºberspringen und direkt zu den High-Level-API-Notebooks wechseln.

## ‚úçÔ∏è √úbungen: Frameworks

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

Low-Level API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
High-Level API| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Nachdem Sie die Frameworks gemeistert haben, lassen Sie uns das Konzept des Overfittings rekapitulieren.

# Overfitting

Overfitting ist ein √§u√üerst wichtiges Konzept im maschinellen Lernen, und es ist sehr wichtig, es richtig zu verstehen!

Betrachten Sie das folgende Problem der Ann√§herung an 5 Punkte (dargestellt durch `x` in den untenstehenden Diagrammen):

![linear](../../../../../lessons/3-NeuralNetworks/images/overfit1.jpg) | ![overfit](../../../../../lessons/3-NeuralNetworks/images/overfit2.jpg)
-------------------------|--------------------------
**Lineares Modell, 2 Parameter** | **Nicht-lineares Modell, 7 Parameter**
Trainingsfehler = 5.3 | Trainingsfehler = 0
Validierungsfehler = 5.1 | Validierungsfehler = 20

* Links sehen wir eine gute lineare Ann√§herung. Da die Anzahl der Parameter angemessen ist, erfasst das Modell die Verteilung der Punkte korrekt.
* Rechts ist das Modell zu m√§chtig. Da wir nur 5 Punkte haben und das Modell 7 Parameter hat, kann es sich so anpassen, dass es durch alle Punkte verl√§uft, wodurch der Trainingsfehler 0 wird. Dies verhindert jedoch, dass das Modell das richtige Muster hinter den Daten versteht, was zu einem sehr hohen Validierungsfehler f√ºhrt.

Es ist sehr wichtig, ein korrektes Gleichgewicht zwischen der Komplexit√§t des Modells (Anzahl der Parameter) und der Anzahl der Trainingsbeispiele zu finden.

## Warum Overfitting auftritt

  * Zu wenig Trainingsdaten
  * Zu m√§chtiges Modell
  * Zu viel Rauschen in den Eingabedaten

## Wie man Overfitting erkennt

Wie Sie aus dem obigen Diagramm sehen k√∂nnen, kann Overfitting durch einen sehr niedrigen Trainingsfehler und einen hohen Validierungsfehler erkannt werden. Normalerweise sehen wir w√§hrend des Trainings, dass sowohl der Trainings- als auch der Validierungsfehler zun√§chst abnehmen. An einem bestimmten Punkt k√∂nnte der Validierungsfehler jedoch aufh√∂ren zu sinken und anfangen zu steigen. Dies ist ein Zeichen f√ºr Overfitting und ein Hinweis darauf, dass wir das Training an diesem Punkt wahrscheinlich stoppen sollten (oder zumindest einen Schnappschuss des Modells machen sollten).

![overfitting](../../../../../lessons/3-NeuralNetworks/images/Overfitting.png)

## Wie man Overfitting verhindert

Wenn Sie feststellen, dass Overfitting auftritt, k√∂nnen Sie Folgendes tun:

 * Die Menge der Trainingsdaten erh√∂hen
 * Die Komplexit√§t des Modells verringern
 * Eine [Regularisierungstechnik](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md) verwenden, wie z. B. [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), die wir sp√§ter betrachten werden.

## Overfitting und Bias-Varianz-Abw√§gung

Overfitting ist tats√§chlich ein Fall eines allgemeineren Problems in der Statistik, das als [Bias-Varianz-Abw√§gung](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) bezeichnet wird. Wenn wir die m√∂glichen Fehlerquellen in unserem Modell betrachten, k√∂nnen wir zwei Arten von Fehlern erkennen:

* **Bias-Fehler** entstehen dadurch, dass unser Algorithmus die Beziehung zwischen den Trainingsdaten nicht korrekt erfassen kann. Dies kann darauf zur√ºckzuf√ºhren sein, dass unser Modell nicht m√§chtig genug ist (**Underfitting**).
* **Varianz-Fehler**, die dadurch entstehen, dass das Modell Rauschen in den Eingabedaten anstelle einer sinnvollen Beziehung approximiert (**Overfitting**).

W√§hrend des Trainings nimmt der Bias-Fehler ab (da unser Modell lernt, die Daten zu approximieren), und der Varianz-Fehler nimmt zu. Es ist wichtig, das Training zu stoppen - entweder manuell (wenn wir Overfitting erkennen) oder automatisch (durch Einf√ºhrung von Regularisierung) -, um Overfitting zu verhindern.

## Fazit

In dieser Lektion haben Sie die Unterschiede zwischen den verschiedenen APIs der beiden beliebtesten KI-Frameworks, TensorFlow und PyTorch, kennengelernt. Au√üerdem haben Sie ein sehr wichtiges Thema, Overfitting, behandelt.

## üöÄ Herausforderung

In den begleitenden Notebooks finden Sie 'Aufgaben' am Ende; arbeiten Sie die Notebooks durch und l√∂sen Sie die Aufgaben.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## √úberpr√ºfung & Selbststudium

Recherchieren Sie zu den folgenden Themen:

- TensorFlow
- PyTorch
- Overfitting

Stellen Sie sich die folgenden Fragen:

- Was ist der Unterschied zwischen TensorFlow und PyTorch?
- Was ist der Unterschied zwischen Overfitting und Underfitting?

## [Aufgabe](lab/README.md)

In diesem Labor sollen Sie zwei Klassifikationsprobleme mit ein- und mehrschichtigen vollst√§ndig verbundenen Netzwerken mithilfe von PyTorch oder TensorFlow l√∂sen.

* [Anleitung](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.