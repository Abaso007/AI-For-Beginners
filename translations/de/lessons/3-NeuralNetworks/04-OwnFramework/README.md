<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-24T09:39:49+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "de"
}
-->
# EinfÃ¼hrung in Neuronale Netze. Mehrschichtiger Perzeptron

Im vorherigen Abschnitt hast du das einfachste Modell eines neuronalen Netzes kennengelernt â€“ den einlagigen Perzeptron, ein lineares Klassifikationsmodell fÃ¼r zwei Klassen.

In diesem Abschnitt erweitern wir dieses Modell zu einem flexibleren Framework, das uns ermÃ¶glicht:

* **Mehrklassenklassifikation** zusÃ¤tzlich zur Zweiklassenklassifikation durchzufÃ¼hren
* **Regressionsprobleme** zusÃ¤tzlich zur Klassifikation zu lÃ¶sen
* Klassen zu trennen, die nicht linear separierbar sind

Wir werden auÃŸerdem unser eigenes modulares Framework in Python entwickeln, mit dem wir verschiedene Architekturen neuronaler Netze erstellen kÃ¶nnen.

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalisierung des maschinellen Lernens

Beginnen wir mit der Formalisierung des Problems des maschinellen Lernens. Angenommen, wir haben einen Trainingsdatensatz **X** mit Labels **Y**, und wir mÃ¼ssen ein Modell *f* erstellen, das mÃ¶glichst genaue Vorhersagen trifft. Die QualitÃ¤t der Vorhersagen wird durch die **Verlustfunktion** â„’ gemessen. Die folgenden Verlustfunktionen werden hÃ¤ufig verwendet:

* FÃ¼r Regressionsprobleme, bei denen wir eine Zahl vorhersagen mÃ¼ssen, kÃ¶nnen wir den **absoluten Fehler** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| oder den **quadratischen Fehler** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> verwenden.
* FÃ¼r Klassifikationen verwenden wir den **0-1-Verlust** (der im Wesentlichen der **Genauigkeit** des Modells entspricht) oder den **logistischen Verlust**.

FÃ¼r den einlagigen Perzeptron wurde die Funktion *f* als lineare Funktion definiert: *f(x)=wx+b* (hierbei ist *w* die Gewichtsmatrix, *x* der Vektor der Eingabefeatures und *b* der Bias-Vektor). FÃ¼r verschiedene Architekturen neuronaler Netze kann diese Funktion eine komplexere Form annehmen.

> Im Fall der Klassifikation ist es oft wÃ¼nschenswert, Wahrscheinlichkeiten der entsprechenden Klassen als Ausgabe des Netzes zu erhalten. Um beliebige Zahlen in Wahrscheinlichkeiten umzuwandeln (z. B. um die Ausgabe zu normalisieren), verwenden wir oft die **Softmax-Funktion** Ïƒ, und die Funktion *f* wird zu *f(x)=Ïƒ(wx+b)*.

In der obigen Definition von *f* werden *w* und *b* als **Parameter** Î¸=âŸ¨*w,b*âŸ© bezeichnet. Gegeben den Datensatz âŸ¨**X**,**Y**âŸ©, kÃ¶nnen wir den Gesamterror fÃ¼r den gesamten Datensatz als Funktion der Parameter Î¸ berechnen.

> âœ… **Das Ziel des Trainings eines neuronalen Netzes ist es, den Fehler durch Variation der Parameter Î¸ zu minimieren.**

## Gradient-Descent-Optimierung

Es gibt eine bekannte Methode zur Optimierung von Funktionen, die als **Gradient Descent** bezeichnet wird. Die Idee ist, dass wir eine Ableitung (im mehrdimensionalen Fall als **Gradient** bezeichnet) der Verlustfunktion in Bezug auf die Parameter berechnen und die Parameter so variieren kÃ¶nnen, dass der Fehler abnimmt. Dies kann wie folgt formalisiert werden:

* Initialisiere die Parameter mit zufÃ¤lligen Werten w<sup>(0)</sup>, b<sup>(0)</sup>
* Wiederhole die folgenden Schritte viele Male:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

WÃ¤hrend des Trainings sollten die Optimierungsschritte unter BerÃ¼cksichtigung des gesamten Datensatzes berechnet werden (denk daran, dass der Verlust als Summe Ã¼ber alle Trainingsbeispiele berechnet wird). In der Praxis nehmen wir jedoch kleine Teile des Datensatzes, sogenannte **Minibatches**, und berechnen die Gradienten basierend auf einem Teil der Daten. Da der Teil jedes Mal zufÃ¤llig ausgewÃ¤hlt wird, wird diese Methode als **stochastischer Gradient Descent** (SGD) bezeichnet.

## Mehrschichtige Perzeptrons und Backpropagation

Ein einlagiges Netzwerk, wie wir oben gesehen haben, ist in der Lage, linear separierbare Klassen zu klassifizieren. Um ein komplexeres Modell zu erstellen, kÃ¶nnen wir mehrere Schichten des Netzwerks kombinieren. Mathematisch bedeutet dies, dass die Funktion *f* eine komplexere Form annimmt und in mehreren Schritten berechnet wird:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Hierbei ist Î± eine **nichtlineare Aktivierungsfunktion**, Ïƒ eine Softmax-Funktion, und die Parameter Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Der Gradient-Descent-Algorithmus bleibt derselbe, aber die Berechnung der Gradienten wird schwieriger. Mithilfe der Kettenregel der Differentiation kÃ¶nnen wir die Ableitungen wie folgt berechnen:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Die Kettenregel der Differentiation wird verwendet, um die Ableitungen der Verlustfunktion in Bezug auf die Parameter zu berechnen.

Beachte, dass der linke Teil all dieser AusdrÃ¼cke identisch ist, und daher kÃ¶nnen wir die Ableitungen effizient berechnen, indem wir von der Verlustfunktion ausgehend "rÃ¼ckwÃ¤rts" durch den Berechnungsgraphen gehen. Daher wird die Methode zum Trainieren eines mehrschichtigen Perzeptrons als **Backpropagation** oder 'Backprop' bezeichnet.

<img alt="Berechnungsgraph" src="images/ComputeGraphGrad.png"/>

> TODO: Bildnachweis

> âœ… Wir werden Backpropagation in unserem Notebook-Beispiel noch viel detaillierter behandeln.

## Fazit

In dieser Lektion haben wir unsere eigene Bibliothek fÃ¼r neuronale Netze erstellt und sie fÃ¼r eine einfache zweidimensionale Klassifikationsaufgabe verwendet.

## ğŸš€ Herausforderung

Im begleitenden Notebook wirst du dein eigenes Framework zur Erstellung und zum Training mehrschichtiger Perzeptrons implementieren. Du wirst im Detail sehen, wie moderne neuronale Netze funktionieren.

Gehe zum [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) Notebook und arbeite es durch.

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## RÃ¼ckblick & Selbststudium

Backpropagation ist ein gÃ¤ngiger Algorithmus in KI und ML, der es wert ist, [im Detail](https://wikipedia.org/wiki/Backpropagation) studiert zu werden.

## [Aufgabe](lab/README.md)

In diesem Labor wirst du das Framework, das du in dieser Lektion erstellt hast, verwenden, um die Klassifikation handgeschriebener Ziffern aus dem MNIST-Datensatz zu lÃ¶sen.

* [Anweisungen](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Ãœbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) Ã¼bersetzt. Obwohl wir uns um Genauigkeit bemÃ¼hen, beachten Sie bitte, dass automatisierte Ãœbersetzungen Fehler oder Ungenauigkeiten enthalten kÃ¶nnen. Das Originaldokument in seiner ursprÃ¼nglichen Sprache sollte als maÃŸgebliche Quelle betrachtet werden. FÃ¼r kritische Informationen wird eine professionelle menschliche Ãœbersetzung empfohlen. Wir Ã¼bernehmen keine Haftung fÃ¼r MissverstÃ¤ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Ãœbersetzung ergeben.