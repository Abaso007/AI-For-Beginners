<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-24T09:34:36+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "de"
}
-->
# Generative Adversarial Networks

Im vorherigen Abschnitt haben wir √ºber **generative Modelle** gelernt: Modelle, die neue Bilder erzeugen k√∂nnen, die den Bildern im Trainingsdatensatz √§hneln. VAE war ein gutes Beispiel f√ºr ein generatives Modell.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Wenn wir jedoch versuchen, etwas wirklich Bedeutungsvolles zu generieren, wie ein Gem√§lde in vern√ºnftiger Aufl√∂sung, werden wir feststellen, dass das Training mit VAE nicht gut konvergiert. F√ºr diesen Anwendungsfall sollten wir eine andere Architektur kennenlernen, die speziell auf generative Modelle ausgerichtet ist - **Generative Adversarial Networks**, oder GANs.

Die Hauptidee eines GANs besteht darin, zwei neuronale Netzwerke zu haben, die gegeneinander trainiert werden:

<img src="images/gan_architecture.png" width="70%"/>

> Bild von [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Ein bisschen Vokabular:
> * **Generator** ist ein Netzwerk, das einen zuf√§lligen Vektor nimmt und daraus ein Bild erzeugt.
> * **Discriminator** ist ein Netzwerk, das ein Bild nimmt und entscheiden soll, ob es sich um ein echtes Bild (aus dem Trainingsdatensatz) oder ein vom Generator erzeugtes Bild handelt. Es ist im Wesentlichen ein Bildklassifikator.

### Discriminator

Die Architektur des Discriminators unterscheidet sich nicht von einem gew√∂hnlichen Bildklassifikationsnetzwerk. Im einfachsten Fall kann es ein vollst√§ndig verbundenes Klassifikationsnetzwerk sein, aber h√∂chstwahrscheinlich wird es ein [Convolutional Network](../07-ConvNets/README.md) sein.

> ‚úÖ Ein GAN, das auf Convolutional Networks basiert, wird als [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) bezeichnet.

Ein CNN-Discriminator besteht aus den folgenden Schichten: mehreren Convolutions+Poolings (mit abnehmender r√§umlicher Gr√∂√üe) und einer oder mehreren vollst√§ndig verbundenen Schichten, um einen "Feature-Vektor" zu erhalten, sowie einem abschlie√üenden bin√§ren Klassifikator.

> ‚úÖ Ein 'Pooling' in diesem Kontext ist eine Technik, die die Gr√∂√üe des Bildes reduziert. "Pooling-Schichten reduzieren die Dimensionen der Daten, indem sie die Ausgaben von Neuronenclustern in einer Schicht zu einem einzelnen Neuron in der n√§chsten Schicht kombinieren." - [Quelle](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

Ein Generator ist etwas komplizierter. Man kann ihn als umgekehrten Discriminator betrachten. Ausgehend von einem latenten Vektor (anstelle eines Feature-Vektors) hat er eine vollst√§ndig verbundene Schicht, um ihn in die erforderliche Gr√∂√üe/Form umzuwandeln, gefolgt von Deconvolutions+Upscaling. Dies √§hnelt dem *Decoder*-Teil eines [Autoencoders](../09-Autoencoders/README.md).

> ‚úÖ Da die Convolution-Schicht als linearer Filter implementiert ist, der das Bild durchl√§uft, ist Deconvolution im Wesentlichen √§hnlich wie Convolution und kann mit derselben Schichtlogik implementiert werden.

<img src="images/gan_arch_detail.png" width="70%"/>

> Bild von [Dmitry Soshnikov](http://soshnikov.com)

### Training des GANs

GANs werden als **adversarial** bezeichnet, weil es einen st√§ndigen Wettbewerb zwischen dem Generator und dem Discriminator gibt. W√§hrend dieses Wettbewerbs verbessern sich sowohl der Generator als auch der Discriminator, sodass das Netzwerk lernt, immer bessere Bilder zu erzeugen.

Das Training erfolgt in zwei Phasen:

* **Training des Discriminators**. Diese Aufgabe ist ziemlich einfach: Wir generieren eine Charge von Bildern mit dem Generator, kennzeichnen sie mit 0 (was f√ºr ein gef√§lschtes Bild steht), und nehmen eine Charge von Bildern aus dem Eingabedatensatz (mit dem Label 1, echtes Bild). Wir erhalten einen *Discriminator Loss* und f√ºhren Backpropagation durch.
* **Training des Generators**. Dies ist etwas komplizierter, da wir die erwartete Ausgabe f√ºr den Generator nicht direkt kennen. Wir nehmen das gesamte GAN-Netzwerk, das aus einem Generator und einem Discriminator besteht, f√ºttern es mit zuf√§lligen Vektoren und erwarten, dass das Ergebnis 1 ist (entsprechend echten Bildern). Dann frieren wir die Parameter des Discriminators ein (wir wollen ihn in diesem Schritt nicht trainieren) und f√ºhren Backpropagation durch.

W√§hrend dieses Prozesses sinken die Verluste des Generators und des Discriminators nicht signifikant. Im Idealfall sollten sie oszillieren, was darauf hinweist, dass beide Netzwerke ihre Leistung verbessern.

## ‚úçÔ∏è √úbungen: GANs

* [GAN Notebook in TensorFlow/Keras](../../../../../lessons/4-ComputerVision/10-GANs/GANTF.ipynb)
* [GAN Notebook in PyTorch](../../../../../lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb)

### Probleme beim Training von GANs

GANs sind daf√ºr bekannt, besonders schwierig zu trainieren zu sein. Hier sind einige Probleme:

* **Mode Collapse**. Damit ist gemeint, dass der Generator lernt, ein erfolgreiches Bild zu erzeugen, das den Discriminator t√§uscht, aber keine Vielfalt an verschiedenen Bildern produziert.
* **Empfindlichkeit gegen√ºber Hyperparametern**. Oft kann man beobachten, dass ein GAN √ºberhaupt nicht konvergiert, und dann pl√∂tzlich durch eine Verringerung der Lernrate zur Konvergenz gelangt.
* Das **Gleichgewicht** zwischen Generator und Discriminator aufrechterhalten. In vielen F√§llen kann der Verlust des Discriminators relativ schnell auf null sinken, was dazu f√ºhrt, dass der Generator nicht weiter trainiert werden kann. Um dies zu √ºberwinden, k√∂nnen wir versuchen, unterschiedliche Lernraten f√ºr Generator und Discriminator festzulegen oder das Training des Discriminators zu √ºberspringen, wenn der Verlust bereits zu niedrig ist.
* Training f√ºr **hohe Aufl√∂sung**. √Ñhnlich wie bei Autoencodern tritt dieses Problem auf, weil das Rekonstruieren zu vieler Schichten eines Convolutional Networks zu Artefakten f√ºhrt. Dieses Problem wird typischerweise durch sogenanntes **progressives Wachstum** gel√∂st, bei dem zun√§chst einige Schichten auf niedrig aufgel√∂sten Bildern trainiert werden und dann Schichten "freigeschaltet" oder hinzugef√ºgt werden. Eine andere L√∂sung w√§re, zus√§tzliche Verbindungen zwischen den Schichten hinzuzuf√ºgen und mehrere Aufl√∂sungen gleichzeitig zu trainieren - siehe dieses [Multi-Scale Gradient GANs Paper](https://arxiv.org/abs/1903.06048) f√ºr Details.

## Style Transfer

GANs sind eine gro√üartige M√∂glichkeit, k√ºnstlerische Bilder zu generieren. Eine weitere interessante Technik ist der sogenannte **Style Transfer**, bei dem ein **Inhaltsbild** genommen und in einem anderen Stil neu gezeichnet wird, indem Filter aus einem **Stilbild** angewendet werden.

So funktioniert es:
* Wir beginnen mit einem zuf√§lligen Rauschbild (oder mit einem Inhaltsbild, aber der Einfachheit halber ist es leichter, mit zuf√§lligem Rauschen zu beginnen).
* Unser Ziel ist es, ein Bild zu erstellen, das sowohl dem Inhaltsbild als auch dem Stilbild nahekommt. Dies wird durch zwei Verlustfunktionen bestimmt:
   - **Content Loss** wird basierend auf den Merkmalen berechnet, die von der CNN in einigen Schichten aus dem aktuellen Bild und dem Inhaltsbild extrahiert werden.
   - **Style Loss** wird zwischen dem aktuellen Bild und dem Stilbild auf clevere Weise unter Verwendung von Gram-Matrizen berechnet (mehr Details im [Beispiel-Notebook](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)).
* Um das Bild glatter zu machen und Rauschen zu entfernen, f√ºhren wir auch einen **Variation Loss** ein, der den durchschnittlichen Abstand zwischen benachbarten Pixeln berechnet.
* Die Hauptoptimierungsschleife passt das aktuelle Bild mithilfe von Gradient Descent (oder einem anderen Optimierungsalgorithmus) an, um den Gesamtabstand zu minimieren, der eine gewichtete Summe aller drei Verluste ist.

## ‚úçÔ∏è Beispiel: [Style Transfer](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Fazit

In dieser Lektion haben Sie √ºber GANs und deren Training gelernt. Sie haben auch die besonderen Herausforderungen kennengelernt, denen diese Art von neuronalen Netzwerken begegnen kann, und einige Strategien, wie man diese √ºberwinden kann.

## üöÄ Herausforderung

Arbeiten Sie das [Style Transfer Notebook](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) mit Ihren eigenen Bildern durch.

## Wiederholung & Selbststudium

Zur Vertiefung lesen Sie mehr √ºber GANs in diesen Ressourcen:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), eine *de facto* GAN-Architektur, die man in Betracht ziehen sollte.
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Aufgabe

Besuchen Sie eines der beiden Notebooks zu dieser Lektion erneut und trainieren Sie das GAN mit Ihren eigenen Bildern. Was k√∂nnen Sie erschaffen?

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.