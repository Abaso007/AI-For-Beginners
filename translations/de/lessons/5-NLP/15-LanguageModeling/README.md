<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T12:23:03+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "de"
}
-->
# Sprachmodellierung

Semantische Einbettungen wie Word2Vec und GloVe sind tats√§chlich ein erster Schritt in Richtung **Sprachmodellierung** ‚Äì Modelle zu erstellen, die irgendwie die *Natur* der Sprache *verstehen* (oder *repr√§sentieren*).

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Die Hauptidee hinter der Sprachmodellierung besteht darin, Modelle auf nicht gekennzeichneten Datens√§tzen in einer un√ºberwachten Weise zu trainieren. Das ist wichtig, da wir riesige Mengen an unbeschriftetem Text zur Verf√ºgung haben, w√§hrend die Menge an beschriftetem Text immer durch den Aufwand begrenzt w√§re, den wir f√ºr die Beschriftung aufbringen k√∂nnen. Meistens k√∂nnen wir Sprachmodelle erstellen, die **fehlende W√∂rter** im Text vorhersagen, da es einfach ist, ein zuf√§lliges Wort im Text auszublenden und es als Trainingsbeispiel zu verwenden.

## Training von Einbettungen

In unseren vorherigen Beispielen haben wir vortrainierte semantische Einbettungen verwendet, aber es ist interessant zu sehen, wie diese Einbettungen trainiert werden k√∂nnen. Es gibt mehrere m√∂gliche Ans√§tze, die verwendet werden k√∂nnen:

* **N-Gramm**-Sprachmodellierung, bei der wir ein Token vorhersagen, indem wir auf die N vorherigen Tokens schauen (N-Gramm).
* **Continuous Bag-of-Words** (CBoW), bei der wir das mittlere Token $W_0$ in einer Token-Sequenz $W_{-N}$, ..., $W_N$ vorhersagen.
* **Skip-Gramm**, bei dem wir eine Menge benachbarter Tokens {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} aus dem mittleren Token $W_0$ vorhersagen.

![Bild aus einer Arbeit √ºber die Umwandlung von W√∂rtern in Vektoren](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.de.png)

> Bild aus [dieser Arbeit](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Beispiel-Notebooks: Training eines CBoW-Modells

Setze dein Lernen mit den folgenden Notebooks fort:

* [Training von CBoW Word2Vec mit TensorFlow](CBoW-TF.ipynb)
* [Training von CBoW Word2Vec mit PyTorch](CBoW-PyTorch.ipynb)

## Fazit

In der vorherigen Lektion haben wir gesehen, dass Wort-Einbettungen wie Magie funktionieren! Jetzt wissen wir, dass das Training von Wort-Einbettungen keine sehr komplexe Aufgabe ist, und wir sollten in der Lage sein, unsere eigenen Wort-Einbettungen f√ºr dom√§nenspezifische Texte zu trainieren, falls erforderlich.

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## Wiederholung & Selbststudium

* [Offizielles PyTorch-Tutorial zur Sprachmodellierung](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Offizielles TensorFlow-Tutorial zum Training eines Word2Vec-Modells](https://www.TensorFlow.org/tutorials/text/word2vec).
* Die Verwendung des **gensim**-Frameworks, um die am h√§ufigsten verwendeten Einbettungen mit wenigen Codezeilen zu trainieren, wird [in dieser Dokumentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) beschrieben.

## üöÄ [Aufgabe: Skip-Gramm-Modell trainieren](lab/README.md)

Im Labor fordern wir dich heraus, den Code aus dieser Lektion zu √§ndern, um ein Skip-Gramm-Modell anstelle von CBoW zu trainieren. [Lies die Details](lab/README.md)

---

