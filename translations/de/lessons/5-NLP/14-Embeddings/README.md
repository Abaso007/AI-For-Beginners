<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-24T09:30:31+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "de"
}
-->
# Einbettungen

## [Quiz vor der Vorlesung](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Beim Training von Klassifikatoren basierend auf BoW oder TF/IDF arbeiteten wir mit hochdimensionalen Bag-of-Words-Vektoren der L√§nge `vocab_size` und konvertierten explizit von niedrigdimensionalen Positionsdarstellungsvektoren in sp√§rliche One-Hot-Darstellungen. Diese One-Hot-Darstellung ist jedoch nicht speichereffizient. Au√üerdem wird jedes Wort unabh√§ngig von den anderen behandelt, d. h. One-Hot-codierte Vektoren dr√ºcken keine semantische √Ñhnlichkeit zwischen W√∂rtern aus.

Die Idee der **Einbettung** besteht darin, W√∂rter durch niedrigdimensionale dichte Vektoren darzustellen, die irgendwie die semantische Bedeutung eines Wortes widerspiegeln. Sp√§ter werden wir besprechen, wie man sinnvolle Wort-Einbettungen erstellt, aber vorerst betrachten wir Einbettungen einfach als eine M√∂glichkeit, die Dimensionalit√§t eines Wortvektors zu reduzieren.

Die Einbettungsschicht w√ºrde also ein Wort als Eingabe nehmen und einen Ausgabesektor mit einer bestimmten `embedding_size` erzeugen. In gewisser Weise ist sie einer `Linear`-Schicht sehr √§hnlich, aber anstatt einen One-Hot-codierten Vektor zu verwenden, kann sie eine Wortnummer als Eingabe akzeptieren, wodurch wir gro√üe One-Hot-codierte Vektoren vermeiden k√∂nnen.

Durch die Verwendung einer Einbettungsschicht als erste Schicht in unserem Klassifikator-Netzwerk k√∂nnen wir von einem Bag-of-Words-Modell zu einem **Embedding-Bag-Modell** wechseln, bei dem wir zun√§chst jedes Wort in unserem Text in die entsprechende Einbettung umwandeln und dann eine Aggregatfunktion √ºber alle diese Einbettungen berechnen, wie z. B. `sum`, `average` oder `max`.

![Bild, das einen Einbettungsklassifikator f√ºr f√ºnf Sequenzw√∂rter zeigt.](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)

> Bild vom Autor

## ‚úçÔ∏è √úbungen: Einbettungen

Setze dein Lernen in den folgenden Notebooks fort:
* [Einbettungen mit PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Einbettungen TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Semantische Einbettungen: Word2Vec

W√§hrend die Einbettungsschicht gelernt hat, W√∂rter in Vektordarstellungen zu √ºberf√ºhren, hat diese Darstellung jedoch nicht unbedingt eine gro√üe semantische Bedeutung. Es w√§re w√ºnschenswert, eine Vektordarstellung zu lernen, bei der √§hnliche W√∂rter oder Synonyme Vektoren entsprechen, die in Bezug auf eine Vektordistanz (z. B. euklidische Distanz) nahe beieinander liegen.

Um dies zu erreichen, m√ºssen wir unser Einbettungsmodell auf einer gro√üen Textsammlung in einer bestimmten Weise vortrainieren. Eine Methode, um semantische Einbettungen zu trainieren, wird [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) genannt. Sie basiert auf zwei Hauptarchitekturen, die verwendet werden, um eine verteilte Darstellung von W√∂rtern zu erzeugen:

 - **Continuous Bag-of-Words** (CBoW) ‚Äî In dieser Architektur trainieren wir das Modell, ein Wort aus dem umgebenden Kontext vorherzusagen. Gegeben das ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, ist das Ziel des Modells, $W_0$ aus $(W_{-2},W_{-1},W_1,W_2)$ vorherzusagen.
 - **Continuous Skip-Gram** ist das Gegenteil von CBoW. Das Modell verwendet das umgebende Fenster von Kontextw√∂rtern, um das aktuelle Wort vorherzusagen.

CBoW ist schneller, w√§hrend Skip-Gram langsamer ist, aber eine bessere Darstellung von seltenen W√∂rtern liefert.

![Bild, das sowohl CBoW- als auch Skip-Gram-Algorithmen zur Umwandlung von W√∂rtern in Vektoren zeigt.](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> Bild aus [diesem Paper](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vec vortrainierte Einbettungen (sowie andere √§hnliche Modelle wie GloVe) k√∂nnen auch anstelle der Einbettungsschicht in neuronalen Netzwerken verwendet werden. Allerdings m√ºssen wir mit Vokabularen umgehen, da das Vokabular, das zum Vortrainieren von Word2Vec/GloVe verwendet wurde, wahrscheinlich von dem Vokabular in unserem Textkorpus abweicht. Schau dir die oben genannten Notebooks an, um zu sehen, wie dieses Problem gel√∂st werden kann.

## Kontextuelle Einbettungen

Eine zentrale Einschr√§nkung traditioneller vortrainierter Einbettungsdarstellungen wie Word2Vec ist das Problem der Wortbedeutungsunterscheidung. W√§hrend vortrainierte Einbettungen einen Teil der Bedeutung von W√∂rtern im Kontext erfassen k√∂nnen, wird jede m√∂gliche Bedeutung eines Wortes in derselben Einbettung kodiert. Dies kann in nachgelagerten Modellen Probleme verursachen, da viele W√∂rter, wie das Wort 'play', je nach Kontext unterschiedliche Bedeutungen haben.

Zum Beispiel hat das Wort 'play' in diesen beiden S√§tzen eine ganz unterschiedliche Bedeutung:

- Ich ging zu einem **Theaterst√ºck**.
- John m√∂chte mit seinen Freunden **spielen**.

Die oben genannten vortrainierten Einbettungen repr√§sentieren beide Bedeutungen des Wortes 'play' in derselben Einbettung. Um diese Einschr√§nkung zu √ºberwinden, m√ºssen wir Einbettungen basierend auf dem **Sprachmodell** erstellen, das auf einem gro√üen Textkorpus trainiert wurde und *wei√ü*, wie W√∂rter in verschiedenen Kontexten zusammengef√ºgt werden k√∂nnen. Die Diskussion √ºber kontextuelle Einbettungen liegt au√üerhalb des Rahmens dieses Tutorials, aber wir werden darauf zur√ºckkommen, wenn wir sp√§ter im Kurs √ºber Sprachmodelle sprechen.

## Fazit

In dieser Lektion hast du gelernt, wie man Einbettungsschichten in TensorFlow und PyTorch erstellt und verwendet, um die semantische Bedeutung von W√∂rtern besser widerzuspiegeln.

## üöÄ Herausforderung

Word2Vec wurde f√ºr einige interessante Anwendungen verwendet, einschlie√ülich der Generierung von Songtexten und Gedichten. Schau dir [diesen Artikel](https://www.politetype.com/blog/word2vec-color-poems) an, der erkl√§rt, wie der Autor Word2Vec verwendet hat, um Gedichte zu generieren. Sieh dir auch [dieses Video von Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) an, um eine andere Erkl√§rung dieser Technik zu entdecken. Versuche dann, diese Techniken auf deinen eigenen Textkorpus anzuwenden, vielleicht aus Kaggle.

## [Quiz nach der Vorlesung](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## Wiederholung & Selbststudium

Lies dieses Paper √ºber Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Aufgabe: Notebooks](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.