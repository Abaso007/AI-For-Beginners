<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b708c9b85b833864c73c6281f1e6b96e",
  "translation_date": "2025-09-23T12:24:39+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "de"
}
-->
# Einbettungen

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Beim Training von Klassifikatoren basierend auf BoW oder TF/IDF haben wir mit hochdimensionalen Bag-of-Words-Vektoren gearbeitet, deren L√§nge `vocab_size` entspricht. Dabei haben wir explizit von niedrigdimensionalen Positionsdarstellungsvektoren in sp√§rliche One-Hot-Darstellungen umgewandelt. Diese One-Hot-Darstellung ist jedoch nicht speichereffizient. Au√üerdem wird jedes Wort unabh√§ngig von den anderen behandelt, d. h. One-Hot-codierte Vektoren dr√ºcken keine semantische √Ñhnlichkeit zwischen W√∂rtern aus.

Die Idee der **Einbettung** besteht darin, W√∂rter durch niedrigdimensionale dichte Vektoren darzustellen, die irgendwie die semantische Bedeutung eines Wortes widerspiegeln. Sp√§ter werden wir besprechen, wie man sinnvolle Wort-Einbettungen erstellt, aber vorerst k√∂nnen wir Einbettungen als eine Methode betrachten, die Dimensionalit√§t eines Wortvektors zu reduzieren.

Die Einbettungsschicht w√ºrde also ein Wort als Eingabe nehmen und einen Ausgabesektor mit einer festgelegten `embedding_size` erzeugen. In gewisser Weise √§hnelt sie einer `Linear`-Schicht, aber anstatt einen One-Hot-codierten Vektor zu verwenden, kann sie eine Wortnummer als Eingabe akzeptieren, wodurch wir gro√üe One-Hot-codierte Vektoren vermeiden k√∂nnen.

Durch die Verwendung einer Einbettungsschicht als erste Schicht in unserem Klassifikator-Netzwerk k√∂nnen wir von einem Bag-of-Words-Modell zu einem **Embedding-Bag-Modell** wechseln, bei dem wir zun√§chst jedes Wort in unserem Text in die entsprechende Einbettung umwandeln und dann eine Aggregatfunktion √ºber alle diese Einbettungen berechnen, wie z. B. `sum`, `average` oder `max`.

![Bild zeigt einen Einbettungsklassifikator f√ºr f√ºnf Sequenzw√∂rter.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.de.png)

> Bild vom Autor

## ‚úçÔ∏è √úbungen: Einbettungen

Setze dein Lernen in den folgenden Notebooks fort:
* [Einbettungen mit PyTorch](EmbeddingsPyTorch.ipynb)
* [Einbettungen mit TensorFlow](EmbeddingsTF.ipynb)

## Semantische Einbettungen: Word2Vec

W√§hrend die Einbettungsschicht gelernt hat, W√∂rter in Vektordarstellungen zu √ºberf√ºhren, hat diese Darstellung jedoch nicht unbedingt eine semantische Bedeutung. Es w√§re w√ºnschenswert, eine Vektordarstellung zu lernen, bei der √§hnliche W√∂rter oder Synonyme Vektoren entsprechen, die in Bezug auf eine Vektordistanz (z. B. euklidische Distanz) nahe beieinander liegen.

Um dies zu erreichen, m√ºssen wir unser Einbettungsmodell auf einer gro√üen Textsammlung in einer spezifischen Weise vortrainieren. Eine Methode, semantische Einbettungen zu trainieren, nennt sich [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Es basiert auf zwei Hauptarchitekturen, die verwendet werden, um eine verteilte Darstellung von W√∂rtern zu erzeugen:

 - **Continuous Bag-of-Words** (CBoW) ‚Äî In dieser Architektur trainieren wir das Modell darauf, ein Wort aus dem umgebenden Kontext vorherzusagen. Gegeben das Ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, ist das Ziel des Modells, $W_0$ aus $(W_{-2},W_{-1},W_1,W_2)$ vorherzusagen.
 - **Continuous Skip-Gram** ist das Gegenteil von CBoW. Das Modell verwendet das umgebende Fenster von Kontextw√∂rtern, um das aktuelle Wort vorherzusagen.

CBoW ist schneller, w√§hrend Skip-Gram langsamer ist, aber eine bessere Darstellung von seltenen W√∂rtern liefert.

![Bild zeigt sowohl CBoW- als auch Skip-Gram-Algorithmen zur Umwandlung von W√∂rtern in Vektoren.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.de.png)

> Bild aus [diesem Paper](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vec vortrainierte Einbettungen (sowie andere √§hnliche Modelle wie GloVe) k√∂nnen auch anstelle der Einbettungsschicht in neuronalen Netzwerken verwendet werden. Allerdings m√ºssen wir uns mit Vokabularen auseinandersetzen, da das Vokabular, das zum Vortrainieren von Word2Vec/GloVe verwendet wurde, wahrscheinlich von dem Vokabular in unserem Textkorpus abweicht. Schau dir die oben genannten Notebooks an, um zu sehen, wie dieses Problem gel√∂st werden kann.

## Kontextuelle Einbettungen

Eine zentrale Einschr√§nkung traditioneller vortrainierter Einbettungsdarstellungen wie Word2Vec ist das Problem der Mehrdeutigkeit von Wortbedeutungen. W√§hrend vortrainierte Einbettungen einige Bedeutungen von W√∂rtern im Kontext erfassen k√∂nnen, wird jede m√∂gliche Bedeutung eines Wortes in derselben Einbettung kodiert. Dies kann Probleme in nachgelagerten Modellen verursachen, da viele W√∂rter, wie das Wort 'play', je nach Kontext unterschiedliche Bedeutungen haben.

Zum Beispiel hat das Wort 'play' in diesen beiden S√§tzen ganz unterschiedliche Bedeutungen:

- Ich war in einem **Theaterst√ºck**.
- John m√∂chte mit seinen Freunden **spielen**.

Die oben genannten vortrainierten Einbettungen repr√§sentieren beide Bedeutungen des Wortes 'play' in derselben Einbettung. Um diese Einschr√§nkung zu √ºberwinden, m√ºssen wir Einbettungen basierend auf dem **Sprachmodell** erstellen, das auf einem gro√üen Textkorpus trainiert wurde und *wei√ü*, wie W√∂rter in verschiedenen Kontexten zusammengef√ºgt werden k√∂nnen. Die Diskussion √ºber kontextuelle Einbettungen liegt au√üerhalb des Umfangs dieses Tutorials, aber wir werden sp√§ter im Kurs darauf zur√ºckkommen, wenn wir √ºber Sprachmodelle sprechen.

## Fazit

In dieser Lektion hast du gelernt, wie man Einbettungsschichten in TensorFlow und PyTorch erstellt und verwendet, um die semantische Bedeutung von W√∂rtern besser widerzuspiegeln.

## üöÄ Herausforderung

Word2Vec wurde f√ºr einige interessante Anwendungen verwendet, einschlie√ülich der Generierung von Songtexten und Gedichten. Schau dir [diesen Artikel](https://www.politetype.com/blog/word2vec-color-poems) an, der erkl√§rt, wie der Autor Word2Vec verwendet hat, um Gedichte zu generieren. Sieh dir auch [dieses Video von Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) an, um eine andere Erkl√§rung dieser Technik zu entdecken. Versuche dann, diese Techniken auf deinen eigenen Textkorpus anzuwenden, vielleicht aus Kaggle.

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## √úberpr√ºfung & Selbststudium

Lies dieses Paper √ºber Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Aufgabe: Notebooks](assignment.md)

---

