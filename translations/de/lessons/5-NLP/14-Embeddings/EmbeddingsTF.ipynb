{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einbettungen\n",
    "\n",
    "In unserem vorherigen Beispiel haben wir mit hochdimensionalen Bag-of-Words-Vektoren der Länge `vocab_size` gearbeitet und die niedrigdimensionalen Positionsdarstellungsvektoren explizit in eine spärliche One-Hot-Darstellung umgewandelt. Diese One-Hot-Darstellung ist nicht speichereffizient. Außerdem wird jedes Wort unabhängig von den anderen behandelt, sodass One-Hot-codierte Vektoren keine semantischen Ähnlichkeiten zwischen Wörtern ausdrücken.\n",
    "\n",
    "In dieser Einheit werden wir weiterhin den **News AG**-Datensatz untersuchen. Zu Beginn laden wir die Daten und holen einige Definitionen aus der vorherigen Einheit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was ist ein Embedding?\n",
    "\n",
    "Die Idee eines **Embeddings** besteht darin, Wörter mithilfe von niedrigdimensionalen, dichten Vektoren darzustellen, die die semantische Bedeutung des Wortes widerspiegeln. Später werden wir besprechen, wie man sinnvolle Wort-Embeddings erstellt, aber vorerst betrachten wir Embeddings einfach als eine Möglichkeit, die Dimensionalität eines Wortvektors zu reduzieren.\n",
    "\n",
    "Eine Embedding-Schicht nimmt also ein Wort als Eingabe und erzeugt einen Ausgabevektor mit einer festgelegten `embedding_size`. In gewisser Weise ähnelt sie einer `Dense`-Schicht, aber anstatt einen One-Hot-codierten Vektor als Eingabe zu verwenden, kann sie eine Wortnummer verarbeiten.\n",
    "\n",
    "Indem wir eine Embedding-Schicht als erste Schicht in unserem Netzwerk verwenden, können wir von einem Bag-of-Words-Modell zu einem **Embedding-Bag**-Modell wechseln. Dabei wird jedes Wort in unserem Text zunächst in das entsprechende Embedding umgewandelt, und anschließend wird eine Aggregationsfunktion über alle diese Embeddings berechnet, wie z. B. `sum`, `average` oder `max`.\n",
    "\n",
    "![Bild, das einen Embedding-Klassifikator für fünf Sequenzwörter zeigt.](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)\n",
    "\n",
    "Unser Klassifikator-Neuronales-Netzwerk besteht aus den folgenden Schichten:\n",
    "\n",
    "* `TextVectorization`-Schicht, die einen String als Eingabe nimmt und einen Tensor mit Token-Nummern erzeugt. Wir werden eine angemessene Vokabulargröße `vocab_size` festlegen und weniger häufig verwendete Wörter ignorieren. Die Eingabeform wird 1 sein, und die Ausgabeform wird $n$ sein, da wir $n$ Token als Ergebnis erhalten, von denen jedes Zahlen von 0 bis `vocab_size` enthält.\n",
    "* `Embedding`-Schicht, die $n$ Zahlen nimmt und jede Zahl in einen dichten Vektor einer bestimmten Länge (in unserem Beispiel 100) reduziert. Der Eingabetensor der Form $n$ wird somit in einen $n\\times 100$-Tensor umgewandelt.\n",
    "* Aggregationsschicht, die den Durchschnitt dieses Tensors entlang der ersten Achse berechnet, d. h. sie berechnet den Durchschnitt aller $n$ Eingabetensoren, die verschiedenen Wörtern entsprechen. Um diese Schicht zu implementieren, verwenden wir eine `Lambda`-Schicht und übergeben ihr die Funktion zur Berechnung des Durchschnitts. Die Ausgabe wird die Form 100 haben und die numerische Darstellung der gesamten Eingabesequenz sein.\n",
    "* Abschließender `Dense`-linearer Klassifikator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im `summary`-Ausdruck entspricht die erste Tensor-Dimension `None` in der **output shape**-Spalte der Minibatch-Größe, und die zweite entspricht der Länge der Token-Sequenz. Alle Token-Sequenzen im Minibatch haben unterschiedliche Längen. Wir werden im nächsten Abschnitt besprechen, wie man damit umgeht.\n",
    "\n",
    "Jetzt lass uns das Netzwerk trainieren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Hinweis**: Wir erstellen den Vektorisierer basierend auf einem Teil der Daten. Dies wird durchgeführt, um den Prozess zu beschleunigen, und es könnte dazu führen, dass nicht alle Token aus unserem Text im Vokabular enthalten sind. In diesem Fall würden diese Token ignoriert, was zu einer leicht geringeren Genauigkeit führen kann. Allerdings liefert ein Teil des Textes in der Praxis oft eine gute Schätzung des Vokabulars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umgang mit variablen Sequenzgrößen\n",
    "\n",
    "Lassen Sie uns verstehen, wie das Training in Minibatches abläuft. Im obigen Beispiel hat der Eingabetensor die Dimension 1, und wir verwenden 128 lange Minibatches, sodass die tatsächliche Größe des Tensors $128 \\times 1$ beträgt. Allerdings ist die Anzahl der Tokens in jedem Satz unterschiedlich. Wenn wir die `TextVectorization`-Schicht auf eine einzelne Eingabe anwenden, ist die Anzahl der zurückgegebenen Tokens unterschiedlich, abhängig davon, wie der Text tokenisiert wird:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir den Vektorisierer jedoch auf mehrere Sequenzen anwenden, muss er einen Tensor mit rechteckiger Form erzeugen, sodass er nicht verwendete Elemente mit dem PAD-Token (was in unserem Fall null ist) auffüllt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier können wir die Einbettungen sehen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Hinweis**: Um die Menge an Auffüllung zu minimieren, kann es in einigen Fällen sinnvoll sein, alle Sequenzen im Datensatz in der Reihenfolge zunehmender Länge (oder genauer gesagt, Anzahl der Token) zu sortieren. Dies stellt sicher, dass jede Minibatch Sequenzen ähnlicher Länge enthält.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantische Einbettungen: Word2Vec\n",
    "\n",
    "In unserem vorherigen Beispiel hat die Einbettungsschicht gelernt, Wörter in Vektorrepräsentationen abzubilden, jedoch hatten diese Repräsentationen keine semantische Bedeutung. Es wäre wünschenswert, eine Vektorrepräsentation zu erlernen, bei der ähnliche Wörter oder Synonyme Vektoren entsprechen, die in Bezug auf eine Vektordistanz (zum Beispiel euklidische Distanz) nahe beieinander liegen.\n",
    "\n",
    "Um dies zu erreichen, müssen wir unser Einbettungsmodell mit einer großen Textsammlung vortrainieren, indem wir eine Technik wie [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) verwenden. Diese basiert auf zwei Hauptarchitekturen, die verwendet werden, um eine verteilte Repräsentation von Wörtern zu erzeugen:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), bei dem wir das Modell darauf trainieren, ein Wort aus dem umgebenden Kontext vorherzusagen. Gegeben ist das N-Gramm $(W_{-2},W_{-1},W_0,W_1,W_2)$, und das Ziel des Modells ist es, $W_0$ aus $(W_{-2},W_{-1},W_1,W_2)$ vorherzusagen.\n",
    " - **Continuous skip-gram** ist das Gegenteil von CBoW. Das Modell verwendet das umgebende Fenster von Kontextwörtern, um das aktuelle Wort vorherzusagen.\n",
    "\n",
    "CBoW ist schneller, während Skip-Gram zwar langsamer ist, aber eine bessere Repräsentation für seltene Wörter liefert.\n",
    "\n",
    "![Bild, das sowohl die CBoW- als auch die Skip-Gram-Algorithmen zur Umwandlung von Wörtern in Vektoren zeigt.](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)\n",
    "\n",
    "Um mit der Word2Vec-Einbettung, die auf dem Google-News-Datensatz vortrainiert wurde, zu experimentieren, können wir die **gensim**-Bibliothek verwenden. Unten finden wir die Wörter, die 'neural' am ähnlichsten sind.\n",
    "\n",
    "> **Hinweis:** Wenn Sie zum ersten Mal Wortvektoren erstellen, kann das Herunterladen einige Zeit in Anspruch nehmen!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können auch die Vektoreinbettung aus dem Wort extrahieren, um sie beim Training des Klassifikationsmodells zu verwenden. Die Einbettung hat 300 Komponenten, aber hier zeigen wir aus Gründen der Klarheit nur die ersten 20 Komponenten des Vektors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Großartige an semantischen Einbettungen ist, dass man die Vektor-Codierung basierend auf Semantik manipulieren kann. Zum Beispiel können wir nach einem Wort suchen, dessen Vektorrepräsentation so nah wie möglich an den Wörtern *König* und *Frau* ist und so weit wie möglich vom Wort *Mann* entfernt ist:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ein Beispiel oben verwendet einige interne GenSym-Magie, aber die zugrunde liegende Logik ist tatsächlich ziemlich einfach. Eine interessante Sache an Einbettungen ist, dass man normale Vektoroperationen auf Einbettungsvektoren durchführen kann, und das würde Operationen auf Wort**bedeutungen** widerspiegeln. Das obige Beispiel kann in Form von Vektoroperationen ausgedrückt werden: Wir berechnen den Vektor, der **KÖNIG-MANN+FRAU** entspricht (Operationen `+` und `-` werden auf Vektordarstellungen der entsprechenden Wörter durchgeführt), und finden dann das nächstgelegene Wort im Wörterbuch zu diesem Vektor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Wir mussten kleine Koeffizienten zu den *man*- und *woman*-Vektoren hinzufügen – versuchen Sie, diese zu entfernen, um zu sehen, was passiert.\n",
    "\n",
    "Um den nächstgelegenen Vektor zu finden, verwenden wir TensorFlow-Mechanismen, um einen Vektor von Abständen zwischen unserem Vektor und allen Vektoren im Vokabular zu berechnen, und finden dann den Index des minimalen Wortes mit `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während Word2Vec eine großartige Möglichkeit zu sein scheint, die Semantik von Wörtern auszudrücken, hat es viele Nachteile, darunter die folgenden:\n",
    "\n",
    "* Sowohl CBoW- als auch Skip-Gram-Modelle sind **prädiktive Einbettungen** und berücksichtigen nur den lokalen Kontext. Word2Vec nutzt den globalen Kontext nicht.\n",
    "* Word2Vec berücksichtigt nicht die **Morphologie** von Wörtern, d. h. die Tatsache, dass die Bedeutung eines Wortes von verschiedenen Teilen des Wortes, wie z. B. dem Stamm, abhängen kann.\n",
    "\n",
    "**FastText** versucht, die zweite Einschränkung zu überwinden, und baut auf Word2Vec auf, indem es Vektordarstellungen für jedes Wort und die Zeichen-n-Gramme innerhalb jedes Wortes lernt. Die Werte der Darstellungen werden dann bei jedem Trainingsschritt zu einem Vektor gemittelt. Obwohl dies eine Menge zusätzlicher Berechnungen beim Pretraining erfordert, ermöglicht es den Wort-Einbettungen, Subwort-Informationen zu kodieren.\n",
    "\n",
    "Eine andere Methode, **GloVe**, verwendet einen anderen Ansatz für Wort-Einbettungen, der auf der Faktorisierung der Wort-Kontext-Matrix basiert. Zunächst wird eine große Matrix erstellt, die die Anzahl der Wortvorkommen in verschiedenen Kontexten zählt, und dann wird versucht, diese Matrix in niedrigeren Dimensionen so darzustellen, dass der Rekonstruktionsverlust minimiert wird.\n",
    "\n",
    "Die gensim-Bibliothek unterstützt diese Wort-Einbettungen, und Sie können mit ihnen experimentieren, indem Sie den Modell-Ladecode oben ändern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verwendung vortrainierter Embeddings in Keras\n",
    "\n",
    "Wir können das obige Beispiel anpassen, um die Matrix in unserer Embedding-Schicht mit semantischen Embeddings wie Word2Vec vorab zu füllen. Die Vokabulare des vortrainierten Embeddings und des Textkorpus werden wahrscheinlich nicht übereinstimmen, daher müssen wir eines auswählen. Hier untersuchen wir die beiden möglichen Optionen: die Verwendung des Tokenizer-Vokabulars und die Verwendung des Vokabulars aus den Word2Vec-Embeddings.\n",
    "\n",
    "### Verwendung des Tokenizer-Vokabulars\n",
    "\n",
    "Bei der Verwendung des Tokenizer-Vokabulars haben einige Wörter aus dem Vokabular entsprechende Word2Vec-Embeddings, während andere fehlen. Angenommen, unsere Vokabulargröße ist `vocab_size` und die Länge des Word2Vec-Embedding-Vektors ist `embed_size`, wird die Embedding-Schicht durch eine Gewichtsmatrix der Form `vocab_size`$\\times$`embed_size` dargestellt. Wir füllen diese Matrix, indem wir das Vokabular durchgehen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für Wörter, die nicht im Word2Vec-Wortschatz vorhanden sind, können wir sie entweder als Nullen belassen oder einen zufälligen Vektor generieren.\n",
    "\n",
    "Nun können wir eine Einbettungsschicht mit vortrainierten Gewichten definieren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Hinweis**: Beachten Sie, dass wir `trainable=False` setzen, wenn wir die `Embedding` erstellen. Das bedeutet, dass wir die Embedding-Schicht nicht neu trainieren. Dies kann dazu führen, dass die Genauigkeit etwas geringer ist, aber es beschleunigt das Training.\n",
    "\n",
    "### Verwendung des Embedding-Vokabulars\n",
    "\n",
    "Ein Problem bei dem vorherigen Ansatz ist, dass die in der TextVectorization und Embedding verwendeten Vokabulare unterschiedlich sind. Um dieses Problem zu lösen, können wir eine der folgenden Lösungen verwenden:\n",
    "* Das Word2Vec-Modell mit unserem Vokabular neu trainieren.\n",
    "* Unser Dataset mit dem Vokabular aus dem vortrainierten Word2Vec-Modell laden. Die Vokabulare, die zum Laden des Datasets verwendet werden, können während des Ladens angegeben werden.\n",
    "\n",
    "Der zweite Ansatz scheint einfacher zu sein, also setzen wir ihn um. Zunächst erstellen wir eine `TextVectorization`-Schicht mit dem angegebenen Vokabular, das aus den Word2Vec-Embeddings stammt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gensim-Wort-Einbettungsbibliothek enthält eine praktische Funktion, `get_keras_embeddings`, die automatisch die entsprechende Keras-Einbettungsschicht für Sie erstellt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einer der Gründe, warum wir keine höhere Genauigkeit sehen, ist, dass einige Wörter aus unserem Datensatz im vortrainierten GloVe-Vokabular fehlen und daher im Wesentlichen ignoriert werden. Um dies zu überwinden, können wir unsere eigenen Einbettungen basierend auf unserem Datensatz trainieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontextuelle Einbettungen\n",
    "\n",
    "Eine zentrale Einschränkung traditioneller vortrainierter Einbettungsrepräsentationen wie Word2Vec ist die Tatsache, dass sie zwar eine gewisse Bedeutung eines Wortes erfassen können, aber nicht zwischen verschiedenen Bedeutungen unterscheiden können. Dies kann in nachgelagerten Modellen zu Problemen führen.\n",
    "\n",
    "Zum Beispiel hat das Wort „play“ in diesen beiden Sätzen unterschiedliche Bedeutungen:\n",
    "- Ich war in einem **Theaterstück** im Theater.\n",
    "- John möchte mit seinen Freunden **spielen**.\n",
    "\n",
    "Die vortrainierten Einbettungen, über die wir gesprochen haben, repräsentieren beide Bedeutungen des Wortes „play“ in derselben Einbettung. Um diese Einschränkung zu überwinden, müssen wir Einbettungen basierend auf dem **Sprachmodell** erstellen, das auf einem großen Textkorpus trainiert wurde und *versteht*, wie Wörter in unterschiedlichen Kontexten zusammengefügt werden können. Die Diskussion über kontextuelle Einbettungen liegt außerhalb des Umfangs dieses Tutorials, aber wir werden darauf zurückkommen, wenn wir im nächsten Abschnitt über Sprachmodelle sprechen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-31T17:10:29+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}