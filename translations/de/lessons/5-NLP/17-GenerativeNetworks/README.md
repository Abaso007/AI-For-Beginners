<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-24T09:30:59+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "de"
}
-->
# Generative Netzwerke

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Rekurrente Neuronale Netzwerke (RNNs) und ihre Varianten mit Gated Cells wie Long Short Term Memory Cells (LSTMs) und Gated Recurrent Units (GRUs) bieten einen Mechanismus f√ºr Sprachmodellierung, da sie die Wortreihenfolge lernen und Vorhersagen f√ºr das n√§chste Wort in einer Sequenz treffen k√∂nnen. Dies erm√∂glicht es uns, RNNs f√ºr **generative Aufgaben** zu nutzen, wie z. B. die Generierung von gew√∂hnlichem Text, maschinelle √úbersetzung und sogar Bildbeschreibungen.

> ‚úÖ Denke an all die Male, in denen du von generativen Aufgaben wie der automatischen Textvervollst√§ndigung profitiert hast. Recherchiere zu deinen Lieblingsanwendungen, um herauszufinden, ob sie RNNs verwendet haben.

In der RNN-Architektur, die wir in der vorherigen Einheit besprochen haben, erzeugte jede RNN-Einheit den n√§chsten versteckten Zustand als Ausgabe. Wir k√∂nnen jedoch auch eine weitere Ausgabe zu jeder rekurrenten Einheit hinzuf√ºgen, die es uns erm√∂glicht, eine **Sequenz** auszugeben (die genauso lang ist wie die urspr√ºngliche Sequenz). Dar√ºber hinaus k√∂nnen wir RNN-Einheiten verwenden, die bei jedem Schritt keine Eingabe akzeptieren, sondern nur einen anf√§nglichen Zustandsvektor aufnehmen und dann eine Sequenz von Ausgaben erzeugen.

Dies erm√∂glicht verschiedene neuronale Architekturen, die im folgenden Bild dargestellt sind:

![Bild, das g√§ngige Muster von rekurrenten neuronalen Netzwerken zeigt.](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/unreasonable-effectiveness-of-rnn.jpg)

> Bild aus dem Blogpost [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) von [Andrej Karpaty](http://karpathy.github.io/)

* **One-to-one** ist ein traditionelles neuronales Netzwerk mit einer Eingabe und einer Ausgabe.
* **One-to-many** ist eine generative Architektur, die einen Eingabewert akzeptiert und eine Sequenz von Ausgabewerten erzeugt. Zum Beispiel, wenn wir ein **Bildbeschreibungsnetzwerk** trainieren m√∂chten, das eine textuelle Beschreibung eines Bildes erzeugt, k√∂nnen wir ein Bild als Eingabe verwenden, es durch ein CNN leiten, um seinen versteckten Zustand zu erhalten, und dann eine rekurrente Kette Wort f√ºr Wort die Beschreibung generieren lassen.
* **Many-to-one** entspricht den RNN-Architekturen, die wir in der vorherigen Einheit beschrieben haben, wie z. B. Textklassifikation.
* **Many-to-many**, oder **Sequenz-zu-Sequenz**, entspricht Aufgaben wie der **maschinellen √úbersetzung**, bei der ein erstes RNN alle Informationen aus der Eingabesequenz in den versteckten Zustand sammelt und eine andere RNN-Kette diesen Zustand in die Ausgabesequenz entfaltet.

In dieser Einheit konzentrieren wir uns auf einfache generative Modelle, die uns helfen, Text zu generieren. Der Einfachheit halber verwenden wir eine Tokenisierung auf Zeichenebene.

Wir werden dieses RNN trainieren, um Text Schritt f√ºr Schritt zu generieren. Bei jedem Schritt nehmen wir eine Zeichenfolge der L√§nge `nchars` und lassen das Netzwerk das n√§chste Zeichen f√ºr jedes Eingabezeichen generieren:

![Bild, das ein Beispiel f√ºr die RNN-Generierung des Wortes 'HELLO' zeigt.](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate.png)

Beim Generieren von Text (w√§hrend der Inferenz) beginnen wir mit einem **Prompt**, der durch die RNN-Zellen geleitet wird, um seinen Zwischenzustand zu erzeugen. Von diesem Zustand aus beginnt die Generierung. Wir generieren ein Zeichen nach dem anderen und √ºbergeben den Zustand und das generierte Zeichen an eine weitere RNN-Zelle, um das n√§chste zu generieren, bis wir gen√ºgend Zeichen erzeugt haben.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Bild vom Autor

## ‚úçÔ∏è √úbungen: Generative Netzwerke

Setze dein Lernen in den folgenden Notebooks fort:

* [Generative Netzwerke mit PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Generative Netzwerke mit TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Weiche Textgenerierung und Temperatur

Die Ausgabe jeder RNN-Zelle ist eine Wahrscheinlichkeitsverteilung von Zeichen. Wenn wir immer das Zeichen mit der h√∂chsten Wahrscheinlichkeit als n√§chstes Zeichen im generierten Text ausw√§hlen, kann der Text oft in "Schleifen" geraten, in denen sich dieselben Zeichenfolgen immer wiederholen, wie in diesem Beispiel:

```
today of the second the company and a second the company ...
```

Wenn wir jedoch die Wahrscheinlichkeitsverteilung f√ºr das n√§chste Zeichen betrachten, k√∂nnte es sein, dass der Unterschied zwischen den h√∂chsten Wahrscheinlichkeiten nicht gro√ü ist, z. B. k√∂nnte ein Zeichen eine Wahrscheinlichkeit von 0,2 haben, ein anderes 0,19 usw. Zum Beispiel k√∂nnte das n√§chste Zeichen in der Sequenz '*play*' genauso gut ein Leerzeichen oder ein **e** sein (wie im Wort *player*).

Das f√ºhrt uns zu der Erkenntnis, dass es nicht immer "fair" ist, das Zeichen mit der h√∂chsten Wahrscheinlichkeit auszuw√§hlen, da die Wahl des zweitwahrscheinlichsten Zeichens dennoch zu sinnvollem Text f√ºhren k√∂nnte. Es ist kl√ºger, Zeichen aus der Wahrscheinlichkeitsverteilung zu **samplen**, die durch die Netzwerkausgabe gegeben ist. Wir k√∂nnen auch einen Parameter, die **Temperatur**, verwenden, um die Wahrscheinlichkeitsverteilung abzuflachen, falls wir mehr Zuf√§lligkeit hinzuf√ºgen m√∂chten, oder sie steiler machen, wenn wir uns st√§rker an die Zeichen mit der h√∂chsten Wahrscheinlichkeit halten wollen.

Erforsche, wie diese weiche Textgenerierung in den oben verlinkten Notebooks implementiert ist.

## Fazit

W√§hrend die Textgenerierung an sich n√ºtzlich sein kann, liegen die Hauptvorteile in der F√§higkeit, Text mit RNNs aus einem anf√§nglichen Merkmalsvektor zu generieren. Zum Beispiel wird die Textgenerierung als Teil der maschinellen √úbersetzung verwendet (Sequenz-zu-Sequenz, in diesem Fall wird der Zustandsvektor des *Encoders* verwendet, um die √ºbersetzte Nachricht zu generieren oder zu *decodieren*), oder um eine textuelle Beschreibung eines Bildes zu erzeugen (in diesem Fall w√ºrde der Merkmalsvektor aus einem CNN-Extraktor stammen).

## üöÄ Herausforderung

Nimm an einigen Lektionen auf Microsoft Learn zu diesem Thema teil:

* Textgenerierung mit [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## R√ºckblick & Selbststudium

Hier sind einige Artikel, um dein Wissen zu erweitern:

* Verschiedene Ans√§tze zur Textgenerierung mit Markov-Ketten, LSTM und GPT-2: [Blogpost](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Beispiel zur Textgenerierung in der [Keras-Dokumentation](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Aufgabe](lab/README.md)

Wir haben gesehen, wie man Text Zeichen f√ºr Zeichen generiert. Im Labor wirst du die Textgenerierung auf Wortebene erkunden.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.