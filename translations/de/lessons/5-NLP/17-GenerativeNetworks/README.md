<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "51be6057374d01d70e07dd5ec88ebc0d",
  "translation_date": "2025-09-23T12:22:45+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "de"
}
-->
# Generative Netzwerke

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Recurrent Neural Networks (RNNs) und ihre Varianten mit gated Zellen wie Long Short Term Memory Cells (LSTMs) und Gated Recurrent Units (GRUs) bieten eine M√∂glichkeit zur Sprachmodellierung, da sie die Wortreihenfolge lernen und Vorhersagen f√ºr das n√§chste Wort in einer Sequenz treffen k√∂nnen. Dies erm√∂glicht es uns, RNNs f√ºr **generative Aufgaben** zu nutzen, wie z. B. gew√∂hnliche Textgenerierung, maschinelle √úbersetzung und sogar Bildbeschriftung.

> ‚úÖ Denke an all die Male, in denen du von generativen Aufgaben wie der Textvervollst√§ndigung beim Tippen profitiert hast. Recherchiere zu deinen Lieblingsanwendungen, um herauszufinden, ob sie RNNs verwendet haben.

In der RNN-Architektur, die wir in der vorherigen Einheit besprochen haben, erzeugte jede RNN-Einheit den n√§chsten versteckten Zustand als Ausgabe. Wir k√∂nnen jedoch auch eine weitere Ausgabe zu jeder rekurrenten Einheit hinzuf√ºgen, die es uns erm√∂glicht, eine **Sequenz** auszugeben (die genauso lang ist wie die urspr√ºngliche Sequenz). Dar√ºber hinaus k√∂nnen wir RNN-Einheiten verwenden, die bei jedem Schritt keine Eingabe akzeptieren, sondern nur einen anf√§nglichen Zustandsvektor nehmen und dann eine Sequenz von Ausgaben erzeugen.

Dies erm√∂glicht verschiedene neuronale Architekturen, die im folgenden Bild dargestellt sind:

![Bild zeigt g√§ngige Muster von rekurrenten neuronalen Netzwerken.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.de.jpg)

> Bild aus dem Blogpost [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) von [Andrej Karpaty](http://karpathy.github.io/)

* **One-to-one** ist ein traditionelles neuronales Netzwerk mit einer Eingabe und einer Ausgabe.
* **One-to-many** ist eine generative Architektur, die einen Eingabewert akzeptiert und eine Sequenz von Ausgabewerten erzeugt. Zum Beispiel, wenn wir ein **Bildbeschriftungsnetzwerk** trainieren m√∂chten, das eine textuelle Beschreibung eines Bildes erzeugt, k√∂nnen wir ein Bild als Eingabe nehmen, es durch ein CNN leiten, um seinen versteckten Zustand zu erhalten, und dann eine rekurrente Kette Wort f√ºr Wort die Beschriftung generieren lassen.
* **Many-to-one** entspricht den RNN-Architekturen, die wir in der vorherigen Einheit beschrieben haben, wie z. B. Textklassifikation.
* **Many-to-many**, oder **sequence-to-sequence**, entspricht Aufgaben wie **maschineller √úbersetzung**, bei denen wir zuerst ein RNN alle Informationen aus der Eingabesequenz in den versteckten Zustand sammeln lassen und eine andere RNN-Kette diesen Zustand in die Ausgabesequenz entfaltet.

In dieser Einheit konzentrieren wir uns auf einfache generative Modelle, die uns helfen, Text zu generieren. Der Einfachheit halber verwenden wir eine Tokenisierung auf Zeichenebene.

Wir werden dieses RNN trainieren, um Text Schritt f√ºr Schritt zu generieren. Bei jedem Schritt nehmen wir eine Zeichenfolge der L√§nge `nchars` und bitten das Netzwerk, das n√§chste Ausgabesymbol f√ºr jedes Eingabesymbol zu generieren:

![Bild zeigt ein Beispiel f√ºr die RNN-Generierung des Wortes 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.de.png)

Bei der Textgenerierung (w√§hrend der Inferenz) beginnen wir mit einem **Prompt**, der durch die RNN-Zellen geleitet wird, um seinen Zwischenzustand zu erzeugen, und dann beginnt die Generierung aus diesem Zustand. Wir generieren ein Zeichen nach dem anderen und √ºbergeben den Zustand und das generierte Zeichen an eine andere RNN-Zelle, um das n√§chste zu generieren, bis wir gen√ºgend Zeichen erzeugt haben.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Bild vom Autor

## ‚úçÔ∏è √úbungen: Generative Netzwerke

Setze dein Lernen in den folgenden Notebooks fort:

* [Generative Netzwerke mit PyTorch](GenerativePyTorch.ipynb)
* [Generative Netzwerke mit TensorFlow](GenerativeTF.ipynb)

## Weiche Textgenerierung und Temperatur

Die Ausgabe jeder RNN-Zelle ist eine Wahrscheinlichkeitsverteilung von Zeichen. Wenn wir immer das Zeichen mit der h√∂chsten Wahrscheinlichkeit als n√§chstes Zeichen im generierten Text nehmen, kann der Text oft "zyklisch" werden und sich zwischen denselben Zeichenfolgen wiederholen, wie in diesem Beispiel:

```
today of the second the company and a second the company ...
```

Wenn wir jedoch die Wahrscheinlichkeitsverteilung f√ºr das n√§chste Zeichen betrachten, k√∂nnte es sein, dass der Unterschied zwischen den h√∂chsten Wahrscheinlichkeiten nicht gro√ü ist, z. B. k√∂nnte ein Zeichen eine Wahrscheinlichkeit von 0,2 haben, ein anderes - 0,19 usw. Zum Beispiel k√∂nnte das n√§chste Zeichen in der Sequenz '*play*' genauso gut ein Leerzeichen oder **e** sein (wie im Wort *player*).

Dies f√ºhrt uns zu der Erkenntnis, dass es nicht immer "fair" ist, das Zeichen mit der h√∂chsten Wahrscheinlichkeit auszuw√§hlen, da die Wahl des zweitbesten Zeichens immer noch zu sinnvollem Text f√ºhren k√∂nnte. Es ist kl√ºger, **Zeichen** aus der Wahrscheinlichkeitsverteilung zu **samplen**, die durch die Netzwerkausgabe gegeben wird. Wir k√∂nnen auch einen Parameter, **Temperatur**, verwenden, der die Wahrscheinlichkeitsverteilung abflacht, falls wir mehr Zuf√§lligkeit hinzuf√ºgen m√∂chten, oder sie steiler macht, wenn wir uns st√§rker an die Zeichen mit der h√∂chsten Wahrscheinlichkeit halten m√∂chten.

Erkunde, wie diese weiche Textgenerierung in den oben verlinkten Notebooks implementiert ist.

## Fazit

W√§hrend die Textgenerierung an sich n√ºtzlich sein kann, liegen die Hauptvorteile in der F√§higkeit, Text mithilfe von RNNs aus einem anf√§nglichen Feature-Vektor zu generieren. Zum Beispiel wird die Textgenerierung als Teil der maschinellen √úbersetzung verwendet (sequence-to-sequence, in diesem Fall wird der Zustandsvektor vom *Encoder* verwendet, um die √ºbersetzte Nachricht zu generieren oder zu *decodieren*), oder um eine textuelle Beschreibung eines Bildes zu erzeugen (in diesem Fall w√ºrde der Feature-Vektor von einem CNN-Extraktor stammen).

## üöÄ Herausforderung

Nimm einige Lektionen auf Microsoft Learn zu diesem Thema:

* Textgenerierung mit [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## R√ºckblick & Selbststudium

Hier sind einige Artikel, um dein Wissen zu erweitern:

* Verschiedene Ans√§tze zur Textgenerierung mit Markov-Kette, LSTM und GPT-2: [Blogpost](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Beispiel zur Textgenerierung in der [Keras-Dokumentation](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Aufgabe](lab/README.md)

Wir haben gesehen, wie man Text Zeichen f√ºr Zeichen generiert. Im Labor wirst du die Textgenerierung auf Wortebene erkunden.

---

