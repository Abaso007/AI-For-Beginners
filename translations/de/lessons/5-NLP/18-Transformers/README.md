<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f335dfcb4a993920504c387973a36957",
  "translation_date": "2025-09-23T12:23:12+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "de"
}
-->
# Aufmerksamkeitsmechanismen und Transformer

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Eines der wichtigsten Probleme im Bereich der NLP ist die **maschinelle √úbersetzung**, eine zentrale Aufgabe, die Tools wie Google Translate zugrunde liegt. In diesem Abschnitt konzentrieren wir uns auf die maschinelle √úbersetzung oder allgemeiner auf jede *Sequence-to-Sequence*-Aufgabe (auch **Satztransduktion** genannt).

Mit RNNs wird Sequence-to-Sequence durch zwei rekurrente Netzwerke implementiert, wobei ein Netzwerk, der **Encoder**, eine Eingabesequenz in einen versteckten Zustand zusammenfasst, w√§hrend ein anderes Netzwerk, der **Decoder**, diesen versteckten Zustand in ein √ºbersetztes Ergebnis entfaltet. Es gibt jedoch einige Probleme bei diesem Ansatz:

* Der Endzustand des Encoder-Netzwerks hat Schwierigkeiten, sich an den Anfang eines Satzes zu erinnern, was zu einer schlechten Modellqualit√§t bei langen S√§tzen f√ºhrt.
* Alle W√∂rter in einer Sequenz haben den gleichen Einfluss auf das Ergebnis. In der Realit√§t haben jedoch bestimmte W√∂rter in der Eingabesequenz oft mehr Einfluss auf die sequentiellen Ausgaben als andere.

**Aufmerksamkeitsmechanismen** bieten eine M√∂glichkeit, den kontextuellen Einfluss jedes Eingabevektors auf jede Ausgabewahrscheinlichkeit des RNN zu gewichten. Dies wird durch die Erstellung von Abk√ºrzungen zwischen den Zwischenzust√§nden des Eingabe-RNN und des Ausgabe-RNN umgesetzt. Auf diese Weise ber√ºcksichtigen wir beim Generieren des Ausgabesymbols y<sub>t</sub> alle versteckten Eingabezust√§nde h<sub>i</sub>, mit unterschiedlichen Gewichtungskoeffizienten &alpha;<sub>t,i</sub>.

![Bild zeigt ein Encoder/Decoder-Modell mit einer additiven Aufmerksamkeits-Schicht](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.de.png)

> Das Encoder-Decoder-Modell mit additivem Aufmerksamkeitsmechanismus aus [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), zitiert aus [diesem Blogbeitrag](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Die Aufmerksamkeitsmatrix {&alpha;<sub>i,j</sub>} repr√§sentiert den Grad, in dem bestimmte Eingabew√∂rter bei der Generierung eines bestimmten Wortes in der Ausgabesequenz eine Rolle spielen. Unten ist ein Beispiel f√ºr eine solche Matrix:

![Bild zeigt eine Beispielausrichtung, gefunden von RNNsearch-50, entnommen aus Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.de.png)

> Abbildung aus [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Aufmerksamkeitsmechanismen sind verantwortlich f√ºr viele der aktuellen oder nahezu aktuellen Spitzenleistungen in der NLP. Das Hinzuf√ºgen von Aufmerksamkeit erh√∂ht jedoch die Anzahl der Modellparameter erheblich, was zu Skalierungsproblemen bei RNNs f√ºhrte. Eine zentrale Einschr√§nkung bei der Skalierung von RNNs ist, dass die rekurrente Natur der Modelle es schwierig macht, das Training zu batchen und zu parallelisieren. In einem RNN muss jedes Element einer Sequenz in der Reihenfolge verarbeitet werden, was eine einfache Parallelisierung verhindert.

![Encoder Decoder mit Aufmerksamkeit](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Abbildung aus [Googles Blog](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Die Einf√ºhrung von Aufmerksamkeitsmechanismen in Kombination mit dieser Einschr√§nkung f√ºhrte zur Entwicklung der heute bekannten und genutzten Transformer-Modelle, wie BERT und Open-GPT3.

## Transformer-Modelle

Eine der Hauptideen hinter Transformern ist es, die sequentielle Natur von RNNs zu vermeiden und ein Modell zu schaffen, das w√§hrend des Trainings parallelisierbar ist. Dies wird durch die Implementierung von zwei Ideen erreicht:

* Positionskodierung
* Verwendung des Self-Attention-Mechanismus, um Muster zu erfassen, anstelle von RNNs (oder CNNs) (deshalb hei√üt das Paper, das Transformer einf√ºhrt, *[Attention is all you need](https://arxiv.org/abs/1706.03762)*).

### Positionskodierung/Einbettung

Die Idee der Positionskodierung ist folgende:  
1. Bei der Verwendung von RNNs wird die relative Position der Tokens durch die Anzahl der Schritte dargestellt und muss daher nicht explizit repr√§sentiert werden.  
2. Sobald wir jedoch zu Aufmerksamkeit wechseln, m√ºssen wir die relativen Positionen der Tokens innerhalb einer Sequenz kennen.  
3. Um Positionskodierung zu erhalten, erweitern wir unsere Sequenz von Tokens mit einer Sequenz von Token-Positionen in der Sequenz (d.h. eine Sequenz von Zahlen 0,1, ...).  
4. Wir mischen dann die Token-Position mit einem Token-Einbettungsvektor. Um die Position (Ganzzahl) in einen Vektor zu transformieren, k√∂nnen wir verschiedene Ans√§tze verwenden:

* Trainierbare Einbettung, √§hnlich wie Token-Einbettung. Dies ist der Ansatz, den wir hier betrachten. Wir wenden Einbettungsschichten sowohl auf Tokens als auch auf ihre Positionen an, was zu Einbettungsvektoren mit denselben Dimensionen f√ºhrt, die wir dann zusammen addieren.
* Feste Positionskodierungsfunktion, wie im urspr√ºnglichen Paper vorgeschlagen.

<img src="images/pos-embedding.png" width="50%"/>

> Bild vom Autor

Das Ergebnis, das wir mit Positionskodierung erhalten, bettet sowohl das urspr√ºngliche Token als auch dessen Position innerhalb einer Sequenz ein.

### Multi-Head Self-Attention

Als N√§chstes m√ºssen wir einige Muster innerhalb unserer Sequenz erfassen. Um dies zu tun, verwenden Transformer einen **Self-Attention**-Mechanismus, der im Wesentlichen Aufmerksamkeit ist, die auf dieselbe Sequenz als Eingabe und Ausgabe angewendet wird. Die Anwendung von Self-Attention erm√∂glicht es uns, den **Kontext** innerhalb des Satzes zu ber√ºcksichtigen und zu sehen, welche W√∂rter miteinander in Beziehung stehen. Zum Beispiel erm√∂glicht es uns zu sehen, auf welche W√∂rter durch Koreferenzen wie *es* verwiesen wird, und auch den Kontext zu ber√ºcksichtigen:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.de.png)

> Bild aus dem [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

In Transformern verwenden wir **Multi-Head Attention**, um dem Netzwerk die F√§higkeit zu geben, verschiedene Arten von Abh√§ngigkeiten zu erfassen, z. B. langfristige vs. kurzfristige Wortbeziehungen, Koreferenzen vs. etwas anderes usw.

[TensorFlow Notebook](TransformersTF.ipynb) enth√§lt weitere Details zur Implementierung von Transformer-Schichten.

### Encoder-Decoder-Aufmerksamkeit

In Transformern wird Aufmerksamkeit an zwei Stellen verwendet:

* Um Muster innerhalb des Eingabetextes mit Self-Attention zu erfassen
* Um Sequenz√ºbersetzung durchzuf√ºhren ‚Äì dies ist die Aufmerksamkeits-Schicht zwischen Encoder und Decoder.

Encoder-Decoder-Aufmerksamkeit ist der Aufmerksamkeitsmechanismus, der in RNNs verwendet wird, wie zu Beginn dieses Abschnitts beschrieben. Dieses animierte Diagramm erkl√§rt die Rolle der Encoder-Decoder-Aufmerksamkeit.

![Animiertes GIF zeigt, wie die Bewertungen in Transformer-Modellen durchgef√ºhrt werden.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Da jede Eingabeposition unabh√§ngig von jeder Ausgabeposition abgebildet wird, k√∂nnen Transformer besser parallelisieren als RNNs, was viel gr√∂√üere und ausdrucksst√§rkere Sprachmodelle erm√∂glicht. Jeder Aufmerksamkeitskopf kann verwendet werden, um verschiedene Beziehungen zwischen W√∂rtern zu lernen, was die nachgelagerten Aufgaben der nat√ºrlichen Sprachverarbeitung verbessert.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) ist ein sehr gro√ües mehrschichtiges Transformer-Netzwerk mit 12 Schichten f√ºr *BERT-base* und 24 f√ºr *BERT-large*. Das Modell wird zun√§chst auf einem gro√üen Textkorpus (Wikipedia + B√ºcher) mit un√ºberwachtem Training (Vorhersage maskierter W√∂rter in einem Satz) vortrainiert. W√§hrend des Vortrainings nimmt das Modell ein erhebliches Ma√ü an Sprachverst√§ndnis auf, das dann mit anderen Datens√§tzen durch Feintuning genutzt werden kann. Dieser Prozess wird als **Transfer Learning** bezeichnet.

![Bild von http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.de.png)

> Bild [Quelle](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è √úbungen: Transformer

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [Transformer in PyTorch](TransformersPyTorch.ipynb)
* [Transformer in TensorFlow](TransformersTF.ipynb)

## Fazit

In dieser Lektion haben Sie etwas √ºber Transformer und Aufmerksamkeitsmechanismen gelernt, alles wesentliche Werkzeuge im NLP-Werkzeugkasten. Es gibt viele Variationen von Transformer-Architekturen, einschlie√ülich BERT, DistilBERT, BigBird, OpenGPT3 und mehr, die fein abgestimmt werden k√∂nnen. Das [HuggingFace-Paket](https://github.com/huggingface/) bietet ein Repository f√ºr das Training vieler dieser Architekturen mit sowohl PyTorch als auch TensorFlow.

## üöÄ Herausforderung

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## √úberpr√ºfung & Selbststudium

* [Blogbeitrag](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), der das klassische [Attention is all you need](https://arxiv.org/abs/1706.03762)-Paper √ºber Transformer erkl√§rt.
* [Eine Serie von Blogbeitr√§gen](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) √ºber Transformer, die die Architektur im Detail erkl√§ren.

## [Aufgabe](assignment.md)

---

