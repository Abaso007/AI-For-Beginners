<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2273cc150380a5e191903cea858f021",
  "translation_date": "2025-09-23T12:24:13+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "de"
}
-->
# Rekurrente Neuronale Netze

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/31)

In den vorherigen Abschnitten haben wir reichhaltige semantische Repr√§sentationen von Text und einen einfachen linearen Klassifikator auf den Einbettungen verwendet. Diese Architektur erfasst die aggregierte Bedeutung der W√∂rter in einem Satz, ber√ºcksichtigt jedoch nicht die **Reihenfolge** der W√∂rter, da die Aggregationsoperation auf den Einbettungen diese Information aus dem urspr√ºnglichen Text entfernt. Da diese Modelle die Wortreihenfolge nicht modellieren k√∂nnen, sind sie nicht in der Lage, komplexere oder mehrdeutige Aufgaben wie Textgenerierung oder Fragebeantwortung zu l√∂sen.

Um die Bedeutung einer Textsequenz zu erfassen, m√ºssen wir eine andere Architektur f√ºr neuronale Netze verwenden, die als **rekurrentes neuronales Netz** oder RNN bezeichnet wird. Im RNN geben wir unseren Satz ein Symbol nach dem anderen durch das Netzwerk, und das Netzwerk erzeugt einen **Zustand**, den wir dann mit dem n√§chsten Symbol erneut in das Netzwerk einspeisen.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.de.png)

> Bild vom Autor

Angenommen, wir haben eine Eingabesequenz von Token X<sub>0</sub>,...,X<sub>n</sub>, erstellt das RNN eine Sequenz von neuronalen Netzwerkbl√∂cken und trainiert diese Sequenz end-to-end mittels Backpropagation. Jeder Netzwerkblock nimmt ein Paar (X<sub>i</sub>,S<sub>i</sub>) als Eingabe und erzeugt S<sub>i+1</sub> als Ergebnis. Der finale Zustand S<sub>n</sub> oder (Ausgabe Y<sub>n</sub>) wird in einen linearen Klassifikator eingespeist, um das Ergebnis zu erzeugen. Alle Netzwerkbl√∂cke teilen sich die gleichen Gewichte und werden end-to-end mit einem Backpropagation-Durchgang trainiert.

Da die Zustandsvektoren S<sub>0</sub>,...,S<sub>n</sub> durch das Netzwerk weitergegeben werden, kann es die sequentiellen Abh√§ngigkeiten zwischen W√∂rtern lernen. Zum Beispiel, wenn das Wort *nicht* irgendwo in der Sequenz erscheint, kann es lernen, bestimmte Elemente innerhalb des Zustandsvektors zu negieren, was zu einer Verneinung f√ºhrt.

> ‚úÖ Da die Gewichte aller RNN-Bl√∂cke im obigen Bild geteilt werden, kann dasselbe Bild als ein Block (rechts) mit einer rekurrenten R√ºckkopplungsschleife dargestellt werden, die den Ausgabestatus des Netzwerks wieder an die Eingabe weitergibt.

## Anatomie einer RNN-Zelle

Schauen wir uns an, wie eine einfache RNN-Zelle organisiert ist. Sie akzeptiert den vorherigen Zustand S<sub>i-1</sub> und das aktuelle Symbol X<sub>i</sub> als Eingaben und muss den Ausgabestatus S<sub>i</sub> erzeugen (und manchmal sind wir auch an einer anderen Ausgabe Y<sub>i</sub> interessiert, wie im Fall von generativen Netzwerken).

Eine einfache RNN-Zelle hat zwei Gewichtsmatrizen: eine transformiert ein Eingabesymbol (wir nennen sie W), und eine andere transformiert einen Eingabezustand (H). In diesem Fall wird die Ausgabe des Netzwerks als &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b) berechnet, wobei &sigma; die Aktivierungsfunktion und b ein zus√§tzlicher Bias ist.

<img alt="Anatomie einer RNN-Zelle" src="images/rnn-anatomy.png" width="50%"/>

> Bild vom Autor

In vielen F√§llen werden Eingabetoken vor dem Eintritt in das RNN durch die Einbettungsschicht geleitet, um die Dimensionalit√§t zu reduzieren. In diesem Fall, wenn die Dimension der Eingabevektoren *emb_size* ist und der Zustandsvektor *hid_size* ist - betr√§gt die Gr√∂√üe von W *emb_size*&times;*hid_size*, und die Gr√∂√üe von H ist *hid_size*&times;*hid_size*.

## Long Short Term Memory (LSTM)

Eines der Hauptprobleme klassischer RNNs ist das sogenannte **Problem der verschwindenden Gradienten**. Da RNNs end-to-end in einem Backpropagation-Durchgang trainiert werden, haben sie Schwierigkeiten, Fehler zu den ersten Schichten des Netzwerks zu propagieren, und k√∂nnen daher keine Beziehungen zwischen weit entfernten Token lernen. Eine M√∂glichkeit, dieses Problem zu vermeiden, besteht darin, **explizites Zustandsmanagement** durch sogenannte **Gates** einzuf√ºhren. Es gibt zwei bekannte Architekturen dieser Art: **Long Short Term Memory** (LSTM) und **Gated Relay Unit** (GRU).

![Bild eines Beispiels f√ºr eine Long Short Term Memory-Zelle](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Bildquelle TBD

Das LSTM-Netzwerk ist √§hnlich wie ein RNN organisiert, aber es gibt zwei Zust√§nde, die von Schicht zu Schicht weitergegeben werden: der eigentliche Zustand C und der versteckte Vektor H. In jeder Einheit wird der versteckte Vektor H<sub>i</sub> mit der Eingabe X<sub>i</sub> verkettet, und sie steuern, was mit dem Zustand C √ºber **Gates** geschieht. Jedes Gate ist ein neuronales Netzwerk mit Sigmoid-Aktivierung (Ausgabe im Bereich [0,1]), das als bitweises Maskieren betrachtet werden kann, wenn es mit dem Zustandsvektor multipliziert wird. Es gibt die folgenden Gates (von links nach rechts im obigen Bild):

* Das **Vergessens-Gate** nimmt einen versteckten Vektor und bestimmt, welche Komponenten des Vektors C wir vergessen und welche wir durchlassen m√ºssen.
* Das **Eingabe-Gate** nimmt einige Informationen aus den Eingabe- und versteckten Vektoren und f√ºgt sie in den Zustand ein.
* Das **Ausgabe-Gate** transformiert den Zustand √ºber eine lineare Schicht mit *tanh*-Aktivierung und w√§hlt dann einige seiner Komponenten mithilfe eines versteckten Vektors H<sub>i</sub> aus, um einen neuen Zustand C<sub>i+1</sub> zu erzeugen.

Komponenten des Zustands C k√∂nnen als Flags betrachtet werden, die ein- und ausgeschaltet werden k√∂nnen. Zum Beispiel, wenn wir in der Sequenz den Namen *Alice* begegnen, k√∂nnten wir annehmen, dass er sich auf eine weibliche Figur bezieht, und das Flag im Zustand setzen, dass wir ein weibliches Substantiv im Satz haben. Wenn wir sp√§ter die Phrase *und Tom* begegnen, setzen wir das Flag, dass wir ein Plural-Substantiv haben. Durch die Manipulation des Zustands k√∂nnen wir also angeblich die grammatikalischen Eigenschaften von Satzteilen verfolgen.

> ‚úÖ Eine ausgezeichnete Ressource, um die Interna von LSTM zu verstehen, ist dieser gro√üartige Artikel [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) von Christopher Olah.

## Bidirektionale und mehrschichtige RNNs

Wir haben rekurrente Netzwerke besprochen, die in eine Richtung arbeiten, vom Anfang einer Sequenz bis zum Ende. Das erscheint nat√ºrlich, da es der Art und Weise √§hnelt, wie wir lesen und Sprache h√∂ren. Da wir jedoch in vielen praktischen F√§llen zuf√§lligen Zugriff auf die Eingabesequenz haben, k√∂nnte es sinnvoll sein, die rekurrente Berechnung in beide Richtungen auszuf√ºhren. Solche Netzwerke werden als **bidirektionale** RNNs bezeichnet. Bei einem bidirektionalen Netzwerk ben√∂tigen wir zwei versteckte Zustandsvektoren, einen f√ºr jede Richtung.

Ein rekurrentes Netzwerk, sei es eindirektional oder bidirektional, erfasst bestimmte Muster innerhalb einer Sequenz und kann sie in einem Zustandsvektor speichern oder in die Ausgabe weitergeben. Wie bei konvolutionalen Netzwerken k√∂nnen wir eine weitere rekurrente Schicht auf die erste aufbauen, um h√∂herstufige Muster zu erfassen und aus den niedrigstufigen Mustern zu bauen, die von der ersten Schicht extrahiert wurden. Dies f√ºhrt uns zum Konzept eines **mehrschichtigen RNN**, das aus zwei oder mehr rekurrenten Netzwerken besteht, wobei die Ausgabe der vorherigen Schicht als Eingabe an die n√§chste Schicht weitergegeben wird.

![Bild eines mehrschichtigen Long Short Term Memory-RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.de.jpg)

*Bild aus [diesem wunderbaren Beitrag](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) von Fernando L√≥pez*

## ‚úçÔ∏è √úbungen: Einbettungen

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [RNNs mit PyTorch](RNNPyTorch.ipynb)
* [RNNs mit TensorFlow](RNNTF.ipynb)

## Fazit

In dieser Einheit haben wir gesehen, dass RNNs f√ºr die Sequenzklassifikation verwendet werden k√∂nnen, aber tats√§chlich k√∂nnen sie viele weitere Aufgaben bew√§ltigen, wie Textgenerierung, maschinelle √úbersetzung und mehr. Diese Aufgaben werden wir in der n√§chsten Einheit betrachten.

## üöÄ Herausforderung

Lesen Sie einige Literatur √ºber LSTMs und denken Sie √ºber deren Anwendungen nach:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## √úberpr√ºfung & Selbststudium

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) von Christopher Olah.

## [Aufgabe: Notebooks](assignment.md)

---

