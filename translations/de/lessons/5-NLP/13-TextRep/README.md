<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-24T09:31:28+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "de"
}
-->
# Darstellung von Text als Tensoren

## [Quiz vor der Vorlesung](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Textklassifikation

Im ersten Teil dieses Abschnitts konzentrieren wir uns auf die Aufgabe der **Textklassifikation**. Wir verwenden das [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)-Datenset, das Nachrichtenartikel wie den folgenden enth√§lt:

* Kategorie: Wissenschaft/Technik  
* Titel: Ky. Unternehmen erh√§lt Zuschuss zur Erforschung von Peptiden (AP)  
* Inhalt: AP - Ein Unternehmen, das von einem Chemieforscher der Universit√§t von Louisville gegr√ºndet wurde, erhielt einen Zuschuss zur Entwicklung...

Unser Ziel wird es sein, die Nachricht basierend auf dem Text einer der Kategorien zuzuordnen.

## Darstellung von Text

Wenn wir Aufgaben der nat√ºrlichen Sprachverarbeitung (NLP) mit neuronalen Netzwerken l√∂sen wollen, ben√∂tigen wir eine Methode, um Text als Tensoren darzustellen. Computer stellen Zeichen bereits als Zahlen dar, die mithilfe von Codierungen wie ASCII oder UTF-8 auf die Schriftarten auf Ihrem Bildschirm abgebildet werden.

<img alt="Bild zeigt ein Diagramm, das ein Zeichen einer ASCII- und bin√§ren Darstellung zuordnet" src="images/ascii-character-map.png" width="50%"/>

> [Bildquelle](https://www.seobility.net/en/wiki/ASCII)

Als Menschen verstehen wir, was jeder Buchstabe **repr√§sentiert** und wie alle Zeichen zusammenkommen, um die W√∂rter eines Satzes zu bilden. Computer hingegen haben von sich aus kein solches Verst√§ndnis, und ein neuronales Netzwerk muss die Bedeutung w√§hrend des Trainings lernen.

Daher k√∂nnen wir verschiedene Ans√§tze verwenden, um Text darzustellen:

* **Zeichenbasierte Darstellung**, bei der wir Text darstellen, indem wir jedes Zeichen als Zahl behandeln. Angenommen, wir haben *C* verschiedene Zeichen in unserem Textkorpus, dann w√ºrde das Wort *Hello* durch einen 5x*C*-Tensor dargestellt. Jedes Zeichen w√ºrde einer Spalte im Tensor in One-Hot-Codierung entsprechen.  
* **Wortbasierte Darstellung**, bei der wir ein **Vokabular** aller W√∂rter in unserem Text erstellen und dann W√∂rter mithilfe von One-Hot-Codierung darstellen. Dieser Ansatz ist in gewisser Weise besser, da jedes Zeichen f√ºr sich genommen nicht viel Bedeutung hat. Durch die Verwendung h√∂herer semantischer Konzepte ‚Äì W√∂rter ‚Äì vereinfachen wir die Aufgabe f√ºr das neuronale Netzwerk. Aufgrund der gro√üen W√∂rterbuchgr√∂√üe m√ºssen wir jedoch mit hochdimensionalen, sp√§rlichen Tensoren umgehen.

Unabh√§ngig von der Darstellung m√ºssen wir den Text zun√§chst in eine Sequenz von **Tokens** umwandeln, wobei ein Token entweder ein Zeichen, ein Wort oder manchmal sogar ein Teil eines Wortes ist. Anschlie√üend konvertieren wir das Token in eine Zahl, typischerweise mithilfe eines **Vokabulars**, und diese Zahl kann mithilfe von One-Hot-Codierung in ein neuronales Netzwerk eingespeist werden.

## N-Gramme

In der nat√ºrlichen Sprache kann die genaue Bedeutung von W√∂rtern nur im Kontext bestimmt werden. Zum Beispiel haben *neuronales Netzwerk* und *Fischernetz* v√∂llig unterschiedliche Bedeutungen. Eine M√∂glichkeit, dies zu ber√ºcksichtigen, besteht darin, unser Modell auf Wortpaaren aufzubauen und Wortpaare als separate Vokabular-Tokens zu betrachten. Auf diese Weise wird der Satz *Ich gehe gerne angeln* durch die folgende Sequenz von Tokens dargestellt: *Ich gehe*, *gehe gerne*, *gerne angeln*. Das Problem bei diesem Ansatz ist, dass die W√∂rterbuchgr√∂√üe erheblich w√§chst und Kombinationen wie *gerne angeln* und *gerne einkaufen* durch unterschiedliche Tokens dargestellt werden, die trotz des gleichen Verbs keine semantische √Ñhnlichkeit teilen.

In einigen F√§llen k√∂nnen wir auch Tri-Gramme ‚Äì Kombinationen aus drei W√∂rtern ‚Äì in Betracht ziehen. Daher wird dieser Ansatz oft als **n-Gramme** bezeichnet. Es macht auch Sinn, n-Gramme mit zeichenbasierter Darstellung zu verwenden, wobei n-Gramme in diesem Fall grob verschiedenen Silben entsprechen.

## Bag-of-Words und TF/IDF

Bei Aufgaben wie der Textklassifikation m√ºssen wir in der Lage sein, Text durch einen festen Vektor darzustellen, den wir als Eingabe f√ºr den abschlie√üenden dichten Klassifikator verwenden. Eine der einfachsten M√∂glichkeiten, dies zu tun, besteht darin, alle einzelnen Wortdarstellungen zu kombinieren, z. B. indem wir sie addieren. Wenn wir die One-Hot-Codierungen jedes Wortes addieren, erhalten wir einen Frequenzvektor, der zeigt, wie oft jedes Wort im Text vorkommt. Eine solche Darstellung von Text wird als **Bag-of-Words** (BoW) bezeichnet.

<img src="images/bow.png" width="90%"/>

> Bild vom Autor

Ein BoW zeigt im Wesentlichen, welche W√∂rter im Text vorkommen und in welchen Mengen, was tats√§chlich ein guter Hinweis darauf sein kann, worum es im Text geht. Ein Nachrichtenartikel √ºber Politik enth√§lt beispielsweise wahrscheinlich W√∂rter wie *Pr√§sident* und *Land*, w√§hrend eine wissenschaftliche Ver√∂ffentlichung Begriffe wie *Kollider*, *entdeckt* usw. enthalten k√∂nnte. Daher k√∂nnen Wortfrequenzen in vielen F√§llen ein guter Indikator f√ºr den Textinhalt sein.

Das Problem bei BoW ist, dass bestimmte h√§ufige W√∂rter wie *und*, *ist* usw. in den meisten Texten vorkommen und die h√∂chsten Frequenzen aufweisen, wodurch die wirklich wichtigen W√∂rter in den Hintergrund treten. Wir k√∂nnen die Bedeutung dieser W√∂rter verringern, indem wir die H√§ufigkeit ber√ºcksichtigen, mit der W√∂rter in der gesamten Dokumentensammlung vorkommen. Dies ist die Hauptidee hinter dem TF/IDF-Ansatz, der in den zu dieser Lektion geh√∂renden Notebooks ausf√ºhrlicher behandelt wird.

Allerdings k√∂nnen keine dieser Ans√§tze die **Semantik** des Textes vollst√§ndig ber√ºcksichtigen. Daf√ºr ben√∂tigen wir leistungsst√§rkere neuronale Netzwerkmodelle, die wir sp√§ter in diesem Abschnitt besprechen werden.

## ‚úçÔ∏è √úbungen: Textdarstellung

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [Textdarstellung mit PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Textdarstellung mit TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Fazit

Bisher haben wir Techniken untersucht, die Frequenzgewichte f√ºr verschiedene W√∂rter hinzuf√ºgen k√∂nnen. Sie sind jedoch nicht in der Lage, Bedeutung oder Reihenfolge darzustellen. Wie der ber√ºhmte Linguist J. R. Firth 1935 sagte: "Die vollst√§ndige Bedeutung eines Wortes ist immer kontextabh√§ngig, und keine Untersuchung der Bedeutung ohne Kontext kann ernst genommen werden." Sp√§ter im Kurs werden wir lernen, wie man kontextuelle Informationen aus Text mithilfe von Sprachmodellen erfasst.

## üöÄ Herausforderung

Probieren Sie einige andere √úbungen mit Bag-of-Words und verschiedenen Datenmodellen aus. Vielleicht inspiriert Sie dieser [Wettbewerb auf Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [Quiz nach der Vorlesung](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Wiederholung & Selbststudium

√úben Sie Ihre F√§higkeiten mit Text-Embeddings und Bag-of-Words-Techniken auf [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste).

## [Aufgabe: Notebooks](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.