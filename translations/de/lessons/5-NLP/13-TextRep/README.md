<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbd3f73e4139f030ecb2e20387d70fee",
  "translation_date": "2025-09-23T12:24:58+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "de"
}
-->
# Darstellung von Text als Tensoren

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Textklassifikation

Im ersten Teil dieses Abschnitts konzentrieren wir uns auf die Aufgabe der **Textklassifikation**. Wir verwenden das [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)-Dataset, das Nachrichtenartikel wie die folgenden enth√§lt:

* Kategorie: Wissenschaft/Technik
* Titel: Ky. Unternehmen erh√§lt Zuschuss zur Erforschung von Peptiden (AP)
* Text: AP - Ein Unternehmen, das von einem Chemieforscher der Universit√§t von Louisville gegr√ºndet wurde, erhielt einen Zuschuss zur Entwicklung...

Unser Ziel wird es sein, den Nachrichtenartikel basierend auf dem Text einer der Kategorien zuzuordnen.

## Darstellung von Text

Um Aufgaben der nat√ºrlichen Sprachverarbeitung (NLP) mit neuronalen Netzwerken zu l√∂sen, ben√∂tigen wir eine Methode, um Text als Tensoren darzustellen. Computer repr√§sentieren Textzeichen bereits als Zahlen, die mit Codierungen wie ASCII oder UTF-8 auf die Schriftarten auf Ihrem Bildschirm abgebildet werden.

<img alt="Bild zeigt ein Diagramm, das ein Zeichen mit einer ASCII- und Bin√§rdarstellung verkn√ºpft" src="images/ascii-character-map.png" width="50%"/>

> [Bildquelle](https://www.seobility.net/en/wiki/ASCII)

Als Menschen verstehen wir, was jeder Buchstabe **darstellt** und wie alle Zeichen zusammenkommen, um die W√∂rter eines Satzes zu bilden. Computer hingegen haben von sich aus kein solches Verst√§ndnis, und ein neuronales Netzwerk muss die Bedeutung w√§hrend des Trainings lernen.

Daher k√∂nnen wir verschiedene Ans√§tze verwenden, um Text darzustellen:

* **Zeichenbasierte Darstellung**, bei der wir Text darstellen, indem wir jedes Zeichen als Zahl behandeln. Angenommen, wir haben *C* verschiedene Zeichen in unserem Textkorpus, dann w√ºrde das Wort *Hello* durch einen 5x*C*-Tensor dargestellt. Jeder Buchstabe w√ºrde einer Spalte des Tensors in One-Hot-Codierung entsprechen.
* **Wortbasierte Darstellung**, bei der wir ein **Vokabular** aller W√∂rter in unserem Text erstellen und dann W√∂rter mit One-Hot-Codierung darstellen. Dieser Ansatz ist in gewisser Weise besser, da jeder Buchstabe f√ºr sich genommen nicht viel Bedeutung hat. Durch die Verwendung h√∂herer semantischer Konzepte ‚Äì W√∂rter ‚Äì vereinfachen wir die Aufgabe f√ºr das neuronale Netzwerk. Allerdings m√ºssen wir aufgrund der gro√üen W√∂rterbuchgr√∂√üe mit hochdimensionalen, sp√§rlichen Tensoren umgehen.

Unabh√§ngig von der Darstellung m√ºssen wir den Text zun√§chst in eine Sequenz von **Tokens** umwandeln, wobei ein Token entweder ein Zeichen, ein Wort oder manchmal sogar ein Teil eines Wortes sein kann. Anschlie√üend konvertieren wir das Token in eine Zahl, typischerweise mithilfe eines **Vokabulars**, und diese Zahl kann mithilfe von One-Hot-Codierung in ein neuronales Netzwerk eingespeist werden.

## N-Gramme

In der nat√ºrlichen Sprache kann die genaue Bedeutung von W√∂rtern nur im Kontext bestimmt werden. Zum Beispiel haben *neuronales Netzwerk* und *Fischernetz* v√∂llig unterschiedliche Bedeutungen. Eine M√∂glichkeit, dies zu ber√ºcksichtigen, besteht darin, unser Modell auf Wortpaaren aufzubauen und Wortpaare als separate Vokabular-Tokens zu betrachten. Auf diese Weise wird der Satz *Ich gehe gerne angeln* durch die folgende Sequenz von Tokens dargestellt: *Ich gehe*, *gehe gerne*, *gerne angeln*. Das Problem bei diesem Ansatz ist, dass die W√∂rterbuchgr√∂√üe erheblich w√§chst und Kombinationen wie *gerne angeln* und *gerne einkaufen* durch unterschiedliche Tokens dargestellt werden, die trotz des gleichen Verbs keine semantische √Ñhnlichkeit teilen.

In einigen F√§llen k√∂nnen wir auch die Verwendung von Tri-Grammen ‚Äì Kombinationen aus drei W√∂rtern ‚Äì in Betracht ziehen. Daher wird dieser Ansatz oft als **n-Gramme** bezeichnet. Es macht auch Sinn, n-Gramme mit zeichenbasierter Darstellung zu verwenden, wobei n-Gramme ungef√§hr verschiedenen Silben entsprechen.

## Bag-of-Words und TF/IDF

Bei Aufgaben wie der Textklassifikation m√ºssen wir in der Lage sein, Text durch einen festen Vektor darzustellen, den wir als Eingabe f√ºr den abschlie√üenden dichten Klassifikator verwenden. Eine der einfachsten M√∂glichkeiten, dies zu tun, besteht darin, alle individuellen Wortdarstellungen zu kombinieren, z. B. durch Addition. Wenn wir die One-Hot-Codierungen jedes Wortes addieren, erhalten wir einen Frequenzvektor, der zeigt, wie oft jedes Wort im Text vorkommt. Eine solche Darstellung von Text wird als **Bag-of-Words** (BoW) bezeichnet.

<img src="images/bow.png" width="90%"/>

> Bild vom Autor

Ein BoW zeigt im Wesentlichen, welche W√∂rter im Text vorkommen und in welchen Mengen, was tats√§chlich ein guter Hinweis darauf sein kann, worum es im Text geht. Beispielsweise enth√§lt ein Nachrichtenartikel √ºber Politik wahrscheinlich W√∂rter wie *Pr√§sident* und *Land*, w√§hrend eine wissenschaftliche Ver√∂ffentlichung Begriffe wie *Collider*, *entdeckt* usw. enthalten k√∂nnte. Daher k√∂nnen Wortfrequenzen in vielen F√§llen ein guter Indikator f√ºr den Textinhalt sein.

Das Problem bei BoW ist, dass bestimmte h√§ufig vorkommende W√∂rter wie *und*, *ist* usw. in den meisten Texten erscheinen und die h√∂chsten Frequenzen haben, wodurch die wirklich wichtigen W√∂rter √ºberdeckt werden. Wir k√∂nnen die Bedeutung dieser W√∂rter verringern, indem wir die H√§ufigkeit ber√ºcksichtigen, mit der W√∂rter in der gesamten Dokumentensammlung vorkommen. Dies ist die Hauptidee hinter dem TF/IDF-Ansatz, der in den zu dieser Lektion beigef√ºgten Notebooks ausf√ºhrlicher behandelt wird.

Keiner dieser Ans√§tze kann jedoch die **Semantik** des Textes vollst√§ndig ber√ºcksichtigen. Um dies zu erreichen, ben√∂tigen wir leistungsst√§rkere Modelle f√ºr neuronale Netzwerke, die wir sp√§ter in diesem Abschnitt besprechen werden.

## ‚úçÔ∏è √úbungen: Textdarstellung

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [Textdarstellung mit PyTorch](TextRepresentationPyTorch.ipynb)
* [Textdarstellung mit TensorFlow](TextRepresentationTF.ipynb)

## Fazit

Bisher haben wir Techniken untersucht, die Frequenzgewichtungen f√ºr verschiedene W√∂rter hinzuf√ºgen k√∂nnen. Sie sind jedoch nicht in der Lage, Bedeutung oder Reihenfolge darzustellen. Wie der ber√ºhmte Linguist J. R. Firth 1935 sagte: "Die vollst√§ndige Bedeutung eines Wortes ist immer kontextuell, und keine Untersuchung der Bedeutung au√üerhalb des Kontexts kann ernst genommen werden." Sp√§ter im Kurs werden wir lernen, wie man kontextuelle Informationen aus Text mithilfe von Sprachmodellen erfasst.

## üöÄ Herausforderung

Probieren Sie einige andere √úbungen mit Bag-of-Words und verschiedenen Datenmodellen aus. Lassen Sie sich vielleicht von diesem [Wettbewerb auf Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) inspirieren.

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## √úberpr√ºfung & Selbststudium

√úben Sie Ihre F√§higkeiten mit Text-Einbettungen und Bag-of-Words-Techniken auf [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Aufgabe: Notebooks](assignment.md)

---

