{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tréning RL na vyvažovanie Cartpole\n",
    "\n",
    "Tento notebook je súčasťou [kurikula AI pre začiatočníkov](http://aka.ms/ai-beginners). Bol inšpirovaný [oficiálnym tutoriálom PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) a [touto implementáciou Cartpole v PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "V tomto príklade použijeme RL na natrénovanie modelu, ktorý dokáže vyvažovať tyč na vozíku, ktorý sa môže pohybovať doľava a doprava na horizontálnej osi. Na simuláciu tyče použijeme prostredie [OpenAI Gym](https://www.gymlibrary.ml/).\n",
    "\n",
    "> **Note**: Kód tejto lekcie môžete spustiť lokálne (napr. vo Visual Studio Code), v takom prípade sa simulácia otvorí v novom okne. Pri spúšťaní kódu online môže byť potrebné vykonať niektoré úpravy kódu, ako je popísané [tu](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Začneme tým, že sa uistíme, že Gym je nainštalovaný:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz vytvorme prostredie CartPole a pozrime sa, ako s ním pracovať. Prostredie má nasledujúce vlastnosti:\n",
    "\n",
    "* **Akčný priestor** je množina možných akcií, ktoré môžeme vykonať v každom kroku simulácie  \n",
    "* **Pozorovací priestor** je priestor pozorovaní, ktoré môžeme vykonať  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pozrime sa, ako funguje simulácia. Nasledujúca slučka spúšťa simuláciu, až kým `env.step` nevráti ukončovací príznak `done`. Náhodne vyberieme akcie pomocou `env.action_space.sample()`, čo znamená, že experiment pravdepodobne veľmi rýchlo zlyhá (prostredie CartPole sa ukončí, keď rýchlosť CartPole, jeho poloha alebo uhol prekročia určité limity).\n",
    "\n",
    "> Simulácia sa otvorí v novom okne. Kód môžete spustiť viackrát a sledovať, ako sa správa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Môžete si všimnúť, že pozorovania obsahujú 4 čísla. Sú to:\n",
    "- Poloha vozíka\n",
    "- Rýchlosť vozíka\n",
    "- Uhol tyče\n",
    "- Rýchlosť otáčania tyče\n",
    "\n",
    "`rew` je odmena, ktorú dostávame pri každom kroku. V prostredí CartPole získavate 1 bod za každý simulačný krok a cieľom je maximalizovať celkovú odmenu, t. j. čas, počas ktorého dokáže CartPole udržať rovnováhu bez pádu.\n",
    "\n",
    "Počas posilňovaného učenia je naším cieľom natrénovať **politiku** $\\pi$, ktorá nám pre každý stav $s$ povie, akú akciu $a$ máme vykonať, teda v podstate $a = \\pi(s)$.\n",
    "\n",
    "Ak chcete pravdepodobnostné riešenie, môžete si politiku predstaviť ako návratnosť množiny pravdepodobností pre každú akciu, t. j. $\\pi(a|s)$ by znamenalo pravdepodobnosť, že by sme mali vykonať akciu $a$ v stave $s$.\n",
    "\n",
    "## Metóda Gradientu Politiky\n",
    "\n",
    "V najjednoduchšom RL algoritme, nazývanom **Gradient Politiky**, budeme trénovať neurónovú sieť, aby predpovedala ďalšiu akciu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budeme trénovať sieť vykonávaním mnohých experimentov a aktualizáciou našej siete po každom spustení. Definujme funkciu, ktorá vykoná experiment a vráti výsledky (tzv. **stopu**) - všetky stavy, akcie (a ich odporúčané pravdepodobnosti) a odmeny:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Môžete spustiť jednu epizódu s netrénovanou sieťou a pozorovať, že celková odmena (t.j. dĺžka epizódy) je veľmi nízka:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedným z náročných aspektov algoritmu gradientu politiky je použitie **diskontovaných odmien**. Myšlienka spočíva v tom, že vypočítame vektor celkových odmien v každom kroku hry a počas tohto procesu diskontujeme skoré odmeny pomocou nejakého koeficientu $gamma$. Výsledný vektor tiež normalizujeme, pretože ho použijeme ako váhu na ovplyvnenie nášho tréningu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz sa pustíme do samotného tréningu! Spustíme 300 epizód, pričom v každej epizóde vykonáme nasledujúce kroky:\n",
    "\n",
    "1. Spustíme experiment a zhromaždíme sledovanie.\n",
    "1. Vypočítame rozdiel (`gradients`) medzi vykonanými akciami a predpokladanými pravdepodobnosťami. Čím menší je rozdiel, tým viac si môžeme byť istí, že sme vykonali správnu akciu.\n",
    "1. Vypočítame diskontované odmeny a vynásobíme gradienty diskontovanými odmenami - tým zabezpečíme, že kroky s vyššími odmenami budú mať väčší vplyv na konečný výsledok než tie s nižšími odmenami.\n",
    "1. Očakávané cieľové akcie pre našu neurónovú sieť budú čiastočne odvodené z predpokladaných pravdepodobností počas behu a čiastočne z vypočítaných gradientov. Použijeme parameter `alpha` na určenie, do akej miery sa berú do úvahy gradienty a odmeny - toto sa nazýva *rýchlosť učenia* algoritmu posilneného učenia.\n",
    "1. Nakoniec trénujeme našu sieť na stavoch a očakávaných akciách a proces opakujeme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz spustime epizódu s vykresľovaním, aby sme videli výsledok:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dúfam, že vidíte, že tyč sa teraz dokáže celkom dobre vyvážiť!\n",
    "\n",
    "## Model Actor-Critic\n",
    "\n",
    "Model Actor-Critic je ďalším vývojom gradientov politiky, v ktorom vytvárame neurónovú sieť na učenie sa politiky aj odhadovaných odmien. Sieť bude mať dva výstupy (alebo ju môžete vnímať ako dve samostatné siete):\n",
    "* **Actor** odporučí akciu, ktorú treba vykonať, tým, že nám poskytne pravdepodobnostné rozdelenie stavov, podobne ako v modeli gradientu politiky.\n",
    "* **Critic** odhadne, aké by mohli byť odmeny z týchto akcií. Vracia celkové odhadované odmeny v budúcnosti pre daný stav.\n",
    "\n",
    "Definujme takýto model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budeme musieť mierne upraviť naše funkcie `discounted_rewards` a `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz spustíme hlavnú tréningovú slučku. Použijeme manuálny proces tréningu siete výpočtom správnych stratových funkcií a aktualizáciou parametrov siete:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhrnutie\n",
    "\n",
    "V tejto ukážke sme videli dva algoritmy posilneného učenia: jednoduchý policy gradient a sofistikovanejší actor-critic. Môžete si všimnúť, že tieto algoritmy pracujú s abstraktnými pojmami ako stav, akcia a odmena – preto ich možno aplikovať na veľmi odlišné prostredia.\n",
    "\n",
    "Posilnené učenie nám umožňuje naučiť sa najlepšiu stratégiu na vyriešenie problému len na základe sledovania konečnej odmeny. Skutočnosť, že nepotrebujeme označené dátové súbory, nám umožňuje opakovane simulovať a optimalizovať naše modely. Napriek tomu však v oblasti posilneného učenia existuje množstvo výziev, ktoré môžete objaviť, ak sa rozhodnete venovať viac pozornosti tejto zaujímavej oblasti umelej inteligencie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Upozornenie**:  \nTento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keď sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T22:58:11+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "sk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}