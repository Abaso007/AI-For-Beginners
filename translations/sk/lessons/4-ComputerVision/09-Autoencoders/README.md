<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1b8d9e1b3a6f1daa864b1ff3dfc3076d",
  "translation_date": "2025-09-23T14:05:01+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "sk"
}
-->
# Autoenkod√©ry

Pri tr√©novan√≠ CNN je jedn√Ωm z probl√©mov, ≈æe potrebujeme veƒæk√© mno≈æstvo oznaƒçen√Ωch d√°t. V pr√≠pade klasifik√°cie obr√°zkov mus√≠me obr√°zky rozdeli≈• do r√¥znych tried, ƒço si vy≈æaduje manu√°lnu pr√°cu.

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Av≈°ak, m√¥≈æeme chcie≈• pou≈æi≈• surov√© (neoznaƒçen√©) d√°ta na tr√©novanie CNN extraktorov vlastnost√≠, ƒço sa naz√Ωva **samoriaden√© uƒçenie**. Namiesto oznaƒçen√≠ pou≈æijeme tr√©novacie obr√°zky ako vstup aj v√Ωstup siete. Hlavn√° my≈°lienka **autoenkod√©ra** je, ≈æe budeme ma≈• **enkod√©rov√∫ sie≈•**, ktor√° konvertuje vstupn√Ω obr√°zok do urƒçit√©ho **latentn√©ho priestoru** (zvyƒçajne je to len vektor men≈°ej veƒækosti), a potom **dekod√©rov√∫ sie≈•**, ktorej cieƒæom bude rekon≈°truova≈• p√¥vodn√Ω obr√°zok.

> ‚úÖ [Autoenkod√©r](https://wikipedia.org/wiki/Autoencoder) je "typ umelej neur√≥novej siete, ktor√° sa pou≈æ√≠va na uƒçenie efekt√≠vneho k√≥dovania neoznaƒçen√Ωch d√°t."

Keƒè≈æe tr√©nujeme autoenkod√©r, aby zachytil ƒço najviac inform√°ci√≠ z p√¥vodn√©ho obr√°zku na presn√∫ rekon≈°trukciu, sie≈• sa sna≈æ√≠ n√°js≈• najlep≈°ie **zobrazenie** vstupn√Ωch obr√°zkov, aby zachytila ich v√Ωznam.

![Sch√©ma Autoenkod√©ra](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.sk.jpg)

> Obr√°zok z [blogu Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Scen√°re pou≈æitia autoenkod√©rov

Aj keƒè rekon≈°trukcia p√¥vodn√Ωch obr√°zkov nemus√≠ by≈• sama o sebe u≈æitoƒçn√°, existuje niekoƒæko scen√°rov, kde s√∫ autoenkod√©ry obzvl√°≈°≈• u≈æitoƒçn√©:

* **Zn√≠≈æenie dimenzie obr√°zkov na vizualiz√°ciu** alebo **tr√©novanie zobrazen√≠ obr√°zkov**. Autoenkod√©ry zvyƒçajne poskytuj√∫ lep≈°ie v√Ωsledky ako PCA, preto≈æe ber√∫ do √∫vahy priestorov√∫ povahu obr√°zkov a hierarchick√© vlastnosti.
* **Odstra≈àovanie ≈°umu**, t.j. odstr√°nenie ≈°umu z obr√°zku. Keƒè≈æe ≈°um obsahuje veƒæa nepotrebn√Ωch inform√°ci√≠, autoenkod√©r ho nedok√°≈æe v≈°etok zap√≠sa≈• do relat√≠vne mal√©ho latentn√©ho priestoru, a tak zachyt√≠ iba d√¥le≈æit√∫ ƒças≈• obr√°zku. Pri tr√©novan√≠ odstra≈àovaƒçov ≈°umu zaƒç√≠name s p√¥vodn√Ωmi obr√°zkami a pou≈æ√≠vame obr√°zky s umelo pridan√Ωm ≈°umom ako vstup pre autoenkod√©r.
* **Super-rezol√∫cia**, zv√Ω≈°enie rozl√≠≈°enia obr√°zkov. Zaƒç√≠name s obr√°zkami vo vysokom rozl√≠≈°en√≠ a pou≈æ√≠vame obr√°zky s ni≈æ≈°√≠m rozl√≠≈°en√≠m ako vstup pre autoenkod√©r.
* **Generat√≠vne modely**. Po natr√©novan√≠ autoenkod√©ra m√¥≈æeme dekod√©rov√∫ ƒças≈• pou≈æi≈• na vytv√°ranie nov√Ωch objektov zaƒç√≠naj√∫c od n√°hodn√Ωch latentn√Ωch vektorov.

## Variabiln√© autoenkod√©ry (VAE)

Tradiƒçn√© autoenkod√©ry zni≈æuj√∫ dimenziu vstupn√Ωch d√°t urƒçit√Ωm sp√¥sobom, priƒçom identifikuj√∫ d√¥le≈æit√© vlastnosti vstupn√Ωch obr√°zkov. Av≈°ak, latentn√© vektory ƒçasto ned√°vaj√∫ veƒæk√Ω zmysel. In√Ωmi slovami, ak vezmeme dataset MNIST ako pr√≠klad, zisti≈•, ktor√© ƒç√≠slice zodpovedaj√∫ r√¥znym latentn√Ωm vektorom, nie je jednoduch√° √∫loha, preto≈æe bl√≠zke latentn√© vektory nemusia nevyhnutne zodpoveda≈• rovnak√Ωm ƒç√≠sliciam.

Na druhej strane, na tr√©novanie *generat√≠vnych* modelov je lep≈°ie ma≈• urƒçit√∫ predstavu o latentnom priestore. T√°to my≈°lienka n√°s priv√°dza k **variabiln√©mu autoenkod√©ru** (VAE).

VAE je autoenkod√©r, ktor√Ω sa uƒç√≠ predpoveda≈• *≈°tatistick√© rozdelenie* latentn√Ωch parametrov, tzv. **latentn√© rozdelenie**. Napr√≠klad m√¥≈æeme chcie≈•, aby latentn√© vektory boli norm√°lne rozdelen√© s urƒçit√Ωm priemerom z<sub>mean</sub> a ≈°tandardnou odch√Ωlkou z<sub>sigma</sub> (oba, priemer aj ≈°tandardn√° odch√Ωlka, s√∫ vektory urƒçitej dimenzie d). Enkod√©r vo VAE sa uƒç√≠ predpoveda≈• tieto parametre, a potom dekod√©r vezme n√°hodn√Ω vektor z tohto rozdelenia na rekon≈°trukciu objektu.

Zhrnutie:

 * Zo vstupn√©ho vektora predpoved√°me `z_mean` a `z_log_sigma` (namiesto predpovedania samotnej ≈°tandardnej odch√Ωlky predpoved√°me jej logaritmus)
 * Vzorkujeme vektor `sample` z rozdelenia N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Dekod√©r sa sna≈æ√≠ dek√≥dova≈• p√¥vodn√Ω obr√°zok pomocou `sample` ako vstupn√©ho vektora

 <img src="images/vae.png" width="50%">

> Obr√°zok z [tohto blogov√©ho pr√≠spevku](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) od Isaaka Dykemana

Variabiln√© autoenkod√©ry pou≈æ√≠vaj√∫ komplexn√∫ funkciu straty, ktor√° pozost√°va z dvoch ƒçast√≠:

* **Rekon≈°trukƒçn√° strata** je funkcia straty, ktor√° ukazuje, ako bl√≠zko je rekon≈°truovan√Ω obr√°zok k cieƒæu (m√¥≈æe to by≈• Mean Squared Error, alebo MSE). Je to rovnak√° funkcia straty ako pri be≈æn√Ωch autoenkod√©roch.
* **KL strata**, ktor√° zabezpeƒçuje, ≈æe rozdelenie latentn√Ωch premenn√Ωch zost√°va bl√≠zko norm√°lnemu rozdeleniu. Je zalo≈æen√° na pojme [Kullback-Leiblerova divergencia](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - metrika na odhad, ako podobn√© s√∫ dve ≈°tatistick√© rozdelenia.

Jednou z d√¥le≈æit√Ωch v√Ωhod VAE je, ≈æe n√°m umo≈æ≈àuj√∫ generova≈• nov√© obr√°zky relat√≠vne jednoducho, preto≈æe vieme, z ktor√©ho rozdelenia vzorkova≈• latentn√© vektory. Napr√≠klad, ak tr√©nujeme VAE s 2D latentn√Ωm vektorom na MNIST, m√¥≈æeme potom meni≈• komponenty latentn√©ho vektora, aby sme z√≠skali r√¥zne ƒç√≠slice:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Obr√°zok od [Dmitry Soshnikov](http://soshnikov.com)

Pozorujte, ako sa obr√°zky prel√≠naj√∫, keƒè zaƒç√≠name z√≠skava≈• latentn√© vektory z r√¥znych ƒçast√≠ latentn√©ho priestoru parametrov. Tento priestor m√¥≈æeme tie≈æ vizualizova≈• v 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Obr√°zok od [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Cviƒçenia: Autoenkod√©ry

Z√≠skajte viac inform√°ci√≠ o autoenkod√©roch v t√Ωchto pr√≠slu≈°n√Ωch notebookoch:

* [Autoenkod√©ry v TensorFlow](AutoencodersTF.ipynb)
* [Autoenkod√©ry v PyTorch](AutoEncodersPyTorch.ipynb)

## Vlastnosti autoenkod√©rov

* **≈†pecifick√© pre d√°ta** - funguj√∫ dobre iba s typom obr√°zkov, na ktor√Ωch boli tr√©novan√©. Napr√≠klad, ak tr√©nujeme sie≈• na super-rezol√∫ciu na kvetoch, nebude dobre fungova≈• na portr√©toch. Je to preto, ≈æe sie≈• dok√°≈æe vytvori≈• obr√°zok vo vy≈°≈°om rozl√≠≈°en√≠ t√Ωm, ≈æe berie jemn√© detaily z vlastnost√≠ nauƒçen√Ωch z tr√©novacieho datasetu.
* **Stratov√©** - rekon≈°truovan√Ω obr√°zok nie je rovnak√Ω ako p√¥vodn√Ω obr√°zok. Povaha straty je definovan√° *funkciou straty* pou≈æitou poƒças tr√©novania.
* Funguje na **neoznaƒçen√Ωch d√°tach**

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Z√°ver

V tejto lekcii ste sa nauƒçili o r√¥znych typoch autoenkod√©rov dostupn√Ωch pre AI vedca. Nauƒçili ste sa, ako ich vytvori≈• a ako ich pou≈æi≈• na rekon≈°trukciu obr√°zkov. Tie≈æ ste sa nauƒçili o VAE a ako ich pou≈æi≈• na generovanie nov√Ωch obr√°zkov.

## üöÄ V√Ωzva

V tejto lekcii ste sa nauƒçili pou≈æ√≠va≈• autoenkod√©ry na obr√°zky. Ale m√¥≈æu by≈• pou≈æit√© aj na hudbu! Pozrite si projekt Magenta [MusicVAE](https://magenta.tensorflow.org/music-vae), ktor√Ω pou≈æ√≠va autoenkod√©ry na uƒçenie rekon≈°trukcie hudby. Vysk√∫≈°ajte niektor√© [experimenty](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) s touto kni≈ænicou a zistite, ƒço dok√°≈æete vytvori≈•.

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Prehƒæad a samostatn√© ≈°t√∫dium

Pre referenciu si preƒç√≠tajte viac o autoenkod√©roch v t√Ωchto zdrojoch:

* [Budovanie autoenkod√©rov v Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Blogov√Ω pr√≠spevok na NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Vysvetlenie variabiln√Ωch autoenkod√©rov](https://kvfrans.com/variational-autoencoders-explained/)
* [Podmienen√© variabiln√© autoenkod√©ry](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Zadanie

Na konci [tohto notebooku s TensorFlow](AutoencodersTF.ipynb) n√°jdete '√∫lohu' - pou≈æite ju ako svoje zadanie.

---

