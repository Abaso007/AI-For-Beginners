<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-25T22:39:12+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "sk"
}
-->
# Generat√≠vne adversari√°lne siete

V predch√°dzaj√∫cej ƒçasti sme sa nauƒçili o **generat√≠vnych modeloch**: modeloch, ktor√© dok√°≈æu generova≈• nov√© obr√°zky podobn√© t√Ωm v tr√©ningovej mno≈æine. VAE bol dobr√Ωm pr√≠kladom generat√≠vneho modelu.

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Ak sa v≈°ak pok√∫sime generova≈• nieƒço naozaj zmyslupln√©, napr√≠klad maƒæbu v rozumnej kvalite, pomocou VAE, zist√≠me, ≈æe tr√©ning sa nekonverguje dobre. Pre tento pr√≠pad pou≈æitia by sme sa mali nauƒçi≈• o inej architekt√∫re ≈°pecificky zameranej na generat√≠vne modely - **Generat√≠vne adversari√°lne siete**, alebo GANy.

Hlavn√° my≈°lienka GANov spoƒç√≠va v tom, ≈æe m√°me dve neur√≥nov√© siete, ktor√© sa bud√∫ tr√©nova≈• proti sebe:

<img src="images/gan_architecture.png" width="70%"/>

> Obr√°zok od [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Mal√Ω slovn√≠k:
> * **Gener√°tor** je sie≈•, ktor√° prij√≠ma n√°hodn√Ω vektor a ako v√Ωsledok produkuje obr√°zok.
> * **Diskrimin√°tor** je sie≈•, ktor√° prij√≠ma obr√°zok a mala by urƒçi≈•, ƒçi ide o skutoƒçn√Ω obr√°zok (z tr√©ningovej mno≈æiny), alebo ƒçi bol vygenerovan√Ω gener√°torom. V podstate ide o klasifik√°tor obr√°zkov.

### Diskrimin√°tor

Architekt√∫ra diskrimin√°tora sa nel√≠≈°i od be≈ænej siete na klasifik√°ciu obr√°zkov. V najjednoduch≈°om pr√≠pade m√¥≈æe √≠s≈• o plne prepojen√Ω klasifik√°tor, ale pravdepodobne to bude [konvoluƒçn√° sie≈•](../07-ConvNets/README.md).

> ‚úÖ GAN zalo≈æen√Ω na konvoluƒçn√Ωch sie≈•ach sa naz√Ωva [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)

Diskrimin√°tor CNN pozost√°va z nasleduj√∫cich vrstiev: niekoƒæko konvol√∫ci√≠ + poolingov (s klesaj√∫cou priestorovou veƒækos≈•ou) a jednej alebo viacer√Ωch plne prepojen√Ωch vrstiev na z√≠skanie "vektorov vlastnost√≠", a nakoniec bin√°rneho klasifik√°tora.

> ‚úÖ 'Pooling' v tomto kontexte je technika, ktor√° zmen≈°uje veƒækos≈• obr√°zka. "Poolingov√© vrstvy zni≈æuj√∫ rozmery d√°t kombinovan√≠m v√Ωstupov klastrov neur√≥nov v jednej vrstve do jedn√©ho neur√≥nu v nasleduj√∫cej vrstve." - [zdroj](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Gener√°tor

Gener√°tor je o nieƒço zlo≈æitej≈°√≠. M√¥≈æete si ho predstavi≈• ako obr√°ten√Ω diskrimin√°tor. Zaƒç√≠na sa latentn√Ωm vektorom (namiesto vektora vlastnost√≠), ktor√Ω m√° plne prepojen√∫ vrstvu na jeho konverziu do po≈æadovanej veƒækosti/tvaru, nasledovan√∫ dekonvol√∫ciami + zv√§ƒç≈°ovan√≠m. Toto je podobn√© *dekod√©ru* v [autoenk√≥deri](../09-Autoencoders/README.md).

> ‚úÖ Preto≈æe konvoluƒçn√° vrstva je implementovan√° ako line√°rny filter prech√°dzaj√∫ci obr√°zkom, dekonvol√∫cia je v podstate podobn√° konvol√∫cii a m√¥≈æe by≈• implementovan√° pomocou rovnakej logiky vrstvy.

<img src="images/gan_arch_detail.png" width="70%"/>

> Obr√°zok od [Dmitry Soshnikov](http://soshnikov.com)

### Tr√©ning GANov

GANy sa naz√Ωvaj√∫ **adversari√°lne**, preto≈æe medzi gener√°torom a diskrimin√°torom prebieha neust√°la s√∫≈•a≈æ. Poƒças tejto s√∫≈•a≈æe sa obidva modely zlep≈°uj√∫, ƒç√≠m sa sie≈• uƒç√≠ vytv√°ra≈• st√°le lep≈°ie obr√°zky.

Tr√©ning prebieha v dvoch f√°zach:

* **Tr√©ning diskrimin√°tora**. T√°to √∫loha je pomerne jednoduch√°: vygenerujeme d√°vku obr√°zkov pomocou gener√°tora, oznaƒç√≠me ich ako 0, ƒço znamen√° falo≈°n√Ω obr√°zok, a vezmeme d√°vku obr√°zkov z tr√©ningovej mno≈æiny (s oznaƒçen√≠m 1, skutoƒçn√Ω obr√°zok). Z√≠skame nejak√∫ *stratu diskrimin√°tora* a vykon√°me sp√§tn√© ≈°√≠renie.
* **Tr√©ning gener√°tora**. Toto je o nieƒço zlo≈æitej≈°ie, preto≈æe nepozn√°me oƒçak√°van√Ω v√Ωstup gener√°tora priamo. Vezmeme cel√∫ GAN sie≈• pozost√°vaj√∫cu z gener√°tora nasledovan√©ho diskrimin√°torom, nak≈ïmime ju n√°hodn√Ωmi vektormi a oƒçak√°vame, ≈æe v√Ωsledok bude 1 (zodpovedaj√∫ci skutoƒçn√Ωm obr√°zkom). Potom zmraz√≠me parametre diskrimin√°tora (nechceme, aby sa v tomto kroku tr√©noval) a vykon√°me sp√§tn√© ≈°√≠renie.

Poƒças tohto procesu straty gener√°tora a diskrimin√°tora v√Ωznamne neklesaj√∫. V ide√°lnom pr√≠pade by mali oscilova≈•, ƒço zodpoved√° zlep≈°ovaniu v√Ωkonu oboch siet√≠.

## ‚úçÔ∏è Cviƒçenia: GANy

* [GAN Notebook v TensorFlow/Keras](../../../../../lessons/4-ComputerVision/10-GANs/GANTF.ipynb)
* [GAN Notebook v PyTorch](../../../../../lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb)

### Probl√©my s tr√©ningom GANov

GANy s√∫ zn√°me t√Ωm, ≈æe s√∫ obzvl√°≈°≈• n√°roƒçn√© na tr√©ning. Tu je niekoƒæko probl√©mov:

* **Kolaps m√≥du**. Tento term√≠n znamen√°, ≈æe gener√°tor sa nauƒç√≠ vytv√°ra≈• jeden √∫spe≈°n√Ω obr√°zok, ktor√Ω oklame diskrimin√°tor, a nie r√¥zne obr√°zky.
* **Citlivos≈• na hyperparametre**. ƒåasto sa st√°va, ≈æe GAN nekonverguje v√¥bec, a potom n√°hle zn√≠≈æenie r√Ωchlosti uƒçenia vedie ku konvergencii.
* Udr≈æiavanie **rovnov√°hy** medzi gener√°torom a diskrimin√°torom. V mnoh√Ωch pr√≠padoch m√¥≈æe strata diskrimin√°tora klesn√∫≈• na nulu pomerne r√Ωchlo, ƒço sp√¥sob√≠, ≈æe gener√°tor u≈æ nebude schopn√Ω ƒèalej tr√©nova≈•. Na prekonanie tohto probl√©mu m√¥≈æeme sk√∫si≈• nastavi≈• r√¥zne r√Ωchlosti uƒçenia pre gener√°tor a diskrimin√°tor, alebo preskoƒçi≈• tr√©ning diskrimin√°tora, ak je strata u≈æ pr√≠li≈° n√≠zka.
* Tr√©ning pre **vysok√© rozl√≠≈°enie**. Tento probl√©m, podobne ako pri autoenk√≥deroch, vznik√°, preto≈æe rekon≈°trukcia pr√≠li≈° mnoh√Ωch vrstiev konvoluƒçnej siete vedie k artefaktom. Tento probl√©m sa zvyƒçajne rie≈°i tzv. **progres√≠vnym rastom**, keƒè sa najprv niekoƒæko vrstiev tr√©nuje na obr√°zkoch s n√≠zkym rozl√≠≈°en√≠m, a potom sa vrstvy "odomkn√∫" alebo pridaj√∫. ƒéal≈°√≠m rie≈°en√≠m by bolo pridanie extra spojen√≠ medzi vrstvami a tr√©ning viacer√Ωch rozl√≠≈°en√≠ naraz - podrobnosti n√°jdete v tomto [Multi-Scale Gradient GANs ƒçl√°nku](https://arxiv.org/abs/1903.06048).

## Prenos ≈°t√Ωlu

GANy s√∫ skvel√Ωm sp√¥sobom, ako generova≈• umeleck√© obr√°zky. ƒéal≈°ou zauj√≠mavou technikou je tzv. **prenos ≈°t√Ωlu**, ktor√Ω vezme jeden **obsahov√Ω obr√°zok** a prekresl√≠ ho v inom ≈°t√Ωle, aplikuj√∫c filtre zo **≈°t√Ωlov√©ho obr√°zka**.

Ako to funguje:
* Zaƒçneme s n√°hodn√Ωm ≈°umov√Ωm obr√°zkom (alebo s obsahov√Ωm obr√°zkom, ale pre pochopenie je jednoduch≈°ie zaƒça≈• s n√°hodn√Ωm ≈°umom).
* Na≈°√≠m cieƒæom bude vytvori≈• tak√Ω obr√°zok, ktor√Ω bude bl√≠zky obsahov√©mu obr√°zku aj ≈°t√Ωlov√©mu obr√°zku. To sa urƒç√≠ pomocou dvoch funkci√≠ straty:
   - **Strata obsahu** sa vypoƒç√≠ta na z√°klade vlastnost√≠ extrahovan√Ωch CNN na niektor√Ωch vrstv√°ch z aktu√°lneho obr√°zka a obsahov√©ho obr√°zka.
   - **Strata ≈°t√Ωlu** sa vypoƒç√≠ta medzi aktu√°lnym obr√°zkom a ≈°t√Ωlov√Ωm obr√°zkom ≈°ikovn√Ωm sp√¥sobom pomocou Gramov√Ωch mat√≠c (viac podrobnost√≠ v [pr√≠kladovom notebooku](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)).
* Na vyhladenie obr√°zka a odstr√°nenie ≈°umu zav√°dzame aj **stratu vari√°cie**, ktor√° poƒç√≠ta priemern√∫ vzdialenos≈• medzi susedn√Ωmi pixelmi.
* Hlavn√° optimalizaƒçn√° sluƒçka upravuje aktu√°lny obr√°zok pomocou gradientn√©ho zostupu (alebo in√©ho optimalizaƒçn√©ho algoritmu) na minimaliz√°ciu celkovej straty, ktor√° je v√°≈æen√Ωm s√∫ƒçtom v≈°etk√Ωch troch str√°t.

## ‚úçÔ∏è Pr√≠klad: [Prenos ≈°t√Ωlu](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Zhrnutie

V tejto lekcii ste sa nauƒçili o GANoch a ako ich tr√©nova≈•. Tie≈æ ste sa dozvedeli o ≈°peci√°lnych v√Ωzvach, ktor√Ωm tento typ neur√≥nov√Ωch siet√≠ ƒçel√≠, a o niektor√Ωch strat√©gi√°ch, ako ich prekona≈•.

## üöÄ V√Ωzva

Prejdite si [notebook o prenose ≈°t√Ωlu](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) s pou≈æit√≠m vlastn√Ωch obr√°zkov.

## Prehƒæad a samo≈°t√∫dium

Pre referenciu si preƒç√≠tajte viac o GANoch v t√Ωchto zdrojoch:

* Marco Pasini, [10 lekci√≠, ktor√© som sa nauƒçil pri tr√©ningu GANov za jeden rok](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), *de facto* architekt√∫ra GANov na zv√°≈æenie
* [Vytv√°ranie generat√≠vneho umenia pomocou GANov na Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Zadanie

Znovu si prejdite jeden z dvoch notebookov spojen√Ωch s touto lekciou a pretr√©nujte GAN na vlastn√Ωch obr√°zkoch. ƒåo dok√°≈æete vytvori≈•?

**Upozornenie**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keƒè sa sna≈æ√≠me o presnos≈•, pros√≠m, berte na vedomie, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho rodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nie sme zodpovedn√≠ za ak√©koƒævek nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.