{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanizmy pozornosti a transformery\n",
    "\n",
    "Jednou z hlavných nevýhod rekurentných sietí je, že všetky slová v sekvencii majú rovnaký vplyv na výsledok. To spôsobuje suboptimálny výkon pri štandardných modeloch LSTM encoder-decoder pre úlohy sekvencie na sekvenciu, ako je rozpoznávanie pomenovaných entít alebo strojový preklad. V skutočnosti majú konkrétne slová v vstupnej sekvencii často väčší vplyv na výstupy ako ostatné.\n",
    "\n",
    "Zvážte model sekvencie na sekvenciu, napríklad strojový preklad. Ten je implementovaný pomocou dvoch rekurentných sietí, kde jedna sieť (**encoder**) zhrnie vstupnú sekvenciu do skrytého stavu a druhá (**decoder**) rozvinie tento skrytý stav do preloženého výsledku. Problém s týmto prístupom je, že konečný stav siete má problém zapamätať si začiatok vety, čo vedie k nízkej kvalite modelu pri dlhých vetách.\n",
    "\n",
    "**Mechanizmy pozornosti** poskytujú spôsob, ako vážiť kontextuálny vplyv každého vstupného vektora na každú predikciu výstupu RNN. Implementuje sa to vytvorením skratiek medzi medzistavmi vstupnej RNN a výstupnej RNN. Týmto spôsobom, pri generovaní výstupného symbolu $y_t$, zohľadníme všetky skryté stavy vstupu $h_i$, s rôznymi váhovými koeficientmi $\\alpha_{t,i}$.\n",
    "\n",
    "![Obrázok zobrazujúci model encoder/decoder s vrstvou aditívnej pozornosti](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.sk.png)\n",
    "*Model encoder-decoder s mechanizmom aditívnej pozornosti podľa [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citované z [tohto blogového príspevku](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matica pozornosti $\\{\\alpha_{i,j}\\}$ predstavuje mieru, do akej určité vstupné slová ovplyvňujú generovanie daného slova vo výstupnej sekvencii. Nižšie je príklad takejto matice:\n",
    "\n",
    "![Obrázok zobrazujúci vzorové zarovnanie nájdené RNNsearch-50, prevzaté z Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.sk.png)\n",
    "\n",
    "*Obrázok prevzatý z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Obr.3)*\n",
    "\n",
    "Mechanizmy pozornosti sú zodpovedné za veľkú časť súčasného alebo takmer súčasného stavu umenia v spracovaní prirodzeného jazyka. Pridanie pozornosti však výrazne zvyšuje počet parametrov modelu, čo viedlo k problémom so škálovaním RNN. Kľúčovým obmedzením škálovania RNN je, že rekurentná povaha modelov sťažuje dávkovanie a paralelizáciu tréningu. V RNN musí byť každý prvok sekvencie spracovaný v sekvenčnom poradí, čo znamená, že ho nemožno ľahko paralelizovať.\n",
    "\n",
    "Prijatie mechanizmov pozornosti v kombinácii s týmto obmedzením viedlo k vytvoreniu dnes už špičkových modelov Transformer, ktoré poznáme a používame, od BERT po OpenGPT3.\n",
    "\n",
    "## Modely Transformer\n",
    "\n",
    "Namiesto prenášania kontextu každej predchádzajúcej predikcie do ďalšieho kroku hodnotenia používajú **modely Transformer** **pozíciové kódovania** a **pozornosť**, aby zachytili kontext daného vstupu v rámci poskytnutého okna textu. Nižšie uvedený obrázok ukazuje, ako pozíciové kódovania s pozornosťou dokážu zachytiť kontext v rámci daného okna.\n",
    "\n",
    "![Animovaný GIF zobrazujúci, ako sa hodnotenia vykonávajú v modeloch Transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Keďže každá vstupná pozícia je mapovaná nezávisle na každú výstupnú pozíciu, transformery dokážu lepšie paralelizovať ako RNN, čo umožňuje oveľa väčšie a expresívnejšie jazykové modely. Každá hlava pozornosti môže byť použitá na učenie rôznych vzťahov medzi slovami, čo zlepšuje následné úlohy spracovania prirodzeného jazyka.\n",
    "\n",
    "## Vytvorenie jednoduchého modelu Transformer\n",
    "\n",
    "Keras neobsahuje zabudovanú vrstvu Transformer, ale môžeme si ju vytvoriť sami. Ako predtým, zameriame sa na klasifikáciu textu z datasetu AG News, ale stojí za zmienku, že modely Transformer dosahujú najlepšie výsledky pri náročnejších úlohách NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nové vrstvy v Keras by mali byť podtriedou triedy `Layer` a implementovať metódu `call`. Začnime s vrstvou **Positional Embedding**. Použijeme [niektorý kód z oficiálnej dokumentácie Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Predpokladáme, že všetky vstupné sekvencie doplníme na dĺžku `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Táto vrstva pozostáva z dvoch vrstiev `Embedding`: na vkladanie tokenov (spôsobom, o ktorom sme už hovorili) a na vkladanie pozícií tokenov. Pozície tokenov sa vytvárajú ako postupnosť prirodzených čísel od 0 po `maxlen` pomocou `tf.range`, a potom sa prechádzajú cez vrstvu vkladania. Dva výsledné vektorové vklady sa následne sčítajú, čím sa vytvorí pozične vložená reprezentácia vstupu s tvarom `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Teraz implementujme blok transformátora. Bude prijímať výstup predtým definovanej vrstvy vkladania:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer aplikuje `MultiHeadAttention` na vstup s pozičným kódovaním, aby vytvoril vektor pozornosti s rozmermi `maxlen`$\\times$`embed_dim`, ktorý sa následne zmieša so vstupom a normalizuje pomocou `LayerNormalization`.\n",
    "\n",
    "> **Poznámka**: `LayerNormalization` je podobná `BatchNormalization`, o ktorej sme hovorili v časti *Počítačové videnie* tohto vzdelávacieho kurzu, ale normalizuje výstupy predchádzajúcej vrstvy pre každú vzorku tréningu nezávisle, aby ich priviedla do rozsahu [-1..1].\n",
    "\n",
    "Výstup tejto vrstvy sa potom prechádza cez sieť `Dense` (v našom prípade - dvojvrstvový perceptron) a výsledok sa pridáva k finálnemu výstupu (ktorý sa opäť podrobuje normalizácii).\n",
    "\n",
    "Teraz sme pripravení definovať kompletný model transformera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformer Modely\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) je veľmi veľká viacvrstvová transformerová sieť s 12 vrstvami pre *BERT-base* a 24 vrstvami pre *BERT-large*. Model je najskôr predtrénovaný na veľkom korpuse textových dát (WikiPedia + knihy) pomocou nesupervidovaného učenia (predpovedanie maskovaných slov vo vete). Počas predtrénovania model absorbuje významnú úroveň porozumenia jazyka, ktorú je možné následne využiť s inými datasetmi pomocou jemného doladenia. Tento proces sa nazýva **transferové učenie**.\n",
    "\n",
    "![obrázok z http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.sk.png)\n",
    "\n",
    "Existuje mnoho variácií transformerových architektúr vrátane BERT, DistilBERT, BigBird, OpenGPT3 a ďalších, ktoré je možné jemne doladiť.\n",
    "\n",
    "Pozrime sa, ako môžeme použiť predtrénovaný model BERT na riešenie nášho tradičného problému klasifikácie sekvencií. Požičajme si myšlienku a časť kódu z [oficiálnej dokumentácie](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Na načítanie predtrénovaných modelov použijeme **Tensorflow hub**. Najskôr načítajme BERT-špecifický vektorizér:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je dôležité, aby ste použili rovnaký vektorizačný nástroj, aký bol použitý pri pôvodnom tréningu siete. BERT vektorizačný nástroj vracia tri komponenty:\n",
    "* `input_word_ids`, čo je sekvencia čísel tokenov pre vstupnú vetu\n",
    "* `input_mask`, ktorá ukazuje, ktorá časť sekvencie obsahuje skutočný vstup a ktorá je výplňou. Je podobná maske vytvorenej vrstvou `Masking`\n",
    "* `input_type_ids` sa používa pri úlohách jazykového modelovania a umožňuje špecifikovať dve vstupné vety v jednej sekvencii.\n",
    "\n",
    "Potom môžeme inicializovať BERT extraktor vlastností:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takže, BERT vrstva vracia niekoľko užitočných výsledkov:\n",
    "* `pooled_output` je výsledkom spriemerovania všetkých tokenov v sekvencii. Môžete ho vnímať ako inteligentné sémantické zapuzdrenie celej siete. Je ekvivalentný výstupu vrstvy `GlobalAveragePooling1D` v našom predchádzajúcom modeli.\n",
    "* `sequence_output` je výstup poslednej transformerovej vrstvy (zodpovedá výstupu `TransformerBlock` v našom vyššie uvedenom modeli).\n",
    "* `encoder_outputs` sú výstupy všetkých transformerových vrstiev. Keďže sme načítali 4-vrstvový BERT model (ako pravdepodobne tušíte z názvu, ktorý obsahuje `4_H`), obsahuje 4 tenzory. Posledný z nich je rovnaký ako `sequence_output`.\n",
    "\n",
    "Teraz definujeme end-to-end klasifikačný model. Použijeme *funkcionálnu definíciu modelu*, kde definujeme vstup modelu a potom poskytneme sériu výrazov na výpočet jeho výstupu. Tiež nastavíme váhy BERT modelu ako netrénovateľné a budeme trénovať iba finálny klasifikátor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napriek tomu, že je tu len málo trénovateľných parametrov, proces je pomerne pomalý, pretože extraktor vlastností BERT je výpočtovo náročný. Zdá sa, že sme nedosiahli rozumnú presnosť, buď kvôli nedostatku tréningu, alebo nedostatku parametrov modelu.\n",
    "\n",
    "Skúsme odomknúť váhy BERT a trénovať ich tiež. To si vyžaduje veľmi malú rýchlosť učenia a tiež opatrnejšiu stratégiu tréningu s **warmup**, použitím optimalizátora **AdamW**. Na vytvorenie optimalizátora použijeme balík `tf-models-official`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako môžete vidieť, tréning prebieha pomerne pomaly - ale možno budete chcieť experimentovať a trénovať model niekoľko epoch (5-10) a zistiť, či dosiahnete lepší výsledok v porovnaní s prístupmi, ktoré sme použili predtým.\n",
    "\n",
    "## Knižnica Huggingface Transformers\n",
    "\n",
    "Ďalším veľmi bežným (a o niečo jednoduchším) spôsobom, ako používať modely Transformer, je [balík HuggingFace](https://github.com/huggingface/), ktorý poskytuje jednoduché stavebné bloky pre rôzne NLP úlohy. Je dostupný pre Tensorflow aj PyTorch, ďalší veľmi populárny rámec pre neurónové siete.\n",
    "\n",
    "> **Poznámka**: Ak vás nezaujíma, ako funguje knižnica Transformers - môžete preskočiť na koniec tohto notebooku, pretože neuvidíte nič podstatne odlišné od toho, čo sme robili vyššie. Budeme opakovať rovnaké kroky tréningu modelu BERT pomocou inej knižnice a podstatne väčšieho modelu. Proces preto zahŕňa pomerne dlhý tréning, takže možno budete chcieť len prejsť kódom.\n",
    "\n",
    "Pozrime sa, ako môžeme náš problém vyriešiť pomocou [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prvá vec, ktorú musíme urobiť, je vybrať model, ktorý budeme používať. Okrem niektorých vstavaných modelov obsahuje Huggingface [online úložisko modelov](https://huggingface.co/models), kde môžete nájsť oveľa viac predtrénovaných modelov od komunity. Všetky tieto modely je možné načítať a používať len poskytnutím názvu modelu. Všetky potrebné binárne súbory pre model sa automaticky stiahnu.\n",
    "\n",
    "V určitých prípadoch budete potrebovať načítať svoje vlastné modely, v takom prípade môžete špecifikovať adresár, ktorý obsahuje všetky relevantné súbory, vrátane parametrov pre tokenizer, súboru `config.json` s parametrami modelu, binárnych váh atď.\n",
    "\n",
    "Z názvu modelu môžeme inicializovať model aj tokenizer. Začnime s tokenizerom:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objekt `tokenizer` obsahuje funkciu `encode`, ktorú je možné priamo použiť na kódovanie textu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Môžeme tiež použiť tokenizér na zakódovanie sekvencie spôsobom vhodným na odovzdanie modelu, t.j. vrátane polí `token_ids`, `input_mask` atď. Môžeme tiež špecifikovať, že chceme tenzory Tensorflow poskytnutím argumentu `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V našom prípade budeme používať predtrénovaný BERT model nazývaný `bert-base-uncased`. *Uncased* znamená, že model je necitlivý na veľkosť písmen.\n",
    "\n",
    "Pri trénovaní modelu musíme poskytnúť tokenizovanú sekvenciu ako vstup, a preto navrhneme dátový spracovateľský pipeline. Keďže `tokenizer.encode` je Python funkcia, použijeme rovnaký prístup ako v poslednej jednotke, kde ju voláme pomocou `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz môžeme načítať skutočný model pomocou balíka `BertForSequenceClassification`. To zabezpečuje, že náš model už má požadovanú architektúru pre klasifikáciu, vrátane konečného klasifikátora. Uvidíte varovnú správu, ktorá uvádza, že váhy konečného klasifikátora nie sú inicializované a model by vyžadoval predtréning - to je úplne v poriadku, pretože presne to sa chystáme urobiť!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako môžete vidieť z `summary()`, model obsahuje takmer 110 miliónov parametrov! Predpokladáme, že ak chceme jednoduchú klasifikačnú úlohu na relatívne malom datasete, nechceme trénovať základnú vrstvu BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz sme pripravení začať tréning!\n",
    "\n",
    "> **Poznámka**: Tréning plnohodnotného BERT modelu môže byť veľmi časovo náročný! Preto ho budeme trénovať iba na prvých 32 dávkach. Toto slúži len na ukážku, ako je nastavený tréning modelu. Ak máte záujem vyskúšať plnohodnotný tréning - jednoducho odstráňte parametre `steps_per_epoch` a `validation_steps` a pripravte sa čakať!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ak zvýšite počet iterácií a počkáte dostatočne dlho, a trénujete počas viacerých epoch, môžete očakávať, že klasifikácia pomocou BERT nám poskytne najlepšiu presnosť! Je to preto, že BERT už veľmi dobre rozumie štruktúre jazyka, a my potrebujeme len doladiť záverečný klasifikátor. Avšak, keďže BERT je veľký model, celý proces trénovania trvá dlho a vyžaduje značný výpočtový výkon! (GPU, a ideálne viac ako jednu).\n",
    "\n",
    "> **Note:** V našom príklade sme používali jeden z najmenších predtrénovaných modelov BERT. Existujú väčšie modely, ktoré pravdepodobne prinesú lepšie výsledky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hlavné body\n",
    "\n",
    "V tejto jednotke sme sa oboznámili s najnovšími modelovými architektúrami založenými na **transformeroch**. Aplikovali sme ich na našu úlohu klasifikácie textu, ale podobne môžu byť BERT modely použité na extrakciu entít, odpovedanie na otázky a iné NLP úlohy.\n",
    "\n",
    "Transformačné modely predstavujú súčasný špičkový stav v NLP a vo väčšine prípadov by mali byť prvým riešením, s ktorým začnete experimentovať pri implementácii vlastných NLP riešení. Avšak pochopenie základných princípov rekurentných neurónových sietí, o ktorých sme hovorili v tomto module, je mimoriadne dôležité, ak chcete vytvárať pokročilé neurónové modely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Upozornenie**:  \nTento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho pôvodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-30T00:47:18+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "sk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}