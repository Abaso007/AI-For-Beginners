<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-25T21:55:48+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "sk"
}
-->
# Jazykov√© modelovanie

S√©mantick√© vektory, ako Word2Vec a GloVe, s√∫ v skutoƒçnosti prv√Ωm krokom k **jazykov√©mu modelovaniu** ‚Äì vytv√°raniu modelov, ktor√© nejako *rozumej√∫* (alebo *reprezentuj√∫*) podstatu jazyka.

## [Kv√≠z pred predn√°≈°kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

Hlavnou my≈°lienkou jazykov√©ho modelovania je ich tr√©novanie na neoznaƒçen√Ωch d√°tach v nesupervidovanom re≈æime. To je d√¥le≈æit√©, preto≈æe m√°me k dispoz√≠cii obrovsk√© mno≈æstvo neoznaƒçen√©ho textu, zatiaƒæ ƒço mno≈æstvo oznaƒçen√©ho textu je v≈ædy obmedzen√© √∫sil√≠m, ktor√© m√¥≈æeme venova≈• jeho oznaƒçovaniu. Najƒçastej≈°ie m√¥≈æeme vytvori≈• jazykov√© modely, ktor√© dok√°≈æu **predpoveda≈• ch√Ωbaj√∫ce slov√°** v texte, preto≈æe je jednoduch√© n√°hodne vynecha≈• slovo v texte a pou≈æi≈• ho ako tr√©ningov√Ω vzor.

## Tr√©novanie vektorov

V na≈°ich predch√°dzaj√∫cich pr√≠kladoch sme pou≈æ√≠vali predtr√©novan√© s√©mantick√© vektory, ale je zauj√≠mav√© vidie≈•, ako sa tieto vektory daj√∫ tr√©nova≈•. Existuje niekoƒæko mo≈æn√Ωch pr√≠stupov, ktor√© sa daj√∫ pou≈æi≈•:

* **Jazykov√© modelovanie pomocou N-Gramov**, kde predpoved√°me token na z√°klade N predch√°dzaj√∫cich tokenov (N-gram).
* **Continuous Bag-of-Words** (CBoW), kde predpoved√°me stredn√Ω token $W_0$ v sekvencii tokenov $W_{-N}$, ..., $W_N$.
* **Skip-gram**, kde predpoved√°me mno≈æinu susedn√Ωch tokenov {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} zo stredn√©ho tokenu $W_0$.

![obr√°zok z ƒçl√°nku o konverzii slov na vektory](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sk.png)

> Obr√°zok z [tohto ƒçl√°nku](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Pr√≠kladov√© notebooky: Tr√©novanie CBoW modelu

Pokraƒçujte vo svojom uƒçen√≠ pomocou nasleduj√∫cich notebookov:

* [Tr√©novanie CBoW Word2Vec s TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Tr√©novanie CBoW Word2Vec s PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Z√°ver

V predch√°dzaj√∫cej lekcii sme videli, ≈æe vektory slov funguj√∫ ako k√∫zlo! Teraz vieme, ≈æe tr√©novanie vektorov slov nie je veƒæmi zlo≈æit√° √∫loha, a mali by sme by≈• schopn√≠ tr√©nova≈• vlastn√© vektory slov pre text ≈°pecifick√Ω pre dan√∫ oblas≈•, ak to bude potrebn√©.

## [Kv√≠z po predn√°≈°ke](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Prehƒæad a samostatn√© ≈°t√∫dium

* [Ofici√°lny PyTorch tutori√°l o jazykovom modelovan√≠](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Ofici√°lny TensorFlow tutori√°l o tr√©novan√≠ Word2Vec modelu](https://www.TensorFlow.org/tutorials/text/word2vec).
* Pou≈æitie frameworku **gensim** na tr√©novanie najbe≈ænej≈°ie pou≈æ√≠van√Ωch vektorov v niekoƒæk√Ωch riadkoch k√≥du je pop√≠san√© [v tejto dokument√°cii](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [√öloha: Tr√©novanie Skip-Gram modelu](lab/README.md)

V laborat√≥riu v√°s vyz√Ωvame, aby ste upravili k√≥d z tejto lekcie na tr√©novanie Skip-Gram modelu namiesto CBoW. [Preƒç√≠tajte si podrobnosti](lab/README.md)

**Upozornenie**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa sna≈æ√≠me o presnos≈•, pros√≠m, berte na vedomie, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho rodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nenesieme zodpovednos≈• za ak√©koƒævek nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.