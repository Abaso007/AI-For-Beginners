{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vstavané reprezentácie\n",
    "\n",
    "V našom predchádzajúcom príklade sme pracovali s vysokodimenzionálnymi vektormi bag-of-words s dĺžkou `vocab_size` a explicitne sme prevádzali nízkodimenzionálne pozičné reprezentácie na riedke one-hot reprezentácie. Táto one-hot reprezentácia nie je pamäťovo efektívna. Navyše, každé slovo je spracované nezávisle od ostatných, takže one-hot kódované vektory nevyjadrujú sémantické podobnosti medzi slovami.\n",
    "\n",
    "V tejto jednotke budeme pokračovať v skúmaní datasetu **News AG**. Na začiatok načítajme dáta a získajme niektoré definície z predchádzajúcej jednotky.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Čo je embedding?\n",
    "\n",
    "Myšlienka **embeddingu** spočíva v reprezentácii slov pomocou nízkodimenzionálnych hustých vektorov, ktoré odrážajú sémantický význam slova. Neskôr si povieme, ako vytvoriť zmysluplné word embeddings, ale zatiaľ si embedding predstavme ako spôsob zníženia dimenzionality vektora slova.\n",
    "\n",
    "Embedding vrstva teda prijíma slovo ako vstup a produkuje výstupný vektor so špecifikovanou veľkosťou `embedding_size`. V istom zmysle je veľmi podobná vrstve `Dense`, ale namiesto toho, aby prijímala vektor zakódovaný pomocou one-hot, dokáže prijať číslo reprezentujúce slovo.\n",
    "\n",
    "Použitím embedding vrstvy ako prvej vrstvy v našej sieti môžeme prejsť z modelu bag-of-words na model **embedding bag**, kde najskôr každé slovo v našom texte prevedieme na zodpovedajúci embedding a potom vypočítame nejakú agregačnú funkciu nad všetkými týmito embeddingmi, ako napríklad `sum`, `average` alebo `max`.\n",
    "\n",
    "![Obrázok zobrazujúci embedding klasifikátor pre päť sekvenčných slov.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sk.png)\n",
    "\n",
    "Naša klasifikačná neurónová sieť pozostáva z nasledujúcich vrstiev:\n",
    "\n",
    "* Vrstva `TextVectorization`, ktorá prijíma reťazec ako vstup a produkuje tenzor čísel tokenov. Špecifikujeme rozumnú veľkosť slovníka `vocab_size` a ignorujeme menej často používané slová. Vstupný tvar bude 1 a výstupný tvar bude $n$, pretože dostaneme $n$ tokenov ako výsledok, pričom každý z nich obsahuje čísla od 0 po `vocab_size`.\n",
    "* Vrstva `Embedding`, ktorá prijíma $n$ čísel a redukuje každé číslo na hustý vektor danej dĺžky (v našom príklade 100). Tým sa vstupný tenzor tvaru $n$ transformuje na tenzor $n\\times 100$.\n",
    "* Agregačná vrstva, ktorá vypočíta priemer tohto tenzora pozdĺž prvej osi, t.j. vypočíta priemer všetkých $n$ vstupných tenzorov zodpovedajúcich rôznym slovám. Na implementáciu tejto vrstvy použijeme vrstvu `Lambda` a do nej odovzdáme funkciu na výpočet priemeru. Výstup bude mať tvar 100 a bude predstavovať číselnú reprezentáciu celého vstupného sekvenčného textu.\n",
    "* Záverečný lineárny klasifikátor `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V `summary` výpise, v stĺpci **output shape**, prvý rozmer tenzora `None` zodpovedá veľkosti minibatchu a druhý zodpovedá dĺžke sekvencie tokenov. Všetky sekvencie tokenov v minibatchu majú rôzne dĺžky. O tom, ako s tým pracovať, budeme hovoriť v nasledujúcej sekcii.\n",
    "\n",
    "Teraz poďme trénovať sieť:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Poznámka** že vytvárame vektorizačný nástroj na základe podmnožiny údajov. Toto sa robí s cieľom urýchliť proces, čo môže viesť k situácii, keď nie všetky tokeny z nášho textu budú prítomné vo slovníku. V takom prípade budú tieto tokeny ignorované, čo môže viesť k mierne nižšej presnosti. Avšak v reálnom živote podmnožina textu často poskytuje dobrý odhad slovníka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práca s rôznymi veľkosťami sekvencií premenných\n",
    "\n",
    "Poďme si vysvetliť, ako prebieha trénovanie v minibatchoch. V uvedenom príklade má vstupný tenzor rozmer 1 a používame minibatche s dĺžkou 128, takže skutočná veľkosť tenzora je $128 \\times 1$. Počet tokenov v každej vete je však odlišný. Ak aplikujeme vrstvu `TextVectorization` na jeden vstup, počet vrátených tokenov sa líši v závislosti od toho, ako je text tokenizovaný:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avšak, keď aplikujeme vektorizátor na niekoľko sekvencií, musí vytvoriť tenzor obdĺžnikového tvaru, takže nevyužité prvky vyplní tokenom PAD (ktorý je v našom prípade nula):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu môžeme vidieť vkladania:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Poznámka**: Na minimalizáciu množstva doplňovania má v niektorých prípadoch zmysel zoradiť všetky sekvencie v dátovej sade podľa rastúcej dĺžky (alebo presnejšie podľa počtu tokenov). Tým sa zabezpečí, že každý minibatch bude obsahovať sekvencie podobnej dĺžky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sémantické vektory: Word2Vec\n",
    "\n",
    "V našom predchádzajúcom príklade sa vrstva embedding naučila mapovať slová na vektorové reprezentácie, avšak tieto reprezentácie nemali sémantický význam. Bolo by užitočné naučiť sa vektorovú reprezentáciu tak, aby podobné slová alebo synonymá zodpovedali vektorom, ktoré sú blízko seba z hľadiska nejakej vektorovej vzdialenosti (napríklad euklidovskej vzdialenosti).\n",
    "\n",
    "Aby sme to dosiahli, musíme predtrénovať náš embedding model na veľkej zbierke textov pomocou techniky, ako je [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Táto technika je založená na dvoch hlavných architektúrach, ktoré sa používajú na vytvorenie distribuovanej reprezentácie slov:\n",
    "\n",
    " - **Kontinuálny model bag-of-words** (CBoW), kde trénujeme model na predpovedanie slova na základe okolitého kontextu. Pri danom n-grame $(W_{-2},W_{-1},W_0,W_1,W_2)$ je cieľom modelu predpovedať $W_0$ na základe $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Kontinuálny skip-gram** je opačný k CBoW. Model používa okolité slová z kontextového okna na predpovedanie aktuálneho slova.\n",
    "\n",
    "CBoW je rýchlejší, zatiaľ čo skip-gram je pomalší, ale lepšie reprezentuje zriedkavé slová.\n",
    "\n",
    "![Obrázok zobrazujúci algoritmy CBoW a Skip-Gram na konverziu slov na vektory.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sk.png)\n",
    "\n",
    "Na experimentovanie s embeddingom Word2Vec predtrénovaným na Google News dataset môžeme použiť knižnicu **gensim**. Nižšie nájdeme slová najviac podobné slovu 'neural'.\n",
    "\n",
    "> **Poznámka:** Keď prvýkrát vytvárate vektorové reprezentácie slov, ich sťahovanie môže chvíľu trvať!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Môžeme tiež extrahovať vektorové zakotvenie zo slova, ktoré sa použije pri trénovaní klasifikačného modelu. Zakotvenie má 300 komponentov, ale tu ukazujeme iba prvých 20 komponentov vektora pre prehľadnosť:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skvelá vec na semantických vkladoch je, že môžete manipulovať s vektorovým kódovaním na základe semantiky. Napríklad môžeme požiadať o nájdenie slova, ktorého vektorová reprezentácia je čo najbližšie k slovám *kráľ* a *žena*, a čo najďalej od slova *muž*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Príklad vyššie používa určitú internú mágiu GenSym, ale základná logika je v skutočnosti celkom jednoduchá. Zaujímavou vecou na vkladaní je, že môžete vykonávať bežné vektorové operácie na vektoroch vkladania, a to by odrážalo operácie na **významoch** slov. Príklad vyššie možno vyjadriť pomocou vektorových operácií: vypočítame vektor zodpovedajúci **KRÁĽ-MUŽ+ŽENA** (operácie `+` a `-` sa vykonávajú na vektorových reprezentáciách príslušných slov) a potom nájdeme najbližšie slovo v slovníku k tomuto vektoru:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Museli sme pridať malé koeficienty k vektorom *muž* a *žena* - skúste ich odstrániť, aby ste videli, čo sa stane.\n",
    "\n",
    "Na nájdenie najbližšieho vektora používame TensorFlow mechanizmus na výpočet vektora vzdialeností medzi naším vektorom a všetkými vektormi vo slovníku, a potom nájdeme index minimálneho slova pomocou `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aj keď sa Word2Vec javí ako skvelý spôsob na vyjadrenie sémantiky slov, má mnoho nevýhod, vrátane nasledujúcich:\n",
    "\n",
    "* Modely CBoW a skip-gram sú **prediktívne vektorizácie**, ktoré berú do úvahy iba lokálny kontext. Word2Vec nevyužíva globálny kontext.\n",
    "* Word2Vec neberie do úvahy **morfologickú štruktúru** slov, t. j. skutočnosť, že význam slova môže závisieť od rôznych častí slova, ako je napríklad koreň.\n",
    "\n",
    "**FastText** sa snaží prekonať druhé obmedzenie a stavia na Word2Vec tým, že sa učí vektorové reprezentácie pre každé slovo a n-gramy znakov nachádzajúce sa v rámci každého slova. Hodnoty týchto reprezentácií sa potom pri každom kroku trénovania spriemerujú do jedného vektora. Aj keď to pridáva veľa dodatočných výpočtov počas predtrénovania, umožňuje to vektorovým reprezentáciám slov zakódovať informácie o podslovách.\n",
    "\n",
    "Ďalšia metóda, **GloVe**, používa odlišný prístup k vektorovým reprezentáciám slov, založený na faktorizácii matice slov a kontextov. Najprv vytvorí veľkú maticu, ktorá počíta počet výskytov slov v rôznych kontextoch, a potom sa snaží túto maticu reprezentovať v nižších dimenziách tak, aby minimalizovala stratu pri rekonštrukcii.\n",
    "\n",
    "Knižnica gensim podporuje tieto vektorové reprezentácie slov a môžete s nimi experimentovať zmenou kódu na načítanie modelu vyššie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Použitie predtrénovaných embeddingov v Keras\n",
    "\n",
    "Môžeme upraviť vyššie uvedený príklad tak, aby sme predvyplnili maticu v našej embedding vrstve pomocou semantických embeddingov, ako je Word2Vec. Slovníky predtrénovaného embeddingu a textového korpusu sa pravdepodobne nebudú zhodovať, takže si musíme vybrať jeden. Tu skúmame dve možné možnosti: použitie slovníka tokenizéra a použitie slovníka z embeddingov Word2Vec.\n",
    "\n",
    "### Použitie slovníka tokenizéra\n",
    "\n",
    "Pri použití slovníka tokenizéra budú niektoré slová zo slovníka mať zodpovedajúce embeddingy Word2Vec, zatiaľ čo niektoré budú chýbať. Keďže veľkosť nášho slovníka je `vocab_size` a dĺžka embeddingového vektora Word2Vec je `embed_size`, embedding vrstva bude reprezentovaná váhovou maticou tvaru `vocab_size`$\\times$`embed_size`. Túto maticu naplníme prechádzaním slovníka:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre slová, ktoré nie sú prítomné vo Word2Vec slovníku, môžeme buď ponechať hodnoty ako nuly, alebo vygenerovať náhodný vektor.\n",
    "\n",
    "Teraz môžeme definovať vrstvu embedding s predtrénovanými váhami:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Poznámka**: Všimnite si, že sme nastavili `trainable=False` pri vytváraní `Embedding`, čo znamená, že vrstvu Embedding nebudeme znovu trénovať. To môže spôsobiť mierne nižšiu presnosť, ale urýchľuje to tréning.\n",
    "\n",
    "### Použitie slovníka pre embedding\n",
    "\n",
    "Jedným z problémov predchádzajúceho prístupu je, že slovníky použité v TextVectorization a Embedding sú odlišné. Na prekonanie tohto problému môžeme použiť jedno z nasledujúcich riešení:\n",
    "* Znovu natrénovať Word2Vec model na našom slovníku.\n",
    "* Načítať náš dataset so slovníkom z predtrénovaného Word2Vec modelu. Slovníky použité na načítanie datasetu môžeme špecifikovať počas načítania.\n",
    "\n",
    "Druhý prístup sa zdá byť jednoduchší, takže ho implementujeme. Najprv vytvoríme vrstvu `TextVectorization` so špecifikovaným slovníkom, prevzatým z Word2Vec embeddingov:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knižnica word embeddings Gensim obsahuje praktickú funkciu `get_keras_embeddings`, ktorá automaticky vytvorí zodpovedajúcu vrstvu embeddings pre Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedným z dôvodov, prečo nevidíme vyššiu presnosť, je, že niektoré slová z nášho datasetu chýbajú v predtrénovanej slovnej zásobe GloVe, a preto sú v podstate ignorované. Aby sme to prekonali, môžeme trénovať vlastné vektorové reprezentácie na základe nášho datasetu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontextové vkladania\n",
    "\n",
    "Jedným z hlavných obmedzení tradičných predtrénovaných reprezentácií vkladania, ako je Word2Vec, je skutočnosť, že aj keď dokážu zachytiť určitý význam slova, nedokážu rozlíšiť rôzne významy. To môže spôsobiť problémy v následných modeloch.\n",
    "\n",
    "Napríklad slovo „hrať“ má rôzny význam v týchto dvoch vetách:\n",
    "- Išiel som na **hru** do divadla.\n",
    "- John chce **hrať** so svojimi priateľmi.\n",
    "\n",
    "Predtrénované vkladania, o ktorých sme hovorili, reprezentujú oba významy slova „hrať“ rovnakým spôsobom. Aby sme prekonali toto obmedzenie, potrebujeme vytvoriť vkladania založené na **jazykovom modeli**, ktorý je trénovaný na veľkom korpuse textu a *vie*, ako môžu byť slová spájané v rôznych kontextoch. Diskusia o kontextových vkladaní je mimo rozsah tohto tutoriálu, ale vrátime sa k nim, keď budeme hovoriť o jazykových modeloch v ďalšej jednotke.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Upozornenie**:  \nTento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-30T01:02:26+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "sk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}