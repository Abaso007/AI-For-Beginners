<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-25T21:38:56+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "sk"
}
-->
# Vstavan√© reprezent√°cie (Embeddings)

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Pri tr√©novan√≠ klasifik√°torov zalo≈æen√Ωch na BoW alebo TF/IDF sme pracovali s vysokodimenzion√°lnymi vektormi bag-of-words s dƒ∫≈ækou `vocab_size` a explicitne sme prev√°dzali n√≠zkodimenzion√°lne poziƒçn√© reprezent√°cie na riedke one-hot reprezent√°cie. T√°to one-hot reprezent√°cia v≈°ak nie je pam√§≈•ovo efekt√≠vna. Navy≈°e, ka≈æd√© slovo je spracovan√© nez√°visle od ostatn√Ωch, t. j. one-hot k√≥dovan√© vektory nevyjadruj√∫ ≈æiadnu s√©mantick√∫ podobnos≈• medzi slovami.

My≈°lienka **vstavanej reprezent√°cie (embedding)** spoƒç√≠va v tom, ≈æe slov√° s√∫ reprezentovan√© n√≠zkodimenzion√°lnymi hust√Ωmi vektormi, ktor√© urƒçit√Ωm sp√¥sobom odr√°≈æaj√∫ s√©mantick√Ω v√Ωznam slova. Nesk√¥r si povieme, ako vytvori≈• zmyslupln√© vstavan√© reprezent√°cie slov, ale zatiaƒæ ich m√¥≈æeme ch√°pa≈• ako sp√¥sob zn√≠≈æenia dimenzionality vektorov slov.

Vstavan√° vrstva teda prij√≠ma slovo ako vstup a produkuje v√Ωstupn√Ω vektor so ≈°pecifikovanou veƒækos≈•ou `embedding_size`. V istom zmysle je to veƒæmi podobn√© vrstve `Linear`, ale namiesto prij√≠mania one-hot k√≥dovan√©ho vektora dok√°≈æe prija≈• ƒç√≠slo slova ako vstup, ƒç√≠m sa vyhneme vytv√°raniu veƒæk√Ωch one-hot k√≥dovan√Ωch vektorov.

Pou≈æit√≠m vstavanej vrstvy ako prvej vrstvy v na≈°ej klasifikaƒçnej sieti m√¥≈æeme prejs≈• z modelu bag-of-words na model **embedding bag**, kde najprv ka≈æd√© slovo v texte prevedieme na zodpovedaj√∫cu vstavan√∫ reprezent√°ciu a potom vypoƒç√≠tame nejak√∫ agregaƒçn√∫ funkciu nad v≈°etk√Ωmi t√Ωmito reprezent√°ciami, ako napr√≠klad `sum`, `average` alebo `max`.

![Obr√°zok zn√°zor≈àuj√∫ci klasifik√°tor s pou≈æit√≠m vstavanej reprezent√°cie pre p√§≈• slov v sekvencii.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sk.png)

> Obr√°zok od autora

## ‚úçÔ∏è Cviƒçenia: Vstavan√© reprezent√°cie

Pokraƒçujte v uƒçen√≠ v nasleduj√∫cich notebookoch:
* [Vstavan√© reprezent√°cie s PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Vstavan√© reprezent√°cie s TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## S√©mantick√© vstavan√© reprezent√°cie: Word2Vec

Hoci sa vstavan√° vrstva nauƒçila mapova≈• slov√° na vektorov√© reprezent√°cie, tieto reprezent√°cie nemusia nevyhnutne nies≈• veƒæa s√©mantick√©ho v√Ωznamu. Bolo by u≈æitoƒçn√© nauƒçi≈• sa vektorov√∫ reprezent√°ciu, kde podobn√© slov√° alebo synonym√° zodpovedaj√∫ vektorom, ktor√© s√∫ si bl√≠zke podƒæa nejakej vektorovej vzdialenosti (napr. Euklidovskej vzdialenosti).

Na to potrebujeme predtr√©nova≈• n√°≈° model vstavanej reprezent√°cie na veƒækej zbierke textov ≈°pecifick√Ωm sp√¥sobom. Jedn√Ωm zo sp√¥sobov tr√©novania s√©mantick√Ωch vstavan√Ωch reprezent√°ci√≠ je met√≥da [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). T√°to met√≥da je zalo≈æen√° na dvoch hlavn√Ωch architekt√∫rach, ktor√© sa pou≈æ√≠vaj√∫ na vytv√°ranie distribuovan√Ωch reprezent√°ci√≠ slov:

 - **Continuous bag-of-words** (CBoW) ‚Äî v tejto architekt√∫re tr√©nujeme model na predpovedanie slova na z√°klade okolit√Ωch slov v kontexte. Pri n-grame $(W_{-2},W_{-1},W_0,W_1,W_2)$ je cieƒæom modelu predpoveda≈• $W_0$ na z√°klade $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** je opakom CBoW. Model pou≈æ√≠va okolit√Ω kontextov√Ω r√°mec slov na predpovedanie aktu√°lneho slova.

CBoW je r√Ωchlej≈°√≠, zatiaƒæ ƒço skip-gram je pomal≈°√≠, ale lep≈°ie reprezentuje zriedkav√© slov√°.

![Obr√°zok zn√°zor≈àuj√∫ci algoritmy CBoW a Skip-Gram na prevod slov na vektory.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sk.png)

> Obr√°zok z [tohto ƒçl√°nku](https://arxiv.org/pdf/1301.3781.pdf)

Predtr√©novan√© vstavan√© reprezent√°cie Word2Vec (ako aj in√© podobn√© modely, napr√≠klad GloVe) m√¥≈æu by≈• pou≈æit√© namiesto vstavanej vrstvy v neur√≥nov√Ωch sie≈•ach. Mus√≠me sa v≈°ak vysporiada≈• so slovn√≠kmi, preto≈æe slovn√≠k pou≈æit√Ω na predtr√©novanie Word2Vec/GloVe sa pravdepodobne l√≠≈°i od slovn√≠ka v na≈°om textovom korpuse. Pozrite si vy≈°≈°ie uveden√© notebooky, aby ste zistili, ako tento probl√©m vyrie≈°i≈•.

## Kontextov√© vstavan√© reprezent√°cie

Jedn√Ωm z hlavn√Ωch obmedzen√≠ tradiƒçn√Ωch predtr√©novan√Ωch vstavan√Ωch reprezent√°ci√≠, ako je Word2Vec, je probl√©m rozl√≠≈°enia v√Ωznamu slova. Hoci predtr√©novan√© vstavan√© reprezent√°cie dok√°≈æu zachyti≈• urƒçit√Ω v√Ωznam slov v kontexte, ka≈æd√Ω mo≈æn√Ω v√Ωznam slova je zak√≥dovan√Ω do tej istej reprezent√°cie. To m√¥≈æe sp√¥sobi≈• probl√©my v n√°sledn√Ωch modeloch, preto≈æe mnoh√© slov√°, ako napr√≠klad slovo ‚Äûhra≈•‚Äú, maj√∫ r√¥zne v√Ωznamy v z√°vislosti od kontextu, v ktorom sa pou≈æ√≠vaj√∫.

Napr√≠klad slovo ‚Äûhra≈•‚Äú m√° v t√Ωchto dvoch vet√°ch √∫plne odli≈°n√Ω v√Ωznam:

- I≈°iel som na **hru** do divadla.
- J√°n chce **hra≈•** so svojimi priateƒæmi.

Predtr√©novan√© vstavan√© reprezent√°cie vy≈°≈°ie reprezentuj√∫ oba tieto v√Ωznamy slova ‚Äûhra≈•‚Äú v tej istej reprezent√°cii. Na prekonanie tohto obmedzenia potrebujeme vytvori≈• vstavan√© reprezent√°cie zalo≈æen√© na **jazykovom modeli**, ktor√Ω je tr√©novan√Ω na veƒækom korpuse textov a ‚Äûvie‚Äú, ako sa slov√° m√¥≈æu sklada≈• v r√¥znych kontextoch. Diskusia o kontextov√Ωch vstavan√Ωch reprezent√°ci√°ch presahuje r√°mec tohto tutori√°lu, ale vr√°time sa k nim pri diskusii o jazykov√Ωch modeloch nesk√¥r v kurze.

## Z√°ver

V tejto lekcii ste sa nauƒçili, ako vytv√°ra≈• a pou≈æ√≠va≈• vstavan√© vrstvy v TensorFlow a PyTorch na lep≈°ie vyjadrenie s√©mantick√©ho v√Ωznamu slov.

## üöÄ V√Ωzva

Word2Vec bol pou≈æit√Ω na niektor√© zauj√≠mav√© aplik√°cie, vr√°tane generovania textov piesn√≠ a po√©zie. Pozrite si [tento ƒçl√°nok](https://www.politetype.com/blog/word2vec-color-poems), ktor√Ω popisuje, ako autor pou≈æil Word2Vec na generovanie po√©zie. Pozrite si aj [toto video od Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), kde n√°jdete in√© vysvetlenie tejto techniky. Potom sa pok√∫ste aplikova≈• tieto techniky na svoj vlastn√Ω textov√Ω korpus, mo≈æno z√≠skan√Ω z Kaggle.

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Prehƒæad a samostatn√© ≈°t√∫dium

Preƒç√≠tajte si tento ƒçl√°nok o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [√öloha: Notebooky](assignment.md)

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keƒè sa sna≈æ√≠me o presnos≈•, pros√≠m, berte na vedomie, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho rodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nenesieme zodpovednos≈• za ak√©koƒævek nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.