<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b708c9b85b833864c73c6281f1e6b96e",
  "translation_date": "2025-09-23T14:11:48+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "sk"
}
-->
# Vstavan√© reprezent√°cie (Embeddings)

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Pri tr√©novan√≠ klasifik√°torov zalo≈æen√Ωch na BoW alebo TF/IDF sme pracovali s vysokodimenzion√°lnymi vektormi bag-of-words s dƒ∫≈ækou `vocab_size` a explicitne sme prev√°dzali n√≠zkodimenzion√°lne poziƒçn√© reprezent√°cie na riedke one-hot reprezent√°cie. T√°to one-hot reprezent√°cia v≈°ak nie je pam√§≈•ovo efekt√≠vna. Navy≈°e, ka≈æd√© slovo je pova≈æovan√© za nez√°visl√© od ostatn√Ωch, t.j. one-hot k√≥dovan√© vektory nevyjadruj√∫ ≈æiadnu s√©mantick√∫ podobnos≈• medzi slovami.

My≈°lienka **vstavan√Ωch reprezent√°ci√≠ (embedding)** spoƒç√≠va v reprezentovan√≠ slov pomocou n√≠zkodimenzion√°lnych hust√Ωch vektorov, ktor√© urƒçit√Ωm sp√¥sobom odr√°≈æaj√∫ s√©mantick√Ω v√Ωznam slova. Nesk√¥r si povieme, ako vytvori≈• zmyslupln√© vstavan√© reprezent√°cie slov, ale zatiaƒæ ich m√¥≈æeme ch√°pa≈• ako sp√¥sob zn√≠≈æenia dimenzionality vektora slova.

Vstavan√° vrstva (embedding layer) teda prij√≠ma slovo ako vstup a produkuje v√Ωstupn√Ω vektor so ≈°pecifikovanou veƒækos≈•ou `embedding_size`. V istom zmysle je veƒæmi podobn√° vrstve `Linear`, ale namiesto prij√≠mania one-hot k√≥dovan√©ho vektora dok√°≈æe prija≈• ƒç√≠slo slova ako vstup, ƒç√≠m sa vyhneme vytv√°raniu veƒæk√Ωch one-hot k√≥dovan√Ωch vektorov.

Pou≈æit√≠m vstavanej vrstvy ako prvej vrstvy v na≈°ej klasifikaƒçnej sieti m√¥≈æeme prejs≈• z modelu bag-of-words na model **embedding bag**, kde najprv ka≈æd√© slovo v texte prevedieme na zodpovedaj√∫cu vstavan√∫ reprezent√°ciu a potom vypoƒç√≠tame nejak√∫ agregaƒçn√∫ funkciu nad v≈°etk√Ωmi t√Ωmito reprezent√°ciami, ako napr√≠klad `sum`, `average` alebo `max`.

![Obr√°zok zobrazuj√∫ci klasifik√°tor s pou≈æit√≠m vstavan√Ωch reprezent√°ci√≠ pre p√§≈• slov v sekvencii.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sk.png)

> Obr√°zok od autora

## ‚úçÔ∏è Cviƒçenia: Vstavan√© reprezent√°cie

Pokraƒçujte v uƒçen√≠ v nasleduj√∫cich notebookoch:
* [Vstavan√© reprezent√°cie s PyTorch](EmbeddingsPyTorch.ipynb)
* [Vstavan√© reprezent√°cie s TensorFlow](EmbeddingsTF.ipynb)

## S√©mantick√© vstavan√© reprezent√°cie: Word2Vec

Aj keƒè sa vstavan√° vrstva nauƒçila mapova≈• slov√° na vektorov√∫ reprezent√°ciu, t√°to reprezent√°cia nemus√≠ nevyhnutne obsahova≈• veƒæa s√©mantick√©ho v√Ωznamu. Bolo by u≈æitoƒçn√© nauƒçi≈• sa vektorov√∫ reprezent√°ciu, kde podobn√© slov√° alebo synonym√° zodpovedaj√∫ vektorom, ktor√© s√∫ si bl√≠zke podƒæa nejakej vektorovej vzdialenosti (napr. Euklidovskej vzdialenosti).

Na to potrebujeme predtr√©nova≈• n√°≈° model vstavan√Ωch reprezent√°ci√≠ na veƒækej zbierke textov ≈°pecifick√Ωm sp√¥sobom. Jedn√Ωm zo sp√¥sobov tr√©novania s√©mantick√Ωch vstavan√Ωch reprezent√°ci√≠ je met√≥da [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). T√°to met√≥da je zalo≈æen√° na dvoch hlavn√Ωch architekt√∫rach, ktor√© sa pou≈æ√≠vaj√∫ na vytv√°ranie distribuovan√Ωch reprezent√°ci√≠ slov:

 - **Kontinu√°lny bag-of-words** (CBoW) ‚Äî v tejto architekt√∫re tr√©nujeme model na predpovedanie slova na z√°klade okolit√Ωch slov v kontexte. Pre ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ je cieƒæom modelu predpoveda≈• $W_0$ na z√°klade $(W_{-2},W_{-1},W_1,W_2)$.
 - **Kontinu√°lny skip-gram** je opakom CBoW. Model pou≈æ√≠va okno kontextov√Ωch slov na predpovedanie aktu√°lneho slova.

CBoW je r√Ωchlej≈°√≠, zatiaƒæ ƒço skip-gram je pomal≈°√≠, ale lep≈°ie reprezentuje zriedkav√© slov√°.

![Obr√°zok zobrazuj√∫ci algoritmy CBoW a Skip-Gram na konverziu slov na vektory.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sk.png)

> Obr√°zok z [tohto ƒçl√°nku](https://arxiv.org/pdf/1301.3781.pdf)

Predtr√©novan√© vstavan√© reprezent√°cie Word2Vec (ako aj in√© podobn√© modely, napr√≠klad GloVe) m√¥≈æu by≈• pou≈æit√© namiesto vstavanej vrstvy v neur√≥nov√Ωch sie≈•ach. Mus√≠me sa v≈°ak vysporiada≈• so slovn√≠kmi, preto≈æe slovn√≠k pou≈æit√Ω na predtr√©novanie Word2Vec/GloVe sa pravdepodobne l√≠≈°i od slovn√≠ka v na≈°om textovom korpuse. Pozrite si vy≈°≈°ie uveden√© notebooky, aby ste zistili, ako tento probl√©m vyrie≈°i≈•.

## Kontextov√© vstavan√© reprezent√°cie

Jedn√Ωm z hlavn√Ωch obmedzen√≠ tradiƒçn√Ωch predtr√©novan√Ωch vstavan√Ωch reprezent√°ci√≠, ako je Word2Vec, je probl√©m rozli≈°ovania v√Ωznamu slov (word sense disambiguation). Zatiaƒæ ƒço predtr√©novan√© reprezent√°cie dok√°≈æu zachyti≈• urƒçit√Ω v√Ωznam slov v kontexte, ka≈æd√Ω mo≈æn√Ω v√Ωznam slova je zak√≥dovan√Ω do tej istej reprezent√°cie. To m√¥≈æe sp√¥sobi≈• probl√©my v n√°sledn√Ωch modeloch, preto≈æe mnoh√© slov√°, ako napr√≠klad slovo 'play', maj√∫ r√¥zne v√Ωznamy v z√°vislosti od kontextu, v ktorom s√∫ pou≈æit√©.

Napr√≠klad slovo 'play' m√° v t√Ωchto dvoch vet√°ch √∫plne odli≈°n√Ω v√Ωznam:

- I≈°iel som na **hru** do divadla.
- John chce **hra≈•** so svojimi priateƒæmi.

Vy≈°≈°ie uveden√© predtr√©novan√© reprezent√°cie reprezentuj√∫ oba tieto v√Ωznamy slova 'play' rovnakou reprezent√°ciou. Na prekonanie tohto obmedzenia potrebujeme vytvori≈• reprezent√°cie zalo≈æen√© na **jazykovom modeli**, ktor√Ω je tr√©novan√Ω na veƒækom korpuse textov a *vie*, ako sa slov√° m√¥≈æu sklada≈• v r√¥znych kontextoch. Diskusia o kontextov√Ωch vstavan√Ωch reprezent√°ci√°ch presahuje r√°mec tohto tutori√°lu, ale vr√°time sa k nim, keƒè budeme nesk√¥r hovori≈• o jazykov√Ωch modeloch.

## Z√°ver

V tejto lekcii ste sa nauƒçili, ako vytv√°ra≈• a pou≈æ√≠va≈• vstavan√© vrstvy v TensorFlow a PyTorch na lep≈°ie odr√°≈æanie s√©mantick√Ωch v√Ωznamov slov.

## üöÄ V√Ωzva

Word2Vec bol pou≈æit√Ω na niektor√© zauj√≠mav√© aplik√°cie, vr√°tane generovania textov piesn√≠ a po√©zie. Pozrite si [tento ƒçl√°nok](https://www.politetype.com/blog/word2vec-color-poems), ktor√Ω popisuje, ako autor pou≈æil Word2Vec na generovanie po√©zie. Pozrite si aj [toto video od Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), kde n√°jdete in√© vysvetlenie tejto techniky. Potom sa pok√∫ste aplikova≈• tieto techniky na svoj vlastn√Ω textov√Ω korpus, mo≈æno z√≠skan√Ω z Kaggle.

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## Prehƒæad a samostatn√© ≈°t√∫dium

Preƒç√≠tajte si tento ƒçl√°nok o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [√öloha: Notebooky](assignment.md)

---

