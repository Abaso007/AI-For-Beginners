{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vstavané reprezentácie\n",
    "\n",
    "V našom predchádzajúcom príklade sme pracovali s vysokodimenzionálnymi vektormi bag-of-words s dĺžkou `vocab_size` a explicitne sme prevádzali nízkodimenzionálne pozičné reprezentácie na riedke one-hot reprezentácie. Táto one-hot reprezentácia nie je pamäťovo efektívna, navyše, každé slovo je spracované nezávisle od ostatných, t.j. one-hot zakódované vektory nevyjadrujú žiadnu sémantickú podobnosť medzi slovami.\n",
    "\n",
    "V tejto časti budeme pokračovať v skúmaní datasetu **News AG**. Na začiatok načítajme dáta a získajme niektoré definície z predchádzajúceho notebooku.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Čo je embedding?\n",
    "\n",
    "Myšlienka **embeddingu** spočíva v reprezentácii slov pomocou nižšej dimenzie hustých vektorov, ktoré určitým spôsobom odrážajú sémantický význam slova. Neskôr si povieme, ako vytvoriť zmysluplné word embeddings, ale zatiaľ si predstavme embeddingy ako spôsob zníženia dimenzionality vektora slova.\n",
    "\n",
    "Embedding vrstva teda prijme slovo ako vstup a vygeneruje výstupný vektor s určenou veľkosťou `embedding_size`. V istom zmysle je veľmi podobná vrstve `Linear`, ale namiesto prijímania vektora zakódovaného v one-hot formáte dokáže prijať číslo slova ako vstup.\n",
    "\n",
    "Použitím embedding vrstvy ako prvej vrstvy v našej sieti môžeme prejsť od modelu bag-of-words k modelu **embedding bag**, kde najskôr každé slovo v našom texte prevedieme na zodpovedajúci embedding a potom vypočítame nejakú agregačnú funkciu nad všetkými týmito embeddingami, ako napríklad `sum`, `average` alebo `max`.\n",
    "\n",
    "![Obrázok zobrazujúci embedding klasifikátor pre päť slov v sekvencii.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sk.png)\n",
    "\n",
    "Naša klasifikačná neurónová sieť začne embedding vrstvou, potom agregačnou vrstvou a na vrchu bude lineárny klasifikátor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práca s rôznou veľkosťou sekvencie premenných\n",
    "\n",
    "V dôsledku tejto architektúry je potrebné vytvárať minibatchy pre našu sieť určitým spôsobom. V predchádzajúcej jednotke, pri použití metódy bag-of-words, mali všetky BoW tenzory v minibatchi rovnakú veľkosť `vocab_size`, bez ohľadu na skutočnú dĺžku textovej sekvencie. Keď prejdeme na slovné vektory (word embeddings), skončíme s rôznym počtom slov v každom textovom vzorku, a pri kombinovaní týchto vzoriek do minibatchov bude potrebné použiť nejaké doplnenie (padding).\n",
    "\n",
    "Toto je možné dosiahnuť použitím rovnakej techniky, ktorá zahŕňa poskytnutie funkcie `collate_fn` pre zdroj dát:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tréning klasifikátora embeddingov\n",
    "\n",
    "Teraz, keď sme definovali správny dataloader, môžeme model trénovať pomocou tréningovej funkcie, ktorú sme definovali v predchádzajúcej jednotke:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Poznámka**: Trénujeme tu iba na 25k záznamoch (menej ako jeden celý epoch) kvôli úspore času, ale môžete pokračovať v tréningu, napísať funkciu na tréning počas viacerých epoch a experimentovať s parametrom rýchlosti učenia, aby ste dosiahli vyššiu presnosť. Mali by ste byť schopní dosiahnuť presnosť približne 90%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vrstva EmbeddingBag a reprezentácia sekvencií s premenlivou dĺžkou\n",
    "\n",
    "V predchádzajúcej architektúre sme museli všetky sekvencie doplniť na rovnakú dĺžku, aby sa zmestili do minibatchu. Toto nie je najefektívnejší spôsob reprezentácie sekvencií s premenlivou dĺžkou - iný prístup by bol použiť **offset** vektor, ktorý by obsahoval posuny všetkých sekvencií uložených v jednom veľkom vektore.\n",
    "\n",
    "![Obrázok zobrazujúci reprezentáciu sekvencií pomocou offset vektora](../../../../../translated_images/offset-sequence-representation.eb73fcefb29b46eecfbe74466077cfeb7c0f93a4f254850538a2efbc63517479.sk.png)\n",
    "\n",
    "> **Note**: Na obrázku vyššie je zobrazená sekvencia znakov, ale v našom príklade pracujeme so sekvenciami slov. Avšak, základný princíp reprezentácie sekvencií pomocou offset vektora zostáva rovnaký.\n",
    "\n",
    "Na prácu s offset reprezentáciou používame vrstvu [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html). Je podobná vrstve `Embedding`, ale ako vstup berie obsahový vektor a offset vektor, a navyše obsahuje vrstvu na priemerovanie, ktorá môže byť `mean`, `sum` alebo `max`.\n",
    "\n",
    "Tu je upravená sieť, ktorá používa `EmbeddingBag`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prípravu dátovej sady na trénovanie musíme poskytnúť konverznú funkciu, ktorá pripraví vektor posunu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Všimnite si, že na rozdiel od všetkých predchádzajúcich príkladov naša sieť teraz prijíma dva parametre: dátový vektor a offsetový vektor, ktoré majú rôzne veľkosti. Podobne aj náš načítač dát nám poskytuje 3 hodnoty namiesto 2: ako vlastnosti sú poskytované textové aj offsetové vektory. Preto musíme mierne upraviť našu tréningovú funkciu, aby sme to zohľadnili:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sémantické vektory: Word2Vec\n",
    "\n",
    "V našom predchádzajúcom príklade sa vrstva modelu na vkladanie naučila mapovať slová na vektorové reprezentácie, avšak táto reprezentácia nemala veľký sémantický význam. Bolo by dobré naučiť sa takú vektorovú reprezentáciu, kde by podobné slová alebo synonymá zodpovedali vektorom, ktoré sú si blízke podľa nejakej vektorovej vzdialenosti (napr. euklidovskej vzdialenosti).\n",
    "\n",
    "Aby sme to dosiahli, musíme náš model na vkladanie predtrénovať na veľkej zbierke textov špecifickým spôsobom. Jedným z prvých spôsobov, ako trénovať sémantické vektory, je metóda nazývaná [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Táto metóda je založená na dvoch hlavných architektúrach, ktoré sa používajú na vytvorenie distribuovanej reprezentácie slov:\n",
    "\n",
    " - **Kontinuálny model bag-of-words** (CBoW) — v tejto architektúre trénujeme model na predpovedanie slova na základe okolitých slov v kontexte. Ak máme ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, cieľom modelu je predpovedať $W_0$ na základe $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Kontinuálny skip-gram** je opakom CBoW. Model používa okno okolitých slov v kontexte na predpovedanie aktuálneho slova.\n",
    "\n",
    "CBoW je rýchlejší, zatiaľ čo skip-gram je pomalší, ale lepšie reprezentuje zriedkavé slová.\n",
    "\n",
    "![Obrázok zobrazujúci algoritmy CBoW a Skip-Gram na konverziu slov na vektory.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sk.png)\n",
    "\n",
    "Na experimentovanie s Word2Vec vektormi predtrénovanými na Google News dataset môžeme použiť knižnicu **gensim**. Nižšie nájdeme slová najviac podobné slovu 'neural'.\n",
    "\n",
    "> **Note:** Keď prvýkrát vytvárate vektorové reprezentácie slov, ich sťahovanie môže chvíľu trvať!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Môžeme tiež vypočítať vektorové zakotvenia zo slova, ktoré sa použijú pri trénovaní klasifikačného modelu (z dôvodu prehľadnosti zobrazujeme iba prvých 20 komponentov vektora):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skvelá vec na semantických vkladoch je, že môžete manipulovať s vektorovým kódovaním na zmenu semantiky. Napríklad môžeme požiadať o nájdenie slova, ktorého vektorová reprezentácia by bola čo najbližšie k slovám *kráľ* a *žena*, a čo najďalej od slova *muž*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obe metódy, CBoW a Skip-Grams, sú „prediktívne“ vektory, pretože berú do úvahy iba lokálne kontexty. Word2Vec nevyužíva globálny kontext.\n",
    "\n",
    "**FastText** rozširuje Word2Vec tým, že sa učí vektorové reprezentácie pre každé slovo a znakové n-gramy nachádzajúce sa v rámci každého slova. Hodnoty týchto reprezentácií sa potom pri každom kroku tréningu spriemerujú do jedného vektora. Aj keď to pridáva veľa dodatočných výpočtov počas pred-tréningu, umožňuje to, aby vektorové reprezentácie slov obsahovali informácie o podslovách.\n",
    "\n",
    "Ďalšia metóda, **GloVe**, využíva myšlienku matice spoluvýskytu a používa neurónové metódy na rozklad matice spoluvýskytu do výraznejších a nelineárnych vektorov slov.\n",
    "\n",
    "Môžete si vyskúšať príklad zmenou vektorových reprezentácií na FastText a GloVe, keďže gensim podporuje niekoľko rôznych modelov vektorových reprezentácií slov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Použitie predtrénovaných vektorov v PyTorch\n",
    "\n",
    "Môžeme upraviť vyššie uvedený príklad tak, aby sme predvyplnili maticu v našej vrstve embedding pomocou sémantických vektorov, ako je Word2Vec. Musíme brať do úvahy, že slovníky predtrénovaných vektorov a nášho textového korpusu sa pravdepodobne nebudú zhodovať, takže inicializujeme váhy pre chýbajúce slová náhodnými hodnotami:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz poďme trénovať náš model. Všimnite si, že čas potrebný na trénovanie modelu je výrazne dlhší ako v predchádzajúcom príklade, kvôli väčšej veľkosti vrstvy embedding a teda oveľa vyššiemu počtu parametrov. Tiež, kvôli tomu môžeme potrebovať trénovať náš model na viacerých príkladoch, ak chceme vyhnúť sa overfittingu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V našom prípade nepozorujeme výrazné zvýšenie presnosti, čo pravdepodobne súvisí s odlišnými slovníkmi.  \n",
    "Na prekonanie problému s rôznymi slovníkmi môžeme použiť jedno z nasledujúcich riešení:  \n",
    "* Znovu natrénovať model word2vec na našom slovníku  \n",
    "* Načítať náš dataset so slovníkom z predtrénovaného modelu word2vec. Slovník použitý na načítanie datasetu je možné špecifikovať počas načítania.  \n",
    "\n",
    "Druhý prístup sa zdá jednoduchší, najmä preto, že PyTorch `torchtext` framework obsahuje vstavanú podporu pre embeddingy. Môžeme napríklad vytvoriť slovník založený na GloVe nasledujúcim spôsobom:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Načítaná slovná zásoba má nasledujúce základné operácie:\n",
    "* Slovník `vocab.stoi` nám umožňuje previesť slovo na jeho index v slovníku\n",
    "* `vocab.itos` robí opak - prevádza číslo na slovo\n",
    "* `vocab.vectors` je pole vektorov zapuzdrenia, takže na získanie zapuzdrenia slova `s` musíme použiť `vocab.vectors[vocab.stoi[s]]`\n",
    "\n",
    "Tu je príklad manipulácie s vektormi na demonštráciu rovnice **kind-man+woman = queen** (musel som trochu upraviť koeficient, aby to fungovalo):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na natrénovanie klasifikátora pomocou týchto embeddingov najprv musíme zakódovať náš dataset pomocou slovníka GloVe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako sme videli vyššie, všetky vektorové vloženia sú uložené v matici `vocab.vectors`. To umožňuje veľmi jednoduché načítanie týchto váh do váh vrstvy vloženia pomocou jednoduchého kopírovania:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedným z dôvodov, prečo nevidíme významné zvýšenie presnosti, je skutočnosť, že niektoré slová z nášho datasetu chýbajú v predtrénovanej slovnej zásobe GloVe, a preto sú v podstate ignorované. Aby sme túto skutočnosť prekonali, môžeme trénovať vlastné vektorové reprezentácie na našom datasete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontextové vkladania\n",
    "\n",
    "Jedným z hlavných obmedzení tradičných predtrénovaných reprezentácií vkladania, ako je Word2Vec, je problém rozlíšenia významu slov. Hoci predtrénované vkladania dokážu zachytiť určitý význam slov v kontexte, každý možný význam slova je zakódovaný do toho istého vkladania. To môže spôsobovať problémy v následných modeloch, pretože mnohé slová, ako napríklad slovo 'hrať', majú rôzne významy v závislosti od kontextu, v ktorom sú použité.\n",
    "\n",
    "Napríklad slovo 'hrať' v týchto dvoch vetách má úplne odlišný význam:\n",
    "- Išiel som na **hru** do divadla.\n",
    "- Ján chce **hrať** so svojimi priateľmi.\n",
    "\n",
    "Predtrénované vkladania vyššie reprezentujú oba tieto významy slova 'hrať' v tom istom vkladaní. Aby sme prekonali toto obmedzenie, musíme vytvoriť vkladania založené na **jazykovom modeli**, ktorý je trénovaný na veľkom korpuse textu a *vie*, ako môžu byť slová poskladané v rôznych kontextoch. Diskusia o kontextových vkladaní je mimo rozsahu tohto tutoriálu, ale vrátime sa k nim pri rozprávaní o jazykových modeloch v ďalšej jednotke.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Upozornenie**:  \nTento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keď sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho pôvodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-30T01:06:55+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "sk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}