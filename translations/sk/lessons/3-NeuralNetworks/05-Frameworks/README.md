<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-25T23:50:49+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "sk"
}
-->
# Frameworky pre neur√≥nov√© siete

Ako sme sa u≈æ nauƒçili, na efekt√≠vne tr√©novanie neur√≥nov√Ωch siet√≠ mus√≠me urobi≈• dve veci:

* Operova≈• s tenzormi, napr. n√°sobi≈•, sƒç√≠ta≈• a poƒç√≠ta≈• niektor√© funkcie ako sigmoid alebo softmax
* Poƒç√≠ta≈• gradienty v≈°etk√Ωch v√Ωrazov, aby sme mohli vykon√°va≈• optimaliz√°ciu pomocou gradientn√©ho zostupu

## [Kv√≠z pred predn√°≈°kou](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

K√Ωm kni≈ænica `numpy` dok√°≈æe vykon√°va≈• prv√∫ ƒças≈•, potrebujeme mechanizmus na v√Ωpoƒçet gradientov. V [na≈°om frameworku](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), ktor√Ω sme vyvinuli v predch√°dzaj√∫cej sekcii, sme museli manu√°lne programova≈• v≈°etky deriv√°cie vo vn√∫tri met√≥dy `backward`, ktor√° vykon√°va sp√§tn√∫ propag√°ciu. Ide√°lne by mal framework umo≈æni≈• v√Ωpoƒçet gradientov *ak√©hokoƒævek v√Ωrazu*, ktor√Ω m√¥≈æeme definova≈•.

ƒéal≈°ou d√¥le≈æitou vecou je schopnos≈• vykon√°va≈• v√Ωpoƒçty na GPU alebo in√Ωch ≈°pecializovan√Ωch v√Ωpoƒçtov√Ωch jednotk√°ch, ako napr√≠klad [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Tr√©novanie hlbok√Ωch neur√≥nov√Ωch siet√≠ vy≈æaduje *veƒæk√© mno≈æstvo* v√Ωpoƒçtov, a mo≈ænos≈• paralelizova≈• tieto v√Ωpoƒçty na GPU je veƒæmi d√¥le≈æit√°.

> ‚úÖ Term√≠n 'paralelizova≈•' znamen√° rozlo≈æi≈• v√Ωpoƒçty na viacer√© zariadenia.

V s√∫ƒçasnosti s√∫ najpopul√°rnej≈°ie frameworky pre neur√≥nov√© siete: [TensorFlow](http://TensorFlow.org) a [PyTorch](https://pytorch.org/). Oba poskytuj√∫ n√≠zko√∫rov≈àov√© API na oper√°cie s tenzormi na CPU aj GPU. Na vrchole n√≠zko√∫rov≈àov√©ho API existuje aj vysoko√∫rov≈àov√© API, naz√Ωvan√© [Keras](https://keras.io/) a [PyTorch Lightning](https://pytorchlightning.ai/) zodpovedaj√∫co.

N√≠zko√∫rov≈àov√© API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
Vysoko√∫rov≈àov√© API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**N√≠zko√∫rov≈àov√© API** v oboch frameworkoch umo≈æ≈àuj√∫ vytv√°ra≈• tzv. **v√Ωpoƒçtov√© grafy**. Tento graf definuje, ako vypoƒç√≠ta≈• v√Ωstup (zvyƒçajne funkciu straty) s dan√Ωmi vstupn√Ωmi parametrami, a m√¥≈æe by≈• posunut√Ω na v√Ωpoƒçet na GPU, ak je dostupn√Ω. Existuj√∫ funkcie na diferenci√°ciu tohto v√Ωpoƒçtov√©ho grafu a v√Ωpoƒçet gradientov, ktor√© m√¥≈æu by≈• n√°sledne pou≈æit√© na optimaliz√°ciu parametrov modelu.

**Vysoko√∫rov≈àov√© API** pova≈æuj√∫ neur√≥nov√© siete za **sekvenciu vrstiev** a uƒæahƒçuj√∫ kon≈°trukciu v√§ƒç≈°iny neur√≥nov√Ωch siet√≠. Tr√©novanie modelu zvyƒçajne vy≈æaduje pr√≠pravu d√°t a n√°sledn√© zavolanie funkcie `fit`, ktor√° vykon√° pr√°cu.

Vysoko√∫rov≈àov√© API umo≈æ≈àuj√∫ veƒæmi r√Ωchlo kon≈°truova≈• typick√© neur√≥nov√© siete bez starost√≠ o mno≈æstvo detailov. Z√°rove≈à n√≠zko√∫rov≈àov√© API pon√∫kaj√∫ oveƒæa v√§ƒç≈°iu kontrolu nad procesom tr√©novania, a preto sa ƒçasto pou≈æ√≠vaj√∫ vo v√Ωskume, keƒè pracujete s nov√Ωmi architekt√∫rami neur√≥nov√Ωch siet√≠.

Je tie≈æ d√¥le≈æit√© pochopi≈•, ≈æe m√¥≈æete pou≈æ√≠va≈• obe API spoloƒçne, napr. m√¥≈æete vyvin√∫≈• vlastn√∫ architekt√∫ru vrstvy siete pomocou n√≠zko√∫rov≈àov√©ho API a potom ju pou≈æi≈• vo v√§ƒç≈°ej sieti kon≈°truovanej a tr√©novanej pomocou vysoko√∫rov≈àov√©ho API. Alebo m√¥≈æete definova≈• sie≈• pomocou vysoko√∫rov≈àov√©ho API ako sekvenciu vrstiev a potom pou≈æi≈• vlastn√Ω n√≠zko√∫rov≈àov√Ω tr√©ningov√Ω cyklus na vykonanie optimaliz√°cie. Obe API pou≈æ√≠vaj√∫ rovnak√© z√°kladn√© koncepty a s√∫ navrhnut√© tak, aby spolu dobre fungovali.

## Uƒçenie

V tomto kurze pon√∫kame v√§ƒç≈°inu obsahu pre PyTorch aj TensorFlow. M√¥≈æete si vybra≈• preferovan√Ω framework a prejs≈• iba zodpovedaj√∫ce notebooky. Ak si nie ste ist√≠, ktor√Ω framework si vybra≈•, preƒç√≠tajte si diskusie na internete o **PyTorch vs. TensorFlow**. M√¥≈æete sa tie≈æ pozrie≈• na oba frameworky, aby ste z√≠skali lep≈°ie pochopenie.

Kde je to mo≈æn√©, pou≈æijeme vysoko√∫rov≈àov√© API pre jednoduchos≈•. Ver√≠me v≈°ak, ≈æe je d√¥le≈æit√© pochopi≈•, ako neur√≥nov√© siete funguj√∫ od z√°kladov, preto na zaƒçiatku zaƒç√≠name pr√°cou s n√≠zko√∫rov≈àov√Ωm API a tenzormi. Ak v≈°ak chcete zaƒça≈• r√Ωchlo a nechcete tr√°vi≈• veƒæa ƒçasu uƒçen√≠m t√Ωchto detailov, m√¥≈æete ich preskoƒçi≈• a √≠s≈• priamo do notebookov s vysoko√∫rov≈àov√Ωm API.

## ‚úçÔ∏è Cviƒçenia: Frameworky

Pokraƒçujte vo svojom uƒçen√≠ v nasleduj√∫cich notebookoch:

N√≠zko√∫rov≈àov√© API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
Vysoko√∫rov≈àov√© API| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Po zvl√°dnut√≠ frameworkov si zopakujme koncept overfittingu.

# Overfitting

Overfitting je mimoriadne d√¥le≈æit√Ω koncept v strojovom uƒçen√≠, a je veƒæmi d√¥le≈æit√© ho spr√°vne pochopi≈•!

Zv√°≈æte nasleduj√∫ci probl√©m aproxim√°cie 5 bodov (reprezentovan√Ωch `x` na grafoch ni≈æ≈°ie):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.sk.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.sk.jpg)
-------------------------|--------------------------
**Line√°rny model, 2 parametre** | **Neline√°rny model, 7 parametrov**
Chyba tr√©novania = 5.3 | Chyba tr√©novania = 0
Chyba valid√°cie = 5.1 | Chyba valid√°cie = 20

* Naƒæavo vid√≠me dobr√∫ aproxim√°ciu priamkou. Preto≈æe poƒçet parametrov je primeran√Ω, model spr√°vne pochop√≠ rozlo≈æenie bodov.
* Napravo je model pr√≠li≈° v√Ωkonn√Ω. Preto≈æe m√°me iba 5 bodov a model m√° 7 parametrov, m√¥≈æe sa prisp√¥sobi≈• tak, aby pre≈°iel v≈°etk√Ωmi bodmi, ƒç√≠m sa chyba tr√©novania stane 0. To v≈°ak br√°ni modelu pochopi≈• spr√°vny vzor v d√°tach, a preto je chyba valid√°cie veƒæmi vysok√°.

Je veƒæmi d√¥le≈æit√© n√°js≈• spr√°vnu rovnov√°hu medzi bohatstvom modelu (poƒçet parametrov) a poƒçtom tr√©novac√≠ch vzoriek.

## Preƒço doch√°dza k overfittingu

  * Nedostatok tr√©novac√≠ch d√°t
  * Pr√≠li≈° v√Ωkonn√Ω model
  * Pr√≠li≈° veƒæa ≈°umu vo vstupn√Ωch d√°tach

## Ako detekova≈• overfitting

Ako m√¥≈æete vidie≈• na grafe vy≈°≈°ie, overfitting mo≈æno detekova≈• veƒæmi n√≠zkou chybou tr√©novania a vysokou chybou valid√°cie. Norm√°lne poƒças tr√©novania vid√≠me, ≈æe chyby tr√©novania aj valid√°cie zaƒç√≠naj√∫ klesa≈•, a potom v urƒçitom bode chyba valid√°cie m√¥≈æe presta≈• klesa≈• a zaƒça≈• st√∫pa≈•. Toto bude znak overfittingu a indik√°tor, ≈æe by sme mali pravdepodobne zastavi≈• tr√©novanie (alebo aspo≈à urobi≈• sn√≠mku modelu).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.sk.png)

## Ako pred√≠s≈• overfittingu

Ak vid√≠te, ≈æe doch√°dza k overfittingu, m√¥≈æete urobi≈• jedno z nasleduj√∫cich:

 * Zv√Ω≈°i≈• mno≈æstvo tr√©novac√≠ch d√°t
 * Zn√≠≈æi≈• komplexnos≈• modelu
 * Pou≈æi≈• nejak√∫ [regularizaƒçn√∫ techniku](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), ako napr√≠klad [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), ktor√∫ si nesk√¥r rozoberieme.

## Overfitting a tradeoff medzi biasom a varianciou

Overfitting je vlastne pr√≠pad v≈°eobecnej≈°ieho probl√©mu v ≈°tatistike naz√Ωvan√©ho [tradeoff medzi biasom a varianciou](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Ak zv√°≈æime mo≈æn√© zdroje chyby v na≈°om modeli, m√¥≈æeme vidie≈• dva typy ch√Ωb:

* **Chyby biasu** s√∫ sp√¥soben√© t√Ωm, ≈æe n√°≈° algoritmus nedok√°≈æe spr√°vne zachyti≈• vz≈•ah medzi tr√©novac√≠mi d√°tami. M√¥≈æe to by≈• v√Ωsledok toho, ≈æe n√°≈° model nie je dostatoƒçne v√Ωkonn√Ω (**underfitting**).
* **Chyby variancie**, ktor√© s√∫ sp√¥soben√© t√Ωm, ≈æe model aproximuje ≈°um vo vstupn√Ωch d√°tach namiesto zmyslupln√©ho vz≈•ahu (**overfitting**).

Poƒças tr√©novania chyba biasu kles√° (keƒè sa n√°≈° model uƒç√≠ aproximova≈• d√°ta) a chyba variancie st√∫pa. Je d√¥le≈æit√© zastavi≈• tr√©novanie - buƒè manu√°lne (keƒè detekujeme overfitting) alebo automaticky (zaveden√≠m regulariz√°cie) - aby sme predi≈°li overfittingu.

## Z√°ver

V tejto lekcii ste sa nauƒçili rozdiely medzi r√¥znymi API pre dva najpopul√°rnej≈°ie AI frameworky, TensorFlow a PyTorch. Okrem toho ste sa nauƒçili o veƒæmi d√¥le≈æitej t√©me, overfittingu.

## üöÄ V√Ωzva

V sprievodn√Ωch notebookoch n√°jdete '√∫lohy' na konci; prejdite si notebooky a dokonƒçite √∫lohy.

## [Kv√≠z po predn√°≈°ke](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Prehƒæad & Samo≈°t√∫dium

Presk√∫majte nasleduj√∫ce t√©my:

- TensorFlow
- PyTorch
- Overfitting

Polo≈æte si nasleduj√∫ce ot√°zky:

- Ak√Ω je rozdiel medzi TensorFlow a PyTorch?
- Ak√Ω je rozdiel medzi overfittingom a underfittingom?

## [Zadanie](lab/README.md)

V tomto laborat√≥riu m√°te za √∫lohu vyrie≈°i≈• dva klasifikaƒçn√© probl√©my pomocou jedno- a viacvrstvov√Ωch plne prepojen√Ωch siet√≠ pomocou PyTorch alebo TensorFlow.

* [In≈°trukcie](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Upozornenie**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keƒè sa sna≈æ√≠me o presnos≈•, pros√≠m, berte na vedomie, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho rodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nie sme zodpovedn√≠ za ≈æiadne nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.