{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dikkat Mekanizmaları ve Transformerlar\n",
    "\n",
    "Tekrarlayan ağların (recurrent networks) en büyük dezavantajlarından biri, bir dizideki tüm kelimelerin sonuca aynı etkiyi yapmasıdır. Bu durum, Adlandırılmış Varlık Tanıma (Named Entity Recognition) ve Makine Çevirisi gibi dizi-dizi görevlerinde standart LSTM encoder-decoder modelleriyle optimal olmayan bir performansa yol açar. Gerçekte, giriş dizisindeki belirli kelimeler genellikle ardışık çıktılar üzerinde diğerlerinden daha fazla etkiye sahiptir.\n",
    "\n",
    "Makine çevirisi gibi bir dizi-dizi modelini düşünün. Bu model, iki tekrarlayan ağ tarafından uygulanır: biri (**encoder**) giriş dizisini gizli bir duruma sıkıştırır, diğeri (**decoder**) bu gizli durumu çevrilmiş bir sonuca açar. Bu yaklaşımın sorunu, ağın son durumunun bir cümlenin başlangıcını hatırlamakta zorlanmasıdır, bu da uzun cümlelerde modelin kalitesinin düşmesine neden olur.\n",
    "\n",
    "**Dikkat Mekanizmaları**, her bir giriş vektörünün RNN'nin her bir çıktı tahmini üzerindeki bağlamsal etkisini ağırlıklandırmanın bir yolunu sağlar. Bu, giriş RNN'nin ara durumları ile çıkış RNN arasında kısayollar oluşturarak uygulanır. Bu şekilde, çıktı sembolü $y_t$ oluşturulurken, farklı ağırlık katsayıları $\\alpha_{t,i}$ ile tüm giriş gizli durumlarını $h_i$ dikkate alırız.\n",
    "\n",
    "![Eklenecek bir dikkat katmanı ile encoder/decoder modelini gösteren görsel](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.tr.png)\n",
    "*[Bahdanau ve ark., 2015](https://arxiv.org/pdf/1409.0473.pdf) çalışmasından alınan ve [bu blog yazısından](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) alıntılanan eklemeli dikkat mekanizmalı encoder-decoder modeli]*\n",
    "\n",
    "Dikkat matrisi $\\{\\alpha_{i,j}\\}$, belirli giriş kelimelerinin çıktı dizisindeki bir kelimenin oluşturulmasında oynadığı rol derecesini temsil eder. Aşağıda böyle bir matrisin örneği verilmiştir:\n",
    "\n",
    "![Bahdanau - arviz.org'dan alınan RNNsearch-50 tarafından bulunan örnek bir hizalamayı gösteren görsel](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.tr.png)\n",
    "\n",
    "*[Bahdanau ve ark., 2015](https://arxiv.org/pdf/1409.0473.pdf) çalışmasından alınan (Şekil 3)]*\n",
    "\n",
    "Dikkat mekanizmaları, Doğal Dil İşleme alanında mevcut veya yakın zamanda mevcut olan en iyi performansın büyük bir kısmından sorumludur. Ancak dikkat eklemek, model parametrelerinin sayısını büyük ölçüde artırır ve bu da RNN'lerde ölçekleme sorunlarına yol açar. RNN'lerin ölçeklenmesindeki temel kısıtlama, modellerin tekrarlayan doğasının eğitimde toplu işlem ve paralelleştirmeyi zorlaştırmasıdır. Bir RNN'de bir dizinin her bir öğesi sıralı bir şekilde işlenmelidir, bu da kolayca paralelleştirilemeyeceği anlamına gelir.\n",
    "\n",
    "Dikkat mekanizmalarının benimsenmesi ve bu kısıtlama, bugün BERT'ten OpenGPT3'e kadar bildiğimiz ve kullandığımız en iyi performanslı Transformer Modellerinin oluşturulmasına yol açtı.\n",
    "\n",
    "## Transformer Modelleri\n",
    "\n",
    "Her bir önceki tahminin bağlamını bir sonraki değerlendirme adımına iletmek yerine, **transformer modelleri** bir metin penceresi içinde verilen bir girişin bağlamını yakalamak için **konumsal kodlamalar** ve dikkat mekanizmasını kullanır. Aşağıdaki görsel, konumsal kodlamaların dikkat ile bir pencere içinde bağlamı nasıl yakalayabileceğini göstermektedir.\n",
    "\n",
    "![Transformer modellerinde değerlendirmelerin nasıl yapıldığını gösteren animasyonlu GIF.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Her bir giriş pozisyonu bağımsız olarak her bir çıkış pozisyonuna eşlendiğinden, transformerlar RNN'lere göre daha iyi paralelleştirilebilir, bu da çok daha büyük ve daha ifade gücü yüksek dil modellerini mümkün kılar. Her bir dikkat başlığı, kelimeler arasındaki farklı ilişkileri öğrenmek için kullanılabilir ve bu da Doğal Dil İşleme görevlerini iyileştirir.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers), *BERT-base* için 12 katman ve *BERT-large* için 24 katman içeren çok büyük bir çok katmanlı transformer ağıdır. Model, büyük bir metin veri kümesi (WikiPedia + kitaplar) üzerinde denetimsiz eğitim (bir cümledeki maskelenmiş kelimeleri tahmin etme) kullanılarak önce önceden eğitilir. Önceden eğitim sırasında model, önemli bir dil anlayışı seviyesini emer ve bu daha sonra diğer veri kümeleriyle ince ayar yapılarak kullanılabilir. Bu sürece **transfer öğrenme** denir.\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ adresinden alınan görsel](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.tr.png)\n",
    "\n",
    "Transformer mimarilerinin BERT, DistilBERT, BigBird, OpenGPT3 ve daha fazlası gibi birçok varyasyonu vardır ve bunlar ince ayar yapılabilir. [HuggingFace paketi](https://github.com/huggingface/) PyTorch ile bu mimarilerin birçoğunu eğitmek için bir depo sağlar.\n",
    "\n",
    "## BERT'i Metin Sınıflandırması için Kullanma\n",
    "\n",
    "Şimdi önceden eğitilmiş BERT modelini geleneksel görevimizi çözmek için nasıl kullanabileceğimize bakalım: dizi sınıflandırması. Orijinal AG News veri setimizi sınıflandıracağız.\n",
    "\n",
    "Öncelikle HuggingFace kütüphanesini ve veri setimizi yükleyelim:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Önceden eğitilmiş BERT modelini kullanacağımız için, belirli bir tokenizer kullanmamız gerekecek. İlk olarak, önceden eğitilmiş BERT modeliyle ilişkili bir tokenizer yükleyeceğiz.\n",
    "\n",
    "HuggingFace kütüphanesi, yalnızca isimlerini `from_pretrained` fonksiyonlarına argüman olarak belirterek kullanabileceğiniz önceden eğitilmiş modellerin bir deposunu içerir. Model için gerekli olan tüm ikili dosyalar otomatik olarak indirilecektir.\n",
    "\n",
    "Ancak, bazı durumlarda kendi modellerinizi yüklemeniz gerekebilir. Bu durumda, tokenizer için parametreler, model parametrelerini içeren `config.json` dosyası, ikili ağırlıklar ve diğer ilgili dosyaları içeren dizini belirtebilirsiniz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` nesnesi, metni doğrudan kodlamak için kullanılabilecek `encode` işlevini içerir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonra, eğitim sırasında verilere erişmek için kullanacağımız yineleyiciler oluşturalım. Çünkü BERT kendi kodlama fonksiyonunu kullanır, daha önce tanımladığımız `padify`'a benzer bir doldurma fonksiyonu tanımlamamız gerekecek:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bizim durumumuzda, `bert-base-uncased` adlı önceden eğitilmiş BERT modelini kullanacağız. Modeli `BertForSequenceClassfication` paketi kullanarak yükleyelim. Bu, modelimizin sınıflandırma için gerekli mimariye, son sınıflandırıcı dahil, zaten sahip olmasını sağlar. Son sınıflandırıcının ağırlıklarının başlatılmadığını ve modelin ön eğitim gerektireceğini belirten bir uyarı mesajı göreceksiniz - bu tamamen normaldir, çünkü tam olarak yapacağımız şey budur!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artık eğitime başlamaya hazırız! BERT zaten önceden eğitildiği için, başlangıç ağırlıklarını bozmamak adına oldukça küçük bir öğrenme oranıyla başlamak istiyoruz.\n",
    "\n",
    "Tüm ağır işi `BertForSequenceClassification` modeli yapıyor. Modeli eğitim verileri üzerinde çağırdığımızda, giriş minibatch'i için hem kayıp (loss) hem de ağ çıktısını döndürüyor. Kayıp, parametre optimizasyonu için kullanılıyor (`loss.backward()` geri yayılımı gerçekleştirir) ve `out`, elde edilen etiketleri `argmax` kullanarak hesaplanan `labs` ile beklenen `labels` değerlerini karşılaştırarak eğitim doğruluğunu hesaplamak için kullanılıyor.\n",
    "\n",
    "Süreci kontrol edebilmek için, birkaç yineleme boyunca kayıp ve doğruluğu biriktiriyor ve her `report_freq` eğitim döngüsünde bunları yazdırıyoruz.\n",
    "\n",
    "Bu eğitim muhtemelen oldukça uzun sürecek, bu yüzden yineleme sayısını sınırlıyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT sınıflandırmasının (özellikle yineleme sayısını artırıp yeterince beklerseniz) oldukça iyi bir doğruluk sağladığını görebilirsiniz! Bunun nedeni, BERT'in dilin yapısını zaten oldukça iyi anlaması ve bizim yalnızca son sınıflandırıcıyı ince ayar yapmamızın gerekmesidir. Ancak, BERT büyük bir model olduğu için tüm eğitim süreci uzun sürer ve ciddi bir hesaplama gücü gerektirir! (GPU ve tercihen birden fazla).\n",
    "\n",
    "> **Not:** Örneğimizde, önceden eğitilmiş en küçük BERT modellerinden birini kullanıyoruz. Daha iyi sonuçlar verme olasılığı yüksek olan daha büyük modeller de mevcuttur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performansını değerlendirme\n",
    "\n",
    "Şimdi modelimizin test veri seti üzerindeki performansını değerlendirebiliriz. Değerlendirme döngüsü, eğitim döngüsüne oldukça benzerdir, ancak `model.eval()` çağrısı yaparak modeli değerlendirme moduna geçirmeyi unutmamalıyız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Özet\n",
    "\n",
    "Bu bölümde, **transformers** kütüphanesinden önceden eğitilmiş bir dil modelini alıp, metin sınıflandırma görevimize uyarlamanın ne kadar kolay olduğunu gördük. Benzer şekilde, BERT modelleri varlık çıkarımı, soru yanıtlama ve diğer NLP görevleri için de kullanılabilir.\n",
    "\n",
    "Transformer modelleri, NLP alanında güncel en iyi durumu temsil eder ve çoğu durumda özel NLP çözümleri uygularken denemeye başlayacağınız ilk çözüm olmalıdır. Ancak, bu modülde ele alınan tekrarlayan sinir ağlarının temel prensiplerini anlamak, gelişmiş sinir modelleri oluşturmak istiyorsanız son derece önemlidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Feragatname**:  \nBu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluğu sağlamak için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T14:13:03+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "tr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}