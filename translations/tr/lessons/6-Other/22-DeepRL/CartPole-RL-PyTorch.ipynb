{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Dengeleme için RL Eğitimi\n",
    "\n",
    "Bu not defteri, [AI for Beginners Müfredatı](http://aka.ms/ai-beginners) kapsamında yer almaktadır. [Resmi PyTorch eğitimi](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) ve [bu Cartpole PyTorch uygulamasından](https://github.com/yc930401/Actor-Critic-pytorch) ilham alınarak hazırlanmıştır.\n",
    "\n",
    "Bu örnekte, bir modelin yatay bir düzlemde sağa ve sola hareket edebilen bir arabada bir direği dengelemesini sağlamak için RL kullanacağız. Direği simüle etmek için [OpenAI Gym](https://www.gymlibrary.ml/) ortamını kullanacağız.\n",
    "\n",
    "> **Not**: Bu dersin kodunu yerel olarak (örneğin, Visual Studio Code'dan) çalıştırabilirsiniz; bu durumda simülasyon yeni bir pencerede açılacaktır. Kodu çevrimiçi çalıştırırken, [burada](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) açıklandığı gibi kodda bazı değişiklikler yapmanız gerekebilir.\n",
    "\n",
    "Gym'in yüklü olduğundan emin olarak başlayacağız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi CartPole ortamını oluşturalım ve üzerinde nasıl işlem yapacağımızı görelim. Bir ortamın şu özellikleri vardır:\n",
    "\n",
    "* **Eylem alanı**, simülasyonun her adımında gerçekleştirebileceğimiz olası eylemlerin kümesidir  \n",
    "* **Gözlem alanı**, yapabileceğimiz gözlemlerin alanıdır  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simülasyonun nasıl çalıştığını görelim. Aşağıdaki döngü, `env.step` sonlandırma bayrağı `done` döndürmediği sürece simülasyonu çalıştırır. Eylemleri rastgele seçmek için `env.action_space.sample()` kullanacağız, bu da deneyin muhtemelen çok hızlı bir şekilde başarısız olacağı anlamına gelir (CartPole ortamı, CartPole'un hızı, konumu veya açısı belirli sınırların dışına çıktığında sonlanır).\n",
    "\n",
    "> Simülasyon yeni bir pencerede açılacaktır. Kodu birkaç kez çalıştırabilir ve nasıl davrandığını görebilirsiniz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gözlemlerin 4 sayı içerdiğini fark edebilirsiniz. Bunlar şunlardır:\n",
    "- Arabanın konumu\n",
    "- Arabanın hızı\n",
    "- Direğin açısı\n",
    "- Direğin dönüş hızı\n",
    "\n",
    "`rew`, her adımda aldığımız ödüldür. CartPole ortamında, her simülasyon adımı için 1 puan ödül alırsınız ve amaç toplam ödülü maksimize etmektir, yani CartPole'un düşmeden dengede kalabildiği süreyi artırmaktır.\n",
    "\n",
    "Pekiştirmeli öğrenme sırasında, amacımız her durum $s$ için hangi eylemi $a$ yapmamız gerektiğini söyleyen bir **politika** $\\pi$ eğitmek, yani temelde $a = \\pi(s)$.\n",
    "\n",
    "Eğer olasılıksal bir çözüm istiyorsanız, politikayı her eylem için bir olasılık seti döndüren bir şey olarak düşünebilirsiniz, yani $\\pi(a|s)$, durum $s$'de eylem $a$'yı yapmamız gerektiği olasılığı anlamına gelir.\n",
    "\n",
    "## Politika Gradyan Yöntemi\n",
    "\n",
    "En basit RL algoritmasında, **Politika Gradyanı** olarak adlandırılan yöntemde, bir sinir ağı eğiterek bir sonraki eylemi tahmin edeceğiz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ağı eğitmek için birçok deney yaparak ve her çalıştırmadan sonra ağımızı güncelleyerek ilerleyeceğiz. Deneyi çalıştıracak ve sonuçları (sözde **iz**) - tüm durumlar, eylemler (ve önerilen olasılıkları) ve ödülleri döndürecek bir fonksiyon tanımlayalım:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğitilmemiş ağ ile bir bölüm çalıştırabilir ve toplam ödülün (diğer bir deyişle bölümün uzunluğu) çok düşük olduğunu gözlemleyebilirsiniz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Politika gradyan algoritmasının zorlu yönlerinden biri **indirimli ödüller** kullanmaktır. Fikir, oyunun her adımında toplam ödüller vektörünü hesaplamak ve bu süreçte erken ödülleri bir $gamma$ katsayısı kullanarak indirimli hale getirmektir. Ayrıca, elde edilen vektörü normalize ederiz çünkü bunu eğitimimizi etkilemek için ağırlık olarak kullanacağız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi gerçek eğitime başlayalım! 300 bölüm çalıştıracağız ve her bölümde aşağıdaki adımları gerçekleştireceğiz:\n",
    "\n",
    "1. Deneyi çalıştır ve izleri topla\n",
    "1. Alınan eylemler ile tahmin edilen olasılıklar arasındaki farkı (`gradients`) hesapla. Fark ne kadar az olursa, doğru eylemi seçtiğimizden o kadar emin oluruz.\n",
    "1. İndirimli ödülleri hesapla ve `gradients` ile çarp - bu, daha yüksek ödüllere sahip adımların nihai sonuç üzerinde daha fazla etkisi olmasını, düşük ödüllü adımların ise daha az etkisi olmasını sağlar.\n",
    "1. Sinir ağımız için beklenen hedef eylemler, kısmen çalıştırma sırasında tahmin edilen olasılıklardan, kısmen de hesaplanan `gradients` değerlerinden alınacaktır. `alpha` parametresini kullanarak `gradients` ve ödüllerin ne ölçüde dikkate alınacağını belirleyeceğiz - bu, pekiştirme algoritmasının *öğrenme oranı* olarak adlandırılır.\n",
    "1. Son olarak, ağımızı durumlar ve beklenen eylemler üzerinde eğitiyoruz ve süreci tekrarlıyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi sonucu görmek için bölümü render ederek çalıştıralım:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umarım artık çubuğun oldukça iyi dengede durabildiğini görebiliyorsunuz!\n",
    "\n",
    "## Aktör-Kritik Modeli\n",
    "\n",
    "Aktör-Kritik modeli, politika gradyanlarının daha ileri bir geliştirmesidir. Bu modelde, hem politikayı hem de tahmini ödülleri öğrenmek için bir sinir ağı oluştururuz. Ağın iki çıktısı olacaktır (ya da bunu iki ayrı ağ olarak düşünebilirsiniz):\n",
    "* **Aktör**, politika gradyan modelinde olduğu gibi, durum olasılık dağılımını vererek hangi eylemin yapılması gerektiğini önerecektir.\n",
    "* **Kritik**, bu eylemlerden elde edilecek ödüllerin ne olacağını tahmin eder. Verilen durumda gelecekteki toplam tahmini ödülleri döndürür.\n",
    "\n",
    "Şimdi böyle bir modeli tanımlayalım:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`discounted_rewards` ve `run_episode` fonksiyonlarımızı biraz değiştirmemiz gerekecek:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi ana eğitim döngüsünü çalıştıracağız. Uygun kayıp fonksiyonlarını hesaplayarak ve ağ parametrelerini güncelleyerek manuel ağ eğitim sürecini kullanacağız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Özet\n",
    "\n",
    "Bu demoda iki RL algoritması gördük: basit politika gradyanı ve daha sofistike aktör-eleştirmen. Bu algoritmaların durum, eylem ve ödül gibi soyut kavramlarla çalıştığını görebilirsiniz - bu nedenle çok farklı ortamlara uygulanabilirler.\n",
    "\n",
    "Pekiştirmeli öğrenme, yalnızca nihai ödüle bakarak problemi çözmek için en iyi stratejiyi öğrenmemizi sağlar. Etiketlenmiş veri kümelerine ihtiyaç duymamamız, modellerimizi optimize etmek için simülasyonları birçok kez tekrarlamamıza olanak tanır. Ancak, RL'de hâlâ birçok zorluk bulunmaktadır ve bu ilginç AI alanına daha fazla odaklanmaya karar verirseniz bunları öğrenebilirsiniz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Feragatname**:  \nBu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluğu sağlamak için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul edilmez.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T12:49:15+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "tr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}