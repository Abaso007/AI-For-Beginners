<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "789d6c3fb6fc7948a470b33078a5983a",
  "translation_date": "2025-09-23T08:43:26+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "tr"
}
-->
# Sinir AÄŸlarÄ±na GiriÅŸ. Ã‡ok KatmanlÄ± Perceptron

Ã–nceki bÃ¶lÃ¼mde, en basit sinir aÄŸÄ± modeli olan tek katmanlÄ± perceptron, yani doÄŸrusal iki sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma modelini Ã¶ÄŸrendiniz.

Bu bÃ¶lÃ¼mde, bu modeli daha esnek bir Ã§erÃ§eveye geniÅŸleteceÄŸiz. Bu sayede:

* Ä°ki sÄ±nÄ±fÄ±n yanÄ± sÄ±ra **Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma** yapabiliriz  
* SÄ±nÄ±flandÄ±rmanÄ±n yanÄ± sÄ±ra **regresyon problemlerini** Ã§Ã¶zebiliriz  
* DoÄŸrusal olarak ayrÄ±labilir olmayan sÄ±nÄ±flarÄ± ayÄ±rabiliriz  

AyrÄ±ca, farklÄ± sinir aÄŸÄ± mimarileri oluÅŸturabilmemizi saÄŸlayacak kendi modÃ¼ler Python Ã§erÃ§evemizi geliÅŸtireceÄŸiz.

## [Ders Ã–ncesi Testi](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Makine Ã–ÄŸreniminin Formalizasyonu

Makine Ã–ÄŸrenimi problemini formalize ederek baÅŸlayalÄ±m. Diyelim ki **X** adlÄ± bir eÄŸitim veri setimiz ve **Y** adlÄ± etiketlerimiz var ve en doÄŸru tahminleri yapacak bir model *f* inÅŸa etmemiz gerekiyor. Tahminlerin kalitesi **KayÄ±p Fonksiyonu** &lagran; ile Ã¶lÃ§Ã¼lÃ¼r. AÅŸaÄŸÄ±daki kayÄ±p fonksiyonlarÄ± sÄ±kÃ§a kullanÄ±lÄ±r:

* Bir sayÄ± tahmin etmemiz gereken regresyon problemlerinde, **mutlak hata** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| veya **kare hata** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> kullanÄ±labilir.  
* SÄ±nÄ±flandÄ±rma iÃ§in, **0-1 kaybÄ±** (temelde modelin **doÄŸruluÄŸu** ile aynÄ±dÄ±r) veya **lojistik kayÄ±p** kullanÄ±lÄ±r.

Tek katmanlÄ± perceptron iÃ§in, *f* fonksiyonu doÄŸrusal bir fonksiyon olarak tanÄ±mlanÄ±r: *f(x)=wx+b* (burada *w* aÄŸÄ±rlÄ±k matrisi, *x* giriÅŸ Ã¶zelliklerinin vektÃ¶rÃ¼ ve *b* bias vektÃ¶rÃ¼dÃ¼r). FarklÄ± sinir aÄŸÄ± mimarileri iÃ§in bu fonksiyon daha karmaÅŸÄ±k bir form alabilir.

> SÄ±nÄ±flandÄ±rma durumunda, aÄŸ Ã§Ä±ktÄ±sÄ± olarak ilgili sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±nÄ± elde etmek genellikle istenir. Rastgele sayÄ±larÄ± olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rmek (Ã¶rneÄŸin, Ã§Ä±ktÄ±yÄ± normalize etmek) iÃ§in genellikle **softmax** fonksiyonu &sigma; kullanÄ±lÄ±r ve *f* fonksiyonu *f(x)=&sigma;(wx+b)* haline gelir.

YukarÄ±daki *f* tanÄ±mÄ±nda, *w* ve *b* **parametreler** olarak adlandÄ±rÄ±lÄ±r: &theta;=âŸ¨*w,b*âŸ©. Veri seti âŸ¨**X**,**Y**âŸ© verildiÄŸinde, tÃ¼m veri seti Ã¼zerindeki toplam hatayÄ± parametrelerin bir fonksiyonu olarak hesaplayabiliriz.

> âœ… **Sinir aÄŸÄ± eÄŸitiminin amacÄ±, parametreleri &theta; deÄŸiÅŸtirerek hatayÄ± minimize etmektir.**

## Gradyan Ä°niÅŸi Optimizasyonu

**Gradyan iniÅŸi** adÄ± verilen iyi bilinen bir fonksiyon optimizasyon yÃ¶ntemi vardÄ±r. Bu yÃ¶ntemin temel fikri, kayÄ±p fonksiyonunun parametrelere gÃ¶re tÃ¼revini (Ã§ok boyutlu durumda **gradyan** olarak adlandÄ±rÄ±lÄ±r) hesaplayarak, hatayÄ± azaltacak ÅŸekilde parametreleri deÄŸiÅŸtirmektir. Bu ÅŸu ÅŸekilde formalize edilebilir:

* Parametreleri rastgele deÄŸerlerle baÅŸlat: w<sup>(0)</sup>, b<sup>(0)</sup>  
* AÅŸaÄŸÄ±daki adÄ±mÄ± birÃ§ok kez tekrarla:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w  
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b  

EÄŸitim sÄ±rasÄ±nda, optimizasyon adÄ±mlarÄ± tÃ¼m veri seti dikkate alÄ±narak hesaplanmalÄ±dÄ±r (hatÄ±rlayÄ±n, kayÄ±p tÃ¼m eÄŸitim Ã¶rnekleri Ã¼zerinden bir toplam olarak hesaplanÄ±r). Ancak, gerÃ§ek hayatta veri setinin kÃ¼Ã§Ã¼k parÃ§alarÄ± olan **minibatch'ler** alÄ±nÄ±r ve gradyanlar bir alt veri kÃ¼mesine dayanarak hesaplanÄ±r. Alt kÃ¼me her seferinde rastgele alÄ±ndÄ±ÄŸÄ± iÃ§in bu yÃ¶nteme **stokastik gradyan iniÅŸi** (SGD) denir.

## Ã‡ok KatmanlÄ± Perceptronlar ve Geri YayÄ±lÄ±m

YukarÄ±da gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, tek katmanlÄ± bir aÄŸ doÄŸrusal olarak ayrÄ±labilir sÄ±nÄ±flarÄ± sÄ±nÄ±flandÄ±rabilir. Daha zengin bir model oluÅŸturmak iÃ§in, aÄŸÄ±n birkaÃ§ katmanÄ±nÄ± birleÅŸtirebiliriz. Matematiksel olarak bu, *f* fonksiyonunun daha karmaÅŸÄ±k bir form alacaÄŸÄ± ve birkaÃ§ adÄ±mda hesaplanacaÄŸÄ± anlamÄ±na gelir:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>  
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>  
* f = &sigma;(z<sub>2</sub>)  

Burada, &alpha; bir **doÄŸrusal olmayan aktivasyon fonksiyonu**, &sigma; bir softmax fonksiyonu ve parametreler &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>'dir.

Gradyan iniÅŸi algoritmasÄ± aynÄ± kalÄ±r, ancak gradyanlarÄ± hesaplamak daha zor hale gelir. Zincir tÃ¼rev kuralÄ± kullanÄ±larak tÃ¼revler ÅŸu ÅŸekilde hesaplanabilir:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)  
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)  

> âœ… Zincir tÃ¼rev kuralÄ±, kayÄ±p fonksiyonunun parametrelere gÃ¶re tÃ¼revlerini hesaplamak iÃ§in kullanÄ±lÄ±r.

Dikkat edin, bu ifadelerin en sol kÄ±smÄ± aynÄ±dÄ±r ve bu nedenle tÃ¼revleri etkili bir ÅŸekilde kayÄ±p fonksiyonundan baÅŸlayarak "geri" doÄŸru hesaplayabiliriz. Bu nedenle, Ã§ok katmanlÄ± perceptron eÄŸitme yÃ¶ntemi **geri yayÄ±lÄ±m** veya 'backprop' olarak adlandÄ±rÄ±lÄ±r.

<img alt="hesaplama grafiÄŸi" src="images/ComputeGraphGrad.png"/>

> TODO: gÃ¶rsel kaynaÄŸÄ±

> âœ… Geri yayÄ±lÄ±mÄ±, notebook Ã¶rneÄŸimizde Ã§ok daha ayrÄ±ntÄ±lÄ± bir ÅŸekilde ele alacaÄŸÄ±z.

## SonuÃ§

Bu derste, kendi sinir aÄŸÄ± kÃ¼tÃ¼phanemizi oluÅŸturduk ve bunu basit bir iki boyutlu sÄ±nÄ±flandÄ±rma gÃ¶revi iÃ§in kullandÄ±k.

## ğŸš€ Meydan Okuma

EÅŸlik eden notebook'ta, Ã§ok katmanlÄ± perceptronlar oluÅŸturmak ve eÄŸitmek iÃ§in kendi Ã§erÃ§evenizi uygulayacaksÄ±nÄ±z. Modern sinir aÄŸlarÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± ayrÄ±ntÄ±lÄ± bir ÅŸekilde gÃ¶rebileceksiniz.

[OwnFramework](OwnFramework.ipynb) notebook'una geÃ§in ve Ã¼zerinde Ã§alÄ±ÅŸÄ±n.

## [Ders SonrasÄ± Testi](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

Geri yayÄ±lÄ±m, AI ve ML'de yaygÄ±n olarak kullanÄ±lan bir algoritmadÄ±r ve [daha ayrÄ±ntÄ±lÄ±](https://wikipedia.org/wiki/Backpropagation) Ã§alÄ±ÅŸmaya deÄŸerdir.

## [Ã–dev](lab/README.md)

Bu laboratuvarda, bu derste oluÅŸturduÄŸunuz Ã§erÃ§eveyi kullanarak MNIST el yazÄ±sÄ± rakam sÄ±nÄ±flandÄ±rmasÄ±nÄ± Ã§Ã¶zmeniz isteniyor.

* [Talimatlar](lab/README.md)  
* [Notebook](lab/MyFW_MNIST.ipynb)  

---

