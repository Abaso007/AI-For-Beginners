<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-31T17:43:10+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "en"
}
-->
# Generative Adversarial Networks

In the previous section, we explored **generative models**: models capable of creating new images similar to those in the training dataset. VAE was a good example of such a model.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/19)

However, if we attempt to generate something truly meaningful, like a painting with reasonable resolution, using VAE, we‚Äôll notice that training doesn‚Äôt converge well. For this purpose, we need to learn about another architecture specifically designed for generative models‚Äî**Generative Adversarial Networks**, or GANs.

The core idea of a GAN is to have two neural networks that are trained in opposition to each other:

<img src="images/gan_architecture.png" width="70%"/>

> Image by [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ A quick glossary:
> * **Generator**: A network that takes a random vector and produces an image as output.
> * **Discriminator**: A network that takes an image and determines whether it‚Äôs a real image (from the training dataset) or one generated by the generator. Essentially, it acts as an image classifier.

### Discriminator

The discriminator‚Äôs architecture is similar to a standard image classification network. In its simplest form, it can be a fully-connected classifier, but it‚Äôs more likely to be a [convolutional network](../07-ConvNets/README.md).

> ‚úÖ A GAN that uses convolutional networks is called a [DCGAN](https://arxiv.org/pdf/1511.06434.pdf).

A CNN-based discriminator typically includes several convolution and pooling layers (with decreasing spatial dimensions) followed by one or more fully-connected layers to produce a "feature vector" and a final binary classifier.

> ‚úÖ "Pooling" refers to a technique that reduces the size of the image. "Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer." - [source](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Generator

The generator is a bit more complex. You can think of it as a reversed discriminator. Starting with a latent vector (instead of a feature vector), it uses a fully-connected layer to reshape it into the desired size/shape, followed by deconvolutions and upscaling. This is similar to the *decoder* part of an [autoencoder](../09-Autoencoders/README.md).

> ‚úÖ Since convolution layers are implemented as linear filters that traverse the image, deconvolution is essentially similar to convolution and can be implemented using the same layer logic.

<img src="images/gan_arch_detail.png" width="70%"/>

> Image by [Dmitry Soshnikov](http://soshnikov.com)

### Training the GAN

GANs are called **adversarial** because the generator and discriminator are constantly competing against each other. Through this competition, both networks improve, enabling the GAN to generate increasingly realistic images.

Training occurs in two stages:

* **Training the discriminator**: This step is straightforward. We generate a batch of images using the generator, label them as 0 (fake images), and take a batch of images from the input dataset labeled as 1 (real images). We calculate the *discriminator loss* and perform backpropagation.
* **Training the generator**: This step is more challenging because we don‚Äôt directly know the expected output for the generator. We use the entire GAN network (generator followed by discriminator), feed it random vectors, and expect the output to be 1 (indicating real images). We freeze the discriminator‚Äôs parameters (to prevent it from training during this step) and perform backpropagation.

During training, the losses for both the generator and discriminator don‚Äôt decrease significantly. Ideally, they should oscillate, reflecting the improvement of both networks.

## ‚úçÔ∏è Exercises: GANs

* [GAN Notebook in TensorFlow/Keras](GANTF.ipynb)
* [GAN Notebook in PyTorch](GANPyTorch.ipynb)

### Challenges in GAN Training

GANs are notoriously difficult to train. Here are some common issues:

* **Mode Collapse**: This occurs when the generator learns to produce a single successful image that consistently fools the discriminator, rather than generating a variety of images.
* **Sensitivity to hyperparameters**: GANs often fail to converge, but a small adjustment in hyperparameters (like learning rate) can suddenly lead to convergence.
* Maintaining a **balance** between the generator and discriminator: Sometimes, the discriminator‚Äôs loss drops to zero quickly, making it impossible for the generator to improve further. To address this, you can try using different learning rates for the generator and discriminator or skip discriminator training when its loss is already very low.
* Training for **high resolution**: Similar to the challenges faced by autoencoders, reconstructing high-resolution images can lead to artifacts. This issue is often addressed using **progressive growing**, where training starts with low-resolution images and gradually adds layers or "unblocks" them. Another approach involves adding extra connections between layers and training multiple resolutions simultaneously‚Äîsee this [Multi-Scale Gradient GANs paper](https://arxiv.org/abs/1903.06048) for more details.

## Style Transfer

GANs are excellent for generating artistic images. Another fascinating technique is **style transfer**, which takes a **content image** and redraws it in a different style using filters from a **style image**.

Here‚Äôs how it works:
* Start with a random noise image (or a content image, but for simplicity, we‚Äôll use random noise).
* The goal is to create an image that resembles both the content image and the style image. This is achieved using two loss functions:
   - **Content loss**: Calculated based on features extracted by the CNN at certain layers from the current image and the content image.
   - **Style loss**: Computed between the current image and the style image using Gram matrices (more details in the [example notebook](StyleTransfer.ipynb)).
* To smooth the image and reduce noise, we introduce **Variation loss**, which calculates the average distance between neighboring pixels.
* The main optimization loop adjusts the current image using gradient descent (or another optimization algorithm) to minimize the total loss, which is a weighted sum of all three losses.

## ‚úçÔ∏è Example: [Style Transfer](StyleTransfer.ipynb)

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## Conclusion

In this lesson, you learned about GANs and their training process. You also explored the unique challenges associated with this type of neural network and strategies to overcome them.

## üöÄ Challenge

Try running the [Style Transfer notebook](StyleTransfer.ipynb) using your own images.

## Review & Self Study

For further reading, explore these resources on GANs:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), a widely-used GAN architecture
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Assignment

Revisit one of the two notebooks from this lesson and retrain the GAN using your own images. What can you create?

---

**Disclaimer**:  
This document has been translated using the AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we aim for accuracy, please note that automated translations may include errors or inaccuracies. The original document in its native language should be regarded as the authoritative source. For critical information, professional human translation is advised. We are not responsible for any misunderstandings or misinterpretations resulting from the use of this translation.