<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-24T10:40:26+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "pl"
}
-->
# Wprowadzenie do Sieci Neuronowych. Perceptron Wielowarstwowy

W poprzedniej sekcji nauczyÅ‚eÅ› siÄ™ o najprostszym modelu sieci neuronowej - perceptronie jednowarstwowym, bÄ™dÄ…cym liniowym modelem klasyfikacji dwuklasowej.

W tej sekcji rozszerzymy ten model do bardziej elastycznego frameworka, ktÃ³ry pozwoli nam:

* wykonywaÄ‡ **klasyfikacjÄ™ wieloklasowÄ…** oprÃ³cz dwuklasowej
* rozwiÄ…zywaÄ‡ **problemy regresji** oprÃ³cz klasyfikacji
* rozdzielaÄ‡ klasy, ktÃ³re nie sÄ… liniowo separowalne

Stworzymy rÃ³wnieÅ¼ wÅ‚asny moduÅ‚owy framework w Pythonie, ktÃ³ry umoÅ¼liwi nam budowanie rÃ³Å¼nych architektur sieci neuronowych.

## [Quiz przed wykÅ‚adem](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalizacja Uczenia Maszynowego

Zacznijmy od formalizacji problemu Uczenia Maszynowego. ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych treningowych **X** z etykietami **Y**, i musimy zbudowaÄ‡ model *f*, ktÃ³ry bÄ™dzie dokonywaÅ‚ jak najdokÅ‚adniejszych przewidywaÅ„. JakoÅ›Ä‡ przewidywaÅ„ mierzymy za pomocÄ… **funkcji straty** â„’. CzÄ™sto uÅ¼ywane funkcje straty to:

* W przypadku problemu regresji, gdy musimy przewidzieÄ‡ liczbÄ™, moÅ¼emy uÅ¼yÄ‡ **bÅ‚Ä™du bezwzglÄ™dnego** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| lub **bÅ‚Ä™du kwadratowego** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* W przypadku klasyfikacji uÅ¼ywamy **straty 0-1** (ktÃ³ra jest zasadniczo tym samym co **dokÅ‚adnoÅ›Ä‡** modelu) lub **straty logistycznej**.

Dla perceptronu jednowarstwowego funkcja *f* byÅ‚a zdefiniowana jako funkcja liniowa *f(x)=wx+b* (gdzie *w* to macierz wag, *x* to wektor cech wejÅ›ciowych, a *b* to wektor przesuniÄ™cia). Dla rÃ³Å¼nych architektur sieci neuronowych funkcja ta moÅ¼e przyjmowaÄ‡ bardziej zÅ‚oÅ¼onÄ… formÄ™.

> W przypadku klasyfikacji czÄ™sto poÅ¼Ä…dane jest uzyskanie prawdopodobieÅ„stw odpowiadajÄ…cych klas jako wyjÅ›cia sieci. Aby przeksztaÅ‚ciÄ‡ dowolne liczby na prawdopodobieÅ„stwa (np. znormalizowaÄ‡ wyjÅ›cie), czÄ™sto uÅ¼ywamy funkcji **softmax** Ïƒ, a funkcja *f* staje siÄ™ *f(x)=Ïƒ(wx+b)*

W definicji *f* powyÅ¼ej, *w* i *b* nazywane sÄ… **parametrami** Î¸=âŸ¨*w,b*âŸ©. MajÄ…c zbiÃ³r danych âŸ¨**X**,**Y**âŸ©, moÅ¼emy obliczyÄ‡ caÅ‚kowity bÅ‚Ä…d dla caÅ‚ego zbioru danych jako funkcjÄ™ parametrÃ³w Î¸.

> âœ… **Celem trenowania sieci neuronowej jest minimalizacja bÅ‚Ä™du poprzez modyfikacjÄ™ parametrÃ³w Î¸**

## Optymalizacja za pomocÄ… Gradientu Prostego

Istnieje dobrze znana metoda optymalizacji funkcji zwana **gradientem prostym**. Idea polega na tym, Å¼e moÅ¼emy obliczyÄ‡ pochodnÄ… (w przypadku wielowymiarowym nazywanÄ… **gradientem**) funkcji straty wzglÄ™dem parametrÃ³w i zmieniaÄ‡ parametry w taki sposÃ³b, aby bÅ‚Ä…d malaÅ‚. MoÅ¼na to sformalizowaÄ‡ w nastÄ™pujÄ…cy sposÃ³b:

* Inicjalizuj parametry losowymi wartoÅ›ciami w<sup>(0)</sup>, b<sup>(0)</sup>
* Powtarzaj nastÄ™pujÄ…cy krok wiele razy:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Podczas treningu kroki optymalizacji powinny byÄ‡ obliczane z uwzglÄ™dnieniem caÅ‚ego zbioru danych (pamiÄ™taj, Å¼e strata jest obliczana jako suma dla wszystkich prÃ³bek treningowych). Jednak w praktyce bierzemy maÅ‚e porcje zbioru danych zwane **minipaczkami** i obliczamy gradienty na podstawie podzbioru danych. PoniewaÅ¼ podzbiÃ³r jest wybierany losowo za kaÅ¼dym razem, taka metoda nazywana jest **stochastycznym gradientem prostym** (SGD).

## Perceptrony Wielowarstwowe i Wsteczna Propagacja

SieÄ‡ jednowarstwowa, jak widzieliÅ›my powyÅ¼ej, jest w stanie klasyfikowaÄ‡ klasy liniowo separowalne. Aby zbudowaÄ‡ bogatszy model, moÅ¼emy poÅ‚Ä…czyÄ‡ kilka warstw sieci. Matematycznie oznaczaÅ‚oby to, Å¼e funkcja *f* przyjmie bardziej zÅ‚oÅ¼onÄ… formÄ™ i bÄ™dzie obliczana w kilku krokach:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Tutaj Î± to **nieliniowa funkcja aktywacji**, Ïƒ to funkcja softmax, a parametry Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algorytm gradientu prostego pozostanie taki sam, ale obliczanie gradientÃ³w bÄ™dzie trudniejsze. Zgodnie z reguÅ‚Ä… rÃ³Å¼niczkowania Å‚aÅ„cuchowego moÅ¼emy obliczyÄ‡ pochodne jako:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… ReguÅ‚a rÃ³Å¼niczkowania Å‚aÅ„cuchowego jest uÅ¼ywana do obliczania pochodnych funkcji straty wzglÄ™dem parametrÃ³w.

ZauwaÅ¼, Å¼e lewa czÄ™Å›Ä‡ wszystkich tych wyraÅ¼eÅ„ jest taka sama, dziÄ™ki czemu moÅ¼emy efektywnie obliczaÄ‡ pochodne, zaczynajÄ…c od funkcji straty i idÄ…c "wstecz" przez graf obliczeniowy. Dlatego metoda trenowania perceptronu wielowarstwowego nazywana jest **wstecznÄ… propagacjÄ…**, lub 'backprop'.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: cytowanie obrazu

> âœ… OmÃ³wimy wstecznÄ… propagacjÄ™ znacznie bardziej szczegÃ³Å‚owo w naszym przykÅ‚adzie w notatniku.  

## Podsumowanie

W tej lekcji stworzyliÅ›my wÅ‚asnÄ… bibliotekÄ™ sieci neuronowych i uÅ¼yliÅ›my jej do prostego zadania klasyfikacji dwuwymiarowej.

## ğŸš€ Wyzwanie

W doÅ‚Ä…czonym notatniku zaimplementujesz wÅ‚asny framework do budowania i trenowania perceptronÃ³w wielowarstwowych. BÄ™dziesz mÃ³gÅ‚ zobaczyÄ‡ szczegÃ³Å‚owo, jak dziaÅ‚ajÄ… nowoczesne sieci neuronowe.

PrzejdÅº do notatnika [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) i przepracuj go.

## [Quiz po wykÅ‚adzie](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## PrzeglÄ…d i Samodzielna Nauka

Wsteczna propagacja to powszechny algorytm uÅ¼ywany w AI i ML, warto go [zgÅ‚Ä™biÄ‡ bardziej szczegÃ³Å‚owo](https://wikipedia.org/wiki/Backpropagation)

## [Zadanie](lab/README.md)

W tym laboratorium zostaniesz poproszony o uÅ¼ycie frameworka stworzonego w tej lekcji do rozwiÄ…zania problemu klasyfikacji rÄ™cznie pisanych cyfr MNIST.

* [Instrukcje](lab/README.md)
* [Notatnik](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy mieÄ‡ na uwadze, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za wiarygodne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia wykonanego przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.