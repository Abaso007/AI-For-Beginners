<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "789d6c3fb6fc7948a470b33078a5983a",
  "translation_date": "2025-09-23T13:56:46+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "pl"
}
-->
# Wprowadzenie do sieci neuronowych. Wielowarstwowy perceptron

W poprzedniej sekcji poznaliÅ›my najprostszy model sieci neuronowej - jednowarstwowy perceptron, liniowy model klasyfikacji dla dwÃ³ch klas.

W tej sekcji rozszerzymy ten model, tworzÄ…c bardziej elastyczne ramy, ktÃ³re pozwolÄ… nam:

* przeprowadzaÄ‡ **klasyfikacjÄ™ wieloklasowÄ…** oprÃ³cz klasyfikacji dwuklasowej,
* rozwiÄ…zywaÄ‡ **problemy regresji** oprÃ³cz klasyfikacji,
* rozdzielaÄ‡ klasy, ktÃ³re nie sÄ… liniowo separowalne.

Dodatkowo stworzymy wÅ‚asne moduÅ‚owe ramy w Pythonie, ktÃ³re umoÅ¼liwiÄ… konstruowanie rÃ³Å¼nych architektur sieci neuronowych.

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## Formalizacja uczenia maszynowego

Zacznijmy od formalizacji problemu uczenia maszynowego. ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych treningowych **X** z etykietami **Y**, i musimy zbudowaÄ‡ model *f*, ktÃ³ry bÄ™dzie generowaÅ‚ jak najdokÅ‚adniejsze przewidywania. JakoÅ›Ä‡ przewidywaÅ„ mierzymy za pomocÄ… **funkcji straty** &lagran;. CzÄ™sto stosowane funkcje straty to:

* W przypadku problemu regresji, gdy musimy przewidzieÄ‡ wartoÅ›Ä‡ liczbowÄ…, moÅ¼emy uÅ¼yÄ‡ **bÅ‚Ä™du absolutnego** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| lub **bÅ‚Ä™du kwadratowego** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>.
* W przypadku klasyfikacji stosujemy **0-1 loss** (ktÃ³ry zasadniczo odpowiada **dokÅ‚adnoÅ›ci** modelu) lub **logistycznÄ… funkcjÄ™ straty**.

Dla jednowarstwowego perceptronu funkcja *f* byÅ‚a zdefiniowana jako funkcja liniowa *f(x)=wx+b* (gdzie *w* to macierz wag, *x* to wektor cech wejÅ›ciowych, a *b* to wektor przesuniÄ™cia). W przypadku rÃ³Å¼nych architektur sieci neuronowych funkcja ta moÅ¼e przyjmowaÄ‡ bardziej zÅ‚oÅ¼onÄ… formÄ™.

> W przypadku klasyfikacji czÄ™sto poÅ¼Ä…dane jest uzyskanie prawdopodobieÅ„stw odpowiadajÄ…cych klasom jako wynik sieci. Aby przeksztaÅ‚ciÄ‡ dowolne liczby w prawdopodobieÅ„stwa (np. znormalizowaÄ‡ wynik), czÄ™sto uÅ¼ywamy funkcji **softmax** &sigma;, a funkcja *f* staje siÄ™ *f(x)=&sigma;(wx+b)*.

W powyÅ¼szej definicji *f*, *w* i *b* nazywane sÄ… **parametrami** &theta;=âŸ¨*w,b*âŸ©. MajÄ…c zbiÃ³r danych âŸ¨**X**,**Y**âŸ©, moÅ¼emy obliczyÄ‡ caÅ‚kowity bÅ‚Ä…d dla caÅ‚ego zbioru danych jako funkcjÄ™ parametrÃ³w &theta;.

> âœ… **Celem treningu sieci neuronowej jest minimalizacja bÅ‚Ä™du poprzez zmienianie parametrÃ³w &theta;**

## Optymalizacja metodÄ… gradientu prostego

Istnieje dobrze znana metoda optymalizacji funkcji zwana **gradientem prostym**. Polega ona na obliczeniu pochodnej (w przypadku wielowymiarowym zwanej **gradientem**) funkcji straty wzglÄ™dem parametrÃ³w i zmienianiu parametrÃ³w w taki sposÃ³b, aby bÅ‚Ä…d siÄ™ zmniejszaÅ‚. MoÅ¼na to sformalizowaÄ‡ w nastÄ™pujÄ…cy sposÃ³b:

* Zainicjalizuj parametry losowymi wartoÅ›ciami w<sup>(0)</sup>, b<sup>(0)</sup>.
* Powtarzaj nastÄ™pujÄ…cy krok wiele razy:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Podczas treningu kroki optymalizacji powinny byÄ‡ obliczane z uwzglÄ™dnieniem caÅ‚ego zbioru danych (pamiÄ™tajmy, Å¼e strata jest obliczana jako suma dla wszystkich prÃ³bek treningowych). Jednak w praktyce bierzemy maÅ‚e porcje zbioru danych, zwane **minibatchami**, i obliczamy gradienty na podstawie podzbioru danych. PoniewaÅ¼ podzbiÃ³r jest wybierany losowo za kaÅ¼dym razem, taka metoda nazywana jest **stochastycznym gradientem prostym** (SGD).

## Wielowarstwowe perceptrony i propagacja wsteczna

Jednowarstwowa sieÄ‡, jak widzieliÅ›my powyÅ¼ej, jest zdolna do klasyfikacji klas liniowo separowalnych. Aby zbudowaÄ‡ bardziej zaawansowany model, moÅ¼emy poÅ‚Ä…czyÄ‡ kilka warstw sieci. Matematycznie oznaczaÅ‚oby to, Å¼e funkcja *f* przyjmie bardziej zÅ‚oÅ¼onÄ… formÄ™ i bÄ™dzie obliczana w kilku krokach:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Tutaj &alpha; to **nieliniowa funkcja aktywacji**, &sigma; to funkcja softmax, a parametry &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algorytm gradientu prostego pozostanie taki sam, ale obliczanie gradientÃ³w bÄ™dzie bardziej skomplikowane. Zgodnie z reguÅ‚Ä… rÃ³Å¼niczkowania Å‚aÅ„cuchowego moÅ¼emy obliczyÄ‡ pochodne jako:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> âœ… ReguÅ‚a rÃ³Å¼niczkowania Å‚aÅ„cuchowego jest uÅ¼ywana do obliczania pochodnych funkcji straty wzglÄ™dem parametrÃ³w.

ZauwaÅ¼, Å¼e lewa czÄ™Å›Ä‡ wszystkich tych wyraÅ¼eÅ„ jest taka sama, dziÄ™ki czemu moÅ¼emy efektywnie obliczaÄ‡ pochodne, zaczynajÄ…c od funkcji straty i przechodzÄ…c "wstecz" przez graf obliczeniowy. Dlatego metoda treningu wielowarstwowego perceptronu nazywana jest **propagacjÄ… wstecznÄ…**, lub 'backprop'.

<img alt="graf obliczeniowy" src="images/ComputeGraphGrad.png"/>

> TODO: cytowanie obrazu

> âœ… OmÃ³wimy propagacjÄ™ wstecznÄ… znacznie bardziej szczegÃ³Å‚owo w naszym przykÅ‚adzie w notebooku.  

## Podsumowanie

W tej lekcji stworzyliÅ›my wÅ‚asnÄ… bibliotekÄ™ sieci neuronowych i uÅ¼yliÅ›my jej do prostego zadania klasyfikacji dwuwymiarowej.

## ğŸš€ Wyzwanie

W doÅ‚Ä…czonym notebooku zaimplementujesz wÅ‚asne ramy do budowy i treningu wielowarstwowych perceptronÃ³w. BÄ™dziesz mÃ³gÅ‚ szczegÃ³Å‚owo zobaczyÄ‡, jak dziaÅ‚ajÄ… nowoczesne sieci neuronowe.

PrzejdÅº do notebooka [OwnFramework](OwnFramework.ipynb) i wykonaj zadania.

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## PrzeglÄ…d i samodzielna nauka

Propagacja wsteczna to powszechny algorytm stosowany w AI i ML, warto go [zgÅ‚Ä™biÄ‡](https://wikipedia.org/wiki/Backpropagation).

## [Zadanie](lab/README.md)

W tym laboratorium masz za zadanie uÅ¼yÄ‡ ram, ktÃ³re stworzyÅ‚eÅ› w tej lekcji, do rozwiÄ…zania problemu klasyfikacji rÄ™cznie pisanych cyfr MNIST.

* [Instrukcje](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

---

