{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanizmy uwagi i transformatory\n",
    "\n",
    "Jednym z głównych ograniczeń sieci rekurencyjnych jest to, że wszystkie słowa w sekwencji mają taki sam wpływ na wynik. Powoduje to suboptymalną wydajność standardowych modeli LSTM typu encoder-decoder w zadaniach sekwencja na sekwencję, takich jak rozpoznawanie nazwanych jednostek czy tłumaczenie maszynowe. W rzeczywistości konkretne słowa w sekwencji wejściowej często mają większy wpływ na wyniki sekwencyjne niż inne.\n",
    "\n",
    "Rozważmy model sekwencja na sekwencję, taki jak tłumaczenie maszynowe. Jest on realizowany za pomocą dwóch sieci rekurencyjnych, gdzie jedna sieć (**encoder**) kompresuje sekwencję wejściową do stanu ukrytego, a druga, **decoder**, rozwija ten stan ukryty w przetłumaczony wynik. Problem z tym podejściem polega na tym, że końcowy stan sieci ma trudności z zapamiętaniem początku zdania, co skutkuje niską jakością modelu w przypadku długich zdań.\n",
    "\n",
    "**Mechanizmy uwagi** umożliwiają ważenie kontekstowego wpływu każdego wektora wejściowego na każdą prognozę wyjściową RNN. Realizuje się to poprzez tworzenie skrótów między stanami pośrednimi RNN wejściowego a RNN wyjściowego. W ten sposób, generując symbol wyjściowy $y_t$, uwzględniamy wszystkie stany ukryte wejścia $h_i$, z różnymi współczynnikami wagowymi $\\alpha_{t,i}$. \n",
    "\n",
    "![Obraz przedstawiający model encoder-decoder z warstwą uwagi addytywnej](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)\n",
    "*Model encoder-decoder z mechanizmem uwagi addytywnej w [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cytowany z [tego wpisu na blogu](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Macierz uwagi $\\{\\alpha_{i,j}\\}$ reprezentuje stopień, w jakim określone słowa wejściowe wpływają na generowanie danego słowa w sekwencji wyjściowej. Poniżej znajduje się przykład takiej macierzy:\n",
    "\n",
    "![Obraz przedstawiający przykładowe wyrównanie znalezione przez RNNsearch-50, zaczerpnięty z Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)\n",
    "\n",
    "*Rysunek zaczerpnięty z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Rys.3)*\n",
    "\n",
    "Mechanizmy uwagi są odpowiedzialne za dużą część obecnych lub bliskich obecnym osiągnięć w przetwarzaniu języka naturalnego. Dodanie uwagi znacznie zwiększa jednak liczbę parametrów modelu, co prowadzi do problemów ze skalowaniem w przypadku RNN. Kluczowym ograniczeniem skalowania RNN jest to, że rekurencyjny charakter modeli utrudnia grupowanie i równoległe trenowanie. W RNN każdy element sekwencji musi być przetwarzany w kolejności sekwencyjnej, co oznacza, że nie można go łatwo zrównoleglić.\n",
    "\n",
    "Zastosowanie mechanizmów uwagi w połączeniu z tym ograniczeniem doprowadziło do powstania obecnych modeli transformatorowych, które znamy i używamy dzisiaj, takich jak BERT czy OpenGPT3.\n",
    "\n",
    "## Modele transformatorowe\n",
    "\n",
    "Zamiast przekazywania kontekstu każdej poprzedniej prognozy do kolejnego kroku ewaluacji, **modele transformatorowe** wykorzystują **kodowania pozycyjne** i **uwagę**, aby uchwycić kontekst danego wejścia w ramach dostarczonego okna tekstowego. Poniższy obraz pokazuje, jak kodowania pozycyjne z uwagą mogą uchwycić kontekst w danym oknie.\n",
    "\n",
    "![Animowany GIF pokazujący, jak przeprowadzane są ewaluacje w modelach transformatorowych.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Ponieważ każda pozycja wejściowa jest mapowana niezależnie na każdą pozycję wyjściową, transformatory mogą być lepiej równoleglone niż RNN, co umożliwia tworzenie znacznie większych i bardziej ekspresyjnych modeli językowych. Każda głowica uwagi może być używana do nauki różnych relacji między słowami, co poprawia zadania przetwarzania języka naturalnego.\n",
    "\n",
    "## Budowanie prostego modelu transformatorowego\n",
    "\n",
    "Keras nie zawiera wbudowanej warstwy transformatora, ale możemy zbudować własną. Jak wcześniej, skupimy się na klasyfikacji tekstu z wykorzystaniem zbioru danych AG News, ale warto wspomnieć, że modele transformatorowe osiągają najlepsze wyniki w bardziej wymagających zadaniach NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowe warstwy w Keras powinny dziedziczyć klasę `Layer` i implementować metodę `call`. Zacznijmy od warstwy **Positional Embedding**. Skorzystamy z [kodu z oficjalnej dokumentacji Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Założymy, że wszystkie wejściowe sekwencje są uzupełniane do długości `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta warstwa składa się z dwóch warstw `Embedding`: jednej do osadzania tokenów (w sposób, o którym wcześniej rozmawialiśmy) oraz drugiej do osadzania pozycji tokenów. Pozycje tokenów są tworzone jako sekwencja liczb naturalnych od 0 do `maxlen` za pomocą `tf.range`, a następnie przekazywane przez warstwę osadzania. Dwa wynikowe wektory osadzeń są następnie dodawane, tworząc pozycyjnie osadzone reprezentacje wejścia o kształcie `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Teraz zaimplementujmy blok transformera. Będzie on przyjmował wyjście wcześniej zdefiniowanej warstwy osadzania:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer stosuje `MultiHeadAttention` do wejścia zakodowanego pozycyjnie, aby wygenerować wektor uwagi o wymiarze `maxlen`$\\times$`embed_dim`, który następnie jest mieszany z wejściem i normalizowany za pomocą `LayerNormalization`.\n",
    "\n",
    "> **Uwaga**: `LayerNormalization` jest podobne do `BatchNormalization`, omawianego w części *Computer Vision* tej ścieżki nauki, ale normalizuje wyjścia poprzedniej warstwy dla każdej próbki treningowej niezależnie, aby sprowadzić je do zakresu [-1..1].\n",
    "\n",
    "Wyjście tej warstwy jest następnie przekazywane przez sieć `Dense` (w naszym przypadku - dwuwarstwowy perceptron), a wynik jest dodawany do końcowego wyjścia (które ponownie przechodzi normalizację).\n",
    "\n",
    "Teraz jesteśmy gotowi, aby zdefiniować kompletny model transformera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele Transformer BERT\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) to bardzo duża, wielowarstwowa sieć transformatorowa z 12 warstwami dla *BERT-base* i 24 dla *BERT-large*. Model jest najpierw wstępnie trenowany na dużym korpusie danych tekstowych (Wikipedia + książki) przy użyciu uczenia nienadzorowanego (przewidywanie zamaskowanych słów w zdaniu). Podczas wstępnego treningu model przyswaja znaczący poziom zrozumienia języka, który można następnie wykorzystać z innymi zbiorami danych poprzez dostrajanie. Ten proces nazywa się **uczeniem transferowym**.\n",
    "\n",
    "![obrazek z http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)\n",
    "\n",
    "Istnieje wiele wariantów architektur Transformer, takich jak BERT, DistilBERT, BigBird, OpenGPT3 i inne, które można dostrajać.\n",
    "\n",
    "Zobaczmy, jak możemy użyć wstępnie wytrenowanego modelu BERT do rozwiązania naszego tradycyjnego problemu klasyfikacji sekwencji. Skorzystamy z pomysłu i części kodu z [oficjalnej dokumentacji](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Aby załadować wstępnie wytrenowane modele, użyjemy **Tensorflow hub**. Najpierw załadujmy wektoryzator specyficzny dla BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ważne jest, aby użyć tego samego wektoryzatora, który został użyty podczas trenowania oryginalnej sieci. Wektoryzator BERT zwraca trzy komponenty:\n",
    "* `input_word_ids`, czyli sekwencję numerów tokenów dla zdania wejściowego\n",
    "* `input_mask`, wskazującą, która część sekwencji zawiera rzeczywiste dane wejściowe, a która jest wypełnieniem. Jest to podobne do maski generowanej przez warstwę `Masking`\n",
    "* `input_type_ids` jest używane w zadaniach modelowania języka i pozwala określić dwa zdania wejściowe w jednej sekwencji.\n",
    "\n",
    "Następnie możemy zainicjować ekstraktor cech BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warstwa BERT zwraca kilka przydatnych wyników:\n",
    "* `pooled_output` to wynik uśrednienia wszystkich tokenów w sekwencji. Można to traktować jako inteligentne semantyczne osadzenie całej sieci. Jest to równoważne z wynikiem warstwy `GlobalAveragePooling1D` w naszym poprzednim modelu.\n",
    "* `sequence_output` to wynik ostatniej warstwy transformera (odpowiada wynikowi `TransformerBlock` w naszym modelu powyżej).\n",
    "* `encoder_outputs` to wyniki wszystkich warstw transformera. Ponieważ załadowaliśmy 4-warstwowy model BERT (co można łatwo zgadnąć z nazwy, która zawiera `4_H`), ma on 4 tensory. Ostatni z nich jest taki sam jak `sequence_output`.\n",
    "\n",
    "Teraz zdefiniujemy model klasyfikacji end-to-end. Użyjemy *funkcjonalnej definicji modelu*, w której definiujemy wejście modelu, a następnie podajemy serię wyrażeń do obliczenia jego wyniku. Ustawimy również wagi modelu BERT jako nieuczestniczące w treningu i będziemy trenować jedynie końcowy klasyfikator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomimo niewielkiej liczby trenowalnych parametrów, proces jest dość wolny, ponieważ ekstraktor cech BERT jest obciążeniem obliczeniowym. Wygląda na to, że nie udało nam się osiągnąć zadowalającej dokładności, albo z powodu braku treningu, albo z powodu ograniczonej liczby parametrów modelu.\n",
    "\n",
    "Spróbujmy odblokować wagi BERT i również je wytrenować. Wymaga to bardzo małej szybkości uczenia oraz bardziej ostrożnej strategii treningowej z **warmup**, przy użyciu optymalizatora **AdamW**. Użyjemy pakietu `tf-models-official`, aby stworzyć optymalizator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzisz, trening przebiega dość wolno - ale możesz spróbować poeksperymentować i przeprowadzić trening modelu przez kilka epok (5-10), aby sprawdzić, czy uda Ci się uzyskać lepszy wynik w porównaniu do podejść, które stosowaliśmy wcześniej.\n",
    "\n",
    "## Biblioteka Huggingface Transformers\n",
    "\n",
    "Innym bardzo popularnym (i nieco prostszym) sposobem korzystania z modeli Transformer jest [pakiet HuggingFace](https://github.com/huggingface/), który dostarcza proste elementy budulcowe do różnych zadań NLP. Jest on dostępny zarówno dla Tensorflow, jak i PyTorch, kolejnego bardzo popularnego frameworka sieci neuronowych.\n",
    "\n",
    "> **Note**: Jeśli nie interesuje Cię, jak działa biblioteka Transformers - możesz przejść na koniec tego notatnika, ponieważ nie zobaczysz tu nic zasadniczo innego niż to, co zrobiliśmy wcześniej. Będziemy powtarzać te same kroki trenowania modelu BERT, używając innej biblioteki i znacznie większego modelu. Proces ten obejmuje dość długie treningi, więc możesz po prostu przejrzeć kod.\n",
    "\n",
    "Zobaczmy, jak nasz problem można rozwiązać za pomocą [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszą rzeczą, którą musimy zrobić, jest wybór modelu, którego będziemy używać. Oprócz kilku wbudowanych modeli, Huggingface zawiera [internetowe repozytorium modeli](https://huggingface.co/models), gdzie można znaleźć wiele więcej modeli wstępnie wytrenowanych przez społeczność. Wszystkie te modele można załadować i używać, podając jedynie nazwę modelu. Wszystkie wymagane pliki binarne dla modelu zostaną automatycznie pobrane.\n",
    "\n",
    "Czasami może być konieczne załadowanie własnych modeli. W takim przypadku można określić katalog zawierający wszystkie odpowiednie pliki, w tym parametry dla tokenizer'a, plik `config.json` z parametrami modelu, wagi binarne itp.\n",
    "\n",
    "Na podstawie nazwy modelu możemy zainicjalizować zarówno model, jak i tokenizer. Zacznijmy od tokenizer'a:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obiekt `tokenizer` zawiera funkcję `encode`, która może być bezpośrednio użyta do kodowania tekstu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy również użyć tokenizera do zakodowania sekwencji w sposób odpowiedni do przekazania modelowi, tj. uwzględniając pola `token_ids`, `input_mask` itd. Możemy również określić, że chcemy tensory Tensorflow, podając argument `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W naszym przypadku będziemy używać wstępnie wytrenowanego modelu BERT o nazwie `bert-base-uncased`. *Uncased* oznacza, że model jest niewrażliwy na wielkość liter.\n",
    "\n",
    "Podczas trenowania modelu musimy dostarczyć tokenizowaną sekwencję jako wejście, dlatego zaprojektujemy proces przetwarzania danych. Ponieważ `tokenizer.encode` jest funkcją w Pythonie, zastosujemy tę samą metodę, co w poprzedniej jednostce, wywołując ją za pomocą `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy załadować rzeczywisty model za pomocą pakietu `BertForSequenceClassification`. Zapewnia to, że nasz model ma już wymaganą architekturę do klasyfikacji, w tym końcowy klasyfikator. Zobaczysz komunikat ostrzegawczy informujący, że wagi końcowego klasyfikatora nie są zainicjalizowane, a model wymaga wstępnego treningu - to jest całkowicie w porządku, ponieważ dokładnie to zamierzamy zrobić!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać z `summary()`, model zawiera prawie 110 milionów parametrów! Przypuszczalnie, jeśli chcemy wykonać proste zadanie klasyfikacji na stosunkowo małym zbiorze danych, nie chcemy trenować podstawowej warstwy BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy rozpocząć trening!\n",
    "\n",
    "> **Uwaga**: Trenowanie pełnoskalowego modelu BERT może być bardzo czasochłonne! Dlatego przeszkolimy go tylko na pierwszych 32 partiach. To jedynie pokazuje, jak skonfigurować proces treningu modelu. Jeśli chcesz spróbować pełnoskalowego treningu - po prostu usuń parametry `steps_per_epoch` i `validation_steps`, i przygotuj się na długie oczekiwanie!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli zwiększysz liczbę iteracji, poczekasz wystarczająco długo i przeprowadzisz trening przez kilka epok, możesz oczekiwać, że klasyfikacja za pomocą BERT zapewni najlepszą dokładność! Dzieje się tak, ponieważ BERT już bardzo dobrze rozumie strukturę języka, a my musimy jedynie dostosować końcowy klasyfikator. Jednakże, ponieważ BERT jest dużym modelem, cały proces treningu zajmuje dużo czasu i wymaga znacznej mocy obliczeniowej! (GPU, a najlepiej więcej niż jednego).\n",
    "\n",
    "> **Note:** W naszym przykładzie używamy jednego z najmniejszych wstępnie wytrenowanych modeli BERT. Istnieją większe modele, które prawdopodobnie dadzą lepsze rezultaty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kluczowe informacje\n",
    "\n",
    "W tej jednostce omówiliśmy najnowsze architektury modeli oparte na **transformerach**. Zastosowaliśmy je do naszego zadania klasyfikacji tekstu, ale podobnie modele BERT mogą być używane do ekstrakcji jednostek, odpowiadania na pytania i innych zadań związanych z NLP.\n",
    "\n",
    "Modele transformerów reprezentują obecny stan wiedzy w dziedzinie NLP i w większości przypadków powinny być pierwszym rozwiązaniem, które warto wypróbować podczas wdrażania niestandardowych rozwiązań NLP. Jednak zrozumienie podstawowych zasad działania rekurencyjnych sieci neuronowych, omówionych w tym module, jest niezwykle ważne, jeśli chcesz budować zaawansowane modele neuronowe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony za pomocą usługi tłumaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za wiarygodne źródło. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-31T13:54:04+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}