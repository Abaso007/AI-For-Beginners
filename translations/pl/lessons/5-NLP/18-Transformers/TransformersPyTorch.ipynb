{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanizmy uwagi i transformatory\n",
    "\n",
    "Jednym z głównych ograniczeń sieci rekurencyjnych jest to, że wszystkie słowa w sekwencji mają taki sam wpływ na wynik. Powoduje to suboptymalne działanie standardowych modeli LSTM typu encoder-decoder w zadaniach sekwencja-do-sekwencji, takich jak rozpoznawanie nazwanych jednostek (Named Entity Recognition) czy tłumaczenie maszynowe. W rzeczywistości konkretne słowa w sekwencji wejściowej często mają większy wpływ na wyniki sekwencyjne niż inne.\n",
    "\n",
    "Rozważmy model sekwencja-do-sekwencji, taki jak tłumaczenie maszynowe. Jest on realizowany za pomocą dwóch sieci rekurencyjnych, gdzie jedna sieć (**encoder**) kompresuje sekwencję wejściową do stanu ukrytego, a druga, **decoder**, rozwija ten stan ukryty w przetłumaczony wynik. Problem z tym podejściem polega na tym, że końcowy stan sieci ma trudności z zapamiętaniem początku zdania, co prowadzi do niskiej jakości modelu w przypadku długich zdań.\n",
    "\n",
    "**Mechanizmy uwagi** umożliwiają ważenie kontekstowego wpływu każdego wektora wejściowego na każdą prognozę wyjściową RNN. Realizuje się to poprzez tworzenie skrótów między stanami pośrednimi RNN wejściowego a RNN wyjściowego. W ten sposób, generując symbol wyjściowy $y_t$, uwzględniamy wszystkie stany ukryte wejścia $h_i$, z różnymi współczynnikami wagowymi $\\alpha_{t,i}$.\n",
    "\n",
    "![Obraz przedstawiający model encoder/decoder z warstwą uwagi addytywnej](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)\n",
    "*Model encoder-decoder z mechanizmem uwagi addytywnej w [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cytowany z [tego wpisu na blogu](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Macierz uwagi $\\{\\alpha_{i,j}\\}$ reprezentuje stopień, w jakim określone słowa wejściowe wpływają na generowanie danego słowa w sekwencji wyjściowej. Poniżej znajduje się przykład takiej macierzy:\n",
    "\n",
    "![Obraz przedstawiający przykładowe dopasowanie znalezione przez RNNsearch-50, zaczerpnięty z Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)\n",
    "\n",
    "*Rysunek zaczerpnięty z [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Rys.3)*\n",
    "\n",
    "Mechanizmy uwagi są odpowiedzialne za dużą część obecnych lub bliskich obecnym stanom sztuki osiągnięć w przetwarzaniu języka naturalnego. Dodanie mechanizmu uwagi znacznie jednak zwiększa liczbę parametrów modelu, co prowadzi do problemów ze skalowaniem w przypadku RNN. Kluczowym ograniczeniem skalowania RNN jest to, że rekurencyjny charakter modeli utrudnia grupowanie i równoległe trenowanie. W RNN każdy element sekwencji musi być przetwarzany w kolejności sekwencyjnej, co oznacza, że nie można go łatwo zrównoleglić.\n",
    "\n",
    "Zastosowanie mechanizmów uwagi w połączeniu z tym ograniczeniem doprowadziło do powstania obecnych modeli Transformer, które znamy i używamy dzisiaj, takich jak BERT czy OpenGPT3.\n",
    "\n",
    "## Modele Transformer\n",
    "\n",
    "Zamiast przekazywać kontekst każdej poprzedniej prognozy do kolejnego kroku ewaluacji, **modele Transformer** wykorzystują **kodowania pozycyjne** i mechanizmy uwagi, aby uchwycić kontekst danego wejścia w określonym oknie tekstowym. Poniższy obraz pokazuje, jak kodowania pozycyjne z uwagą mogą uchwycić kontekst w danym oknie.\n",
    "\n",
    "![Animowany GIF pokazujący, jak przeprowadzane są ewaluacje w modelach Transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Ponieważ każda pozycja wejściowa jest mapowana niezależnie na każdą pozycję wyjściową, transformatory mogą być lepiej zrównoleglane niż RNN, co umożliwia tworzenie większych i bardziej ekspresywnych modeli językowych. Każda głowica uwagi może być używana do nauki różnych relacji między słowami, co poprawia zadania przetwarzania języka naturalnego.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) to bardzo duża, wielowarstwowa sieć Transformer z 12 warstwami dla *BERT-base* i 24 dla *BERT-large*. Model jest najpierw wstępnie trenowany na dużym korpusie danych tekstowych (Wikipedia + książki) za pomocą uczenia nienadzorowanego (przewidywanie zamaskowanych słów w zdaniu). Podczas wstępnego trenowania model przyswaja znaczący poziom zrozumienia języka, który można następnie wykorzystać z innymi zbiorami danych za pomocą dostrajania. Ten proces nazywa się **uczeniem transferowym**.\n",
    "\n",
    "![obrazek z http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)\n",
    "\n",
    "Istnieje wiele wariantów architektur Transformer, w tym BERT, DistilBERT, BigBird, OpenGPT3 i inne, które można dostrajać. Pakiet [HuggingFace](https://github.com/huggingface/) udostępnia repozytorium do trenowania wielu z tych architektur z wykorzystaniem PyTorch.\n",
    "\n",
    "## Wykorzystanie BERT do klasyfikacji tekstu\n",
    "\n",
    "Zobaczmy, jak możemy wykorzystać wstępnie wytrenowany model BERT do rozwiązania naszego tradycyjnego zadania: klasyfikacji sekwencji. Będziemy klasyfikować nasz oryginalny zbiór danych AG News.\n",
    "\n",
    "Najpierw załadujmy bibliotekę HuggingFace i nasz zbiór danych:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ będziemy korzystać z wstępnie wytrenowanego modelu BERT, musimy użyć odpowiedniego tokenizatora. Najpierw załadujemy tokenizator powiązany z wstępnie wytrenowanym modelem BERT.\n",
    "\n",
    "Biblioteka HuggingFace zawiera repozytorium wstępnie wytrenowanych modeli, z których można korzystać, podając ich nazwy jako argumenty do funkcji `from_pretrained`. Wszystkie wymagane pliki binarne dla modelu zostaną automatycznie pobrane.\n",
    "\n",
    "Jednakże, w niektórych przypadkach konieczne będzie załadowanie własnych modeli. W takim przypadku można wskazać katalog zawierający wszystkie odpowiednie pliki, w tym parametry dla tokenizatora, plik `config.json` z parametrami modelu, wagi binarne itp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obiekt `tokenizer` zawiera funkcję `encode`, która może być bezpośrednio użyta do kodowania tekstu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie stwórzmy iteratory, które będziemy używać podczas treningu do uzyskiwania dostępu do danych. Ponieważ BERT używa własnej funkcji kodowania, musielibyśmy zdefiniować funkcję dopełniania podobną do `padify`, którą zdefiniowaliśmy wcześniej:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W naszym przypadku będziemy używać wstępnie wytrenowanego modelu BERT o nazwie `bert-base-uncased`. Załadujmy model za pomocą pakietu `BertForSequenceClassfication`. To zapewnia, że nasz model ma już wymaganą architekturę do klasyfikacji, w tym końcowy klasyfikator. Zobaczysz komunikat ostrzegawczy informujący, że wagi końcowego klasyfikatora nie są zainicjalizowane i model wymaga wstępnego treningu - to jest całkowicie w porządku, ponieważ dokładnie to zamierzamy zrobić!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy rozpocząć trening! Ponieważ BERT jest już wstępnie wytrenowany, chcemy zacząć od dość małej wartości współczynnika uczenia, aby nie zniszczyć początkowych wag.\n",
    "\n",
    "Całą ciężką pracę wykonuje model `BertForSequenceClassification`. Gdy wywołujemy model na danych treningowych, zwraca on zarówno stratę, jak i wynik sieci dla wejściowej mini-paczki. Stratę wykorzystujemy do optymalizacji parametrów (`loss.backward()` wykonuje propagację wsteczną), a `out` do obliczania dokładności treningu, porównując uzyskane etykiety `labs` (obliczone za pomocą `argmax`) z oczekiwanymi `labels`.\n",
    "\n",
    "Aby kontrolować proces, akumulujemy stratę i dokładność przez kilka iteracji i wypisujemy je co `report_freq` cykli treningowych.\n",
    "\n",
    "Ten trening prawdopodobnie zajmie dość dużo czasu, więc ograniczamy liczbę iteracji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możesz zauważyć (szczególnie jeśli zwiększysz liczbę iteracji i poczekasz wystarczająco długo), że klasyfikacja za pomocą BERT daje nam całkiem dobrą dokładność! Dzieje się tak, ponieważ BERT już bardzo dobrze rozumie strukturę języka, a my musimy jedynie dostroić końcowy klasyfikator. Jednakże, ponieważ BERT to duży model, cały proces treningu zajmuje dużo czasu i wymaga dużej mocy obliczeniowej! (GPU, a najlepiej więcej niż jednego).\n",
    "\n",
    "> **Note:** W naszym przykładzie używamy jednego z najmniejszych wstępnie wytrenowanych modeli BERT. Istnieją większe modele, które prawdopodobnie przyniosą lepsze wyniki.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ocena wydajności modelu\n",
    "\n",
    "Teraz możemy ocenić wydajność naszego modelu na zestawie testowym. Pętla oceny jest bardzo podobna do pętli treningowej, ale nie możemy zapomnieć o przełączeniu modelu w tryb oceny, wywołując `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kluczowe informacje\n",
    "\n",
    "W tej jednostce zobaczyliśmy, jak łatwo można wykorzystać wstępnie wytrenowany model językowy z biblioteki **transformers** i dostosować go do naszego zadania klasyfikacji tekstu. Podobnie, modele BERT mogą być używane do ekstrakcji encji, odpowiadania na pytania i innych zadań związanych z NLP.\n",
    "\n",
    "Modele transformerów reprezentują obecny stan wiedzy w dziedzinie NLP i w większości przypadków powinny być pierwszym rozwiązaniem, od którego zaczynasz eksperymenty przy implementacji niestandardowych rozwiązań NLP. Jednak zrozumienie podstawowych zasad działania rekurencyjnych sieci neuronowych, omówionych w tym module, jest niezwykle ważne, jeśli chcesz budować zaawansowane modele neuronowe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za źródło autorytatywne. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-31T13:50:36+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}