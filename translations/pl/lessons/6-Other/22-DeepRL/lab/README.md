<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7bd8dc72040e98e35e7225e34058cd4e",
  "translation_date": "2025-08-24T10:38:32+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/lab/README.md",
  "language_code": "pl"
}
-->
## Środowisko

Środowisko Mountain Car składa się z samochodu uwięzionego w dolinie. Twoim celem jest wyskoczyć z doliny i dotrzeć do flagi. Możesz wykonywać następujące akcje: przyspieszać w lewo, w prawo lub nic nie robić. Możesz obserwować pozycję samochodu na osi x oraz jego prędkość.

## Rozpoczęcie pracy z notebookiem

Rozpocznij pracę z laboratorium, otwierając [MountainCar.ipynb](../../../../../../lessons/6-Other/22-DeepRL/lab/MountainCar.ipynb)

## Wnioski

Podczas tego laboratorium powinieneś nauczyć się, że dostosowanie algorytmów RL do nowego środowiska jest często dość proste, ponieważ OpenAI Gym ma ten sam interfejs dla wszystkich środowisk, a algorytmy w dużej mierze nie zależą od charakteru środowiska. Możesz nawet przeorganizować kod w Pythonie w taki sposób, aby przekazywać dowolne środowisko do algorytmu RL jako parametr.

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż staramy się zapewnić dokładność, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za wiarygodne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.