<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-28T19:59:19+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "he"
}
-->
# רשתות גנרטיביות

## [שאלון לפני ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/33)

רשתות עצביות חוזרות (RNNs) וגרסאות התאים שלהן כמו תאי זיכרון ארוך-קצר (LSTMs) ויחידות חוזרות עם שערים (GRUs) מספקות מנגנון למידול שפה בכך שהן יכולות ללמוד את סדר המילים ולספק תחזיות למילה הבאה ברצף. זה מאפשר לנו להשתמש ב-RNNs למשימות **גנרטיביות**, כמו יצירת טקסט רגיל, תרגום מכונה ואפילו יצירת כיתוב לתמונות.

> ✅ חשבו על כל הפעמים שבהן נהניתם ממשימות גנרטיביות כמו השלמת טקסט בזמן ההקלדה. בצעו מחקר על האפליקציות האהובות עליכם כדי לבדוק אם הן השתמשו ב-RNNs.

בארכיטקטורת RNN שדנו בה ביחידה הקודמת, כל יחידת RNN ייצרה את מצב החבוי הבא כתוצאה. עם זאת, ניתן גם להוסיף פלט נוסף לכל יחידה חוזרת, מה שיאפשר לנו להפיק **רצף** (ששווה באורכו לרצף המקורי). יתרה מכך, ניתן להשתמש ביחידות RNN שאינן מקבלות קלט בכל שלב, אלא רק לוקחות וקטור מצב התחלתי ומייצרות רצף של פלטים.

זה מאפשר ארכיטקטורות עצביות שונות שמוצגות בתמונה למטה:

![תמונה המציגה דפוסים נפוצים של רשתות עצביות חוזרות.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.he.jpg)

> תמונה מתוך פוסט בבלוג [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) מאת [Andrej Karpaty](http://karpathy.github.io/)

* **אחד-לאחד** הוא רשת עצבית מסורתית עם קלט אחד ופלט אחד
* **אחד-לרבים** הוא ארכיטקטורה גנרטיבית שמקבלת ערך קלט אחד ומייצרת רצף של ערכי פלט. לדוגמה, אם נרצה לאמן רשת **כיתוב תמונה** שתפיק תיאור טקסטואלי של תמונה, ניתן לקחת תמונה כקלט, להעביר אותה דרך CNN כדי לקבל את המצב החבוי שלה, ואז שרשרת חוזרת תייצר את הכיתוב מילה-אחר-מילה
* **רבים-לאחד** מתאימה לארכיטקטורות RNN שתיארנו ביחידה הקודמת, כמו סיווג טקסט
* **רבים-לרבים**, או **רצף-לרצף**, מתאימה למשימות כמו **תרגום מכונה**, שבהן RNN ראשון אוסף את כל המידע מהרצף הקלט לתוך המצב החבוי, ושרשרת RNN אחרת פורסת את המצב הזה לרצף הפלט.

ביחידה זו נתמקד במודלים גנרטיביים פשוטים שעוזרים לנו ליצור טקסט. לשם פשטות, נשתמש בטוקניזציה ברמת תווים.

נאמן את ה-RNN הזה לייצר טקסט שלב אחר שלב. בכל שלב, ניקח רצף של תווים באורך `nchars`, ונבקש מהרשת לייצר את התו הבא עבור כל תו קלט:

![תמונה המציגה דוגמה ליצירת המילה 'HELLO' באמצעות RNN.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.he.png)

בעת יצירת טקסט (בזמן הסקה), נתחיל עם **הנחיה** כלשהי, שתועבר דרך תאי RNN כדי ליצור את המצב הביניים שלה, ואז מהמצב הזה מתחילה היצירה. ניצור תו אחד בכל פעם, ונעביר את המצב ואת התו שנוצר לתא RNN נוסף כדי ליצור את הבא, עד שניצור מספיק תווים.

<img src="images/rnn-generate-inf.png" width="60%"/>

> תמונה מאת המחבר

## ✍️ תרגילים: רשתות גנרטיביות

המשיכו ללמוד במחברות הבאות:

* [רשתות גנרטיביות עם PyTorch](GenerativePyTorch.ipynb)
* [רשתות גנרטיביות עם TensorFlow](GenerativeTF.ipynb)

## יצירת טקסט רכה וטמפרטורה

הפלט של כל תא RNN הוא התפלגות הסתברות של תווים. אם תמיד נבחר את התו עם ההסתברות הגבוהה ביותר כתו הבא בטקסט שנוצר, הטקסט יכול לעיתים להפוך ל"מחזורי" בין אותם רצפי תווים שוב ושוב, כמו בדוגמה הזו:

```
today of the second the company and a second the company ...
```

עם זאת, אם נבחן את התפלגות ההסתברות לתו הבא, ייתכן שההבדל בין כמה הסתברויות גבוהות אינו גדול, למשל תו אחד יכול להיות בעל הסתברות של 0.2, ותו אחר - 0.19, וכו'. לדוגמה, כאשר מחפשים את התו הבא ברצף '*play*', התו הבא יכול להיות באותה מידה רווח או **e** (כמו במילה *player*).

זה מוביל אותנו למסקנה שלא תמיד "הוגן" לבחור את התו עם ההסתברות הגבוהה ביותר, כי בחירת השני הגבוה ביותר עדיין יכולה להוביל לטקסט משמעותי. חכם יותר **לדגום** תווים מהתפלגות ההסתברות שניתנת על ידי פלט הרשת. ניתן גם להשתמש בפרמטר, **טמפרטורה**, שיחליק את התפלגות ההסתברות, אם נרצה להוסיף יותר אקראיות, או להפוך אותה לתלולה יותר, אם נרצה להיצמד יותר לתווים בעלי ההסתברות הגבוהה ביותר.

חקור כיצד יצירת טקסט רכה מיושמת במחברות המקושרות למעלה.

## סיכום

בעוד שיצירת טקסט יכולה להיות שימושית בפני עצמה, היתרונות העיקריים מגיעים מהיכולת ליצור טקסט באמצעות RNNs מוקטור תכונות התחלתי כלשהו. לדוגמה, יצירת טקסט משמשת כחלק מתרגום מכונה (רצף-לרצף, במקרה זה וקטור מצב מ-*מקודד* משמש ליצירת או *פענוח* הודעה מתורגמת), או יצירת תיאור טקסטואלי של תמונה (במקרה זה וקטור התכונות יגיע ממחלץ CNN).

## 🚀 אתגר

קחו שיעורים ב-Microsoft Learn בנושא זה

* יצירת טקסט עם [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [שאלון לאחר ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## סקירה ולימוד עצמי

הנה כמה מאמרים להרחבת הידע שלכם

* גישות שונות ליצירת טקסט עם שרשרת מרקוב, LSTM ו-GPT-2: [פוסט בבלוג](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* דוגמת יצירת טקסט בתיעוד [Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [מטלה](lab/README.md)

ראינו כיצד ליצור טקסט תו-אחר-תו. במעבדה, תחקור יצירת טקסט ברמת מילים.

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.