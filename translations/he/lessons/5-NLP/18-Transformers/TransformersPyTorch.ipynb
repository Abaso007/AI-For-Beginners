{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# מנגנוני קשב ומודלים של טרנספורמרים\n",
    "\n",
    "חיסרון מרכזי של רשתות חוזרות (RNN) הוא שכל המילים ברצף משפיעות באותה מידה על התוצאה. זה מוביל לביצועים תת-אופטימליים במודלים סטנדרטיים של LSTM עבור משימות רצף לרצף, כמו זיהוי ישויות בשם ותרגום מכונה. במציאות, למילים מסוימות ברצף הקלט יש השפעה רבה יותר על הפלט מאשר לאחרות.\n",
    "\n",
    "נבחן מודל רצף לרצף, כמו תרגום מכונה. הוא מיושם באמצעות שתי רשתות חוזרות, כאשר רשת אחת (**מקודד**) דוחסת את רצף הקלט למצב מוסתר, ורשת אחרת, **מפענח**, פורסת את המצב המוסתר הזה לתוצאה מתורגמת. הבעיה בגישה זו היא שהמצב הסופי של הרשת מתקשה לזכור את תחילת המשפט, מה שמוביל לאיכות ירודה של המודל במשפטים ארוכים.\n",
    "\n",
    "**מנגנוני קשב** מספקים דרך לשקלל את ההשפעה ההקשרית של כל וקטור קלט על כל תחזית פלט של ה-RNN. זה מיושם על ידי יצירת קיצורי דרך בין המצבים הביניים של ה-RNN של הקלט לבין ה-RNN של הפלט. כך, בעת יצירת סמל פלט $y_t$, ניקח בחשבון את כל המצבים המוסתרים של הקלט $h_i$, עם מקדמי משקל שונים $\\alpha_{t,i}$.\n",
    "\n",
    "![תמונה המציגה מודל מקודד/מפענח עם שכבת קשב אדיטיבית](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.he.png)\n",
    "*מודל מקודד-מפענח עם מנגנון קשב אדיטיבי מתוך [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), מצוטט מתוך [פוסט הבלוג הזה](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "מטריצת הקשב $\\{\\alpha_{i,j}\\}$ מייצגת את המידה שבה מילים מסוימות בקלט משפיעות על יצירת מילה מסוימת ברצף הפלט. להלן דוגמה למטריצה כזו:\n",
    "\n",
    "![תמונה המציגה יישור דוגמה שנמצא על ידי RNNsearch-50, מתוך Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.he.png)\n",
    "\n",
    "*תמונה מתוך [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (איור 3)*\n",
    "\n",
    "מנגנוני קשב אחראים לחלק ניכר מהמצב הנוכחי או הקרוב למצב הנוכחי של האמנות בתחום עיבוד שפה טבעית. עם זאת, הוספת קשב מגדילה מאוד את מספר הפרמטרים במודל, מה שהוביל לבעיות קנה מידה עם RNNs. מגבלה מרכזית בקנה מידה של RNNs היא שהאופי החוזר של המודלים מקשה על ביצוע אצווה (batching) והקבלה במקביל. ב-RNN, כל אלמנט ברצף צריך להיות מעובד בסדר רציף, מה שאומר שלא ניתן להקביל בקלות את התהליך.\n",
    "\n",
    "האימוץ של מנגנוני קשב בשילוב עם מגבלה זו הוביל ליצירת מודלים של טרנספורמרים, שהם כיום המצב המתקדם ביותר (State of the Art) שאנו מכירים ומשתמשים בהם כיום, כמו BERT ו-OpenGPT3.\n",
    "\n",
    "## מודלים של טרנספורמרים\n",
    "\n",
    "במקום להעביר את ההקשר של כל תחזית קודמת לשלב ההערכה הבא, **מודלים של טרנספורמרים** משתמשים ב**קידודים מיקום** ובקשב כדי ללכוד את ההקשר של קלט נתון בתוך חלון טקסט מסוים. התמונה למטה מראה כיצד קידודי מיקום עם קשב יכולים ללכוד הקשר בתוך חלון נתון.\n",
    "\n",
    "![GIF מונפש המציג כיצד מתבצעות ההערכות במודלים של טרנספורמרים.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "מכיוון שכל מיקום קלט ממופה באופן עצמאי לכל מיקום פלט, טרנספורמרים יכולים להקביל טוב יותר מ-RNNs, מה שמאפשר מודלים לשוניים גדולים ומבטאים יותר. כל ראש קשב יכול לשמש ללמידת יחסים שונים בין מילים, מה שמשפר משימות עיבוד שפה טבעית.\n",
    "\n",
    "**BERT** (ייצוגי מקודד דו-כיווניים מטרנספורמרים) הוא רשת טרנספורמרים גדולה מאוד עם 12 שכבות עבור *BERT-base* ו-24 עבור *BERT-large*. המודל מאומן מראש על מאגר טקסטים גדול (ויקיפדיה + ספרים) באמצעות אימון לא מפוקח (חיזוי מילים מוסתרות במשפט). במהלך האימון המוקדם, המודל סופג רמה משמעותית של הבנת שפה, שניתן לנצל לאחר מכן עם מערכי נתונים אחרים באמצעות כוונון עדין. תהליך זה נקרא **למידת העברה**.\n",
    "\n",
    "![תמונה מתוך http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.he.png)\n",
    "\n",
    "ישנן וריאציות רבות של ארכיטקטורות טרנספורמרים, כולל BERT, DistilBERT, BigBird, OpenGPT3 ועוד, שניתן לכוונן. חבילת [HuggingFace](https://github.com/huggingface/) מספקת מאגר לאימון רבות מהארכיטקטורות הללו עם PyTorch.\n",
    "\n",
    "## שימוש ב-BERT לסיווג טקסט\n",
    "\n",
    "בואו נראה כיצד ניתן להשתמש במודל BERT מאומן מראש לפתרון המשימה המסורתית שלנו: סיווג רצפים. נסווג את מערך הנתונים המקורי שלנו, AG News.\n",
    "\n",
    "ראשית, נטען את ספריית HuggingFace ואת מערך הנתונים שלנו:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "מכיוון שנשתמש במודל BERT שהוכשר מראש, נצטרך להשתמש בטוקנייזר ספציפי. ראשית, נטען טוקנייזר שמקושר למודל BERT שהוכשר מראש.\n",
    "\n",
    "ספריית HuggingFace מכילה מאגר של מודלים שהוכשרו מראש, שניתן להשתמש בהם פשוט על ידי ציון שמם כארגומנט לפונקציות `from_pretrained`. כל הקבצים הבינאריים הנדרשים עבור המודל יירדו באופן אוטומטי.\n",
    "\n",
    "עם זאת, במקרים מסוימים תצטרכו לטעון מודלים משלכם. במקרה כזה, תוכלו לציין את הספרייה שמכילה את כל הקבצים הרלוונטיים, כולל פרמטרים עבור הטוקנייזר, קובץ `config.json` עם פרמטרי המודל, משקלים בינאריים וכו'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "האובייקט `tokenizer` מכיל את הפונקציה `encode` שניתן להשתמש בה ישירות לקידוד טקסט:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "לאחר מכן, בואו ניצור איטרטורים שנשתמש בהם במהלך האימון כדי לגשת לנתונים. מכיוון ש-BERT משתמש בפונקציית קידוד משלו, נצטרך להגדיר פונקציית ריפוד דומה ל-`padify` שהגדרנו קודם:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "במקרה שלנו, נשתמש במודל BERT מאומן מראש שנקרא `bert-base-uncased`. בואו נטען את המודל באמצעות חבילת `BertForSequenceClassfication`. זה מבטיח שלמודל שלנו כבר יש את הארכיטקטורה הנדרשת לסיווג, כולל המסווג הסופי. תראו הודעת אזהרה שמציינת שהמשקלים של המסווג הסופי לא מאותחלים, והמודל ידרוש אימון מוקדם - זה בסדר גמור, כי זה בדיוק מה שאנחנו עומדים לעשות!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו אנחנו מוכנים להתחיל באימון! מכיוון ש-BERT כבר עבר אימון מוקדם, אנחנו רוצים להתחיל עם קצב למידה קטן יחסית כדי לא להרוס את המשקלים ההתחלתיים.\n",
    "\n",
    "כל העבודה הקשה מתבצעת על ידי המודל `BertForSequenceClassification`. כאשר אנחנו מפעילים את המודל על נתוני האימון, הוא מחזיר גם את ההפסד (loss) וגם את הפלט של הרשת עבור המיני-באטץ' שהוזן. אנחנו משתמשים בהפסד לאופטימיזציה של הפרמטרים (`loss.backward()` מבצע את המעבר לאחור), וב-`out` לחישוב דיוק האימון על ידי השוואת התוויות שהתקבלו `labs` (מחושבות באמצעות `argmax`) עם התוויות הצפויות `labels`.\n",
    "\n",
    "כדי לשלוט בתהליך, אנחנו מצטברים את ההפסד והדיוק לאורך כמה איטרציות, ומדפיסים אותם כל `report_freq` מחזורי אימון.\n",
    "\n",
    "האימון הזה כנראה ייקח זמן רב, ולכן אנחנו מגבילים את מספר האיטרציות.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אתם יכולים לראות (במיוחד אם תגדילו את מספר האיטרציות ותמתינו מספיק זמן) שסיווג באמצעות BERT נותן לנו דיוק די טוב! זאת מכיוון ש-BERT כבר מבין היטב את מבנה השפה, ואנחנו רק צריכים לכוונן את המסווג הסופי. עם זאת, מכיוון ש-BERT הוא מודל גדול, תהליך האימון כולו לוקח זמן רב ודורש כוח חישוב משמעותי! (GPU, ועדיף יותר מאחד).\n",
    "\n",
    "> **Note:** בדוגמה שלנו, השתמשנו באחד מהמודלים הקטנים ביותר של BERT שהוכנו מראש. ישנם מודלים גדולים יותר שסביר להניח שיניבו תוצאות טובות יותר.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## הערכת ביצועי המודל\n",
    "\n",
    "עכשיו נוכל להעריך את ביצועי המודל שלנו על קבוצת הנתונים לבדיקות. לולאת ההערכה דומה מאוד ללולאת האימון, אבל חשוב לא לשכוח להעביר את המודל למצב הערכה על ידי קריאה ל-`model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## נקודות חשובות\n",
    "\n",
    "ביחידה זו ראינו כמה קל לקחת מודל שפה מאומן מראש מספריית **transformers** ולהתאים אותו למשימת סיווג הטקסט שלנו. באופן דומה, ניתן להשתמש במודלים של BERT לחילוץ ישויות, מענה על שאלות ומשימות NLP אחרות.\n",
    "\n",
    "מודלים של Transformer מייצגים את המצב המתקדם ביותר כיום בתחום ה-NLP, וברוב המקרים הם צריכים להיות הפתרון הראשון שבו תתחילו להתנסות כאשר אתם מיישמים פתרונות NLP מותאמים אישית. עם זאת, הבנת העקרונות הבסיסיים של רשתות עצביות חוזרות שנדונו במודול זה היא חשובה מאוד אם אתם רוצים לבנות מודלים עצביים מתקדמים.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**כתב ויתור**:  \nמסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). בעוד שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור הסמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T21:39:58+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "he"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}