<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-28T20:08:03+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "he"
}
-->
# הטמעות

## [שאלון לפני ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/27)

כאשר אימנו מסווגים המבוססים על BoW או TF/IDF, עבדנו עם וקטורי שקי מילים בעלי ממד גבוה באורך `vocab_size`, והמרנו באופן מפורש מווקטורי ייצוג מיקום בעלי ממד נמוך לייצוג דל של אחד-חם. עם זאת, ייצוג אחד-חם אינו יעיל מבחינת זיכרון. בנוסף, כל מילה מטופלת באופן עצמאי, כלומר וקטורי אחד-חם אינם מבטאים שום דמיון סמנטי בין מילים.

הרעיון של **הטמעה** הוא לייצג מילים באמצעות וקטורים צפופים בעלי ממד נמוך יותר, שמשקפים בצורה כלשהי את המשמעות הסמנטית של מילה. נדון בהמשך כיצד לבנות הטמעות מילים משמעותיות, אבל כרגע נחשוב על הטמעות כדרך להקטין את הממדיות של וקטור מילה.

לכן, שכבת ההטמעה תקבל מילה כקלט ותפיק וקטור פלט בגודל `embedding_size` מוגדר. במובן מסוים, זה מאוד דומה לשכבת `Linear`, אבל במקום לקבל וקטור אחד-חם, היא תוכל לקבל מספר מילה כקלט, מה שמאפשר לנו להימנע מיצירת וקטורי אחד-חם גדולים.

על ידי שימוש בשכבת הטמעה כשכבה הראשונה ברשת המסווג שלנו, נוכל לעבור משקי מילים למודל **שקי הטמעות**, שבו אנו ממירים תחילה כל מילה בטקסט שלנו להטמעה המתאימה, ואז מחשבים פונקציית צבירה כלשהי על כל ההטמעות הללו, כמו `sum`, `average` או `max`.

![תמונה המציגה מסווג הטמעות עבור חמש מילים ברצף.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.he.png)

> תמונה מאת המחבר

## ✍️ תרגילים: הטמעות

המשיכו ללמוד במחברות הבאות:
* [הטמעות עם PyTorch](EmbeddingsPyTorch.ipynb)
* [הטמעות TensorFlow](EmbeddingsTF.ipynb)

## הטמעות סמנטיות: Word2Vec

בעוד ששכבת ההטמעה למדה למפות מילים לייצוג וקטורי, ייצוג זה לא בהכרח היה בעל משמעות סמנטית רבה. יהיה נחמד ללמוד ייצוג וקטורי כזה שמילים דומות או מילים נרדפות יתאימו לוקטורים הקרובים זה לזה במונחים של מרחק וקטורי כלשהו (למשל, מרחק אוקלידי).

כדי לעשות זאת, עלינו לאמן מראש את מודל ההטמעה שלנו על אוסף טקסט גדול בצורה מסוימת. אחת הדרכים לאמן הטמעות סמנטיות נקראת [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). היא מבוססת על שתי ארכיטקטורות עיקריות המשמשות לייצוג מבוזר של מילים:

- **שק מילים רציף** (CBoW) — בארכיטקטורה זו, אנו מאמנים את המודל לנבא מילה מתוך הקשר סביבתי. בהינתן הנגרם $(W_{-2},W_{-1},W_0,W_1,W_2)$, מטרת המודל היא לנבא את $W_0$ מתוך $(W_{-2},W_{-1},W_1,W_2)$.
- **סקיפ-גרם רציף** הוא ההפך מ-CBoW. המודל משתמש בחלון הקשר של מילים סביביות כדי לנבא את המילה הנוכחית.

CBoW מהיר יותר, בעוד שסקיפ-גרם איטי יותר, אך עושה עבודה טובה יותר בייצוג מילים נדירות.

![תמונה המציגה את האלגוריתמים CBoW ו-Skip-Gram להמרת מילים לוקטורים.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.he.png)

> תמונה מתוך [המאמר הזה](https://arxiv.org/pdf/1301.3781.pdf)

הטמעות Word2Vec שאומנו מראש (כמו גם מודלים דומים אחרים, כגון GloVe) יכולים לשמש במקום שכבת הטמעה ברשתות נוירונים. עם זאת, עלינו להתמודד עם אוצר מילים, מכיוון שאוצר המילים ששימש לאימון מראש של Word2Vec/GloVe עשוי להיות שונה מאוצר המילים בטקסט שלנו. עיינו במחברות לעיל כדי לראות כיצד ניתן לפתור בעיה זו.

## הטמעות הקשריות

מגבלה מרכזית של ייצוגי הטמעות מאומנים מראש כמו Word2Vec היא בעיית הבהרת המשמעות של מילים. בעוד שהטמעות מאומנות מראש יכולות ללכוד חלק מהמשמעות של מילים בהקשר, כל המשמעויות האפשריות של מילה מקודדות באותה הטמעה. זה יכול לגרום לבעיות במודלים בהמשך, מכיוון שלמילים רבות, כמו המילה 'play', יש משמעויות שונות בהתאם להקשר שבו הן משמשות.

לדוגמה, המילה 'play' בשני המשפטים הבאים יש לה משמעות שונה לחלוטין:

- הלכתי ל**הצגה** בתיאטרון.
- ג'ון רוצה **לשחק** עם חבריו.

ההטמעות המאומנות מראש שהוזכרו לעיל מייצגות את שתי המשמעויות הללו של המילה 'play' באותה הטמעה. כדי להתגבר על מגבלה זו, עלינו לבנות הטמעות המבוססות על **מודל שפה**, שמאומן על קורפוס טקסט גדול ו*יודע* כיצד מילים יכולות להשתלב בהקשרים שונים. דיון בהטמעות הקשריות הוא מחוץ לתחום של מדריך זה, אך נחזור אליהן כשנדבר על מודלי שפה בהמשך הקורס.

## סיכום

בשיעור זה, גיליתם כיצד לבנות ולהשתמש בשכבות הטמעה ב-TensorFlow וב-PyTorch כדי לשקף טוב יותר את המשמעות הסמנטית של מילים.

## 🚀 אתגר

Word2Vec שימש ליישומים מעניינים, כולל יצירת שירי פואטיקה. עיינו ב[מאמר הזה](https://www.politetype.com/blog/word2vec-color-poems) שמסביר כיצד המחבר השתמש ב-Word2Vec ליצירת פואטיקה. צפו גם ב[סרטון הזה של דן שיפמן](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) כדי לגלות הסבר שונה על הטכניקה הזו. לאחר מכן נסו ליישם את הטכניקות הללו על קורפוס הטקסט שלכם, אולי ממקור כמו Kaggle.

## [שאלון אחרי ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## סקירה ולימוד עצמי

קראו את המאמר הזה על Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [מטלה: מחברות](assignment.md)

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.