<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-28T19:47:08+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "he"
}
-->
# מבוא לרשתות נוירונים. פרספטרון רב-שכבתי

בפרק הקודם למדתם על המודל הפשוט ביותר של רשת נוירונים - פרספטרון חד-שכבתי, מודל סיווג ליניארי לשתי קטגוריות.

בפרק זה נרחיב את המודל למסגרת גמישה יותר, שתאפשר לנו:

* לבצע **סיווג רב-קטגורי** בנוסף לסיווג דו-קטגורי  
* לפתור **בעיות רגרסיה** בנוסף לסיווג  
* להפריד בין קטגוריות שאינן ניתנות להפרדה ליניארית  

בנוסף, נפתח מסגרת מודולרית משלנו ב-Python שתאפשר לנו לבנות ארכיטקטורות שונות של רשתות נוירונים.

## [שאלון לפני ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## פורמליזציה של למידת מכונה

נתחיל בפורמליזציה של בעיית למידת המכונה. נניח שיש לנו קבוצת נתוני אימון **X** עם תוויות **Y**, ואנו צריכים לבנות מודל *f* שיבצע תחזיות מדויקות ככל האפשר. איכות התחזיות נמדדת על ידי **פונקציית הפסד** ℒ. פונקציות הפסד נפוצות כוללות:

* עבור בעיית רגרסיה, כאשר אנו צריכים לחזות מספר, ניתן להשתמש ב-**שגיאה מוחלטת** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, או ב-**שגיאה ריבועית** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>  
* עבור סיווג, משתמשים ב-**0-1 loss** (שהוא למעשה **דיוק** המודל), או ב-**logistic loss**.  

בפרספטרון חד-שכבתי, הפונקציה *f* הוגדרה כפונקציה ליניארית *f(x)=wx+b* (כאן *w* הוא מטריצת המשקלים, *x* הוא וקטור התכונות הקלטיות, ו-*b* הוא וקטור ההטיה). בארכיטקטורות שונות של רשתות נוירונים, הפונקציה יכולה לקבל צורה מורכבת יותר.

> במקרה של סיווג, לעיתים קרובות נרצה לקבל הסתברויות של הקטגוריות המתאימות כיציאת הרשת. כדי להמיר מספרים שרירותיים להסתברויות (למשל, לנרמל את היציאה), אנו משתמשים לעיתים קרובות בפונקציית **softmax** σ, ואז הפונקציה *f* הופכת ל-*f(x)=σ(wx+b)*.

בהגדרת *f* לעיל, *w* ו-*b* נקראים **פרמטרים** θ=⟨*w,b*⟩. בהתחשב בקבוצת הנתונים ⟨**X**,**Y**⟩, ניתן לחשב שגיאה כוללת על כל קבוצת הנתונים כפונקציה של הפרמטרים θ.

> ✅ **מטרת אימון רשת נוירונים היא למזער את השגיאה על ידי שינוי הפרמטרים θ**

## אופטימיזציית ירידת גרדיאנט

קיים שיטה ידועה לאופטימיזציית פונקציות הנקראת **ירידת גרדיאנט**. הרעיון הוא שניתן לחשב נגזרת (במקרה רב-ממדי נקראת **גרדיאנט**) של פונקציית ההפסד ביחס לפרמטרים, ולשנות את הפרמטרים כך שהשגיאה תקטן. ניתן לנסח זאת כך:

* אתחול הפרמטרים לערכים אקראיים w<sup>(0)</sup>, b<sup>(0)</sup>  
* חזרה על הצעד הבא פעמים רבות:  
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w  
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b  

במהלך האימון, צעדי האופטימיזציה אמורים להיחשב בהתחשב בכל קבוצת הנתונים (זכרו שהפסד מחושב כסכום על כל דוגמאות האימון). עם זאת, במציאות אנו לוקחים חלקים קטנים מקבוצת הנתונים הנקראים **מיני-בצ'ים**, ומחשבים גרדיאנטים על בסיס תת-קבוצה של הנתונים. מכיוון שהתת-קבוצה נלקחת באופן אקראי בכל פעם, שיטה זו נקראת **ירידת גרדיאנט סטוכסטית** (SGD).

## פרספטרונים רב-שכבתיים ובקפרופגציה

רשת חד-שכבתית, כפי שראינו לעיל, מסוגלת לסווג קטגוריות הניתנות להפרדה ליניארית. כדי לבנות מודל עשיר יותר, ניתן לשלב מספר שכבות ברשת. מתמטית, המשמעות היא שהפונקציה *f* תהיה בעלת צורה מורכבת יותר, ותיחשב במספר שלבים:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>  
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>  
* f = σ(z<sub>2</sub>)  

כאן, α היא **פונקציית הפעלה לא-ליניארית**, σ היא פונקציית softmax, והפרמטרים הם θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

אלגוריתם ירידת הגרדיאנט יישאר זהה, אך חישוב הגרדיאנטים יהיה מורכב יותר. בהתחשב בכלל השרשרת לחישוב נגזרות, ניתן לחשב נגזרות כך:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)  
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)  

> ✅ כלל השרשרת משמש לחישוב נגזרות של פונקציית ההפסד ביחס לפרמטרים.

שימו לב שהחלק השמאלי ביותר בכל הביטויים הללו זהה, ולכן ניתן לחשב נגזרות ביעילות על ידי התחלה מפונקציית ההפסד והתקדמות "אחורה" דרך גרף החישוב. לכן שיטת אימון פרספטרון רב-שכבתי נקראת **בקפרופגציה**, או 'בקפרופ'.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: ציון מקור התמונה

> ✅ נעמיק בנושא הבקפרופ בדוגמה במחברת שלנו.

## סיכום

בשיעור זה בנינו ספריית רשתות נוירונים משלנו, והשתמשנו בה למשימת סיווג דו-ממדית פשוטה.

## 🚀 אתגר

במחברת המצורפת, תיישמו מסגרת משלכם לבניית ואימון פרספטרונים רב-שכבתיים. תוכלו לראות בפירוט כיצד רשתות נוירונים מודרניות פועלות.

המשיכו ל-[OwnFramework](OwnFramework.ipynb) ועבדו עליה.

## [שאלון לאחר ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## סקירה ולימוד עצמי

בקפרופגציה היא אלגוריתם נפוץ ב-AI ו-ML, שכדאי ללמוד [בפירוט נוסף](https://wikipedia.org/wiki/Backpropagation).

## [מטלה](lab/README.md)

במעבדה זו, תתבקשו להשתמש במסגרת שבניתם בשיעור זה כדי לפתור את בעיית סיווג הספרות בכתב יד של MNIST.

* [הוראות](lab/README.md)  
* [מחברת](lab/MyFW_MNIST.ipynb)  

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.