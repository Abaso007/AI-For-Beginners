<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "789d6c3fb6fc7948a470b33078a5983a",
  "translation_date": "2025-09-23T10:21:43+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "he"
}
-->
# מבוא לרשתות נוירונים. פרספטרון רב-שכבתי

בפרק הקודם למדתם על מודל הרשת הנוירונית הפשוט ביותר - פרספטרון חד-שכבתי, מודל סיווג ליניארי לשתי קטגוריות.

בפרק זה נרחיב את המודל למסגרת גמישה יותר, שתאפשר לנו:

* לבצע **סיווג רב-קטגוריות** בנוסף לסיווג דו-קטגוריות  
* לפתור **בעיות רגרסיה** בנוסף לסיווג  
* להפריד בין קטגוריות שאינן ניתנות להפרדה ליניארית  

בנוסף, נפתח מסגרת מודולרית משלנו ב-Python שתאפשר לנו לבנות ארכיטקטורות שונות של רשתות נוירונים.

## [שאלון לפני ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## פורמליזציה של למידת מכונה

נתחיל בפורמליזציה של בעיית למידת המכונה. נניח שיש לנו קבוצת נתונים לאימון **X** עם תוויות **Y**, ואנו צריכים לבנות מודל *f* שיבצע תחזיות מדויקות ככל האפשר. איכות התחזיות נמדדת באמצעות **פונקציית הפסד** &lagran;. פונקציות הפסד נפוצות כוללות:

* עבור בעיית רגרסיה, כאשר אנו צריכים לחזות מספר, ניתן להשתמש ב**שגיאה מוחלטת** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, או ב**שגיאה ריבועית** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>  
* עבור סיווג, משתמשים ב**0-1 loss** (שבעצם זהה ל**דיוק** המודל), או ב**logistic loss**.  

בפרספטרון חד-שכבתי, פונקציה *f* הוגדרה כפונקציה ליניארית *f(x)=wx+b* (כאן *w* הוא מטריצת המשקלים, *x* הוא וקטור התכונות הקלט, ו-*b* הוא וקטור ההטיה). בארכיטקטורות שונות של רשתות נוירונים, פונקציה זו יכולה לקבל צורה מורכבת יותר.

> במקרה של סיווג, לעיתים קרובות רצוי לקבל הסתברויות של הקטגוריות המתאימות כיציאת הרשת. כדי להמיר מספרים שרירותיים להסתברויות (למשל, לנרמל את היציאה), אנו משתמשים לעיתים קרובות בפונקציית **softmax** &sigma;, והפונקציה *f* הופכת ל-*f(x)=&sigma;(wx+b)*  

בהגדרת *f* לעיל, *w* ו-*b* נקראים **פרמטרים** &theta;=⟨*w,b*⟩. בהתחשב בקבוצת הנתונים ⟨**X**,**Y**⟩, ניתן לחשב שגיאה כוללת על כל קבוצת הנתונים כפונקציה של הפרמטרים &theta;.

> ✅ **מטרת אימון הרשת הנוירונית היא למזער את השגיאה על ידי שינוי הפרמטרים &theta;**

## אופטימיזציה באמצעות ירידת גרדיאנט

קיים שיטה ידועה לאופטימיזציה של פונקציות הנקראת **ירידת גרדיאנט**. הרעיון הוא שניתן לחשב נגזרת (במקרה רב-ממדי נקראת **גרדיאנט**) של פונקציית הפסד ביחס לפרמטרים, ולשנות את הפרמטרים כך שהשגיאה תקטן. ניתן לפרמל זאת כך:

* אתחול הפרמטרים בערכים אקראיים w<sup>(0)</sup>, b<sup>(0)</sup>  
* חזרה על הצעד הבא פעמים רבות:  
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w  
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b  

במהלך האימון, הצעדים האופטימיזציה אמורים להיחשב בהתחשב בכל קבוצת הנתונים (זכרו שהפסד מחושב כסכום על כל דוגמאות האימון). עם זאת, בפועל אנו לוקחים חלקים קטנים מהנתונים הנקראים **minibatches**, ומחשבים גרדיאנטים על בסיס תת-קבוצה של הנתונים. מכיוון שהתת-קבוצה נלקחת באופן אקראי בכל פעם, שיטה זו נקראת **ירידת גרדיאנט סטוכסטית** (SGD).

## פרספטרונים רב-שכבתיים ובקפרופגציה

רשת חד-שכבתית, כפי שראינו לעיל, מסוגלת לסווג קטגוריות הניתנות להפרדה ליניארית. כדי לבנות מודל עשיר יותר, ניתן לשלב מספר שכבות ברשת. מבחינה מתמטית, זה אומר שהפונקציה *f* תהיה בעלת צורה מורכבת יותר, ותיחשב במספר שלבים:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>  
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>  
* f = &sigma;(z<sub>2</sub>)  

כאן, &alpha; היא **פונקציית הפעלה לא ליניארית**, &sigma; היא פונקציית softmax, והפרמטרים הם &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

אלגוריתם ירידת הגרדיאנט יישאר זהה, אך יהיה קשה יותר לחשב גרדיאנטים. בהתחשב בכלל השרשרת של נגזרות, ניתן לחשב נגזרות כך:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)  
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)  

> ✅ כלל השרשרת של נגזרות משמש לחישוב נגזרות של פונקציית הפסד ביחס לפרמטרים.

שימו לב שהחלק השמאלי ביותר של כל הביטויים הללו זהה, ולכן ניתן לחשב נגזרות בצורה יעילה על ידי התחלה מפונקציית הפסד והתקדמות "אחורה" דרך גרף החישוב. לכן שיטת האימון של פרספטרון רב-שכבתי נקראת **בקפרופגציה**, או 'backprop'.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: ציון מקור התמונה  

> ✅ נעסוק בבקפרופגציה בפירוט רב יותר בדוגמה במחברת שלנו.  

## סיכום

בשיעור זה, בנינו ספריית רשתות נוירונים משלנו, והשתמשנו בה למשימת סיווג דו-ממדית פשוטה.

## 🚀 אתגר

במחברת המצורפת, תבנו ותאמנו מסגרת משלכם לבניית פרספטרונים רב-שכבתיים. תוכלו לראות בפירוט כיצד רשתות נוירונים מודרניות פועלות.

עברו למחברת [OwnFramework](OwnFramework.ipynb) ועבדו עליה.

## [שאלון אחרי ההרצאה](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## סקירה ולימוד עצמי

בקפרופגציה היא אלגוריתם נפוץ ב-AI ו-ML, שכדאי ללמוד [בפירוט רב יותר](https://wikipedia.org/wiki/Backpropagation)

## [מטלה](lab/README.md)

במעבדה זו, תתבקשו להשתמש במסגרת שבניתם בשיעור זה כדי לפתור את משימת סיווג הספרות בכתב יד של MNIST.

* [הוראות](lab/README.md)  
* [מחברת](lab/MyFW_MNIST.ipynb)  

---

