<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ae074cd940fc2f4dc24fc07b66ccbd99",
  "translation_date": "2025-08-28T19:28:44+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md",
  "language_code": "he"
}
-->
# טריקים לאימון למידה עמוקה

ככל שרשתות עצביות נעשות עמוקות יותר, תהליך האימון שלהן הופך למאתגר יותר ויותר. אחת הבעיות המרכזיות היא מה שמכונה [גרדיאנטים נעלמים](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) או [גרדיאנטים מתפוצצים](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.). [הפוסט הזה](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) מספק מבוא טוב לבעיות הללו.

כדי להפוך את אימון הרשתות העמוקות ליעיל יותר, ישנן מספר טכניקות שניתן להשתמש בהן.

## שמירה על ערכים בטווח סביר

כדי להפוך את החישובים המספריים ליציבים יותר, אנו רוצים לוודא שכל הערכים בתוך הרשת העצבית שלנו נמצאים בטווח סביר, בדרך כלל [-1..1] או [0..1]. זו אינה דרישה מחמירה מאוד, אך טבע החישובים בנקודה צפה הוא כזה שערכים בעלי סדרי גודל שונים אינם יכולים להיות מעובדים יחד בצורה מדויקת. לדוגמה, אם נוסיף 10<sup>-10</sup> ו-10<sup>10</sup>, סביר להניח שנקבל 10<sup>10</sup>, מכיוון שהערך הקטן יותר "יומר" לאותו סדר גודל כמו הגדול יותר, וכך המנטיסה תאבד.

רוב פונקציות האקטיבציה כוללות אי-לינאריות סביב [-1..1], ולכן הגיוני להתאים את כל נתוני הקלט לטווח [-1..1] או [0..1].

## אתחול משקל ראשוני

באופן אידיאלי, אנו רוצים שהערכים יישארו באותו טווח לאחר מעבר דרך שכבות הרשת. לכן חשוב לאתחל את המשקלים בצורה שתשמר את התפלגות הערכים.

התפלגות נורמלית **N(0,1)** אינה רעיון טוב, מכיוון שאם יש לנו *n* קלטים, סטיית התקן של הפלט תהיה *n*, והערכים עשויים לצאת מטווח [0..1].

האתחולים הבאים משמשים לעיתים קרובות:

 * התפלגות אחידה -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/√n_in)** מבטיחה שלקלטים עם ממוצע אפס וסטיית תקן של 1 יישארו אותו ממוצע/סטיית תקן
 * **N(0,√2/(n_in+n_out))** -- מה שמכונה **אתחול Xavier** (`glorot`), הוא עוזר לשמור על האותות בטווח במהלך ההעברה קדימה ואחורה

## נרמול מיני-מקטע (Batch Normalization)

גם עם אתחול משקל נכון, המשקלים יכולים להיות גדולים או קטנים באופן שרירותי במהלך האימון, והם יוציאו את האותות מהטווח הנכון. ניתן להחזיר את האותות לטווח באמצעות אחת מטכניקות **נרמול**. בעוד שיש כמה מהן (נרמול משקל, נרמול שכבה), הנפוצה ביותר היא נרמול מיני-מקטע.

הרעיון של **נרמול מיני-מקטע** הוא לקחת בחשבון את כל הערכים במיני-מקטע, ולבצע נרמול (כלומר להחסיר את הממוצע ולחלק בסטיית התקן) בהתבסס על הערכים הללו. זה מיושם כשכבת רשת שמבצעת את הנרמול הזה לאחר החלת המשקלים, אך לפני פונקציית האקטיבציה. כתוצאה מכך, סביר שנראה דיוק סופי גבוה יותר ואימון מהיר יותר.

הנה [המאמר המקורי](https://arxiv.org/pdf/1502.03167.pdf) על נרמול מיני-מקטע, [ההסבר בוויקיפדיה](https://en.wikipedia.org/wiki/Batch_normalization), ו[פוסט בלוג מבוא טוב](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (ועוד אחד [ברוסית](https://habrahabr.ru/post/309302/)).

## Dropout

**Dropout** היא טכניקה מעניינת שמסירה אחוז מסוים של נוירונים אקראיים במהלך האימון. זה מיושם גם כשכבה עם פרמטר אחד (אחוז הנוירונים להסרה, בדרך כלל 10%-50%), ובמהלך האימון היא מאפסת אלמנטים אקראיים של וקטור הקלט, לפני העברתו לשכבה הבאה.

למרות שזה עשוי להישמע כמו רעיון מוזר, ניתן לראות את ההשפעה של Dropout על אימון מסווג ספרות MNIST במחברת [`Dropout.ipynb`](Dropout.ipynb). זה מאיץ את האימון ומאפשר לנו להשיג דיוק גבוה יותר בפחות אפוקים של אימון.

ניתן להסביר את ההשפעה בכמה דרכים:

 * ניתן לראות בכך גורם הלם אקראי למודל, שמוציא את האופטימיזציה ממינימום מקומי
 * ניתן לראות בכך *מיצוע מודל מרומז*, מכיוון שבמהלך Dropout אנו מאמנים מודל מעט שונה

> *יש אנשים שאומרים שכאשר אדם שיכור מנסה ללמוד משהו, הוא יזכור זאת טוב יותר בבוקר למחרת, בהשוואה לאדם פיכח, מכיוון שמוח עם כמה נוירונים לא מתפקדים מנסה להסתגל טוב יותר כדי לתפוס את המשמעות. מעולם לא בדקנו בעצמנו אם זה נכון או לא.*

## מניעת התאמת יתר

אחד ההיבטים החשובים מאוד בלמידה עמוקה הוא היכולת למנוע [התאמת יתר](../../3-NeuralNetworks/05-Frameworks/Overfitting.md). למרות שזה עשוי להיות מפתה להשתמש במודל רשת עצבית חזק מאוד, תמיד עלינו לאזן בין מספר הפרמטרים של המודל לבין מספר דגימות האימון.

> ודא שאתה מבין את מושג [התאמת היתר](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) שהצגנו קודם!

ישנן מספר דרכים למנוע התאמת יתר:

 * עצירה מוקדמת -- מעקב רציף אחר השגיאה על קבוצת האימות ועצירת האימון כאשר שגיאת האימות מתחילה לעלות.
 * דעיכת משקל מפורשת / רגולריזציה -- הוספת עונש נוסף לפונקציית ההפסד עבור ערכים מוחלטים גבוהים של משקלים, מה שמונע מהמודל להגיע לתוצאות מאוד לא יציבות
 * מיצוע מודל -- אימון מספר מודלים ולאחר מכן מיצוע התוצאה. זה עוזר למזער את השונות.
 * Dropout (מיצוע מודל מרומז)

## אופטימיזרים / אלגוריתמי אימון

היבט חשוב נוסף באימון הוא לבחור אלגוריתם אימון טוב. בעוד ש**ירידת גרדיאנט** קלאסית היא בחירה סבירה, היא יכולה לפעמים להיות איטית מדי, או לגרום לבעיות אחרות.

בלמידה עמוקה, אנו משתמשים ב**ירידת גרדיאנט סטוכסטית** (SGD), שהיא ירידת גרדיאנט המיושמת על מיני-מקטעים, שנבחרו באופן אקראי מתוך קבוצת האימון. המשקלים מותאמים באמצעות הנוסחה הזו:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### מומנטום

ב**SGD עם מומנטום**, אנו שומרים חלק מהגרדיאנט מהשלבים הקודמים. זה דומה למצב שבו אנו נעים עם אינרציה, ומקבלים מכה בכיוון אחר, המסלול שלנו אינו משתנה מיד, אלא שומר על חלק מהתנועה המקורית. כאן אנו מציגים וקטור נוסף v כדי לייצג את *המהירות*:

* v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ  
* w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

כאן הפרמטר γ מציין את המידה שבה אנו לוקחים את האינרציה בחשבון: γ=0 מתאים ל-SGD קלאסי; γ=1 הוא משוואת תנועה טהורה.

### Adam, Adagrad, וכו'

מכיוון שבכל שכבה אנו מכפילים אותות במטריצה W<sub>i</sub>, בהתאם ל-||W<sub>i</sub>||, הגרדיאנט יכול או להיעלם ולהיות קרוב ל-0, או לעלות ללא הגבלה. זו מהות בעיית הגרדיאנטים המתפוצצים/נעלמים.

אחת הפתרונות לבעיה זו היא להשתמש רק בכיוון הגרדיאנט במשוואה, ולהתעלם מהערך המוחלט, כלומר:

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), כאשר ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

אלגוריתם זה נקרא **Adagrad**. אלגוריתמים נוספים שמשתמשים באותה רעיון: **RMSProp**, **Adam**

> **Adam** נחשב לאלגוריתם יעיל מאוד עבור יישומים רבים, אז אם אינך בטוח באיזה להשתמש - השתמש ב-Adam.

### חיתוך גרדיאנט

חיתוך גרדיאנט הוא הרחבה של הרעיון לעיל. כאשר ||∇ℒ|| ≤ θ, אנו מתחשבים בגרדיאנט המקורי באופטימיזציית המשקל, וכאשר ||∇ℒ|| > θ - אנו מחלקים את הגרדיאנט בנורמה שלו. כאן θ הוא פרמטר, ברוב המקרים ניתן לקחת θ=1 או θ=10.

### דעיכת קצב למידה

הצלחה באימון תלויה לעיתים קרובות בפרמטר קצב הלמידה η. הגיוני להניח שערכים גדולים יותר של η מביאים לאימון מהיר יותר, שזה משהו שאנו בדרך כלל רוצים בתחילת האימון, ואז ערכים קטנים יותר של η מאפשרים לנו לכוונן את הרשת. לכן, ברוב המקרים אנו רוצים להקטין את η במהלך תהליך האימון.

ניתן לעשות זאת על ידי הכפלת η במספר כלשהו (למשל 0.98) לאחר כל אפוק של האימון, או באמצעות **תזמון קצב למידה** מורכב יותר.

## ארכיטקטורות רשת שונות

בחירת ארכיטקטורת הרשת הנכונה לבעיה שלך יכולה להיות מאתגרת. בדרך כלל, נבחר ארכיטקטורה שהוכיחה את עצמה עבור המשימה הספציפית שלנו (או משימה דומה). הנה [סקירה טובה](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) של ארכיטקטורות רשת עצבית לראייה ממוחשבת.

> חשוב לבחור ארכיטקטורה שתהיה חזקה מספיק עבור מספר דגימות האימון שיש לנו. בחירת מודל חזק מדי יכולה להוביל ל[התאמת יתר](../../3-NeuralNetworks/05-Frameworks/Overfitting.md).

דרך טובה נוספת תהיה להשתמש בארכיטקטורה שתתאים באופן אוטומטי למורכבות הנדרשת. במידה מסוימת, ארכיטקטורת **ResNet** ו-**Inception** הן בעלות יכולת התאמה עצמית. [עוד על ארכיטקטורות לראייה ממוחשבת](../07-ConvNets/CNN_Architectures.md).

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס AI [Co-op Translator](https://github.com/Azure/co-op-translator). בעוד שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור הסמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.