{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение RL для балансировки Cartpole\n",
    "\n",
    "Этот ноутбук является частью [Учебной программы AI для начинающих](http://aka.ms/ai-beginners). Он был вдохновлен [официальным руководством PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) и [этой реализацией Cartpole на PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "В этом примере мы будем использовать RL для обучения модели, которая сможет балансировать шест на тележке, движущейся влево и вправо по горизонтальной шкале. Мы будем использовать среду [OpenAI Gym](https://www.gymlibrary.ml/) для симуляции этого процесса.\n",
    "\n",
    "> **Note**: Вы можете запустить код этого урока локально (например, из Visual Studio Code), в этом случае симуляция откроется в новом окне. При запуске кода онлайн, возможно, потребуется внести некоторые изменения в код, как описано [здесь](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Начнем с проверки установки Gym:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте создадим среду CartPole и посмотрим, как с ней работать. Среда обладает следующими свойствами:\n",
    "\n",
    "* **Action space** — это набор возможных действий, которые мы можем выполнять на каждом шаге симуляции.\n",
    "* **Observation space** — это пространство наблюдений, которые мы можем получить.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как работает симуляция. Следующий цикл запускает симуляцию до тех пор, пока `env.step` не вернет флаг завершения `done`. Мы будем случайным образом выбирать действия с помощью `env.action_space.sample()`, что, скорее всего, приведет к быстрому провалу эксперимента (среда CartPole завершается, если скорость CartPole, его положение или угол выходят за определенные пределы).\n",
    "\n",
    "> Симуляция откроется в новом окне. Вы можете запустить код несколько раз и посмотреть, как он себя ведет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете заметить, что наблюдения содержат 4 числа. Это:\n",
    "- Положение тележки\n",
    "- Скорость тележки\n",
    "- Угол наклона шеста\n",
    "- Скорость вращения шеста\n",
    "\n",
    "`rew` — это награда, которую мы получаем на каждом шаге. В среде CartPole вы получаете 1 очко за каждый шаг симуляции, и цель состоит в том, чтобы максимизировать общий результат, то есть время, в течение которого CartPole может балансировать, не падая.\n",
    "\n",
    "Во время обучения с подкреплением наша цель — обучить **политику** $\\pi$, которая для каждого состояния $s$ будет определять, какое действие $a$ следует предпринять, то есть, по сути, $a = \\pi(s)$.\n",
    "\n",
    "Если вы хотите вероятностное решение, вы можете рассматривать политику как возвращающую набор вероятностей для каждого действия, то есть $\\pi(a|s)$ будет означать вероятность того, что мы выберем действие $a$ в состоянии $s$.\n",
    "\n",
    "## Метод градиента политики\n",
    "\n",
    "В самом простом алгоритме обучения с подкреплением, называемом **Градиент политики**, мы будем обучать нейронную сеть предсказывать следующее действие.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать сеть, проводя множество экспериментов и обновляя нашу сеть после каждого запуска. Давайте определим функцию, которая будет проводить эксперимент и возвращать результаты (так называемый **трейс**) - все состояния, действия (и их рекомендуемые вероятности) и награды:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете запустить один эпизод с необученной сетью и заметить, что общий награда (или длина эпизода) очень мала:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из сложных аспектов алгоритма градиента политики — использование **дисконтированных вознаграждений**. Идея заключается в том, что мы вычисляем вектор общих вознаграждений на каждом шаге игры, и в процессе этого дисконтируем ранние вознаграждения с использованием некоторого коэффициента $gamma$. Мы также нормализуем полученный вектор, потому что будем использовать его как вес для влияния на наше обучение:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь приступим к обучению! Мы проведем 300 эпизодов, и в каждом эпизоде будем выполнять следующие шаги:\n",
    "\n",
    "1. Запустим эксперимент и соберем трассировку.\n",
    "2. Вычислим разницу (`gradients`) между выполненными действиями и предсказанными вероятностями. Чем меньше разница, тем больше уверенности в том, что было выбрано правильное действие.\n",
    "3. Рассчитаем дисконтированные награды и умножим градиенты на эти награды — это обеспечит, что шаги с более высокими наградами окажут большее влияние на конечный результат, чем шаги с низкими наградами.\n",
    "4. Ожидаемые целевые действия для нашей нейронной сети будут частично взяты из предсказанных вероятностей во время выполнения, а частично из рассчитанных градиентов. Мы будем использовать параметр `alpha`, чтобы определить, в какой степени учитывать градиенты и награды — это называется *скоростью обучения* алгоритма подкрепления.\n",
    "5. Наконец, мы обучим нашу сеть на состояниях и ожидаемых действиях, а затем повторим процесс.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте запустим эпизод с рендерингом, чтобы увидеть результат:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надеюсь, вы заметили, что теперь штанга может довольно хорошо балансировать!\n",
    "\n",
    "## Модель Actor-Critic\n",
    "\n",
    "Модель Actor-Critic представляет собой дальнейшее развитие градиентов политики, в которой мы создаем нейронную сеть для изучения как политики, так и предполагаемых вознаграждений. Сеть будет иметь два выхода (или можно рассматривать это как две отдельные сети):\n",
    "* **Actor** будет рекомендовать действие, которое нужно выполнить, предоставляя распределение вероятностей состояний, как в модели с градиентами политики.\n",
    "* **Critic** будет оценивать, какое вознаграждение можно ожидать от этих действий. Он возвращает общее предполагаемое вознаграждение в будущем для данного состояния.\n",
    "\n",
    "Давайте определим такую модель:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно будет немного изменить наши функции `discounted_rewards` и `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы запустим основной цикл обучения. Мы будем использовать процесс ручного обучения сети, вычисляя соответствующие функции потерь и обновляя параметры сети:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные выводы\n",
    "\n",
    "В этом демонстрационном примере мы рассмотрели два алгоритма обучения с подкреплением: простой градиент политики и более сложный актор-критик. Вы могли заметить, что эти алгоритмы работают с абстрактными понятиями состояния, действия и награды, что позволяет применять их к совершенно различным средам.\n",
    "\n",
    "Обучение с подкреплением дает возможность находить лучшую стратегию для решения задачи, основываясь только на конечной награде. Тот факт, что нам не нужны размеченные наборы данных, позволяет многократно повторять симуляции для оптимизации наших моделей. Однако в области обучения с подкреплением все еще существует множество сложностей, с которыми вы можете познакомиться, если решите углубиться в эту увлекательную сферу искусственного интеллекта.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от ответственности**:  \nЭтот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T10:41:28+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "ru"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}