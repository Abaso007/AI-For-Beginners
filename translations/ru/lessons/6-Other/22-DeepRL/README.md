<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-26T06:45:18+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ru"
}
-->
# Глубокое обучение с подкреплением

Обучение с подкреплением (Reinforcement Learning, RL) считается одной из основных парадигм машинного обучения наряду с обучением с учителем и без учителя. Если в обучении с учителем мы полагаемся на набор данных с известными результатами, то RL основано на **обучении через действия**. Например, когда мы впервые видим компьютерную игру, мы начинаем играть, даже не зная правил, и вскоре можем улучшить свои навыки просто в процессе игры и корректировки своего поведения.

## [Тест перед лекцией](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Для выполнения RL нам нужно:

* **Среда** или **симулятор**, который задает правила игры. Мы должны иметь возможность проводить эксперименты в симуляторе и наблюдать результаты.
* Некоторая **функция вознаграждения**, которая указывает, насколько успешным был наш эксперимент. В случае обучения игре в компьютерную игру вознаграждением будет наш итоговый счет.

На основе функции вознаграждения мы должны корректировать свое поведение и улучшать свои навыки, чтобы в следующий раз играть лучше. Основное отличие RL от других типов машинного обучения заключается в том, что в RL мы обычно не знаем, выиграем мы или проиграем, пока не закончим игру. Таким образом, мы не можем сказать, является ли определенный ход хорошим или плохим - мы получаем вознаграждение только в конце игры.

Во время RL мы обычно проводим множество экспериментов. В каждом эксперименте нам нужно находить баланс между следованием оптимальной стратегии, которую мы изучили до сих пор (**эксплуатация**), и исследованием новых возможных состояний (**исследование**).

## OpenAI Gym

Отличным инструментом для RL является [OpenAI Gym](https://gym.openai.com/) - это **среда симуляции**, которая может моделировать множество различных сред, начиная от игр Atari и заканчивая физикой балансировки шеста. Это одна из самых популярных сред для обучения алгоритмов обучения с подкреплением, поддерживаемая [OpenAI](https://openai.com/).

> **Примечание**: Вы можете увидеть все доступные среды OpenAI Gym [здесь](https://gym.openai.com/envs/#classic_control).

## Балансировка CartPole

Вы, вероятно, видели современные устройства для балансировки, такие как *Segway* или *гироскутеры*. Они автоматически балансируют, регулируя движение колес в ответ на сигналы от акселерометра или гироскопа. В этом разделе мы научимся решать похожую задачу - балансировку шеста. Это похоже на ситуацию, когда цирковой артист балансирует шест на руке, но в данном случае балансировка происходит только в одной плоскости.

Упрощенная версия балансировки известна как задача **CartPole**. В мире CartPole у нас есть горизонтальная платформа, которая может двигаться влево или вправо, и цель - удерживать вертикальный шест на платформе, пока она движется.

<img alt="cartpole" src="images/cartpole.png" width="200"/>

Чтобы создать и использовать эту среду, нам нужно всего несколько строк кода на Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Каждая среда доступна одинаковым образом:
* `env.reset` начинает новый эксперимент.
* `env.step` выполняет шаг симуляции. Он принимает **действие** из **пространства действий** и возвращает **наблюдение** (из пространства наблюдений), а также вознаграждение и флаг завершения.

В приведенном выше примере на каждом шаге выполняется случайное действие, поэтому продолжительность эксперимента очень короткая:

![cartpole без балансировки](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Цель алгоритма RL - обучить модель, так называемую **политику** π, которая будет возвращать действие в ответ на заданное состояние. Политику также можно считать вероятностной, например, для любого состояния *s* и действия *a* она будет возвращать вероятность π(*a*|*s*), что мы должны выполнить *a* в состоянии *s*.

## Алгоритм Policy Gradients

Самый очевидный способ моделирования политики - создание нейронной сети, которая будет принимать состояния на вход и возвращать соответствующие действия (или, скорее, вероятности всех действий). В некотором смысле это похоже на обычную задачу классификации, с одним важным отличием - мы заранее не знаем, какие действия нужно предпринимать на каждом шаге.

Идея заключается в оценке этих вероятностей. Мы строим вектор **накопленных вознаграждений**, который показывает наше общее вознаграждение на каждом шаге эксперимента. Мы также применяем **дисконтирование вознаграждений**, умножая более ранние вознаграждения на некоторый коэффициент γ=0.99, чтобы уменьшить их влияние. Затем мы усиливаем те шаги в эксперименте, которые приносят большее вознаграждение.

> Узнайте больше об алгоритме Policy Gradient и посмотрите его в действии в [примере блокнота](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Алгоритм Actor-Critic

Улучшенная версия подхода Policy Gradients называется **Actor-Critic**. Основная идея заключается в том, что нейронная сеть будет обучена возвращать две вещи:

* Политику, которая определяет, какое действие предпринять. Эта часть называется **actor**.
* Оценку общего вознаграждения, которое мы можем ожидать в данном состоянии - эта часть называется **critic**.

В некотором смысле эта архитектура напоминает [GAN](../../4-ComputerVision/10-GANs/README.md), где у нас есть две сети, обучающиеся друг против друга. В модели actor-critic, actor предлагает действие, которое нужно предпринять, а critic пытается критически оценить результат. Однако наша цель - обучить эти сети совместно.

Поскольку мы знаем как реальные накопленные вознаграждения, так и результаты, возвращаемые критиком во время эксперимента, довольно легко построить функцию потерь, которая минимизирует разницу между ними. Это даст нам **потери критика**. Мы можем вычислить **потери актера**, используя тот же подход, что и в алгоритме Policy Gradient.

После выполнения одного из этих алгоритмов мы можем ожидать, что наш CartPole будет вести себя следующим образом:

![балансирующий cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Упражнения: Policy Gradients и Actor-Critic RL

Продолжите обучение в следующих блокнотах:

* [RL в TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL в PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Другие задачи RL

Обучение с подкреплением сегодня является быстро развивающейся областью исследований. Некоторые интересные примеры применения RL:

* Обучение компьютера играть в **игры Atari**. Сложность этой задачи заключается в том, что у нас нет простого состояния, представленного вектором, а есть скриншот - и нам нужно использовать CNN для преобразования изображения экрана в вектор признаков или извлечения информации о вознаграждении. Игры Atari доступны в Gym.
* Обучение компьютера играть в настольные игры, такие как шахматы и го. Недавно программы, такие как **Alpha Zero**, достигли передового уровня, обучаясь с нуля, когда два агента играют друг против друга и улучшаются на каждом шаге.
* В промышленности RL используется для создания систем управления на основе симуляции. Сервис [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) специально предназначен для этого.

## Заключение

Теперь мы узнали, как обучать агентов достигать хороших результатов, просто предоставляя им функцию вознаграждения, которая определяет желаемое состояние игры, и давая им возможность разумно исследовать пространство поиска. Мы успешно попробовали два алгоритма и достигли хорошего результата за относительно короткий период времени. Однако это только начало вашего пути в RL, и вам определенно стоит рассмотреть возможность прохождения отдельного курса, если вы хотите углубиться в эту тему.

## 🚀 Задание

Изучите приложения, перечисленные в разделе "Другие задачи RL", и попробуйте реализовать одно из них!

## [Тест после лекции](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Обзор и самостоятельное изучение

Узнайте больше о классическом обучении с подкреплением в нашем [курсе "Машинное обучение для начинающих"](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Посмотрите [это отличное видео](https://www.youtube.com/watch?v=qv6UVOQ0F44) о том, как компьютер может научиться играть в Super Mario.

## Задание: [Обучите Mountain Car](lab/README.md)

Ваша цель в этом задании - обучить другую среду Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.