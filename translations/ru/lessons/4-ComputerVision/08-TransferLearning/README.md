<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "717775c4050ccbffbe0c961ad8bf7bf7",
  "translation_date": "2025-08-26T06:43:00+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/README.md",
  "language_code": "ru"
}
-->
# Предобученные сети и перенос обучения

Обучение сверточных нейронных сетей (CNN) может занимать много времени и требует большого объема данных. Однако значительная часть времени уходит на обучение лучшим низкоуровневым фильтрам, которые сеть может использовать для извлечения паттернов из изображений. Возникает естественный вопрос — можно ли использовать нейронную сеть, обученную на одном наборе данных, и адаптировать ее для классификации других изображений без полного процесса обучения?

## [Тест перед лекцией](https://ff-quizzes.netlify.app/en/ai/quiz/15)

Этот подход называется **переносом обучения**, потому что мы переносим часть знаний из одной модели нейронной сети в другую. В переносе обучения мы обычно начинаем с предобученной модели, которая была обучена на большом наборе изображений, например, **ImageNet**. Эти модели уже хорошо справляются с извлечением различных признаков из общих изображений, и во многих случаях достаточно построить классификатор поверх этих извлеченных признаков, чтобы получить хороший результат.

> ✅ Перенос обучения — это термин, который встречается и в других академических областях, таких как образование. Он относится к процессу переноса знаний из одной области в другую.

## Предобученные модели как извлекатели признаков

Сверточные сети, о которых мы говорили в предыдущем разделе, содержат множество слоев, каждый из которых предназначен для извлечения определенных признаков из изображения, начиная с низкоуровневых комбинаций пикселей (таких как горизонтальные/вертикальные линии или штрихи) и заканчивая высокоуровневыми комбинациями признаков, соответствующими, например, глазу или пламени. Если обучить CNN на достаточно большом наборе разнообразных изображений, сеть должна научиться извлекать эти общие признаки.

И Keras, и PyTorch содержат функции для легкой загрузки предобученных весов нейронных сетей для некоторых популярных архитектур, большинство из которых были обучены на изображениях из ImageNet. Наиболее часто используемые описаны на странице [Архитектуры CNN](../07-ConvNets/CNN_Architectures.md) из предыдущего урока. В частности, вы можете рассмотреть использование одной из следующих моделей:

* **VGG-16/VGG-19** — относительно простые модели, которые все же дают хорошую точность. Часто использование VGG в качестве первого эксперимента — хороший выбор, чтобы понять, как работает перенос обучения.
* **ResNet** — семейство моделей, предложенных Microsoft Research в 2015 году. Они содержат больше слоев и требуют больше ресурсов.
* **MobileNet** — семейство моделей уменьшенного размера, подходящих для мобильных устройств. Используйте их, если у вас ограничены ресурсы и вы готовы пожертвовать небольшой точностью.

Вот пример признаков, извлеченных из изображения кота с помощью сети VGG-16:

![Признаки, извлеченные VGG-16](../../../../../translated_images/features.6291f9c7ba3a0b951af88fc9864632b9115365410765680680d30c927dd67354.ru.png)

## Набор данных "Кошки против собак"

В этом примере мы будем использовать набор данных [Кошки и собаки](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), который очень близок к реальной задаче классификации изображений.

## ✍️ Упражнение: Перенос обучения

Давайте посмотрим, как работает перенос обучения, в соответствующих ноутбуках:

* [Перенос обучения — PyTorch](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb)
* [Перенос обучения — TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningTF.ipynb)

## Визуализация идеального кота

Предобученная нейронная сеть содержит различные паттерны внутри своего *"мозга"*, включая представления об **идеальном коте** (а также идеальной собаке, идеальной зебре и т.д.). Было бы интересно как-то **визуализировать это изображение**. Однако это не так просто, потому что паттерны распределены по всем весам сети и организованы в иерархическую структуру.

Один из подходов заключается в том, чтобы начать с случайного изображения и затем использовать технику **оптимизации методом градиентного спуска**, чтобы изменить это изображение так, чтобы сеть начала считать его котом.

![Цикл оптимизации изображения](../../../../../translated_images/ideal-cat-loop.999fbb8ff306e044f997032f4eef9152b453e6a990e449bbfb107de2493cc37e.ru.png)

Однако, если мы сделаем это, мы получим что-то, очень похожее на случайный шум. Это происходит потому, что *существует множество способов заставить сеть считать входное изображение котом*, включая такие, которые визуально не имеют смысла. Хотя эти изображения содержат множество паттернов, типичных для кота, ничего не ограничивает их визуальную выразительность.

Чтобы улучшить результат, мы можем добавить еще один член в функцию потерь, который называется **потеря вариации**. Это метрика, показывающая, насколько похожи соседние пиксели изображения. Минимизация потерь вариации делает изображение более гладким и устраняет шум, раскрывая более визуально привлекательные паттерны. Вот пример таких "идеальных" изображений, которые классифицируются как кот и как зебра с высокой вероятностью:

![Идеальный кот](../../../../../translated_images/ideal-cat.203dd4597643d6b0bd73038b87f9c0464322725e3a06ab145d25d4a861c70592.ru.png) | ![Идеальная зебра](../../../../../translated_images/ideal-zebra.7f70e8b54ee15a7a314000bb5df38a6cfe086ea04d60df4d3ef313d046b98a2b.ru.png)
-----|-----
*Идеальный кот* | *Идеальная зебра*

Аналогичный подход можно использовать для выполнения так называемых **атак с использованием противоречивых примеров** на нейронную сеть. Предположим, мы хотим обмануть нейронную сеть и заставить собаку выглядеть как кот. Если мы возьмем изображение собаки, которое сеть распознает как собаку, мы можем немного изменить его с помощью оптимизации методом градиентного спуска, пока сеть не начнет классифицировать его как кота:

![Изображение собаки](../../../../../translated_images/original-dog.8f68a67d2fe0911f33041c0f7fce8aa4ea919f9d3917ec4b468298522aeb6356.ru.png) | ![Изображение собаки, классифицированное как кот](../../../../../translated_images/adversarial-dog.d9fc7773b0142b89752539bfbf884118de845b3851c5162146ea0b8809fc820f.ru.png)
-----|-----
*Оригинальное изображение собаки* | *Изображение собаки, классифицированное как кот*

Посмотрите код для воспроизведения результатов выше в следующем ноутбуке:

* [Идеальный и противоречивый кот — TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/AdversarialCat_TF.ipynb)

## Заключение

Используя перенос обучения, вы можете быстро создать классификатор для задачи классификации пользовательских объектов и достичь высокой точности. Вы можете заметить, что более сложные задачи, которые мы решаем сейчас, требуют большей вычислительной мощности и не могут быть легко решены на CPU. В следующем разделе мы попробуем использовать более легковесную реализацию для обучения той же модели с использованием меньших вычислительных ресурсов, что приведет к небольшому снижению точности.

## 🚀 Задание

В сопровождающих ноутбуках есть заметки в конце о том, что перенос знаний лучше всего работает с похожими данными для обучения (например, новый вид животных). Проведите эксперименты с совершенно новыми типами изображений, чтобы увидеть, насколько хорошо или плохо работают ваши модели переноса знаний.

## [Тест после лекции](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Обзор и самостоятельное изучение

Прочитайте [TrainingTricks.md](TrainingTricks.md), чтобы углубить свои знания о других способах обучения моделей.

## [Задание](lab/README.md)

В этой лабораторной работе мы будем использовать реальный набор данных [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) с 35 породами кошек и собак и построим классификатор с использованием переноса обучения.

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.