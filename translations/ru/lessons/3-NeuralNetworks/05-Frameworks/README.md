<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-26T06:47:12+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "ru"
}
-->
# Фреймворки для нейронных сетей

Как мы уже узнали, для эффективного обучения нейронных сетей необходимо выполнить два условия:

* Уметь работать с тензорами, например, умножать, складывать и вычислять функции, такие как сигмоида или softmax.
* Уметь вычислять градиенты всех выражений для выполнения оптимизации методом градиентного спуска.

## [Квиз перед лекцией](https://ff-quizzes.netlify.app/en/ai/quiz/9)

Хотя библиотека `numpy` может выполнять первую задачу, нам нужен механизм для вычисления градиентов. В [нашем фреймворке](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), который мы разработали в предыдущем разделе, нам приходилось вручную программировать все функции производных в методе `backward`, который выполняет обратное распространение ошибки. В идеале фреймворк должен предоставлять возможность вычислять градиенты *любого выражения*, которое мы можем определить.

Еще один важный аспект — возможность выполнять вычисления на GPU или других специализированных вычислительных устройствах, таких как [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Обучение глубоких нейронных сетей требует *огромного* количества вычислений, и возможность параллелизовать эти вычисления на GPU крайне важна.

> ✅ Термин "параллелизация" означает распределение вычислений между несколькими устройствами.

На сегодняшний день два самых популярных фреймворка для нейронных сетей — это [TensorFlow](http://TensorFlow.org) и [PyTorch](https://pytorch.org/). Оба предоставляют низкоуровневый API для работы с тензорами как на CPU, так и на GPU. Помимо низкоуровневого API, также существуют высокоуровневые API, такие как [Keras](https://keras.io/) и [PyTorch Lightning](https://pytorchlightning.ai/).

Низкоуровневый API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------------|-------------------------------------|--------------------------------
Высокоуровневый API | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Низкоуровневые API** в обоих фреймворках позволяют создавать так называемые **вычислительные графы**. Этот граф определяет, как вычислять выходные данные (обычно функцию потерь) с заданными входными параметрами, и может быть отправлен на выполнение на GPU, если оно доступно. Существуют функции для дифференцирования этого вычислительного графа и вычисления градиентов, которые затем можно использовать для оптимизации параметров модели.

**Высокоуровневые API** рассматривают нейронные сети как **последовательность слоев**, что значительно упрощает создание большинства нейронных сетей. Обучение модели обычно сводится к подготовке данных и вызову функции `fit`.

Высокоуровневый API позволяет быстро создавать типичные нейронные сети, не вдаваясь в множество деталей. В то же время низкоуровневый API предоставляет гораздо больше контроля над процессом обучения, и поэтому часто используется в исследованиях, когда разрабатываются новые архитектуры нейронных сетей.

Важно понимать, что оба API можно использовать вместе. Например, вы можете разработать собственную архитектуру слоя сети с использованием низкоуровневого API, а затем использовать её внутри более крупной сети, созданной и обученной с помощью высокоуровневого API. Или вы можете определить сеть с помощью высокоуровневого API как последовательность слоев, а затем использовать собственный низкоуровневый цикл обучения для оптимизации. Оба API используют одни и те же базовые концепции и разработаны так, чтобы хорошо работать вместе.

## Обучение

В этом курсе мы предлагаем большинство материалов как для PyTorch, так и для TensorFlow. Вы можете выбрать предпочитаемый фреймворк и изучать только соответствующие ноутбуки. Если вы не уверены, какой фреймворк выбрать, почитайте обсуждения в интернете на тему **PyTorch vs. TensorFlow**. Также вы можете ознакомиться с обоими фреймворками, чтобы лучше их понять.

По возможности мы будем использовать высокоуровневые API для упрощения. Однако мы считаем важным понять, как работают нейронные сети с нуля, поэтому в начале мы начнем с работы с низкоуровневым API и тензорами. Если вы хотите быстро приступить к работе и не тратить много времени на изучение этих деталей, вы можете пропустить их и сразу перейти к ноутбукам с высокоуровневым API.

## ✍️ Упражнения: Фреймворки

Продолжите обучение в следующих ноутбуках:

Низкоуровневый API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------------|-------------------------------------|--------------------------------
Высокоуровневый API | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

После освоения фреймворков давайте вспомним понятие переобучения.

# Переобучение

Переобучение — это чрезвычайно важное понятие в машинном обучении, и его необходимо правильно понимать!

Рассмотрим следующую задачу аппроксимации 5 точек (обозначенных `x` на графиках ниже):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.ru.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.ru.jpg)
-------------------------|--------------------------
**Линейная модель, 2 параметра** | **Нелинейная модель, 7 параметров**
Ошибка на обучении = 5.3 | Ошибка на обучении = 0
Ошибка на валидации = 5.1 | Ошибка на валидации = 20

* На левом графике мы видим хорошую линейную аппроксимацию. Поскольку количество параметров адекватно, модель правильно улавливает распределение точек.
* На правом графике модель слишком мощная. Поскольку у нас всего 5 точек, а модель имеет 7 параметров, она может подстроиться так, чтобы проходить через все точки, что делает ошибку на обучении равной 0. Однако это мешает модели понять правильную закономерность в данных, из-за чего ошибка на валидации становится очень высокой.

Очень важно найти правильный баланс между сложностью модели (количеством параметров) и количеством обучающих примеров.

## Почему возникает переобучение

  * Недостаточно обучающих данных
  * Слишком мощная модель
  * Слишком много шума во входных данных

## Как обнаружить переобучение

Как видно из графика выше, переобучение можно обнаружить по очень низкой ошибке на обучении и высокой ошибке на валидации. Обычно во время обучения мы видим, как обе ошибки — на обучении и валидации — начинают уменьшаться, но в какой-то момент ошибка на валидации может перестать уменьшаться и начать расти. Это будет признаком переобучения и сигналом, что обучение следует остановить (или хотя бы сохранить текущую модель).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.ru.png)

## Как предотвратить переобучение

Если вы видите, что происходит переобучение, вы можете сделать следующее:

 * Увеличить объем обучающих данных
 * Уменьшить сложность модели
 * Использовать [техники регуляризации](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), такие как [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), которые мы рассмотрим позже.

## Переобучение и компромисс между смещением и дисперсией

Переобучение — это частный случай более общей проблемы в статистике, называемой [компромисс между смещением и дисперсией](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Если рассмотреть возможные источники ошибок в нашей модели, можно выделить два типа ошибок:

* **Ошибки смещения** возникают из-за того, что наш алгоритм не может правильно уловить связь между обучающими данными. Это может быть связано с тем, что наша модель недостаточно мощная (**недообучение**).
* **Ошибки дисперсии**, которые возникают из-за того, что модель аппроксимирует шум во входных данных вместо значимых закономерностей (**переобучение**).

Во время обучения ошибка смещения уменьшается (по мере того как модель учится аппроксимировать данные), а ошибка дисперсии увеличивается. Важно остановить обучение — либо вручную (когда мы обнаруживаем переобучение), либо автоматически (с помощью регуляризации) — чтобы предотвратить переобучение.

## Заключение

В этом уроке вы узнали о различиях между различными API для двух самых популярных фреймворков для ИИ — TensorFlow и PyTorch. Кроме того, вы изучили очень важную тему — переобучение.

## 🚀 Задание

В сопровождающих ноутбуках вы найдете "задачи" в конце; пройдите через ноутбуки и выполните задачи.

## [Квиз после лекции](https://ff-quizzes.netlify.app/en/ai/quiz/10)

## Обзор и самостоятельное изучение

Изучите следующие темы:

- TensorFlow
- PyTorch
- Переобучение

Задайте себе следующие вопросы:

- В чем разница между TensorFlow и PyTorch?
- В чем разница между переобучением и недообучением?

## [Задание](lab/README.md)

В этой лабораторной работе вам предстоит решить две задачи классификации с использованием однослойных и многослойных полносвязных сетей с помощью PyTorch или TensorFlow.

* [Инструкции](lab/README.md)
* [Ноутбук](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.