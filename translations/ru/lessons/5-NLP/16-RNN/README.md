<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-26T06:35:14+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "ru"
}
-->
# Рекуррентные нейронные сети

## [Тест перед лекцией](https://ff-quizzes.netlify.app/en/ai/quiz/31)

В предыдущих разделах мы использовали богатые семантические представления текста и простой линейный классификатор поверх эмбеддингов. Такая архитектура позволяет захватывать агрегированное значение слов в предложении, но не учитывает **порядок** слов, так как операция агрегации поверх эмбеддингов удаляет эту информацию из исходного текста. Поскольку такие модели не могут моделировать порядок слов, они не способны решать более сложные или неоднозначные задачи, такие как генерация текста или ответы на вопросы.

Чтобы захватить смысл последовательности текста, необходимо использовать другую архитектуру нейронной сети, которая называется **рекуррентной нейронной сетью** или RNN. В RNN мы пропускаем предложение через сеть по одному символу за раз, и сеть производит некоторое **состояние**, которое затем передается обратно в сеть вместе со следующим символом.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.ru.png)

> Изображение автора

Учитывая входную последовательность токенов X<sub>0</sub>,...,X<sub>n</sub>, RNN создает последовательность блоков нейронной сети и обучает эту последовательность от начала до конца с использованием обратного распространения ошибки. Каждый блок сети принимает пару (X<sub>i</sub>,S<sub>i</sub>) в качестве входных данных и производит S<sub>i+1</sub> в качестве результата. Финальное состояние S<sub>n</sub> или (выход Y<sub>n</sub>) передается в линейный классификатор для получения результата. Все блоки сети имеют одинаковые веса и обучаются от начала до конца с использованием одного прохода обратного распространения.

Поскольку векторы состояния S<sub>0</sub>,...,S<sub>n</sub> передаются через сеть, она способна изучать последовательные зависимости между словами. Например, когда слово *не* появляется где-то в последовательности, сеть может научиться отрицать определенные элементы внутри вектора состояния, что приводит к отрицанию.

> ✅ Поскольку веса всех блоков RNN на изображении выше одинаковы, то же самое изображение можно представить как один блок (справа) с рекуррентной обратной связью, которая передает выходное состояние сети обратно на вход.

## Анатомия ячейки RNN

Давайте рассмотрим, как организована простая ячейка RNN. Она принимает предыдущее состояние S<sub>i-1</sub> и текущий символ X<sub>i</sub> в качестве входных данных и должна производить выходное состояние S<sub>i</sub> (а иногда нас также интересует другой выход Y<sub>i</sub>, как в случае с генеративными сетями).

Простая ячейка RNN имеет две матрицы весов внутри: одна преобразует входной символ (назовем ее W), а другая преобразует входное состояние (H). В этом случае выход сети вычисляется как σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b), где σ — это функция активации, а b — дополнительное смещение.

<img alt="Анатомия ячейки RNN" src="images/rnn-anatomy.png" width="50%"/>

> Изображение автора

Во многих случаях входные токены пропускаются через слой эмбеддинга перед входом в RNN для уменьшения размерности. В этом случае, если размерность входных векторов равна *emb_size*, а вектор состояния — *hid_size*, то размер W равен *emb_size*×*hid_size*, а размер H — *hid_size*×*hid_size*.

## Долгая краткосрочная память (LSTM)

Одна из главных проблем классических RNN — это проблема **затухающих градиентов**. Поскольку RNN обучаются от начала до конца в одном проходе обратного распространения, им сложно передавать ошибку к первым слоям сети, и, следовательно, сеть не может изучать отношения между удаленными токенами. Один из способов избежать этой проблемы — ввести **явное управление состоянием** с помощью так называемых **врат**. Существуют две известные архитектуры такого типа: **долгая краткосрочная память** (LSTM) и **гейтированная рекуррентная единица** (GRU).

![Пример ячейки долгой краткосрочной памяти](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Источник изображения TBD

Сеть LSTM организована аналогично RNN, но есть два состояния, которые передаются от слоя к слою: фактическое состояние C и скрытый вектор H. В каждой единице скрытый вектор H<sub>i</sub> объединяется с входом X<sub>i</sub>, и они управляют тем, что происходит с состоянием C через **врата**. Каждые врата — это нейронная сеть с сигмоидной активацией (выход в диапазоне [0,1]), которую можно рассматривать как побитовую маску при умножении на вектор состояния. Существуют следующие врата (слева направо на изображении выше):

* **Врата забывания** принимают скрытый вектор и определяют, какие компоненты вектора C нужно забыть, а какие пропустить.
* **Входные врата** берут некоторую информацию из входного и скрытого векторов и вставляют ее в состояние.
* **Выходные врата** преобразуют состояние через линейный слой с активацией *tanh*, затем выбирают некоторые из его компонентов с помощью скрытого вектора H<sub>i</sub>, чтобы произвести новое состояние C<sub>i+1</sub>.

Компоненты состояния C можно рассматривать как флаги, которые можно включать и выключать. Например, когда мы встречаем имя *Алиса* в последовательности, мы можем предположить, что оно относится к женскому персонажу, и установить флаг в состоянии, что в предложении есть женский существительный. Когда мы далее встречаем фразу *и Том*, мы установим флаг, что у нас есть множественное число существительных. Таким образом, манипулируя состоянием, мы можем предположительно отслеживать грамматические свойства частей предложения.

> ✅ Отличным ресурсом для понимания внутренней работы LSTM является статья [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Кристофера Олаха.

## Двунаправленные и многослойные RNN

Мы обсудили рекуррентные сети, которые работают в одном направлении — от начала последовательности к концу. Это выглядит естественно, так как напоминает способ, которым мы читаем и слушаем речь. Однако, поскольку во многих практических случаях у нас есть случайный доступ к входной последовательности, имеет смысл выполнять рекуррентные вычисления в обоих направлениях. Такие сети называются **двунаправленными** RNN. При работе с двунаправленной сетью нам понадобятся два скрытых вектора состояния, по одному для каждого направления.

Рекуррентная сеть, будь то однонаправленная или двунаправленная, захватывает определенные шаблоны внутри последовательности и может сохранять их в векторе состояния или передавать в выход. Как и в случае с сверточными сетями, мы можем построить еще один рекуррентный слой поверх первого, чтобы захватывать шаблоны более высокого уровня и строить из шаблонов низкого уровня, извлеченных первым слоем. Это приводит нас к понятию **многослойной RNN**, которая состоит из двух или более рекуррентных сетей, где выход предыдущего слоя передается следующему слою в качестве входа.

![Изображение многослойной LSTM RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.ru.jpg)

*Изображение из [этой замечательной статьи](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) Фернандо Лопеса*

## ✍️ Упражнения: Эмбеддинги

Продолжите обучение в следующих ноутбуках:

* [RNNs с PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs с TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Заключение

В этом разделе мы увидели, что RNN могут использоваться для классификации последовательностей, но на самом деле они могут решать гораздо больше задач, таких как генерация текста, машинный перевод и многое другое. Мы рассмотрим эти задачи в следующем разделе.

## 🚀 Задание

Прочитайте литературу о LSTM и подумайте об их применении:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Тест после лекции](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## Обзор и самостоятельное изучение

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Кристофера Олаха.

## [Задание: Ноутбуки](assignment.md)

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.