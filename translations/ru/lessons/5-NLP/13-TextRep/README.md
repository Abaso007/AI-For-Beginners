<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-26T06:36:47+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "ru"
}
-->
# Представление текста в виде тензоров

## [Тест перед лекцией](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Классификация текста

В первой части этого раздела мы сосредоточимся на задаче **классификации текста**. Мы будем использовать набор данных [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), который содержит новостные статьи, например:

* Категория: Наука/Технологии  
* Заголовок: Компания из Кентукки получила грант на изучение пептидов (AP)  
* Текст: AP - Компания, основанная исследователем химии из Университета Луисвилля, получила грант на разработку...

Наша цель — классифицировать новость в одну из категорий на основе текста.

## Представление текста

Если мы хотим решать задачи обработки естественного языка (NLP) с помощью нейронных сетей, нам нужно каким-то образом представить текст в виде тензоров. Компьютеры уже представляют текстовые символы в виде чисел, которые отображаются на экране с использованием кодировок, таких как ASCII или UTF-8.

<img alt="Изображение, показывающее диаграмму, связывающую символ с его ASCII и двоичным представлением" src="images/ascii-character-map.png" width="50%"/>

> [Источник изображения](https://www.seobility.net/en/wiki/ASCII)

Как люди, мы понимаем, что **означает** каждая буква, и как все символы объединяются, чтобы сформировать слова в предложении. Однако компьютеры сами по себе такого понимания не имеют, и нейронной сети приходится изучать значение во время обучения.

Поэтому мы можем использовать разные подходы для представления текста:

* **Побуквенное представление**, когда мы представляем текст, рассматривая каждый символ как число. Если у нас есть *C* различных символов в корпусе текста, то слово *Hello* будет представлено тензором размером 5x*C*. Каждая буква будет соответствовать столбцу тензора в формате one-hot кодирования.  
* **Пословное представление**, при котором мы создаем **словарь** всех слов в тексте и затем представляем слова с использованием one-hot кодирования. Этот подход несколько лучше, так как каждая буква сама по себе не имеет большого значения, и, используя более высокоуровневые семантические концепции — слова, мы упрощаем задачу для нейронной сети. Однако из-за большого размера словаря приходится работать с высокоразмерными разреженными тензорами.

Независимо от подхода, сначала нужно преобразовать текст в последовательность **токенов**, где токен — это либо символ, либо слово, либо даже часть слова. Затем токен преобразуется в число, обычно с использованием **словаря**, и это число можно подать в нейронную сеть с использованием one-hot кодирования.

## N-граммы

В естественном языке точное значение слов можно определить только в контексте. Например, значения *нейронная сеть* и *рыболовная сеть* совершенно разные. Один из способов учитывать это — строить модель на основе пар слов, рассматривая пары слов как отдельные токены словаря. Таким образом, предложение *Мне нравится ходить на рыбалку* будет представлено следующей последовательностью токенов: *Мне нравится*, *нравится ходить*, *ходить на*, *на рыбалку*. Проблема этого подхода в том, что размер словаря значительно увеличивается, а комбинации вроде *на рыбалку* и *на шопинг* представлены разными токенами, которые не имеют никакого семантического сходства, несмотря на одинаковый глагол.

В некоторых случаях можно использовать триграммы — комбинации из трех слов. Поэтому такой подход часто называют **n-граммами**. Также имеет смысл использовать n-граммы с побуквенным представлением, где n-граммы будут примерно соответствовать различным слогам.

## Мешок слов и TF/IDF

При решении задач, таких как классификация текста, нам нужно уметь представлять текст в виде вектора фиксированного размера, который будет использоваться в качестве входных данных для финального полносвязного классификатора. Один из самых простых способов сделать это — объединить представления отдельных слов, например, сложив их. Если мы сложим one-hot кодирования каждого слова, то получим вектор частот, показывающий, сколько раз каждое слово встречается в тексте. Такое представление текста называется **мешком слов** (BoW).

<img src="images/bow.png" width="90%"/>

> Изображение автора

BoW по сути показывает, какие слова встречаются в тексте и в каком количестве, что действительно может быть хорошим индикатором содержания текста. Например, новостная статья о политике, скорее всего, будет содержать слова *президент* и *страна*, а научная публикация — что-то вроде *коллайдер*, *открытие* и т.д. Таким образом, частоты слов во многих случаях могут быть хорошим индикатором содержания текста.

Проблема BoW в том, что некоторые распространенные слова, такие как *и*, *это* и т.д., встречаются в большинстве текстов и имеют самые высокие частоты, что маскирует действительно важные слова. Мы можем снизить значимость таких слов, учитывая частоту их появления во всей коллекции документов. Это основная идея подхода TF/IDF, который более подробно рассматривается в ноутбуках, приложенных к этому уроку.

Однако ни один из этих подходов не может полностью учитывать **семантику** текста. Для этого нам нужны более мощные модели нейронных сетей, которые мы обсудим позже в этом разделе.

## ✍️ Упражнения: Представление текста

Продолжите обучение в следующих ноутбуках:

* [Представление текста с PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Представление текста с TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## Заключение

На данный момент мы изучили техники, которые могут добавлять вес частоты к различным словам. Однако они не способны представлять значение или порядок. Как сказал известный лингвист Дж. Р. Фёрт в 1935 году: "Полное значение слова всегда контекстуально, и никакое изучение значения вне контекста нельзя считать серьезным". Позже в курсе мы узнаем, как извлекать контекстуальную информацию из текста с помощью языкового моделирования.

## 🚀 Задание

Попробуйте выполнить другие упражнения, используя мешок слов и различные модели данных. Возможно, вас вдохновит [этот конкурс на Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [Тест после лекции](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Обзор и самостоятельное изучение

Практикуйте свои навыки работы с текстовыми встраиваниями и техниками мешка слов на [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Задание: Ноутбуки](assignment.md)

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.