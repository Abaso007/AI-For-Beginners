<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T08:03:18+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "ru"
}
-->
# Предобученные крупные языковые модели

Во всех предыдущих задачах мы обучали нейронную сеть выполнять определённую задачу, используя размеченный набор данных. С крупными трансформерными моделями, такими как BERT, мы используем языковое моделирование в самосупервизируемом режиме, чтобы создать языковую модель, которая затем специализируется на конкретной задаче с дополнительным обучением в определённой области. Однако было доказано, что крупные языковые модели могут решать множество задач без какого-либо обучения, связанного с конкретной областью. Семейство моделей, способных на это, называется **GPT**: Генеративный Предобученный Трансформер.

## [Тест перед лекцией](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## Генерация текста и перплексия

Идея о том, что нейронная сеть может выполнять общие задачи без дополнительного обучения, представлена в статье [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Основная мысль заключается в том, что многие другие задачи можно моделировать с помощью **генерации текста**, поскольку понимание текста фактически означает способность его создавать. Поскольку модель обучена на огромном объёме текста, охватывающем человеческие знания, она также становится осведомлённой в широком круге тем.

> Понимание и способность создавать текст также подразумевают знание чего-то о мире вокруг нас. Люди в значительной степени учатся через чтение, и сеть GPT похожа в этом отношении.

Сети генерации текста работают, предсказывая вероятность следующего слова $$P(w_N)$$. Однако безусловная вероятность следующего слова равна частоте этого слова в текстовом корпусе. GPT может предоставить нам **условную вероятность** следующего слова, учитывая предыдущие: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Подробнее о вероятностях вы можете прочитать в нашем [курсе для начинающих по Data Science](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Качество модели генерации текста можно определить с помощью **перплексии**. Это внутренний метрик, который позволяет измерить качество модели без использования набора данных, связанного с конкретной задачей. Он основан на понятии *вероятности предложения* — модель присваивает высокую вероятность предложению, которое, скорее всего, является реальным (т.е. модель не **озадачена** им), и низкую вероятность предложениям, которые менее осмысленны (например, *Может ли это что?*). Когда мы предоставляем нашей модели предложения из реального текстового корпуса, мы ожидаем, что они будут иметь высокую вероятность и низкую **перплексию**. Математически это определяется как нормализованная обратная вероятность тестового набора:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Вы можете поэкспериментировать с генерацией текста, используя [редактор текста на основе GPT от Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. В этом редакторе вы начинаете писать текст, и нажатие **[TAB]** предложит вам несколько вариантов завершения. Если они слишком короткие или вас не устраивают — нажмите [TAB] снова, и вы получите больше вариантов, включая более длинные фрагменты текста.

## GPT — это семейство моделей

GPT — это не одна модель, а целая коллекция моделей, разработанных и обученных [OpenAI](https://openai.com).

Среди моделей GPT:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| Языковая модель с до 1,5 миллиарда параметров. | Языковая модель с до 175 миллиардов параметров. | 100 триллионов параметров, принимает как изображения, так и текст, а выводит текст. |

Модели GPT-3 и GPT-4 доступны [как когнитивный сервис от Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) и через [API OpenAI](https://openai.com/api/).

## Инженерия запросов (Prompt Engineering)

Поскольку GPT обучен на огромных объёмах данных для понимания языка и кода, он предоставляет ответы на входные данные (запросы). Запросы — это входные данные или вопросы для GPT, где пользователь даёт инструкции модели о задачах, которые она должна выполнить. Чтобы получить желаемый результат, необходимо составить наиболее эффективный запрос, что включает выбор правильных слов, форматов, фраз или даже символов. Этот подход называется [инженерией запросов](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Эта документация](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) предоставляет больше информации об инженерии запросов.

## ✍️ Пример блокнота: [Работа с OpenAI-GPT](GPT-PyTorch.ipynb)

Продолжите обучение с помощью следующих блокнотов:

* [Генерация текста с OpenAI-GPT и Hugging Face Transformers](GPT-PyTorch.ipynb)

## Заключение

Новые универсальные предобученные языковые модели не только моделируют структуру языка, но и содержат огромный объём естественного языка. Таким образом, их можно эффективно использовать для решения некоторых задач обработки естественного языка в режимах zero-shot или few-shot.

## [Тест после лекции](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

