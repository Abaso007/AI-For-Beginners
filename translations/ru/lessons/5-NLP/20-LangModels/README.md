<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-26T08:41:25+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "ru"
}
-->
# Предобученные крупные языковые модели

Во всех предыдущих задачах мы обучали нейронную сеть выполнять определённую задачу, используя размеченный набор данных. С крупными трансформерными моделями, такими как BERT, мы используем языковое моделирование в режиме самоконтроля, чтобы создать языковую модель, которая затем специализируется на конкретной задаче с помощью дополнительного обучения в определённой предметной области. Однако было доказано, что крупные языковые модели могут решать множество задач без какого-либо обучения для конкретной области. Семейство моделей, способных на это, называется **GPT**: Генеративный Предобученный Трансформер.

## [Тест перед лекцией](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/120)

## Генерация текста и перплексия

Идея о том, что нейронная сеть может выполнять общие задачи без дополнительного обучения, представлена в статье [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Основная идея заключается в том, что многие задачи можно моделировать с помощью **генерации текста**, так как понимание текста по сути означает способность его создавать. Поскольку модель обучена на огромном объёме текстов, охватывающих человеческие знания, она также становится осведомлённой в широком круге тем.

> Понимание и способность создавать текст также предполагают знание чего-то о мире вокруг нас. Люди в значительной степени учатся через чтение, и сеть GPT в этом отношении схожа.

Сети генерации текста работают, предсказывая вероятность следующего слова $$P(w_N)$$. Однако безусловная вероятность следующего слова равна частоте этого слова в корпусе текста. GPT способна предоставить нам **условную вероятность** следующего слова, учитывая предыдущие: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Подробнее о вероятностях вы можете прочитать в нашем [курсе "Наука о данных для начинающих"](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Качество модели генерации текста можно определить с помощью метрики **перплексия**. Это внутренняя метрика, которая позволяет измерить качество модели без использования набора данных для конкретной задачи. Она основана на понятии *вероятности предложения* — модель присваивает высокую вероятность предложению, которое, скорее всего, является реальным (т.е. модель не **смущена** этим предложением), и низкую вероятность предложениям, которые менее осмысленны (например, *Может ли это что?*). Когда мы предоставляем модели предложения из реального корпуса текста, мы ожидаем, что они будут иметь высокую вероятность и низкую **перплексию**. Математически это определяется как нормализованная обратная вероятность тестового набора:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Вы можете поэкспериментировать с генерацией текста, используя [редактор текста на базе GPT от Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. В этом редакторе вы начинаете писать текст, и нажатие **[TAB]** предложит вам несколько вариантов завершения. Если они слишком короткие или вас не устраивают, нажмите [TAB] снова, и вы получите больше вариантов, включая более длинные фрагменты текста.

## GPT — это семейство моделей

GPT — это не одна модель, а целая коллекция моделей, разработанных и обученных [OpenAI](https://openai.com).

Среди моделей GPT:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| Языковая модель с до 1,5 миллиарда параметров. | Языковая модель с до 175 миллиардов параметров. | 100 триллионов параметров, принимает как изображения, так и текст на вход, а на выходе выдаёт текст. |

Модели GPT-3 и GPT-4 доступны [в виде когнитивной службы от Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) и через [API OpenAI](https://openai.com/api/).

## Инженерия подсказок

Поскольку GPT обучена на огромных объёмах данных для понимания языка и кода, она предоставляет ответы на входные данные (подсказки). Подсказки — это входные данные или запросы для GPT, где пользователь даёт инструкции модели для выполнения задач. Чтобы получить желаемый результат, необходимо создать наиболее эффективную подсказку, что включает выбор правильных слов, форматов, фраз или даже символов. Этот подход называется [инженерией подсказок](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Эта документация](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) предоставляет больше информации об инженерии подсказок.

## ✍️ Пример блокнота: [Работа с OpenAI-GPT](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

Продолжите обучение с помощью следующих блокнотов:

* [Генерация текста с OpenAI-GPT и Hugging Face Transformers](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## Заключение

Новые универсальные предобученные языковые модели не только моделируют структуру языка, но и содержат огромный объём естественного языка. Таким образом, их можно эффективно использовать для решения некоторых задач обработки естественного языка в режимах zero-shot или few-shot.

## [Тест после лекции](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/220)

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.