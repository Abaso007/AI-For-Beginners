<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-26T08:33:02+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "ru"
}
-->
# Механизмы внимания и трансформеры

## [Предлекционный тест](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Одной из самых важных задач в области обработки естественного языка (NLP) является **машинный перевод**, ключевая задача, лежащая в основе таких инструментов, как Google Translate. В этом разделе мы сосредоточимся на машинном переводе или, более широко, на любой задаче *последовательность-к-последовательности* (также называемой **трансдукцией предложений**).

С помощью RNN задачи последовательность-к-последовательности реализуются двумя рекуррентными сетями, где одна сеть, **кодировщик**, преобразует входную последовательность в скрытое состояние, а другая сеть, **декодировщик**, разворачивает это скрытое состояние в результат перевода. Однако у этого подхода есть несколько проблем:

* Заключительное состояние сети-кодировщика с трудом запоминает начало предложения, что приводит к низкому качеству модели для длинных предложений.
* Все слова в последовательности оказывают одинаковое влияние на результат. На практике же отдельные слова во входной последовательности часто оказывают большее влияние на выходные последовательности, чем другие.

**Механизмы внимания** предоставляют способ взвешивания контекстного влияния каждого входного вектора на каждое предсказание выхода RNN. Это реализуется путем создания "ярлыков" между промежуточными состояниями входной RNN и выходной RNN. Таким образом, при генерации выходного символа y<sub>t</sub> мы учитываем все скрытые состояния входа h<sub>i</sub> с различными весовыми коэффициентами α<sub>t,i</sub>.

![Изображение, показывающее модель кодировщик/декодировщик с аддитивным слоем внимания](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.ru.png)

> Модель кодировщик-декодировщик с механизмом аддитивного внимания из [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитируется из [этого блога](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Матрица внимания {α<sub>i,j</sub>} представляет степень, в которой определенные входные слова участвуют в генерации данного слова в выходной последовательности. Ниже приведен пример такой матрицы:

![Изображение, показывающее пример выравнивания, найденного RNNsearch-50, взято из Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.ru.png)

> Рисунок из [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Рис.3)

Механизмы внимания ответственны за значительную часть текущего или близкого к текущему уровня развития NLP. Однако добавление внимания значительно увеличивает количество параметров модели, что привело к проблемам масштабирования с RNN. Основное ограничение масштабирования RNN заключается в том, что рекуррентная природа моделей затрудняет пакетную обработку и параллелизацию обучения. В RNN каждый элемент последовательности должен обрабатываться в последовательном порядке, что делает параллелизацию сложной.

![Кодировщик-декодировщик с вниманием](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Рисунок из [Google's Blog](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Применение механизмов внимания в сочетании с этим ограничением привело к созданию современных моделей трансформеров, таких как BERT и Open-GPT3, которые мы знаем и используем сегодня.

## Модели трансформеров

Одна из ключевых идей трансформеров заключается в том, чтобы избежать последовательной природы RNN и создать модель, которая может быть параллелизована во время обучения. Это достигается путем реализации двух идей:

* позиционное кодирование
* использование механизма самовнимания для захвата паттернов вместо RNN (или CNN) (именно поэтому статья, представляющая трансформеры, называется *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Позиционное кодирование/встраивание

Идея позиционного кодирования заключается в следующем. 
1. При использовании RNN относительная позиция токенов представляется количеством шагов и, таким образом, не требует явного представления. 
2. Однако при переходе к вниманию необходимо знать относительные позиции токенов в последовательности. 
3. Чтобы получить позиционное кодирование, мы дополняем нашу последовательность токенов последовательностью позиций токенов в последовательности (т.е. последовательностью чисел 0,1, ...).
4. Затем мы смешиваем позицию токена с вектором встраивания токена. Для преобразования позиции (целого числа) в вектор можно использовать различные подходы:

* Обучаемое встраивание, аналогичное встраиванию токенов. Это подход, который мы рассматриваем здесь. Мы применяем слои встраивания как к токенам, так и к их позициям, получая векторы встраивания одинаковых размеров, которые затем складываем.
* Фиксированная функция позиционного кодирования, предложенная в оригинальной статье.

<img src="images/pos-embedding.png" width="50%"/>

> Изображение автора

Результат, который мы получаем с позиционным встраиванием, включает как оригинальный токен, так и его позицию в последовательности.

### Многоголовое самовнимание

Далее нам нужно захватить некоторые паттерны внутри нашей последовательности. Для этого трансформеры используют механизм **самовнимания**, который по сути является вниманием, применяемым к той же последовательности как на входе, так и на выходе. Применение самовнимания позволяет учитывать **контекст** внутри предложения и видеть, какие слова взаимосвязаны. Например, это позволяет видеть, к каким словам относятся кореференции, такие как *it*, и учитывать контекст:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.ru.png)

> Изображение из [Google Blog](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

В трансформерах используется **многоголовое внимание**, чтобы дать сети возможность захватывать несколько различных типов зависимостей, например, долгосрочные и краткосрочные отношения между словами, кореференции и другие.

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) содержит больше деталей о реализации слоев трансформера.

### Внимание кодировщика-декодировщика

В трансформерах внимание используется в двух местах:

* Для захвата паттернов внутри входного текста с помощью самовнимания.
* Для выполнения перевода последовательности - это слой внимания между кодировщиком и декодировщиком.

Внимание кодировщика-декодировщика очень похоже на механизм внимания, используемый в RNN, как описано в начале этого раздела. Этот анимированный диаграмма объясняет роль внимания кодировщика-декодировщика.

![Анимированный GIF, показывающий, как выполняются оценки в моделях трансформеров.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Поскольку каждая входная позиция сопоставляется независимо с каждой выходной позицией, трансформеры могут лучше параллелизоваться, чем RNN, что позволяет создавать гораздо более крупные и выразительные языковые модели. Каждая голова внимания может быть использована для изучения различных отношений между словами, что улучшает задачи обработки естественного языка.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) — это очень большая многослойная сеть трансформеров с 12 слоями для *BERT-base* и 24 для *BERT-large*. Модель сначала предварительно обучается на большом корпусе текстовых данных (WikiPedia + книги) с использованием обучения без учителя (предсказание замаскированных слов в предложении). Во время предварительного обучения модель поглощает значительные уровни понимания языка, которые затем могут быть использованы с другими наборами данных с помощью тонкой настройки. Этот процесс называется **передача обучения**.

![картинка с http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.ru.png)

> Изображение [источник](http://jalammar.github.io/illustrated-bert/)

## ✍️ Упражнения: Трансформеры

Продолжите обучение в следующих ноутбуках:

* [Трансформеры в PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Трансформеры в TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Заключение

В этом уроке вы узнали о трансформерах и механизмах внимания, которые являются важными инструментами в арсенале NLP. Существует множество вариаций архитектур трансформеров, включая BERT, DistilBERT, BigBird, OpenGPT3 и другие, которые можно тонко настроить. Пакет [HuggingFace](https://github.com/huggingface/) предоставляет репозиторий для обучения многих из этих архитектур как с PyTorch, так и с TensorFlow.

## 🚀 Задание

## [Послетекционный тест](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Обзор и самостоятельное изучение

* [Пост в блоге](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), объясняющий классическую статью [Attention is all you need](https://arxiv.org/abs/1706.03762) о трансформерах.
* [Серия постов в блоге](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) о трансформерах, подробно объясняющая архитектуру.

## [Задание](assignment.md)

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.