{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Механизмы внимания и трансформеры\n",
    "\n",
    "Одним из главных недостатков рекуррентных сетей является то, что все слова в последовательности оказывают одинаковое влияние на результат. Это приводит к снижению эффективности стандартных моделей LSTM-энкодер-декодер для задач преобразования последовательностей, таких как распознавание именованных сущностей и машинный перевод. На практике отдельные слова во входной последовательности часто оказывают большее влияние на выходные данные, чем другие.\n",
    "\n",
    "Рассмотрим модель преобразования последовательностей, например, машинный перевод. Она реализуется с помощью двух рекуррентных сетей, где одна сеть (**энкодер**) сжимает входную последовательность в скрытое состояние, а другая сеть (**декодер**) разворачивает это скрытое состояние в переведенный результат. Проблема такого подхода заключается в том, что финальному состоянию сети сложно запомнить начало предложения, что приводит к снижению качества модели на длинных предложениях.\n",
    "\n",
    "**Механизмы внимания** предоставляют способ взвешивания контекстного влияния каждого входного вектора на каждое предсказание RNN. Это реализуется путем создания \"ярлыков\" между промежуточными состояниями входной RNN и выходной RNN. Таким образом, при генерации выходного символа $y_t$ мы будем учитывать все скрытые состояния входа $h_i$ с различными весовыми коэффициентами $\\alpha_{t,i}$.\n",
    "\n",
    "![Изображение, показывающее модель энкодер/декодер с аддитивным слоем внимания](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.ru.png)  \n",
    "*Модель энкодер-декодер с механизмом аддитивного внимания из [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитируется из [этого блога](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Матрица внимания $\\{\\alpha_{i,j}\\}$ представляет степень, с которой определенные входные слова влияют на генерацию конкретного слова в выходной последовательности. Ниже приведен пример такой матрицы:\n",
    "\n",
    "![Изображение, показывающее пример выравнивания, найденного RNNsearch-50, взято из Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.ru.png)  \n",
    "\n",
    "*Рисунок взят из [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Рис.3)*\n",
    "\n",
    "Механизмы внимания ответственны за многие современные или близкие к современным достижениям в области обработки естественного языка. Однако добавление внимания значительно увеличивает количество параметров модели, что привело к проблемам масштабирования с RNN. Одним из ключевых ограничений масштабирования RNN является то, что рекуррентная природа моделей затрудняет пакетную обработку и параллелизацию обучения. В RNN каждый элемент последовательности должен обрабатываться в последовательном порядке, что делает параллелизацию сложной.\n",
    "\n",
    "Применение механизмов внимания в сочетании с этим ограничением привело к созданию современных трансформерных моделей, которые мы знаем и используем сегодня, таких как BERT и OpenGPT3.\n",
    "\n",
    "## Трансформерные модели\n",
    "\n",
    "Вместо передачи контекста каждого предыдущего предсказания на следующий шаг оценки, **трансформерные модели** используют **позиционные кодировки** и внимание для захвата контекста заданного входа в пределах предоставленного окна текста. На изображении ниже показано, как позиционные кодировки с вниманием могут захватывать контекст в пределах заданного окна.\n",
    "\n",
    "![Анимация, показывающая, как выполняются оценки в трансформерных моделях.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Поскольку каждая входная позиция отображается независимо на каждую выходную позицию, трансформеры могут лучше параллелизоваться, чем RNN, что позволяет создавать более крупные и выразительные языковые модели. Каждая \"голова внимания\" может использоваться для изучения различных отношений между словами, что улучшает выполнение задач обработки естественного языка.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) — это очень большая многослойная трансформерная сеть с 12 слоями для *BERT-base* и 24 для *BERT-large*. Модель сначала предварительно обучается на большом корпусе текстовых данных (WikiPedia + книги) с использованием обучения без учителя (предсказание замаскированных слов в предложении). Во время предварительного обучения модель усваивает значительный уровень понимания языка, который затем можно использовать с другими наборами данных с помощью тонкой настройки. Этот процесс называется **трансферным обучением**.\n",
    "\n",
    "![Изображение с сайта http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.ru.png)\n",
    "\n",
    "Существует множество вариаций архитектур трансформеров, включая BERT, DistilBERT, BigBird, OpenGPT3 и другие, которые можно тонко настраивать. Пакет [HuggingFace](https://github.com/huggingface/) предоставляет репозиторий для обучения многих из этих архитектур с использованием PyTorch.\n",
    "\n",
    "## Использование BERT для классификации текста\n",
    "\n",
    "Давайте посмотрим, как мы можем использовать предварительно обученную модель BERT для решения нашей традиционной задачи: классификации последовательностей. Мы будем классифицировать наш оригинальный набор данных AG News.\n",
    "\n",
    "Сначала загрузим библиотеку HuggingFace и наш набор данных:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку мы будем использовать предварительно обученную модель BERT, нам потребуется использовать определенный токенайзер. Сначала мы загрузим токенайзер, связанный с предварительно обученной моделью BERT.\n",
    "\n",
    "Библиотека HuggingFace содержит репозиторий предварительно обученных моделей, которые можно использовать, просто указав их имена в качестве аргументов для функций `from_pretrained`. Все необходимые бинарные файлы для модели будут автоматически загружены.\n",
    "\n",
    "Однако иногда может потребоваться загрузить собственные модели. В этом случае вы можете указать директорию, содержащую все соответствующие файлы, включая параметры для токенайзера, файл `config.json` с параметрами модели, бинарные веса и т. д.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объект `tokenizer` содержит функцию `encode`, которую можно напрямую использовать для кодирования текста:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем давайте создадим итераторы, которые мы будем использовать во время обучения для доступа к данным. Поскольку BERT использует свою собственную функцию кодирования, нам нужно будет определить функцию дополнения, аналогичную `padify`, которую мы определили ранее:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае мы будем использовать предварительно обученную модель BERT под названием `bert-base-uncased`. Давайте загрузим модель с помощью пакета `BertForSequenceClassfication`. Это гарантирует, что наша модель уже имеет необходимую архитектуру для классификации, включая финальный классификатор. Вы увидите предупреждающее сообщение, в котором говорится, что веса финального классификатора не инициализированы, и модель потребует предварительного обучения - это совершенно нормально, потому что именно этим мы собираемся заняться!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы начать обучение! Поскольку BERT уже предварительно обучен, мы хотим использовать довольно маленькую скорость обучения, чтобы не испортить начальные веса.\n",
    "\n",
    "Всю основную работу выполняет модель `BertForSequenceClassification`. Когда мы вызываем модель на обучающих данных, она возвращает как значение потерь (loss), так и выходные данные сети для входного минибатча. Мы используем значение потерь для оптимизации параметров (`loss.backward()` выполняет обратное распространение ошибки), а `out` — для вычисления точности обучения, сравнивая полученные метки `labs` (вычисленные с помощью `argmax`) с ожидаемыми `labels`.\n",
    "\n",
    "Чтобы контролировать процесс, мы накапливаем значения потерь и точности за несколько итераций и выводим их каждые `report_freq` циклов обучения.\n",
    "\n",
    "Это обучение, скорее всего, займет довольно много времени, поэтому мы ограничиваем количество итераций.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете заметить (особенно если увеличить количество итераций и подождать достаточно долго), что классификация с использованием BERT дает довольно хорошую точность! Это связано с тем, что BERT уже достаточно хорошо понимает структуру языка, и нам нужно лишь дообучить финальный классификатор. Однако, поскольку BERT — это большая модель, весь процесс обучения занимает много времени и требует значительных вычислительных ресурсов! (GPU, и желательно больше одного).\n",
    "\n",
    "> **Note:** В нашем примере мы использовали одну из самых маленьких предобученных моделей BERT. Существуют более крупные модели, которые, вероятно, дадут лучшие результаты.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка производительности модели\n",
    "\n",
    "Теперь мы можем оценить производительность нашей модели на тестовом наборе данных. Цикл оценки очень похож на цикл обучения, но не забудьте переключить модель в режим оценки, вызвав `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные выводы\n",
    "\n",
    "В этом разделе мы увидели, как легко взять предварительно обученную языковую модель из библиотеки **transformers** и адаптировать её для задачи классификации текста. Аналогично, модели BERT могут использоваться для извлечения сущностей, ответа на вопросы и других задач обработки естественного языка.\n",
    "\n",
    "Модели трансформеров представляют собой передовой уровень в NLP, и в большинстве случаев они должны быть первым решением, с которого вы начинаете эксперименты при реализации пользовательских NLP-решений. Однако понимание базовых принципов рекуррентных нейронных сетей, обсуждаемых в этом модуле, крайне важно, если вы хотите создавать продвинутые нейронные модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от ответственности**:  \nЭтот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-28T12:09:04+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "ru"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}