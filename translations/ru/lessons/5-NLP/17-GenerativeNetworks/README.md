<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-26T06:36:18+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "ru"
}
-->
# Генеративные сети

## [Тест перед лекцией](https://ff-quizzes.netlify.app/en/ai/quiz/33)

Рекуррентные нейронные сети (RNN) и их варианты с управляемыми ячейками, такие как ячейки долгосрочной памяти (LSTM) и управляемые рекуррентные блоки (GRU), предоставляют механизм для моделирования языка, так как они могут изучать порядок слов и предсказывать следующее слово в последовательности. Это позволяет использовать RNN для **генеративных задач**, таких как обычная генерация текста, машинный перевод и даже создание подписей к изображениям.

> ✅ Вспомните все случаи, когда вы пользовались преимуществами генеративных задач, таких как автозавершение текста при вводе. Исследуйте свои любимые приложения, чтобы узнать, использовали ли они RNN.

В архитектуре RNN, которую мы обсуждали в предыдущем разделе, каждая единица RNN производила следующее скрытое состояние в качестве выхода. Однако мы также можем добавить еще один выход к каждой рекуррентной единице, что позволит нам выводить **последовательность** (равную по длине исходной последовательности). Более того, мы можем использовать RNN, которые не принимают входные данные на каждом шаге, а просто используют некоторый начальный вектор состояния и затем генерируют последовательность выходных данных.

Это позволяет создавать различные нейронные архитектуры, показанные на изображении ниже:

![Изображение, показывающее распространенные шаблоны рекуррентных нейронных сетей.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.ru.jpg)

> Изображение из блога [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) Андрея Карпати ([Andrej Karpaty](http://karpathy.github.io/))

* **Один-к-одному** — традиционная нейронная сеть с одним входом и одним выходом.
* **Один-ко-многим** — генеративная архитектура, которая принимает одно входное значение и генерирует последовательность выходных значений. Например, если мы хотим обучить сеть для **создания подписей к изображениям**, которая будет генерировать текстовое описание изображения, мы можем использовать изображение в качестве входных данных, пропустить его через CNN для получения скрытого состояния, а затем рекуррентная цепочка будет генерировать подпись слово за словом.
* **Многие-к-одному** соответствует архитектурам RNN, которые мы описали в предыдущем разделе, например, классификация текста.
* **Многие-ко-многим**, или **последовательность-к-последовательности**, соответствует задачам, таким как **машинный перевод**, где первая RNN собирает всю информацию из входной последовательности в скрытое состояние, а другая цепочка RNN разворачивает это состояние в выходную последовательность.

В этом разделе мы сосредоточимся на простых генеративных моделях, которые помогают нам генерировать текст. Для простоты мы будем использовать токенизацию на уровне символов.

Мы будем обучать эту RNN генерировать текст шаг за шагом. На каждом шаге мы будем брать последовательность символов длиной `nchars` и просить сеть сгенерировать следующий выходной символ для каждого входного символа:

![Изображение, показывающее пример генерации слова "HELLO" с помощью RNN.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.ru.png)

При генерации текста (во время инференса) мы начинаем с некоторого **запроса**, который передается через ячейки RNN для генерации промежуточного состояния, а затем из этого состояния начинается генерация. Мы генерируем один символ за раз, передаем состояние и сгенерированный символ в следующую ячейку RNN, чтобы сгенерировать следующий символ, пока не будет сгенерировано достаточное количество символов.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Изображение автора

## ✍️ Упражнения: Генеративные сети

Продолжите обучение в следующих ноутбуках:

* [Генеративные сети с PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Генеративные сети с TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Мягкая генерация текста и температура

Выход каждого элемента RNN — это распределение вероятностей символов. Если мы всегда выбираем символ с наивысшей вероятностью в качестве следующего символа в сгенерированном тексте, текст часто может "зацикливаться" на одних и тех же последовательностях символов, как в этом примере:

```
today of the second the company and a second the company ...
```

Однако, если мы посмотрим на распределение вероятностей для следующего символа, может оказаться, что разница между несколькими наивысшими вероятностями незначительна, например, один символ может иметь вероятность 0.2, а другой — 0.19 и т.д. Например, при выборе следующего символа в последовательности '*play*', следующим символом может быть как пробел, так и **e** (как в слове *player*).

Это приводит нас к выводу, что не всегда "справедливо" выбирать символ с наивысшей вероятностью, так как выбор второго по величине также может привести к осмысленному тексту. Более разумно **выбирать** символы из распределения вероятностей, предоставленного выходом сети. Мы также можем использовать параметр **температура**, который сглаживает распределение вероятностей, если мы хотим добавить больше случайности, или делает его более крутым, если мы хотим придерживаться символов с наивысшей вероятностью.

Изучите, как эта мягкая генерация текста реализована в ноутбуках, указанных выше.

## Заключение

Хотя генерация текста может быть полезна сама по себе, основные преимущества заключаются в возможности генерировать текст с помощью RNN из некоторого начального вектора признаков. Например, генерация текста используется как часть машинного перевода (последовательность-к-последовательности, в этом случае вектор состояния от *кодировщика* используется для генерации или *декодирования* переведенного сообщения) или для создания текстового описания изображения (в этом случае вектор признаков будет получен из CNN-экстрактора).

## 🚀 Задание

Пройдите уроки на Microsoft Learn по этой теме:

* Генерация текста с [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Тест после лекции](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## Обзор и самостоятельное изучение

Вот несколько статей для расширения ваших знаний:

* Различные подходы к генерации текста с использованием цепей Маркова, LSTM и GPT-2: [статья](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Пример генерации текста в [документации Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Задание](lab/README.md)

Мы рассмотрели, как генерировать текст символ за символом. В лабораторной работе вы изучите генерацию текста на уровне слов.

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.