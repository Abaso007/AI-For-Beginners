<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "437c988596e751072e41a5aad3fcc5d9",
  "translation_date": "2025-08-26T06:34:30+00:00",
  "source_file": "lessons/7-Ethics/README.md",
  "language_code": "ru"
}
-->
# Этический и ответственный ИИ

Вы почти завершили этот курс, и я надеюсь, что к настоящему моменту вы ясно понимаете, что ИИ основан на ряде формальных математических методов, которые позволяют находить взаимосвязи в данных и обучать модели для воспроизведения некоторых аспектов человеческого поведения. На данном этапе истории мы рассматриваем ИИ как очень мощный инструмент для извлечения закономерностей из данных и применения этих закономерностей для решения новых задач.

## [Тест перед лекцией](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5/)

Однако в научной фантастике мы часто видим истории, где ИИ представляет угрозу для человечества. Обычно такие истории сосредоточены вокруг какого-то восстания ИИ, когда он решает противостоять людям. Это подразумевает, что ИИ обладает какими-то эмоциями или может принимать решения, которые не были предусмотрены его разработчиками.

Тот тип ИИ, о котором мы узнали в этом курсе, — это не что иное, как сложные вычисления с матрицами. Это очень мощный инструмент, который помогает нам решать наши задачи, и, как любой другой мощный инструмент, он может быть использован как во благо, так и во вред. Важно отметить, что его можно *злоупотребить*.

## Принципы ответственного ИИ

Чтобы избежать случайного или преднамеренного злоупотребления ИИ, Microsoft определяет важные [Принципы ответственного ИИ](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-77998-cacaste). Следующие концепции лежат в основе этих принципов:

* **Справедливость** связана с важной проблемой *предвзятости моделей*, которая может возникнуть из-за использования предвзятых данных для обучения. Например, если мы пытаемся предсказать вероятность получения работы разработчика программного обеспечения для человека, модель, скорее всего, отдаст предпочтение мужчинам — просто потому, что обучающий набор данных, вероятно, был предвзят в сторону мужской аудитории. Нам нужно тщательно балансировать обучающие данные и исследовать модель, чтобы избежать предвзятости и убедиться, что модель учитывает более релевантные характеристики.
* **Надежность и безопасность**. По своей природе модели ИИ могут допускать ошибки. Нейронная сеть возвращает вероятности, и мы должны учитывать это при принятии решений. У каждой модели есть определенная точность и полнота, и мы должны понимать это, чтобы предотвратить вред, который может причинить неправильный совет.
* **Конфиденциальность и безопасность** имеют некоторые специфические аспекты, связанные с ИИ. Например, когда мы используем данные для обучения модели, эти данные каким-то образом становятся "интегрированными" в модель. С одной стороны, это повышает безопасность и конфиденциальность, с другой — мы должны помнить, какие данные использовались для обучения модели.
* **Инклюзивность** означает, что мы не создаем ИИ для замены людей, а скорее для их дополнения и повышения творческого потенциала. Это также связано со справедливостью, потому что при работе с недостаточно представленными сообществами большинство собираемых нами наборов данных, скорее всего, будут предвзяты, и мы должны убедиться, что эти сообщества включены и корректно обработаны ИИ.
* **Прозрачность**. Это включает в себя обеспечение того, чтобы мы всегда ясно давали понять, что используется ИИ. Также, где это возможно, мы хотим использовать системы ИИ, которые *интерпретируемы*.
* **Ответственность**. Когда модели ИИ принимают какие-то решения, не всегда ясно, кто несет ответственность за эти решения. Мы должны убедиться, что понимаем, где лежит ответственность за решения ИИ. В большинстве случаев мы хотели бы включить людей в процесс принятия важных решений, чтобы ответственность лежала на реальных людях.

## Инструменты для ответственного ИИ

Microsoft разработала [Набор инструментов для ответственного ИИ](https://github.com/microsoft/responsible-ai-toolbox), который включает в себя набор инструментов:

* Панель интерпретируемости (InterpretML)
* Панель справедливости (FairLearn)
* Панель анализа ошибок
* Панель ответственного ИИ, которая включает:

   - EconML — инструмент для причинного анализа, который фокусируется на вопросах "что если"
   - DiCE — инструмент для контрфактического анализа, позволяющий увидеть, какие характеристики нужно изменить, чтобы повлиять на решение модели

Для получения дополнительной информации об этике ИИ, пожалуйста, посетите [этот урок](https://github.com/microsoft/ML-For-Beginners/tree/main/1-Introduction/3-fairness?WT.mc_id=academic-77998-cacaste) в учебной программе по машинному обучению, который включает задания.

## Обзор и самостоятельное изучение

Пройдите этот [учебный путь](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77998-cacaste), чтобы узнать больше об ответственном ИИ.

## [Тест после лекции](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6/)

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.