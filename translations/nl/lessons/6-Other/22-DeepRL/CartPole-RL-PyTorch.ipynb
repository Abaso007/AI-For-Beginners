{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RL om Cartpole in Balans te Houden\n",
    "\n",
    "Dit notebook maakt deel uit van het [AI for Beginners Curriculum](http://aka.ms/ai-beginners). Het is geïnspireerd door de [officiële PyTorch-tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) en [deze Cartpole PyTorch-implementatie](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "In dit voorbeeld gebruiken we RL om een model te trainen dat een paal in balans houdt op een karretje dat naar links en rechts kan bewegen op een horizontale schaal. We maken gebruik van de [OpenAI Gym](https://www.gymlibrary.ml/) omgeving om de paal te simuleren.\n",
    "\n",
    "> **Opmerking**: Je kunt de code van deze les lokaal uitvoeren (bijvoorbeeld vanuit Visual Studio Code), in welk geval de simulatie in een nieuw venster wordt geopend. Als je de code online uitvoert, moet je mogelijk enkele aanpassingen aan de code doen, zoals beschreven [hier](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "We beginnen met ervoor te zorgen dat Gym is geïnstalleerd:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we de CartPole-omgeving creëren en bekijken hoe we ermee kunnen werken. Een omgeving heeft de volgende eigenschappen:\n",
    "\n",
    "* **Actieruimte** is de verzameling van mogelijke acties die we bij elke stap van de simulatie kunnen uitvoeren  \n",
    "* **Observatieruimte** is de ruimte van waarnemingen die we kunnen doen  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we kijken hoe de simulatie werkt. De volgende lus voert de simulatie uit totdat `env.step` de beëindigingsvlag `done` retourneert. We zullen willekeurige acties kiezen met behulp van `env.action_space.sample()`, wat betekent dat het experiment waarschijnlijk heel snel zal mislukken (de CartPole-omgeving stopt wanneer de snelheid van de CartPole, zijn positie of hoek buiten bepaalde grenzen vallen).\n",
    "\n",
    "> De simulatie wordt geopend in een nieuw venster. Je kunt de code meerdere keren uitvoeren en zien hoe deze zich gedraagt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt zien dat de observaties bestaan uit 4 getallen. Deze zijn:\n",
    "- Positie van het karretje\n",
    "- Snelheid van het karretje\n",
    "- Hoek van de paal\n",
    "- Rotatiesnelheid van de paal\n",
    "\n",
    "`rew` is de beloning die we bij elke stap ontvangen. In de CartPole-omgeving krijg je 1 punt beloning voor elke simulatiestap, en het doel is om de totale beloning te maximaliseren, oftewel de tijd dat de CartPole in balans blijft zonder om te vallen.\n",
    "\n",
    "Tijdens reinforcement learning is ons doel om een **beleid** $\\pi$ te trainen, dat ons voor elke toestand $s$ vertelt welke actie $a$ we moeten nemen, dus in essentie $a = \\pi(s)$.\n",
    "\n",
    "Als je een probabilistische oplossing wilt, kun je het beleid zien als het teruggeven van een set kansen voor elke actie, oftewel $\\pi(a|s)$ zou betekenen de kans dat we actie $a$ moeten nemen in toestand $s$.\n",
    "\n",
    "## Policy Gradient Methode\n",
    "\n",
    "In het eenvoudigste RL-algoritme, genaamd **Policy Gradient**, trainen we een neuraal netwerk om de volgende actie te voorspellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zullen het netwerk trainen door veel experimenten uit te voeren en ons netwerk na elke run bij te werken. Laten we een functie definiëren die het experiment uitvoert en de resultaten retourneert (de zogenaamde **trace**) - alle toestanden, acties (en hun aanbevolen waarschijnlijkheden) en beloningen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt één episode uitvoeren met een niet-getraind netwerk en observeren dat de totale beloning (oftewel de lengte van de episode) erg laag is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een van de lastige aspecten van het policy gradient-algoritme is het gebruik van **gedisconteerde beloningen**. Het idee is dat we de vector van totale beloningen berekenen bij elke stap van het spel, en tijdens dit proces disconteren we de vroege beloningen met een bepaalde coëfficiënt $gamma$. We normaliseren ook de resulterende vector, omdat we deze zullen gebruiken als gewicht om onze training te beïnvloeden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we echt trainen! We zullen 300 episodes uitvoeren, en bij elke episode doen we het volgende:\n",
    "\n",
    "1. Voer het experiment uit en verzamel de trace.\n",
    "2. Bereken het verschil (`gradients`) tussen de genomen acties en de voorspelde waarschijnlijkheden. Hoe kleiner het verschil, hoe zekerder we zijn dat we de juiste actie hebben genomen.\n",
    "3. Bereken de verdisconteerde beloningen en vermenigvuldig de gradients met de verdisconteerde beloningen - dit zorgt ervoor dat stappen met hogere beloningen meer invloed hebben op het eindresultaat dan stappen met lagere beloningen.\n",
    "4. De verwachte doelacties voor ons neuraal netwerk worden deels gehaald uit de voorspelde waarschijnlijkheden tijdens de uitvoering, en deels uit de berekende gradients. We gebruiken de parameter `alpha` om te bepalen in welke mate gradients en beloningen worden meegewogen - dit wordt de *leersnelheid* van het versterkingsalgoritme genoemd.\n",
    "5. Tot slot trainen we ons netwerk op basis van de toestanden en verwachte acties, en herhalen we het proces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we nu de aflevering uitvoeren met rendering om het resultaat te zien:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopelijk kun je zien dat de paal nu behoorlijk goed in balans kan blijven!\n",
    "\n",
    "## Actor-Critic Model\n",
    "\n",
    "Het Actor-Critic model is een verdere ontwikkeling van policy gradients, waarbij we een neuraal netwerk bouwen om zowel het beleid als de geschatte beloningen te leren. Het netwerk zal twee outputs hebben (of je kunt het zien als twee aparte netwerken):\n",
    "* **Actor** zal de actie aanbevelen die moet worden ondernomen door ons de waarschijnlijkheidsverdeling van de toestand te geven, zoals in het policy gradient model.\n",
    "* **Critic** zou inschatten wat de beloning zou zijn van die acties. Het geeft de totale geschatte beloningen in de toekomst terug bij de gegeven toestand.\n",
    "\n",
    "Laten we zo'n model definiëren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zouden onze functies `discounted_rewards` en `run_episode` enigszins moeten aanpassen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we de hoofdtrainingslus uitvoeren. We zullen het handmatige netwerktrainingsproces gebruiken door de juiste verliesfuncties te berekenen en de netwerkparameters bij te werken:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belangrijkste punten\n",
    "\n",
    "We hebben in deze demo twee RL-algoritmen gezien: eenvoudige policy gradient en het meer geavanceerde actor-critic. Je kunt zien dat deze algoritmen werken met abstracte begrippen zoals toestand, actie en beloning - hierdoor kunnen ze worden toegepast op zeer verschillende omgevingen.\n",
    "\n",
    "Reinforcement learning stelt ons in staat om de beste strategie te leren om een probleem op te lossen, enkel door naar de uiteindelijke beloning te kijken. Het feit dat we geen gelabelde datasets nodig hebben, stelt ons in staat om simulaties meerdere keren te herhalen om onze modellen te optimaliseren. Er zijn echter nog steeds veel uitdagingen in RL, die je kunt ontdekken als je besluit je meer te verdiepen in dit interessante gebied van AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nDit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we ons best doen voor nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T20:20:05+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}