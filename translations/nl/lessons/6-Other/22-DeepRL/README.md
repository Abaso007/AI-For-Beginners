<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-28T19:17:46+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "nl"
}
-->
# Deep Reinforcement Learning

Reinforcement learning (RL) wordt gezien als een van de fundamentele machine learning paradigma's, naast supervised learning en unsupervised learning. Terwijl we bij supervised learning vertrouwen op een dataset met bekende uitkomsten, is RL gebaseerd op **leren door te doen**. Bijvoorbeeld, wanneer we voor het eerst een computerspel zien, beginnen we te spelen, zelfs zonder de regels te kennen, en al snel verbeteren we onze vaardigheden simpelweg door te spelen en ons gedrag aan te passen.

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Om RL uit te voeren, hebben we nodig:

* Een **omgeving** of **simulator** die de regels van het spel bepaalt. We moeten experimenten kunnen uitvoeren in de simulator en de resultaten kunnen observeren.
* Een **beloningsfunctie** die aangeeft hoe succesvol ons experiment was. In het geval van leren om een computerspel te spelen, zou de beloning onze eindscore zijn.

Op basis van de beloningsfunctie moeten we ons gedrag kunnen aanpassen en onze vaardigheden verbeteren, zodat we de volgende keer beter spelen. Het belangrijkste verschil tussen andere soorten machine learning en RL is dat we bij RL meestal pas weten of we winnen of verliezen als we het spel hebben voltooid. We kunnen dus niet zeggen of een bepaalde zet op zichzelf goed is of niet - we ontvangen pas een beloning aan het einde van het spel.

Tijdens RL voeren we meestal veel experimenten uit. Bij elk experiment moeten we een balans vinden tussen het volgen van de optimale strategie die we tot nu toe hebben geleerd (**exploitatie**) en het verkennen van nieuwe mogelijke staten (**exploratie**).

## OpenAI Gym

Een geweldig hulpmiddel voor RL is de [OpenAI Gym](https://gym.openai.com/) - een **simulatieomgeving** die veel verschillende omgevingen kan simuleren, van Atari-spellen tot de fysica achter het balanceren van een paal. Het is een van de meest populaire simulatieomgevingen voor het trainen van reinforcement learning-algoritmen en wordt onderhouden door [OpenAI](https://openai.com/).

> **Note**: Je kunt alle beschikbare omgevingen van OpenAI Gym [hier](https://gym.openai.com/envs/#classic_control) bekijken.

## CartPole Balanceren

We hebben allemaal moderne balancerende apparaten gezien, zoals de *Segway* of *Gyroscooters*. Ze kunnen automatisch balanceren door hun wielen aan te passen op basis van een signaal van een versnellingsmeter of gyroscoop. In deze sectie leren we hoe we een vergelijkbaar probleem kunnen oplossen: het balanceren van een paal. Het lijkt op een situatie waarin een circusartiest een paal op zijn hand moet balanceren - maar dit paalbalanceren gebeurt alleen in 1D.

Een vereenvoudigde versie van balanceren staat bekend als het **CartPole**-probleem. In de CartPole-wereld hebben we een horizontale slider die naar links of rechts kan bewegen, en het doel is om een verticale paal bovenop de slider te balanceren terwijl deze beweegt.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Om deze omgeving te cre√´ren en te gebruiken, hebben we een paar regels Python-code nodig:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Elke omgeving kan op exact dezelfde manier worden benaderd:
* `env.reset` start een nieuw experiment
* `env.step` voert een simulatiestap uit. Het ontvangt een **actie** uit de **actie-ruimte** en retourneert een **observatie** (uit de observatieruimte), evenals een beloning en een be√´indigingsvlag.

In het bovenstaande voorbeeld voeren we bij elke stap een willekeurige actie uit, waardoor de levensduur van het experiment erg kort is:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Het doel van een RL-algoritme is om een model te trainen - het zogenaamde **beleid** œÄ - dat de actie retourneert in reactie op een gegeven staat. We kunnen beleid ook als probabilistisch beschouwen, bijvoorbeeld voor elke staat *s* en actie *a* retourneert het de waarschijnlijkheid œÄ(*a*|*s*) dat we *a* moeten nemen in staat *s*.

## Policy Gradients Algoritme

De meest voor de hand liggende manier om een beleid te modelleren is door een neuraal netwerk te maken dat staten als invoer neemt en bijbehorende acties retourneert (of liever de waarschijnlijkheden van alle acties). In zekere zin zou het vergelijkbaar zijn met een normale classificatietaak, met een groot verschil - we weten van tevoren niet welke acties we bij elke stap moeten nemen.

Het idee hier is om die waarschijnlijkheden te schatten. We bouwen een vector van **cumulatieve beloningen** die onze totale beloning bij elke stap van het experiment laat zien. We passen ook **beloningskorting** toe door eerdere beloningen te vermenigvuldigen met een co√´ffici√´nt Œ≥=0.99, om de rol van eerdere beloningen te verminderen. Vervolgens versterken we die stappen langs het experimentpad die grotere beloningen opleveren.

> Leer meer over het Policy Gradient-algoritme en zie het in actie in het [voorbeeldnotebook](CartPole-RL-TF.ipynb).

## Actor-Critic Algoritme

Een verbeterde versie van de Policy Gradients-aanpak wordt **Actor-Critic** genoemd. Het belangrijkste idee hierachter is dat het neurale netwerk wordt getraind om twee dingen te retourneren:

* Het beleid, dat bepaalt welke actie moet worden ondernomen. Dit deel wordt **actor** genoemd.
* De schatting van de totale beloning die we kunnen verwachten in deze staat - dit deel wordt **critic** genoemd.

In zekere zin lijkt deze architectuur op een [GAN](../../4-ComputerVision/10-GANs/README.md), waarbij we twee netwerken hebben die tegen elkaar worden getraind. In het actor-critic model stelt de actor de actie voor die we moeten nemen, en probeert de critic kritisch te zijn en het resultaat te schatten. Ons doel is echter om die netwerken in harmonie te trainen.

Omdat we zowel de echte cumulatieve beloningen als de resultaten die door de critic worden geretourneerd tijdens het experiment kennen, is het relatief eenvoudig om een verliesfunctie te bouwen die het verschil tussen hen minimaliseert. Dat zou ons **critic loss** geven. We kunnen **actor loss** berekenen door dezelfde aanpak te gebruiken als in het policy gradient-algoritme.

Na het uitvoeren van een van deze algoritmen, kunnen we verwachten dat onze CartPole zich zo gedraagt:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Oefeningen: Policy Gradients en Actor-Critic RL

Ga verder met leren in de volgende notebooks:

* [RL in TensorFlow](CartPole-RL-TF.ipynb)
* [RL in PyTorch](CartPole-RL-PyTorch.ipynb)

## Andere RL Taken

Reinforcement Learning is tegenwoordig een snel groeiend onderzoeksgebied. Enkele interessante voorbeelden van reinforcement learning zijn:

* Een computer leren **Atari-spellen** te spelen. Het uitdagende deel van dit probleem is dat we geen eenvoudige staat hebben die wordt weergegeven als een vector, maar eerder een screenshot - en we moeten de CNN gebruiken om deze schermafbeelding om te zetten in een featurevector of om beloningsinformatie te extraheren. Atari-spellen zijn beschikbaar in de Gym.
* Een computer leren bordspellen te spelen, zoals Schaken en Go. Onlangs zijn state-of-the-art programma's zoals **Alpha Zero** vanaf nul getraind door twee agenten tegen elkaar te laten spelen en bij elke stap te verbeteren.
* In de industrie wordt RL gebruikt om controlesystemen te cre√´ren vanuit simulatie. Een service genaamd [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) is speciaal hiervoor ontworpen.

## Conclusie

We hebben nu geleerd hoe we agenten kunnen trainen om goede resultaten te behalen door hen simpelweg een beloningsfunctie te geven die de gewenste staat van het spel definieert, en door hen de mogelijkheid te geven om intelligent de zoekruimte te verkennen. We hebben met succes twee algoritmen geprobeerd en een goed resultaat behaald in een relatief korte tijd. Dit is echter slechts het begin van je reis in RL, en je zou zeker moeten overwegen om een aparte cursus te volgen als je dieper wilt graven.

## üöÄ Uitdaging

Verken de toepassingen die worden vermeld in de sectie 'Andere RL Taken' en probeer er een te implementeren!

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Review & Zelfstudie

Leer meer over klassiek reinforcement learning in ons [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Bekijk [deze geweldige video](https://www.youtube.com/watch?v=qv6UVOQ0F44) waarin wordt uitgelegd hoe een computer kan leren Super Mario te spelen.

## Opdracht: [Train een Mountain Car](lab/README.md)

Je doel tijdens deze opdracht is om een andere Gym-omgeving te trainen - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.