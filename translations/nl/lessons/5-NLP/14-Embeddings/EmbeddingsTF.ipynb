{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inbeddingen\n",
    "\n",
    "In ons vorige voorbeeld werkten we met hoog-dimensionale bag-of-words vectoren met een lengte van `vocab_size`, en we hebben expliciet laag-dimensionale positionele representatievectoren omgezet in een spaarzame one-hot representatie. Deze one-hot representatie is niet geheugen-efficiÃ«nt. Bovendien wordt elk woord onafhankelijk van de andere behandeld, waardoor one-hot gecodeerde vectoren geen semantische overeenkomsten tussen woorden uitdrukken.\n",
    "\n",
    "In deze eenheid gaan we verder met het verkennen van de **News AG** dataset. Om te beginnen, laten we de data laden en enkele definities uit de vorige eenheid ophalen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wat is een embedding?\n",
    "\n",
    "Het idee van een **embedding** is om woorden te representeren met lagere-dimensionale, dense vectoren die de semantische betekenis van het woord weerspiegelen. Later zullen we bespreken hoe je betekenisvolle woordembeddings kunt bouwen, maar voor nu kun je embeddings zien als een manier om de dimensionaliteit van een woordvector te verminderen.\n",
    "\n",
    "Een embedding-laag neemt een woord als invoer en produceert een uitvoervector met een gespecificeerde `embedding_size`. In zekere zin lijkt het erg op een `Dense`-laag, maar in plaats van een one-hot encoded vector als invoer te nemen, kan het een woordnummer verwerken.\n",
    "\n",
    "Door een embedding-laag als de eerste laag in ons netwerk te gebruiken, kunnen we overschakelen van een bag-of-words-model naar een **embedding bag**-model, waarbij we eerst elk woord in onze tekst omzetten naar de bijbehorende embedding en vervolgens een aggregatiefunctie toepassen op al deze embeddings, zoals `sum`, `average` of `max`.\n",
    "\n",
    "![Afbeelding die een embedding-classificator toont voor vijf sequentiewoorden.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.nl.png)\n",
    "\n",
    "Ons classifier-neuraal netwerk bestaat uit de volgende lagen:\n",
    "\n",
    "* `TextVectorization`-laag, die een string als invoer neemt en een tensor van tokennummers produceert. We zullen een redelijke woordenschatgrootte `vocab_size` specificeren en minder vaak gebruikte woorden negeren. De invoervorm zal 1 zijn, en de uitvoervorm zal $n$ zijn, omdat we $n$ tokens als resultaat krijgen, elk met nummers van 0 tot `vocab_size`.\n",
    "* `Embedding`-laag, die $n$ nummers neemt en elk nummer reduceert tot een dense vector van een gegeven lengte (100 in ons voorbeeld). Dus de invoertensor van vorm $n$ wordt getransformeerd naar een $n\\times 100$-tensor.\n",
    "* Aggregatielaag, die het gemiddelde van deze tensor berekent langs de eerste as, d.w.z. het zal het gemiddelde berekenen van alle $n$ invoertensors die overeenkomen met verschillende woorden. Om deze laag te implementeren, gebruiken we een `Lambda`-laag en geven we de functie door om het gemiddelde te berekenen. De uitvoer zal een vorm van 100 hebben en het zal de numerieke representatie van de hele invoersequentie zijn.\n",
    "* De laatste `Dense` lineaire classificator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de `samenvatting`-uitvoer, in de **outputvorm**-kolom, komt de eerste tensor-dimensie `None` overeen met de minibatch-grootte, en de tweede komt overeen met de lengte van de tokenreeks. Alle tokenreeksen in de minibatch hebben verschillende lengtes. We zullen in de volgende sectie bespreken hoe hiermee om te gaan.\n",
    "\n",
    "Laten we nu het netwerk trainen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Opmerking** dat we een vectorizer bouwen op basis van een subset van de gegevens. Dit wordt gedaan om het proces te versnellen, en het kan resulteren in een situatie waarin niet alle tokens uit onze tekst in de woordenschat aanwezig zijn. In dat geval worden die tokens genegeerd, wat kan leiden tot een iets lagere nauwkeurigheid. Echter, in de praktijk geeft een subset van tekst vaak een goede schatting van de woordenschat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omgaan met variabele sequentiegroottes\n",
    "\n",
    "Laten we begrijpen hoe training plaatsvindt in minibatches. In het bovenstaande voorbeeld heeft de invoertensor dimensie 1, en we gebruiken minibatches van 128, zodat de werkelijke grootte van de tensor $128 \\times 1$ is. Echter, het aantal tokens in elke zin is verschillend. Als we de `TextVectorization`-laag toepassen op een enkele invoer, is het aantal geretourneerde tokens verschillend, afhankelijk van hoe de tekst wordt getokeniseerd:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echter, wanneer we de vectorizer toepassen op meerdere sequenties, moet deze een tensor van rechthoekige vorm produceren, dus vult hij ongebruikte elementen met de PAD-token (wat in ons geval nul is):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kunnen we de embeddings zien:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Opmerking**: Om de hoeveelheid opvulling te minimaliseren, is het in sommige gevallen logisch om alle reeksen in de dataset te sorteren op toenemende lengte (of, meer precies, aantal tokens). Dit zorgt ervoor dat elke minibatch reeksen van vergelijkbare lengte bevat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantische embeddings: Word2Vec\n",
    "\n",
    "In ons vorige voorbeeld leerde de embeddinglaag om woorden naar vectorrepresentaties te mappen, maar deze representaties hadden geen semantische betekenis. Het zou handig zijn om een vectorrepresentatie te leren waarbij vergelijkbare woorden of synoniemen overeenkomen met vectoren die dicht bij elkaar liggen in termen van een bepaalde vectordistantie (bijvoorbeeld euclidische afstand).\n",
    "\n",
    "Om dat te bereiken, moeten we ons embeddingmodel vooraf trainen op een grote verzameling tekst met behulp van een techniek zoals [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Dit is gebaseerd op twee hoofdarchitecturen die worden gebruikt om een gedistribueerde representatie van woorden te produceren:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW), waarbij we het model trainen om een woord te voorspellen op basis van de omliggende context. Gegeven de ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, is het doel van het model om $W_0$ te voorspellen op basis van $(W_{-2},W_{-1},W_1,W_2)$.\n",
    " - **Continuous skip-gram** is het tegenovergestelde van CBoW. Het model gebruikt het omliggende venster van contextwoorden om het huidige woord te voorspellen.\n",
    "\n",
    "CBoW is sneller, terwijl skip-gram langzamer is, maar beter presteert bij het representeren van zeldzame woorden.\n",
    "\n",
    "![Afbeelding die zowel de CBoW- als Skip-Gram-algoritmen toont om woorden naar vectoren om te zetten.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.nl.png)\n",
    "\n",
    "Om te experimenteren met de Word2Vec-embedding die vooraf is getraind op de Google News-dataset, kunnen we de **gensim**-bibliotheek gebruiken. Hieronder vinden we de woorden die het meest lijken op 'neural'.\n",
    "\n",
    "> **Opmerking:** Wanneer je voor het eerst woordvectoren aanmaakt, kan het downloaden ervan enige tijd in beslag nemen!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen ook de vectorembedding uit het woord extraheren, om te gebruiken bij het trainen van het classificatiemodel. De embedding heeft 300 componenten, maar hier laten we alleen de eerste 20 componenten van de vector zien voor duidelijkheid:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het geweldige aan semantische embeddings is dat je de vectorcodering kunt manipuleren op basis van semantiek. Bijvoorbeeld, we kunnen vragen om een woord te vinden waarvan de vectorrepresentatie zo dicht mogelijk bij de woorden *koning* en *vrouw* ligt, en zo ver mogelijk van het woord *man*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Een voorbeeld hierboven gebruikt wat interne GenSym-magie, maar de onderliggende logica is eigenlijk vrij eenvoudig. Een interessant aspect van embeddings is dat je normale vectorbewerkingen kunt uitvoeren op embeddingvectoren, en dat zou bewerkingen op woord**betekenissen** weerspiegelen. Het bovenstaande voorbeeld kan worden uitgedrukt in termen van vectorbewerkingen: we berekenen de vector die overeenkomt met **KONING-MAN+VROUW** (bewerkingen `+` en `-` worden uitgevoerd op vectorrepresentaties van overeenkomstige woorden), en vinden vervolgens het dichtstbijzijnde woord in het woordenboek bij die vector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: We moesten een kleine coÃ«fficiÃ«nt toevoegen aan de *man*- en *vrouw*-vectoren - probeer ze te verwijderen om te zien wat er gebeurt.\n",
    "\n",
    "Om de dichtstbijzijnde vector te vinden, gebruiken we TensorFlow-mechanismen om een vector van afstanden te berekenen tussen onze vector en alle vectoren in de woordenschat, en vervolgens vinden we de index van het kleinste woord met behulp van `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoewel Word2Vec een geweldige manier lijkt om woordsemantiek uit te drukken, heeft het verschillende nadelen, waaronder de volgende:\n",
    "\n",
    "* Zowel CBoW- als skip-grammodellen zijn **predictieve embeddings** en houden alleen rekening met lokale context. Word2Vec maakt geen gebruik van globale context.\n",
    "* Word2Vec houdt geen rekening met de **morfologie** van woorden, oftewel het feit dat de betekenis van een woord kan afhangen van verschillende delen van het woord, zoals de stam.\n",
    "\n",
    "**FastText** probeert de tweede beperking te overwinnen en bouwt voort op Word2Vec door vectorrepresentaties te leren voor elk woord en de karakter-n-grams die in elk woord voorkomen. De waarden van deze representaties worden vervolgens gemiddeld tot Ã©Ã©n vector bij elke trainingsstap. Hoewel dit veel extra rekenkracht toevoegt aan de pretraining, stelt het woordembeddings in staat om subwoordinformatie te coderen.\n",
    "\n",
    "Een andere methode, **GloVe**, gebruikt een andere benadering voor woordembeddings, gebaseerd op de factorisatie van de woord-contextmatrix. Eerst bouwt het een grote matrix die het aantal woordvoorkomens in verschillende contexten telt, en vervolgens probeert het deze matrix in lagere dimensies te representeren op een manier die het reconstructieverlies minimaliseert.\n",
    "\n",
    "De gensim-bibliotheek ondersteunt deze woordembeddings, en je kunt ermee experimenteren door de model-laadcode hierboven aan te passen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gebruik van vooraf getrainde embeddings in Keras\n",
    "\n",
    "We kunnen het bovenstaande voorbeeld aanpassen om de matrix in onze embeddinglaag vooraf te vullen met semantische embeddings, zoals Word2Vec. De woordenschat van de vooraf getrainde embedding en de tekstcorpus zullen waarschijnlijk niet overeenkomen, dus we moeten er Ã©Ã©n kiezen. Hier verkennen we de twee mogelijke opties: het gebruik van de tokenizer-woordenschat en het gebruik van de woordenschat van Word2Vec-embeddings.\n",
    "\n",
    "### Gebruik van tokenizer-woordenschat\n",
    "\n",
    "Bij het gebruik van de tokenizer-woordenschat zullen sommige woorden uit de woordenschat overeenkomende Word2Vec-embeddings hebben, terwijl andere ontbreken. Aangezien onze woordenschatgrootte `vocab_size` is en de lengte van de Word2Vec embeddingvector `embed_size` is, zal de embeddinglaag worden weergegeven door een gewichts-matrix met de vorm `vocab_size`$\\times$`embed_size`. We vullen deze matrix door de woordenschat door te nemen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor woorden die niet aanwezig zijn in de Word2Vec-woordenschat, kunnen we ze ofwel als nullen laten, of een willekeurige vector genereren.\n",
    "\n",
    "Nu kunnen we een embeddinglaag definiÃ«ren met vooraf getrainde gewichten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Opmerking**: Merk op dat we `trainable=False` instellen bij het maken van de `Embedding`, wat betekent dat we de Embedding-laag niet opnieuw trainen. Dit kan ervoor zorgen dat de nauwkeurigheid iets lager is, maar het versnelt de training.\n",
    "\n",
    "### Gebruik van embedding vocabulaire\n",
    "\n",
    "Een probleem met de vorige aanpak is dat de woordenschatten die worden gebruikt in de TextVectorization en Embedding verschillend zijn. Om dit probleem op te lossen, kunnen we een van de volgende oplossingen gebruiken:\n",
    "* Het Word2Vec-model opnieuw trainen op onze woordenschat.\n",
    "* Onze dataset laden met de woordenschat van het vooraf getrainde Word2Vec-model. Woordenschatten die worden gebruikt om de dataset te laden, kunnen tijdens het laden worden gespecificeerd.\n",
    "\n",
    "De laatste aanpak lijkt eenvoudiger, dus laten we deze implementeren. Allereerst zullen we een `TextVectorization`-laag maken met de opgegeven woordenschat, afkomstig van de Word2Vec embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De gensim word embeddings-bibliotheek bevat een handige functie, `get_keras_embeddings`, die automatisch de bijbehorende Keras embeddings-laag voor je zal aanmaken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een van de redenen waarom we geen hogere nauwkeurigheid zien, is omdat sommige woorden uit onze dataset ontbreken in de voorgetrainde GloVe-woordenschat en daardoor in feite worden genegeerd. Om dit te verhelpen, kunnen we onze eigen embeddings trainen op basis van onze dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextuele embeddings\n",
    "\n",
    "Een belangrijke beperking van traditionele vooraf getrainde embedding-representaties zoals Word2Vec is dat, hoewel ze enige betekenis van een woord kunnen vastleggen, ze geen onderscheid kunnen maken tussen verschillende betekenissen. Dit kan problemen veroorzaken in modellen die hierop voortbouwen.\n",
    "\n",
    "Bijvoorbeeld, het woord 'play' heeft verschillende betekenissen in deze twee zinnen:\n",
    "- Ik ging naar een **toneelstuk** in het theater.\n",
    "- John wil **spelen** met zijn vrienden.\n",
    "\n",
    "De vooraf getrainde embeddings waar we het over hadden, vertegenwoordigen beide betekenissen van het woord 'play' in dezelfde embedding. Om deze beperking te overwinnen, moeten we embeddings bouwen op basis van het **taalmodel**, dat is getraind op een grote hoeveelheid tekst en *weet* hoe woorden in verschillende contexten samen kunnen worden gebruikt. Het bespreken van contextuele embeddings valt buiten de scope van deze tutorial, maar we komen hierop terug wanneer we taalmodellen bespreken in de volgende eenheid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nDit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T21:53:08+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}