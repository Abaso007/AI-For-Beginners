<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0c37770bba4fff3c71dc00eb261ee61b",
  "translation_date": "2025-08-28T19:48:25+00:00",
  "source_file": "lessons/3-NeuralNetworks/03-Perceptron/README.md",
  "language_code": "nl"
}
-->
# Introductie tot Neurale Netwerken: Perceptron

## [Quiz voor de les](https://ff-quizzes.netlify.app/en/ai/quiz/5)

Een van de eerste pogingen om iets te implementeren dat lijkt op een modern neuraal netwerk werd gedaan door Frank Rosenblatt van het Cornell Aeronautical Laboratory in 1957. Het was een hardware-implementatie genaamd "Mark-1", ontworpen om primitieve geometrische figuren te herkennen, zoals driehoeken, vierkanten en cirkels.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='De Mark 1 Perceptron' />|

> Afbeeldingen [van Wikipedia](https://en.wikipedia.org/wiki/Perceptron)

Een invoerafbeelding werd weergegeven door een 20x20 fotocel-array, waardoor het neurale netwerk 400 inputs en Ã©Ã©n binaire output had. Een eenvoudig netwerk bevatte Ã©Ã©n neuron, ook wel een **threshold logic unit** genoemd. De gewichten van het neurale netwerk fungeerden als potentiometers die handmatig moesten worden aangepast tijdens de trainingsfase.

> âœ… Een potentiometer is een apparaat waarmee de gebruiker de weerstand van een circuit kan aanpassen.

> De New York Times schreef destijds over de perceptron: *het embryo van een elektronische computer waarvan [de marine] verwacht dat het zal kunnen lopen, praten, zien, schrijven, zichzelf reproduceren en zich bewust zijn van zijn bestaan.*

## Perceptron Model

Stel dat we N kenmerken hebben in ons model, in welk geval de invoervector een vector van grootte N zou zijn. Een perceptron is een **binaire classificatie**-model, wat betekent dat het onderscheid kan maken tussen twee klassen van invoergegevens. We gaan ervan uit dat voor elke invoervector x de output van ons perceptron +1 of -1 zal zijn, afhankelijk van de klasse. De output wordt berekend met de formule:

y(x) = f(w<sup>T</sup>x)

waarbij f een stap-activatiefunctie is

<!-- img src="http://www.sciweavers.org/tex2img.php?eq=f%28x%29%20%3D%20%5Cbegin%7Bcases%7D%0A%20%20%20%20%20%20%20%20%20%2B1%20%26%20x%20%5Cgeq%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20-1%20%26%20x%20%3C%200%0A%20%20%20%20%20%20%20%5Cend%7Bcases%7D%20%5C%5C%0A&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0" align="center" border="0" alt="f(x) = \begin{cases} +1 & x \geq 0 \\ -1 & x < 0 \end{cases} \\" width="154" height="50" / -->
<img src="images/activation-func.png"/>

## Het trainen van de Perceptron

Om een perceptron te trainen, moeten we een gewichtsvector w vinden die de meeste waarden correct classificeert, oftewel resulteert in de kleinste **fout**. Deze fout E wordt gedefinieerd door het **perceptroncriterium** op de volgende manier:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

waarbij:

* de som wordt genomen over die trainingsdatapunten i die resulteren in een verkeerde classificatie
* x<sub>i</sub> de invoergegevens zijn, en t<sub>i</sub> -1 of +1 is voor respectievelijk negatieve en positieve voorbeelden.

Dit criterium wordt beschouwd als een functie van de gewichten w, en we moeten het minimaliseren. Vaak wordt een methode genaamd **gradient descent** gebruikt, waarbij we beginnen met enkele initiÃ«le gewichten w<sup>(0)</sup>, en vervolgens bij elke stap de gewichten bijwerken volgens de formule:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Hierbij is Î· de zogenaamde **leersnelheid**, en âˆ‡E(w) de **gradiÃ«nt** van E. Nadat we de gradiÃ«nt hebben berekend, krijgen we:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Het algoritme in Python ziet er als volgt uit:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Conclusie

In deze les heb je geleerd over een perceptron, een binaire classificatie-model, en hoe je het kunt trainen door gebruik te maken van een gewichtsvector.

## ðŸš€ Uitdaging

Als je zelf een perceptron wilt bouwen, probeer dan [deze lab op Microsoft Learn](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/two-class-averaged-perceptron?WT.mc_id=academic-77998-cacaste) die gebruik maakt van de [Azure ML designer](https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer?WT.mc_id=academic-77998-cacaste).

## [Quiz na de les](https://ff-quizzes.netlify.app/en/ai/quiz/6)

## Review & Zelfstudie

Om te zien hoe we een perceptron kunnen gebruiken om een eenvoudig probleem en echte problemen op te lossen, en om verder te leren - ga naar [Perceptron](Perceptron.ipynb) notebook.

Hier is een interessant [artikel over perceptrons](https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590).

## [Opdracht](lab/README.md)

In deze les hebben we een perceptron geÃ¯mplementeerd voor een binaire classificatietaak, en we hebben het gebruikt om onderscheid te maken tussen twee handgeschreven cijfers. In deze lab wordt je gevraagd om het probleem van cijferclassificatie volledig op te lossen, oftewel te bepalen welk cijfer het meest waarschijnlijk overeenkomt met een gegeven afbeelding.

* [Instructies](lab/README.md)
* [Notebook](lab/PerceptronMultiClass.ipynb)

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.