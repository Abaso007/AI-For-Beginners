<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "437c988596e751072e41a5aad3fcc5d9",
  "translation_date": "2025-08-25T21:26:29+00:00",
  "source_file": "lessons/7-Ethics/README.md",
  "language_code": "uk"
}
-->
# Етичний та відповідальний штучний інтелект

Ви майже завершили цей курс, і я сподіваюся, що тепер ви чітко розумієте, що штучний інтелект базується на ряді формальних математичних методів, які дозволяють знаходити взаємозв’язки в даних і навчати моделі відтворювати деякі аспекти людської поведінки. На цьому етапі історії ми вважаємо штучний інтелект дуже потужним інструментом для виявлення закономірностей у даних і застосування цих закономірностей для вирішення нових завдань.

## [Тест перед лекцією](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5/)

Однак у науковій фантастиці ми часто бачимо історії, де штучний інтелект становить загрозу для людства. Зазвичай ці історії зосереджені навколо якогось виду бунту штучного інтелекту, коли він вирішує протистояти людям. Це передбачає, що штучний інтелект має певні емоції або може приймати рішення, які не передбачені його розробниками.

Штучний інтелект, про який ми дізналися в цьому курсі, — це не більше ніж велика матрична арифметика. Це дуже потужний інструмент, який допомагає нам вирішувати наші проблеми, і, як будь-який інший потужний інструмент, його можна використовувати як на благо, так і на шкоду. Важливо, що його можна *зловживати*.

## Принципи відповідального штучного інтелекту

Щоб уникнути випадкового або навмисного зловживання штучним інтелектом, Microsoft визначає важливі [Принципи відповідального штучного інтелекту](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-77998-cacaste). Ці принципи базуються на таких концепціях:

* **Справедливість** стосується важливої проблеми *упередженості моделей*, яка може виникнути через використання упереджених даних для навчання. Наприклад, коли ми намагаємося передбачити ймовірність отримання роботи розробника програмного забезпечення для певної людини, модель, ймовірно, надасть перевагу чоловікам — просто тому, що навчальний набір даних, ймовірно, був упередженим на користь чоловічої аудиторії. Ми повинні ретельно збалансовувати навчальні дані та досліджувати модель, щоб уникнути упередженості та переконатися, що модель враховує більш релевантні характеристики.
* **Надійність і безпека**. За своєю природою моделі штучного інтелекту можуть робити помилки. Нейронна мережа повертає ймовірності, і ми повинні враховувати це під час прийняття рішень. Кожна модель має певну точність і повноту, і ми повинні це розуміти, щоб запобігти шкоді, яку можуть спричинити неправильні рекомендації.
* **Конфіденційність і безпека** мають певні специфічні аспекти для штучного інтелекту. Наприклад, коли ми використовуємо певні дані для навчання моделі, ці дані певним чином стають "інтегрованими" в модель. З одного боку, це підвищує безпеку та конфіденційність, з іншого — ми повинні пам’ятати, на яких даних була навчена модель.
* **Інклюзивність** означає, що ми створюємо штучний інтелект не для того, щоб замінити людей, а щоб доповнити їх і зробити нашу роботу більш творчою. Це також пов’язано зі справедливістю, оскільки при роботі з недостатньо представленими спільнотами більшість зібраних нами наборів даних, ймовірно, будуть упередженими, і ми повинні переконатися, що ці спільноти враховані та правильно оброблені штучним інтелектом.
* **Прозорість**. Це включає в себе забезпечення того, щоб ми завжди чітко вказували на використання штучного інтелекту. Також, де це можливо, ми хочемо використовувати системи штучного інтелекту, які є *інтерпретованими*.
* **Відповідальність**. Коли моделі штучного інтелекту приймають певні рішення, не завжди зрозуміло, хто несе відповідальність за ці рішення. Ми повинні переконатися, що розуміємо, де лежить відповідальність за рішення штучного інтелекту. У більшості випадків ми хотіли б включити людей у процес прийняття важливих рішень, щоб відповідальність несли реальні люди.

## Інструменти для відповідального штучного інтелекту

Microsoft розробила [Набір інструментів для відповідального штучного інтелекту](https://github.com/microsoft/responsible-ai-toolbox), який містить такі інструменти:

* Панель інтерпретації (InterpretML)
* Панель справедливості (FairLearn)
* Панель аналізу помилок
* Панель відповідального штучного інтелекту, яка включає:

   - EconML — інструмент для причинно-наслідкового аналізу, який зосереджується на питаннях "що, якщо"
   - DiCE — інструмент для контрфактичного аналізу, який дозволяє побачити, які характеристики потрібно змінити, щоб вплинути на рішення моделі

Для отримання додаткової інформації про етику штучного інтелекту, відвідайте [цей урок](https://github.com/microsoft/ML-For-Beginners/tree/main/1-Introduction/3-fairness?WT.mc_id=academic-77998-cacaste) у навчальній програмі з машинного навчання, який включає завдання.

## Огляд і самостійне навчання

Пройдіть цей [навчальний шлях](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77998-cacaste), щоб дізнатися більше про відповідальний штучний інтелект.

## [Тест після лекції](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6/)

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, звертаємо вашу увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.