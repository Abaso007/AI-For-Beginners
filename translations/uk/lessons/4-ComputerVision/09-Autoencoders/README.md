<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-25T22:32:25+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "uk"
}
-->
# Автоенкодери

Під час навчання CNN однією з проблем є те, що нам потрібно багато даних із мітками. У випадку класифікації зображень нам потрібно розділити зображення на різні класи, що є ручною роботою.

## [Тест перед лекцією](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Однак ми можемо захотіти використовувати необроблені (без міток) дані для навчання екстракторів ознак CNN, що називається **самонавчанням**. Замість міток ми будемо використовувати навчальні зображення як вхідні та вихідні дані для мережі. Основна ідея **автоенкодера** полягає в тому, що ми матимемо **мережу енкодера**, яка перетворює вхідне зображення в деякий **латентний простір** (зазвичай це просто вектор меншого розміру), а потім **мережу декодера**, метою якої буде реконструкція оригінального зображення.

> ✅ [Автоенкодер](https://wikipedia.org/wiki/Autoencoder) — це "тип штучної нейронної мережі, яка використовується для навчання ефективного кодування даних без міток".

Оскільки ми навчаємо автоенкодер захоплювати якомога більше інформації з оригінального зображення для точної реконструкції, мережа намагається знайти найкраще **вбудовування** вхідних зображень, щоб передати їх зміст.

![Схема автоенкодера](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.uk.jpg)

> Зображення з [блогу Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Сценарії використання автоенкодерів

Хоча реконструкція оригінальних зображень сама по собі може здатися не дуже корисною, є кілька сценаріїв, де автоенкодери особливо корисні:

* **Зниження розмірності зображень для візуалізації** або **навчання вбудовувань зображень**. Зазвичай автоенкодери дають кращі результати, ніж PCA, оскільки враховують просторову природу зображень і ієрархічні ознаки.
* **Шумозаглушення**, тобто видалення шуму із зображення. Оскільки шум містить багато непотрібної інформації, автоенкодер не може вмістити все це в порівняно невеликий латентний простір, і тому він захоплює лише важливу частину зображення. Під час навчання шумозаглушувачів ми починаємо з оригінальних зображень і використовуємо зображення з штучно доданим шумом як вхідні дані для автоенкодера.
* **Суперроздільна здатність**, збільшення роздільної здатності зображення. Ми починаємо з зображень високої роздільної здатності і використовуємо зображення з нижчою роздільною здатністю як вхідні дані для автоенкодера.
* **Генеративні моделі**. Після навчання автоенкодера частину декодера можна використовувати для створення нових об'єктів, починаючи з випадкових латентних векторів.

## Варіаційні автоенкодери (VAE)

Традиційні автоенкодери зменшують розмірність вхідних даних, визначаючи важливі ознаки вхідних зображень. Однак латентні вектори часто не мають особливого сенсу. Іншими словами, якщо взяти набір даних MNIST як приклад, визначити, які цифри відповідають різним латентним вектором, не так просто, оскільки близькі латентні вектори не обов’язково відповідають однаковим цифрам.

З іншого боку, для навчання *генеративних* моделей краще мати певне розуміння латентного простору. Ця ідея приводить нас до **варіаційного автоенкодера** (VAE).

VAE — це автоенкодер, який навчається прогнозувати *статистичний розподіл* латентних параметрів, так званий **латентний розподіл**. Наприклад, ми можемо захотіти, щоб латентні вектори розподілялися нормально з певним середнім z<sub>mean</sub> і стандартним відхиленням z<sub>sigma</sub> (і середнє, і стандартне відхилення є векторами певної розмірності d). Енкодер у VAE навчається прогнозувати ці параметри, а декодер бере випадковий вектор із цього розподілу для реконструкції об’єкта.

Підсумуємо:

 * З вхідного вектора ми прогнозуємо `z_mean` і `z_log_sigma` (замість прогнозування самого стандартного відхилення ми прогнозуємо його логарифм)
 * Ми вибираємо вектор `sample` із розподілу N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Декодер намагається декодувати оригінальне зображення, використовуючи `sample` як вхідний вектор

 <img src="images/vae.png" width="50%">

> Зображення з [цього блогу](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) Ісаака Дайкмана

Варіаційні автоенкодери використовують складну функцію втрат, яка складається з двох частин:

* **Втрати реконструкції** — це функція втрат, яка показує, наскільки реконструйоване зображення близьке до цільового (це може бути середньоквадратична помилка, або MSE). Це та сама функція втрат, що й у звичайних автоенкодерах.
* **KL втрати**, які забезпечують, щоб розподіл латентних змінних залишався близьким до нормального розподілу. Вона базується на понятті [дивергенції Кульбака-Лейблера](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) — метриці для оцінки схожості двох статистичних розподілів.

Однією з важливих переваг VAE є те, що вони дозволяють нам генерувати нові зображення відносно легко, оскільки ми знаємо, з якого розподілу вибирати латентні вектори. Наприклад, якщо ми навчимо VAE з 2D латентним вектором на MNIST, ми зможемо змінювати компоненти латентного вектора, щоб отримати різні цифри:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Зображення Дмитра Сошникова [Dmitry Soshnikov](http://soshnikov.com)

Зверніть увагу, як зображення плавно переходять одне в одне, коли ми починаємо отримувати латентні вектори з різних частин латентного простору параметрів. Ми також можемо візуалізувати цей простір у 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Зображення Дмитра Сошникова [Dmitry Soshnikov](http://soshnikov.com)

## ✍️ Вправи: Автоенкодери

Дізнайтеся більше про автоенкодери в цих відповідних ноутбуках:

* [Автоенкодери в TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [Автоенкодери в PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Властивості автоенкодерів

* **Специфічність даних** - вони добре працюють лише з типом зображень, на яких були навчені. Наприклад, якщо ми навчимо мережу суперроздільної здатності на квітах, вона не буде добре працювати на портретах. Це тому, що мережа може створювати зображення з вищою роздільною здатністю, використовуючи тонкі деталі, отримані з ознак навчального набору даних.
* **Втрати** - реконструйоване зображення не є таким самим, як оригінальне. Характер втрат визначається *функцією втрат*, яка використовується під час навчання.
* Працює з **даними без міток**

## [Тест після лекції](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## Висновок

У цьому уроці ви дізналися про різні типи автоенкодерів, доступних для науковця зі штучного інтелекту. Ви дізналися, як їх створювати та використовувати для реконструкції зображень. Ви також дізналися про VAE і як використовувати його для генерації нових зображень.

## 🚀 Виклик

У цьому уроці ви дізналися про використання автоенкодерів для зображень. Але їх також можна використовувати для музики! Ознайомтеся з проектом Magenta [MusicVAE](https://magenta.tensorflow.org/music-vae), який використовує автоенкодери для навчання реконструкції музики. Проведіть кілька [експериментів](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) з цією бібліотекою, щоб побачити, що ви можете створити.

## [Тест після лекції](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## Огляд і самостійне навчання

Для довідки прочитайте більше про автоенкодери в цих ресурсах:

* [Створення автоенкодерів у Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Блог NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Пояснення варіаційних автоенкодерів](https://kvfrans.com/variational-autoencoders-explained/)
* [Умовні варіаційні автоенкодери](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Завдання

Наприкінці [цього ноутбука з TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb) ви знайдете "завдання" - використовуйте це як своє домашнє завдання.

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, звертаємо вашу увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.