{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Навчання RL для балансування маятника на візку\n",
    "\n",
    "Цей блокнот є частиною [навчальної програми \"AI для початківців\"](http://aka.ms/ai-beginners). Він був натхненний [офіційним підручником PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) та [цією реалізацією Cartpole на PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "У цьому прикладі ми використаємо RL для навчання моделі балансувати маятник на візку, який може рухатися вліво та вправо по горизонтальній шкалі. Ми будемо використовувати середовище [OpenAI Gym](https://www.gymlibrary.ml/) для симуляції маятника.\n",
    "\n",
    "> **Note**: Ви можете запускати код цього уроку локально (наприклад, у Visual Studio Code), у такому випадку симуляція відкриється в новому вікні. Якщо ви запускаєте код онлайн, можливо, вам доведеться внести деякі зміни до коду, як описано [тут](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Ми почнемо з перевірки, чи встановлений Gym:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер створимо середовище CartPole і подивимося, як з ним працювати. Середовище має такі властивості:\n",
    "\n",
    "* **Action space** — набір можливих дій, які ми можемо виконувати на кожному кроці симуляції\n",
    "* **Observation space** — простір спостережень, які ми можемо отримати\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте подивимося, як працює симуляція. Наступний цикл запускає симуляцію, поки `env.step` не поверне прапорець завершення `done`. Ми будемо випадково вибирати дії за допомогою `env.action_space.sample()`, що означає, що експеримент, ймовірно, дуже швидко завершиться невдачею (середовище CartPole завершується, коли швидкість CartPole, його положення або кут виходять за певні межі).\n",
    "\n",
    "> Симуляція відкриється в новому вікні. Ви можете запустити код кілька разів і побачити, як він поводиться.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ви можете помітити, що спостереження містять 4 числа. Це:\n",
    "- Позиція візка\n",
    "- Швидкість візка\n",
    "- Кут нахилу стрижня\n",
    "- Швидкість обертання стрижня\n",
    "\n",
    "`rew` — це винагорода, яку ми отримуємо на кожному кроці. Ви можете побачити, що в середовищі CartPole ви отримуєте 1 бал за кожен крок симуляції, і мета полягає в тому, щоб максимізувати загальну винагороду, тобто час, протягом якого CartPole може балансувати, не падаючи.\n",
    "\n",
    "Під час навчання з підкріпленням наша мета — навчити **політику** $\\pi$, яка для кожного стану $s$ буде вказувати, яку дію $a$ потрібно виконати, тобто, по суті, $a = \\pi(s)$.\n",
    "\n",
    "Якщо ви хочете ймовірнісне рішення, ви можете уявити політику як таку, що повертає набір ймовірностей для кожної дії, тобто $\\pi(a|s)$ означатиме ймовірність того, що ми повинні виконати дію $a$ у стані $s$.\n",
    "\n",
    "## Метод градієнта політики\n",
    "\n",
    "У найпростішому алгоритмі навчання з підкріпленням, який називається **Градієнт політики**, ми будемо навчати нейронну мережу передбачати наступну дію.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми будемо тренувати мережу, проводячи багато експериментів і оновлюючи нашу мережу після кожного запуску. Давайте визначимо функцію, яка буде проводити експеримент і повертати результати (так званий **трейс**) - всі стани, дії (і їх рекомендовані ймовірності) та винагороди:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ви можете запустити один епізод з ненавченим мережевим і спостерігати, що загальна винагорода (тобто тривалість епізоду) дуже низька:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один із складних аспектів алгоритму градієнтної політики — використання **знижених винагород**. Ідея полягає в тому, що ми обчислюємо вектор загальних винагород на кожному кроці гри, і під час цього процесу знижуємо ранні винагороди, використовуючи деякий коефіцієнт $gamma$. Ми також нормалізуємо отриманий вектор, оскільки будемо використовувати його як вагу для впливу на наше навчання:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер почнемо справжнє навчання! Ми проведемо 300 епізодів, і в кожному епізоді виконаємо наступне:\n",
    "\n",
    "1. Запустимо експеримент і зберемо трасу.\n",
    "2. Обчислимо різницю (`gradients`) між виконаними діями та передбаченими ймовірностями. Чим менша різниця, тим більше ми впевнені, що обрали правильну дію.\n",
    "3. Розрахуємо дисконтовані винагороди та помножимо градієнти на дисконтовані винагороди — це забезпечить, що кроки з вищими винагородами матимуть більший вплив на кінцевий результат, ніж ті, що мають нижчі винагороди.\n",
    "4. Очікувані цільові дії для нашої нейронної мережі частково будуть взяті з передбачених ймовірностей під час виконання, а частково — з розрахованих градієнтів. Ми використаємо параметр `alpha`, щоб визначити, наскільки враховуються градієнти та винагороди — це називається *швидкістю навчання* алгоритму підкріплення.\n",
    "5. Нарешті, ми тренуємо нашу мережу на станах і очікуваних діях, і повторюємо процес.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер давайте запустимо епізод із рендерингом, щоб побачити результат:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сподіваємося, тепер ви бачите, що жердина може досить добре балансувати!\n",
    "\n",
    "## Модель Actor-Critic\n",
    "\n",
    "Модель Actor-Critic є подальшим розвитком градієнтів політики, у якій ми створюємо нейронну мережу для навчання як політики, так і оцінених винагород. Мережа матиме два виходи (або можна розглядати це як дві окремі мережі):\n",
    "* **Actor** рекомендуватиме дію, яку слід виконати, надаючи нам розподіл ймовірностей стану, як у моделі градієнтів політики.\n",
    "* **Critic** оцінюватиме, якою буде винагорода від цих дій. Він повертає загальну оцінену винагороду в майбутньому для даного стану.\n",
    "\n",
    "Давайте визначимо таку модель:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам потрібно трохи змінити наші функції `discounted_rewards` та `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми запустимо основний цикл навчання. Ми будемо використовувати процес ручного навчання мережі, обчислюючи відповідні функції втрат і оновлюючи параметри мережі:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основні висновки\n",
    "\n",
    "У цьому демонстраційному прикладі ми розглянули два алгоритми навчання з підкріпленням: простий градієнт політики та більш складний actor-critic. Ви могли помітити, що ці алгоритми працюють з абстрактними поняттями стану, дії та винагороди – тому їх можна застосовувати до дуже різних середовищ.\n",
    "\n",
    "Навчання з підкріпленням дозволяє нам знаходити найкращу стратегію для розв'язання задачі, орієнтуючись лише на кінцеву винагороду. Те, що нам не потрібні розмічені набори даних, дає змогу багаторазово повторювати симуляції для оптимізації наших моделей. Однак у навчанні з підкріпленням все ще існує багато викликів, які ви зможете вивчити, якщо вирішите глибше зануритися в цю захопливу галузь штучного інтелекту.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Відмова від відповідальності**:  \nЦей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-30T08:52:58+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "uk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}