<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7bd8dc72040e98e35e7225e34058cd4e",
  "translation_date": "2025-08-25T23:36:14+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/lab/README.md",
  "language_code": "uk"
}
-->
## Навколишнє середовище

Середовище Mountain Car складається з автомобіля, який застряг у долині. Ваша мета — вистрибнути з долини та досягти прапора. Дії, які ви можете виконувати: прискорення вліво, вправо або нічого не робити. Ви можете спостерігати за положенням автомобіля вздовж осі x та його швидкістю.

## Початковий ноутбук

Розпочніть лабораторну роботу, відкривши [MountainCar.ipynb](../../../../../../lessons/6-Other/22-DeepRL/lab/MountainCar.ipynb)

## Основна ідея

Протягом цієї лабораторної роботи ви маєте зрозуміти, що адаптація алгоритмів підкріплювального навчання (RL) до нового середовища зазвичай є досить простою, оскільки OpenAI Gym має однаковий інтерфейс для всіх середовищ, а алгоритми в цілому не сильно залежать від природи середовища. Ви навіть можете реорганізувати Python-код таким чином, щоб передавати будь-яке середовище до алгоритму RL як параметр.

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, зверніть увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.