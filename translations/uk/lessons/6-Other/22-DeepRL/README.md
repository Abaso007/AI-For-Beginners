<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-25T23:33:58+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "uk"
}
-->
# Глибоке навчання з підкріпленням

Навчання з підкріпленням (RL) вважається одним із базових парадигм машинного навчання поряд із навчанням з учителем та без учителя. У той час як у навчанні з учителем ми покладаємося на набір даних із відомими результатами, RL базується на **навчанні через дію**. Наприклад, коли ми вперше бачимо комп'ютерну гру, ми починаємо грати, навіть не знаючи правил, і незабаром покращуємо свої навички просто завдяки процесу гри та коригуванню своєї поведінки.

## [Тест перед лекцією](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Для виконання RL нам потрібні:

* **Середовище** або **симулятор**, який встановлює правила гри. Ми повинні мати можливість проводити експерименти в симуляторі та спостерігати результати.
* **Функція винагороди**, яка вказує, наскільки успішним був наш експеримент. У випадку навчання гри в комп'ютерну гру винагородою буде наш фінальний рахунок.

На основі функції винагороди ми повинні мати можливість коригувати свою поведінку та покращувати свої навички, щоб наступного разу грати краще. Основна відмінність між іншими типами машинного навчання та RL полягає в тому, що в RL ми зазвичай не знаємо, чи виграли ми, чи програли, поки не закінчимо гру. Таким чином, ми не можемо сказати, чи є певний хід хорошим чи ні — ми отримуємо винагороду лише наприкінці гри.

Під час RL ми зазвичай проводимо багато експериментів. Під час кожного експерименту нам потрібно балансувати між дотриманням оптимальної стратегії, яку ми вивчили до цього часу (**експлуатація**), і дослідженням нових можливих станів (**дослідження**).

## OpenAI Gym

Чудовим інструментом для RL є [OpenAI Gym](https://gym.openai.com/) — **середовище симуляції**, яке може моделювати багато різних середовищ, починаючи від ігор Atari до фізики балансування стовпа. Це одне з найпопулярніших середовищ симуляції для навчання алгоритмів навчання з підкріпленням, яке підтримується [OpenAI](https://openai.com/).

> **Note**: Ви можете переглянути всі доступні середовища OpenAI Gym [тут](https://gym.openai.com/envs/#classic_control).

## Балансування CartPole

Ви, напевно, бачили сучасні пристрої для балансування, такі як *Segway* або *Гіроскутери*. Вони здатні автоматично балансувати, регулюючи свої колеса у відповідь на сигнал від акселерометра або гіроскопа. У цьому розділі ми навчимося вирішувати подібну проблему — балансування стовпа. Це схоже на ситуацію, коли цирковий артист балансує стовп на руці, але це балансування відбувається лише в 1D.

Спрощена версія балансування відома як проблема **CartPole**. У світі CartPole у нас є горизонтальний слайдер, який може рухатися вліво або вправо, і мета полягає в тому, щоб збалансувати вертикальний стовп на вершині слайдера, коли він рухається.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Щоб створити та використовувати це середовище, нам потрібно кілька рядків коду на Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Кожне середовище можна використовувати однаково:
* `env.reset` починає новий експеримент
* `env.step` виконує крок симуляції. Він отримує **дію** з **простору дій** і повертає **спостереження** (з простору спостережень), а також винагороду та прапорець завершення.

У наведеному вище прикладі ми виконуємо випадкову дію на кожному кроці, тому тривалість експерименту дуже коротка:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Мета алгоритму RL — навчити модель — так звану **політику** π, яка буде повертати дію у відповідь на заданий стан. Ми також можемо вважати політику ймовірнісною, тобто для будь-якого стану *s* і дії *a* вона поверне ймовірність π(*a*|*s*), що ми повинні виконати *a* у стані *s*.

## Алгоритм градієнтів політики

Найочевидніший спосіб змоделювати політику — створити нейронну мережу, яка прийматиме стани як вхідні дані та повертатиме відповідні дії (або, скоріше, ймовірності всіх дій). У певному сенсі це було б схоже на звичайне завдання класифікації, з однією суттєвою відмінністю — ми заздалегідь не знаємо, які дії слід виконувати на кожному з кроків.

Ідея полягає в тому, щоб оцінити ці ймовірності. Ми створюємо вектор **накопичених винагород**, який показує нашу загальну винагороду на кожному кроці експерименту. Ми також застосовуємо **дисконтування винагороди**, множачи попередні винагороди на деякий коефіцієнт γ=0.99, щоб зменшити роль попередніх винагород. Потім ми підсилюємо ті кроки вздовж шляху експерименту, які приносять більші винагороди.

> Дізнайтеся більше про алгоритм градієнтів політики та перегляньте його в дії в [прикладному ноутбуці](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Алгоритм Actor-Critic

Покращена версія підходу градієнтів політики називається **Actor-Critic**. Основна ідея полягає в тому, що нейронна мережа буде навчена повертати дві речі:

* Політику, яка визначає, яку дію виконати. Ця частина називається **актор**.
* Оцінку загальної винагороди, яку ми можемо очікувати отримати в цьому стані — ця частина називається **критик**.

У певному сенсі ця архітектура нагадує [GAN](../../4-ComputerVision/10-GANs/README.md), де у нас є дві мережі, які навчаються одна проти одної. У моделі actor-critic актор пропонує дію, яку нам потрібно виконати, а критик намагається бути критичним і оцінити результат. Однак наша мета — навчити ці мережі в унісон.

Оскільки ми знаємо як реальні накопичені винагороди, так і результати, повернуті критиком під час експерименту, відносно легко побудувати функцію втрат, яка мінімізуватиме різницю між ними. Це дасть нам **втрати критика**. Ми можемо обчислити **втрати актора**, використовуючи той самий підхід, що й у алгоритмі градієнтів політики.

Після запуску одного з цих алгоритмів ми можемо очікувати, що наш CartPole буде поводитися так:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Вправи: Градієнти політики та Actor-Critic RL

Продовжуйте навчання в наступних ноутбуках:

* [RL у TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL у PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Інші завдання RL

Навчання з підкріпленням сьогодні є швидкозростаючою галуззю досліджень. Деякі цікаві приклади навчання з підкріпленням:

* Навчання комп'ютера грати в **ігри Atari**. Складність цієї задачі полягає в тому, що у нас немає простого стану, представленого у вигляді вектора, а є скріншот — і нам потрібно використовувати CNN, щоб перетворити це зображення екрана на вектор ознак або витягти інформацію про винагороду. Ігри Atari доступні в Gym.
* Навчання комп'ютера грати в настільні ігри, такі як шахи та го. Нещодавно передові програми, такі як **Alpha Zero**, були навчені з нуля двома агентами, які грали один проти одного та покращувалися на кожному кроці.
* У промисловості RL використовується для створення систем керування на основі симуляції. Сервіс під назвою [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) спеціально розроблений для цього.

## Висновок

Ми дізналися, як навчати агентів досягати хороших результатів, просто надаючи їм функцію винагороди, яка визначає бажаний стан гри, і даючи їм можливість інтелектуально досліджувати простір пошуку. Ми успішно спробували два алгоритми та досягли хорошого результату за відносно короткий період часу. Однак це лише початок вашої подорожі в RL, і вам обов’язково варто розглянути можливість пройти окремий курс, якщо ви хочете заглибитися.

## 🚀 Виклик

Дослідіть застосування, перелічені в розділі "Інші завдання RL", і спробуйте реалізувати одне з них!

## [Тест після лекції](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Огляд і самостійне навчання

Дізнайтеся більше про класичне навчання з підкріпленням у нашій [навчальній програмі з машинного навчання для початківців](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Перегляньте [це чудове відео](https://www.youtube.com/watch?v=qv6UVOQ0F44) про те, як комп'ютер може навчитися грати в Super Mario.

## Завдання: [Навчіть Mountain Car](lab/README.md)

Вашою метою під час цього завдання буде навчити інше середовище Gym — [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, зверніть увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.