<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T15:28:25+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "uk"
}
-->
# Глибоке навчання з підкріпленням

Навчання з підкріпленням (RL) вважається одним із основних парадигм машинного навчання, поряд із навчанням з учителем та без учителя. У той час як у навчанні з учителем ми покладаємося на набір даних із відомими результатами, RL базується на **навчанні через дію**. Наприклад, коли ми вперше бачимо комп'ютерну гру, ми починаємо грати, навіть не знаючи правил, і незабаром можемо покращити свої навички просто через процес гри та коригування своєї поведінки.

## [Тест перед лекцією](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Для виконання RL нам потрібні:

* **Середовище** або **симулятор**, який встановлює правила гри. Ми повинні мати можливість проводити експерименти в симуляторі та спостерігати результати.
* **Функція винагороди**, яка вказує, наскільки успішним був наш експеримент. У випадку навчання гри в комп'ютерну гру винагородою буде наш фінальний рахунок.

На основі функції винагороди ми повинні мати можливість коригувати свою поведінку та покращувати свої навички, щоб наступного разу грати краще. Основна відмінність між іншими типами машинного навчання та RL полягає в тому, що в RL ми зазвичай не знаємо, чи виграємо ми, чи програємо, поки не закінчимо гру. Таким чином, ми не можемо сказати, чи певний хід сам по собі є хорошим чи ні - ми отримуємо винагороду лише наприкінці гри.

Під час RL ми зазвичай проводимо багато експериментів. Під час кожного експерименту нам потрібно балансувати між дотриманням оптимальної стратегії, яку ми вивчили до цього часу (**експлуатація**), і дослідженням нових можливих станів (**дослідження**).

## OpenAI Gym

Чудовим інструментом для RL є [OpenAI Gym](https://gym.openai.com/) - **середовище симуляції**, яке може моделювати багато різних середовищ, починаючи від ігор Atari до фізики балансування стовпа. Це одне з найпопулярніших середовищ симуляції для навчання алгоритмів навчання з підкріпленням, яке підтримується [OpenAI](https://openai.com/).

> **Note**: Ви можете переглянути всі доступні середовища OpenAI Gym [тут](https://gym.openai.com/envs/#classic_control).

## Балансування CartPole

Ви, мабуть, бачили сучасні пристрої для балансування, такі як *Segway* або *Гіроскутери*. Вони здатні автоматично балансувати, регулюючи свої колеса у відповідь на сигнал від акселерометра або гіроскопа. У цьому розділі ми навчимося вирішувати схожу задачу - балансування стовпа. Це схоже на ситуацію, коли цирковий артист повинен балансувати стовп на руці - але це балансування стовпа відбувається лише в 1D.

Спрощена версія балансування відома як задача **CartPole**. У світі CartPole у нас є горизонтальний слайдер, який може рухатися вліво або вправо, і мета полягає в тому, щоб балансувати вертикальний стовп на вершині слайдера, коли він рухається.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Щоб створити та використовувати це середовище, нам потрібно кілька рядків коду на Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Кожне середовище можна використовувати однаково:
* `env.reset` починає новий експеримент
* `env.step` виконує крок симуляції. Він отримує **дію** з **простору дій** і повертає **спостереження** (з простору спостережень), а також винагороду та прапорець завершення.

У наведеному вище прикладі ми виконуємо випадкову дію на кожному кроці, через що тривалість експерименту дуже коротка:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Мета алгоритму RL - навчити модель, так звану **політику** &pi;, яка буде повертати дію у відповідь на заданий стан. Ми також можемо розглядати політику як ймовірнісну, тобто для будь-якого стану *s* і дії *a* вона буде повертати ймовірність &pi;(*a*|*s*), що ми повинні виконати *a* у стані *s*.

## Алгоритм Policy Gradients

Найочевидніший спосіб моделювання політики - створити нейронну мережу, яка буде приймати стани як вхідні дані та повертати відповідні дії (або, скоріше, ймовірності всіх дій). У певному сенсі це було б схоже на звичайну задачу класифікації, з однією важливою відмінністю - ми заздалегідь не знаємо, які дії слід виконувати на кожному з кроків.

Ідея полягає в тому, щоб оцінити ці ймовірності. Ми створюємо вектор **накопичених винагород**, який показує нашу загальну винагороду на кожному кроці експерименту. Ми також застосовуємо **дисконтування винагороди**, множачи ранні винагороди на деякий коефіцієнт &gamma;=0.99, щоб зменшити роль ранніх винагород. Потім ми підсилюємо ті кроки вздовж шляху експерименту, які приносять більші винагороди.

> Дізнайтеся більше про алгоритм Policy Gradient і перегляньте його в дії в [прикладному ноутбуці](CartPole-RL-TF.ipynb).

## Алгоритм Actor-Critic

Покращена версія підходу Policy Gradients називається **Actor-Critic**. Основна ідея полягає в тому, що нейронна мережа буде навчена повертати дві речі:

* Політику, яка визначає, яку дію виконати. Ця частина називається **актор**.
* Оцінку загальної винагороди, яку ми можемо очікувати отримати в цьому стані - ця частина називається **критик**.

У певному сенсі ця архітектура нагадує [GAN](../../4-ComputerVision/10-GANs/README.md), де у нас є дві мережі, які тренуються одна проти одної. У моделі actor-critic актор пропонує дію, яку нам потрібно виконати, а критик намагається бути критичним і оцінити результат. Однак наша мета - навчити ці мережі в унісон.

Оскільки ми знаємо як реальні накопичені винагороди, так і результати, повернуті критиком під час експерименту, відносно легко побудувати функцію втрат, яка мінімізує різницю між ними. Це дасть нам **втрати критика**. Ми можемо обчислити **втрати актора**, використовуючи той самий підхід, що і в алгоритмі Policy Gradient.

Після запуску одного з цих алгоритмів ми можемо очікувати, що наш CartPole буде поводитися так:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Вправи: Policy Gradients та Actor-Critic RL

Продовжуйте навчання в наступних ноутбуках:

* [RL у TensorFlow](CartPole-RL-TF.ipynb)
* [RL у PyTorch](CartPole-RL-PyTorch.ipynb)

## Інші задачі RL

Навчання з підкріпленням сьогодні є швидко зростаючою галуззю досліджень. Деякі цікаві приклади навчання з підкріпленням:

* Навчання комп'ютера грати в **ігри Atari**. Складність цієї задачі полягає в тому, що у нас немає простого стану, представленого як вектор, а є скріншот - і нам потрібно використовувати CNN, щоб перетворити це зображення екрана на вектор ознак або витягти інформацію про винагороду. Ігри Atari доступні в Gym.
* Навчання комп'ютера грати в настільні ігри, такі як шахи та го. Нещодавно програми, такі як **Alpha Zero**, були навчені з нуля двома агентами, які грали один проти одного та покращувалися на кожному кроці.
* У промисловості RL використовується для створення систем управління на основі симуляції. Сервіс [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) спеціально розроблений для цього.

## Висновок

Ми тепер знаємо, як навчати агентів досягати хороших результатів, просто надаючи їм функцію винагороди, яка визначає бажаний стан гри, і даючи їм можливість інтелектуально досліджувати простір пошуку. Ми успішно спробували два алгоритми та досягли хорошого результату за відносно короткий період часу. Однак це лише початок вашої подорожі в RL, і вам обов'язково варто розглянути можливість пройти окремий курс, якщо ви хочете заглибитися.

## 🚀 Виклик

Досліджуйте застосування, наведені в розділі "Інші задачі RL", і спробуйте реалізувати одну з них!

## [Тест після лекції](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## Огляд та самостійне навчання

Дізнайтеся більше про класичне навчання з підкріпленням у нашій [навчальній програмі "Машинне навчання для початківців"](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Перегляньте [це чудове відео](https://www.youtube.com/watch?v=qv6UVOQ0F44), яке розповідає про те, як комп'ютер може навчитися грати в Super Mario.

## Завдання: [Навчіть Mountain Car](lab/README.md)

Вашою метою під час цього завдання буде навчити інше середовище Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

---

