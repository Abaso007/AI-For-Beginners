<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:52:42+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "uk"
}
-->
# Представлення тексту у вигляді тензорів

## [Тест перед лекцією](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Класифікація тексту

Протягом першої частини цього розділу ми зосередимося на завданні **класифікації тексту**. Ми використаємо набір даних [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), який містить новинні статті, наприклад:

* Категорія: Наука/Технології
* Заголовок: Компанія з Кентуккі отримала грант на дослідження пептидів (AP)
* Текст: AP - Компанія, заснована дослідником хімії з Університету Луїсвілля, отримала грант на розробку...

Наша мета — класифікувати новинну статтю в одну з категорій на основі тексту.

## Представлення тексту

Якщо ми хочемо вирішувати завдання обробки природної мови (NLP) за допомогою нейронних мереж, нам потрібен спосіб представлення тексту у вигляді тензорів. Комп’ютери вже представляють текстові символи як числа, які відповідають шрифтам на вашому екрані, використовуючи кодування, такі як ASCII або UTF-8.

<img alt="Зображення, що показує схему перетворення символу в ASCII та двійкове представлення" src="images/ascii-character-map.png" width="50%"/>

> [Джерело зображення](https://www.seobility.net/en/wiki/ASCII)

Як люди, ми розуміємо, що кожна літера **означає**, і як усі символи об’єднуються, щоб утворити слова в реченні. Однак комп’ютери самі по собі не мають такого розуміння, і нейронна мережа повинна навчитися значенню під час тренування.

Тому ми можемо використовувати різні підходи для представлення тексту:

* **Представлення на рівні символів**, коли ми представляємо текст, розглядаючи кожен символ як число. Якщо у нас є *C* різних символів у нашому текстовому корпусі, слово *Hello* буде представлено тензором розміром 5x*C*. Кожна літера відповідатиме стовпцю тензора в one-hot кодуванні.
* **Представлення на рівні слів**, у якому ми створюємо **словник** усіх слів у нашому тексті, а потім представляємо слова за допомогою one-hot кодування. Цей підхід є дещо кращим, оскільки кожна літера сама по собі не має великого значення, і, використовуючи більш високорівневі семантичні концепти — слова — ми спрощуємо завдання для нейронної мережі. Однак через великий розмір словника нам доводиться працювати з високовимірними розрідженими тензорами.

Незалежно від способу представлення, спочатку потрібно перетворити текст у послідовність **токенів**, де один токен — це символ, слово або навіть частина слова. Потім ми перетворюємо токен у число, зазвичай використовуючи **словник**, і це число можна подати в нейронну мережу за допомогою one-hot кодування.

## N-грам

У природній мові точне значення слів можна визначити лише в контексті. Наприклад, значення *нейронна мережа* і *рибальська мережа* абсолютно різні. Один із способів врахувати це — побудувати нашу модель на парах слів, розглядаючи пари слів як окремі токени словника. Таким чином, речення *Мені подобається рибалити* буде представлено наступною послідовністю токенів: *Мені подобається*, *подобається рибалити*. Проблема цього підходу полягає в тому, що розмір словника значно зростає, і комбінації, такі як *рибалити* і *шопінг*, представлені різними токенами, які не мають жодної семантичної схожості, незважаючи на однакове дієслово.

У деяких випадках ми можемо розглянути використання триграм — комбінацій із трьох слів. Таким чином, цей підхід часто називають **n-грамами**. Також має сенс використовувати n-грами з представленням на рівні символів, у цьому випадку n-грами приблизно відповідатимуть різним складам.

## Мішок слів і TF/IDF

Під час вирішення завдань, таких як класифікація тексту, нам потрібно вміти представляти текст одним вектором фіксованого розміру, який ми будемо використовувати як вхід для фінального щільного класифікатора. Один із найпростіших способів зробити це — об’єднати всі окремі представлення слів, наприклад, додаючи їх. Якщо ми додамо one-hot кодування кожного слова, ми отримаємо вектор частот, який показує, скільки разів кожне слово з’являється в тексті. Таке представлення тексту називається **мішок слів** (BoW).

<img src="images/bow.png" width="90%"/>

> Зображення автора

BoW фактично представляє, які слова з’являються в тексті та в якій кількості, що дійсно може бути хорошим показником того, про що йдеться в тексті. Наприклад, новинна стаття про політику, ймовірно, містить слова, такі як *президент* і *країна*, тоді як наукова публікація матиме щось на кшталт *колайдер*, *відкриття* тощо. Таким чином, частоти слів у багатьох випадках можуть бути хорошим показником змісту тексту.

Проблема BoW полягає в тому, що певні загальні слова, такі як *і*, *є* тощо, з’являються в більшості текстів і мають найвищі частоти, приховуючи слова, які дійсно важливі. Ми можемо знизити важливість цих слів, враховуючи частоту, з якою слова зустрічаються в усій колекції документів. Це основна ідея підходу TF/IDF, який детальніше розглядається в зошитах, прикріплених до цього уроку.

Однак жоден із цих підходів не може повністю врахувати **семантику** тексту. Для цього нам потрібні більш потужні моделі нейронних мереж, які ми обговоримо пізніше в цьому розділі.

## ✍️ Вправи: Представлення тексту

Продовжуйте навчання в наступних зошитах:

* [Представлення тексту з PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [Представлення тексту з TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Висновок

На даний момент ми вивчили техніки, які можуть додати вагу частотам різних слів. Вони, однак, не здатні представляти значення або порядок. Як сказав відомий лінгвіст Дж. Р. Ферт у 1935 році: "Повне значення слова завжди є контекстуальним, і жодне дослідження значення поза контекстом не може бути сприйняте серйозно." Ми дізнаємося пізніше в курсі, як захоплювати контекстуальну інформацію з тексту за допомогою мовного моделювання.

## 🚀 Виклик

Спробуйте інші вправи, використовуючи мішок слів і різні моделі даних. Вас може надихнути цей [конкурс на Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Тест після лекції](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Огляд і самостійне навчання

Практикуйте свої навички з текстовими вбудовуваннями та техніками мішка слів на [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Завдання: Зошити](assignment.md)

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, звертаємо вашу увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.