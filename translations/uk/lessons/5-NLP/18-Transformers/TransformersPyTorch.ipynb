{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Механізми уваги та трансформери\n",
    "\n",
    "Одним із головних недоліків рекурентних мереж є те, що всі слова в послідовності мають однаковий вплив на результат. Це призводить до субоптимальної продуктивності стандартних моделей LSTM-енкодер-декодер для задач послідовність-у-послідовність, таких як розпізнавання іменованих сутностей та машинний переклад. Насправді, окремі слова у вхідній послідовності часто мають більший вплив на вихідні результати, ніж інші.\n",
    "\n",
    "Розглянемо модель послідовність-у-послідовність, наприклад, машинний переклад. Вона реалізується за допомогою двох рекурентних мереж, де одна мережа (**енкодер**) стискає вхідну послідовність у прихований стан, а інша, **декодер**, розгортає цей прихований стан у перекладений результат. Проблема цього підходу полягає в тому, що фінальному стану мережі важко запам'ятати початок речення, що призводить до низької якості моделі для довгих речень.\n",
    "\n",
    "**Механізми уваги** забезпечують спосіб зважування контекстуального впливу кожного вхідного вектора на кожне передбачення виходу RNN. Це реалізується шляхом створення \"коротких шляхів\" між проміжними станами вхідної RNN та вихідної RNN. Таким чином, при генерації вихідного символу $y_t$, ми враховуємо всі приховані стани входу $h_i$ з різними ваговими коефіцієнтами $\\alpha_{t,i}$.\n",
    "\n",
    "![Зображення моделі енкодер/декодер з додатковим шаром уваги](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.uk.png)\n",
    "*Модель енкодер-декодер з механізмом додаткової уваги у [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитовано з [цього блогу](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Матриця уваги $\\{\\alpha_{i,j}\\}$ представляє ступінь, до якого певні вхідні слова впливають на генерацію конкретного слова у вихідній послідовності. Нижче наведено приклад такої матриці:\n",
    "\n",
    "![Зображення прикладу вирівнювання, знайденого RNNsearch-50, взято з Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.uk.png)\n",
    "\n",
    "*Зображення взято з [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Рис.3)*\n",
    "\n",
    "Механізми уваги є основою багатьох сучасних або близьких до сучасних досягнень у галузі обробки природної мови. Однак додавання уваги значно збільшує кількість параметрів моделі, що призводить до проблем масштабування з RNN. Основним обмеженням масштабування RNN є те, що рекурентна природа моделей ускладнює пакетну обробку та паралелізацію навчання. У RNN кожен елемент послідовності потрібно обробляти в послідовному порядку, що унеможливлює легку паралелізацію.\n",
    "\n",
    "Використання механізмів уваги разом із цим обмеженням призвело до створення сучасних трансформерних моделей, які ми знаємо і використовуємо сьогодні, таких як BERT та OpenGPT3.\n",
    "\n",
    "## Моделі трансформерів\n",
    "\n",
    "Замість передачі контексту кожного попереднього передбачення на наступний етап оцінки, **моделі трансформерів** використовують **позиційні кодування** та увагу для захоплення контексту заданого входу в межах наданого текстового вікна. Зображення нижче показує, як позиційні кодування з увагою можуть захоплювати контекст у межах заданого вікна.\n",
    "\n",
    "![Анімований GIF, що показує, як виконуються оцінки в моделях трансформерів.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Оскільки кожна вхідна позиція незалежно відображається на кожну вихідну позицію, трансформери можуть краще паралелізуватися, ніж RNN, що дозволяє створювати значно більші та виразніші мовні моделі. Кожна голова уваги може використовуватися для вивчення різних взаємозв’язків між словами, що покращує виконання задач обробки природної мови.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) — це дуже велика багатошарова трансформерна мережа з 12 шарами для *BERT-base* і 24 для *BERT-large*. Модель спочатку проходить попереднє навчання на великому корпусі текстових даних (WikiPedia + книги) за допомогою неконтрольованого навчання (передбачення замаскованих слів у реченні). Під час попереднього навчання модель засвоює значний рівень розуміння мови, який потім можна використовувати з іншими наборами даних за допомогою тонкого налаштування. Цей процес називається **трансферним навчанням**.\n",
    "\n",
    "![Зображення з http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.uk.png)\n",
    "\n",
    "Існує багато варіацій архітектур трансформерів, включаючи BERT, DistilBERT, BigBird, OpenGPT3 та інші, які можна налаштовувати. Пакет [HuggingFace](https://github.com/huggingface/) надає репозиторій для навчання багатьох із цих архітектур за допомогою PyTorch.\n",
    "\n",
    "## Використання BERT для класифікації тексту\n",
    "\n",
    "Давайте подивимося, як можна використовувати попередньо навчену модель BERT для вирішення нашої традиційної задачі: класифікації послідовностей. Ми будемо класифікувати наш оригінальний набір даних AG News.\n",
    "\n",
    "Спочатку завантажимо бібліотеку HuggingFace та наш набір даних:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оскільки ми будемо використовувати попередньо навчений BERT-модель, нам потрібно буде скористатися відповідним токенайзером. Спочатку ми завантажимо токенайзер, пов’язаний із попередньо навченою BERT-моделлю.\n",
    "\n",
    "Бібліотека HuggingFace містить репозиторій попередньо навчених моделей, які можна використовувати, просто вказавши їхні назви як аргументи для функцій `from_pretrained`. Усі необхідні бінарні файли для моделі будуть автоматично завантажені.\n",
    "\n",
    "Однак у деяких випадках вам може знадобитися завантажити власні моделі. У такому разі ви можете вказати каталог, який містить усі відповідні файли, включаючи параметри для токенайзера, файл `config.json` із параметрами моделі, бінарні ваги тощо.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Об'єкт `tokenizer` містить функцію `encode`, яку можна безпосередньо використовувати для кодування тексту:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоді давайте створимо ітератори, які ми будемо використовувати під час навчання для доступу до даних. Оскільки BERT використовує власну функцію кодування, нам потрібно буде визначити функцію заповнення, схожу на `padify`, яку ми визначили раніше:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нашому випадку ми будемо використовувати попередньо навчену модель BERT під назвою `bert-base-uncased`. Давайте завантажимо модель за допомогою пакету `BertForSequenceClassfication`. Це гарантує, що наша модель вже має необхідну архітектуру для класифікації, включаючи фінальний класифікатор. Ви побачите попередження, яке повідомляє, що ваги фінального класифікатора не ініціалізовані, і модель потребуватиме попереднього навчання - це абсолютно нормально, адже саме це ми збираємося зробити!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми готові розпочати навчання! Оскільки BERT вже попередньо навчений, ми хочемо почати з досить малого коефіцієнта навчання, щоб не зруйнувати початкові ваги.\n",
    "\n",
    "Всю важку роботу виконує модель `BertForSequenceClassification`. Коли ми викликаємо модель на навчальних даних, вона повертає як втрати, так і вихід мережі для вхідного мініпакету. Ми використовуємо втрати для оптимізації параметрів (`loss.backward()` виконує зворотний прохід), а `out` — для обчислення точності навчання, порівнюючи отримані мітки `labs` (обчислені за допомогою `argmax`) з очікуваними `labels`.\n",
    "\n",
    "Для контролю процесу ми накопичуємо втрати та точність протягом кількох ітерацій і виводимо їх кожні `report_freq` цикли навчання.\n",
    "\n",
    "Це навчання, ймовірно, займе досить багато часу, тому ми обмежуємо кількість ітерацій.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ви можете побачити (особливо якщо збільшити кількість ітерацій і зачекати достатньо довго), що класифікація за допомогою BERT дає нам досить хорошу точність! Це тому, що BERT вже досить добре розуміє структуру мови, і нам потрібно лише доопрацювати фінальний класифікатор. Однак, через те, що BERT є великою моделлю, весь процес навчання займає багато часу і потребує значних обчислювальних ресурсів! (GPU, і бажано більше ніж один).\n",
    "\n",
    "> **Примітка:** У нашому прикладі ми використовували одну з найменших попередньо навчених моделей BERT. Існують більші моделі, які, ймовірно, дадуть кращі результати.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оцінка продуктивності моделі\n",
    "\n",
    "Тепер ми можемо оцінити продуктивність нашої моделі на тестовому наборі даних. Цикл оцінки дуже схожий на цикл навчання, але не забудьте перевести модель у режим оцінки, викликавши `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основні висновки\n",
    "\n",
    "У цьому розділі ми побачили, як легко взяти попередньо навчений мовний модель із бібліотеки **transformers** і адаптувати її до завдання класифікації тексту. Аналогічно, моделі BERT можна використовувати для вилучення сутностей, відповіді на запитання та інших завдань обробки природної мови.\n",
    "\n",
    "Моделі трансформерів представляють сучасний передовий рівень у NLP, і в більшості випадків це має бути перше рішення, з якого ви починаєте експериментувати при впровадженні власних рішень NLP. Однак розуміння основних принципів рекурентних нейронних мереж, обговорених у цьому модулі, є надзвичайно важливим, якщо ви хочете створювати просунуті нейронні моделі.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Відмова від відповідальності**:  \nЦей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-30T10:18:57+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "uk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}