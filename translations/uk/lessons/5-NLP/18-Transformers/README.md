<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f335dfcb4a993920504c387973a36957",
  "translation_date": "2025-09-23T15:37:46+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "uk"
}
-->
# Механізми уваги та трансформери

## [Тест перед лекцією](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Однією з найважливіших задач у сфері обробки природної мови (NLP) є **машинний переклад**, ключове завдання, яке лежить в основі таких інструментів, як Google Translate. У цьому розділі ми зосередимося на машинному перекладі, або, більш загально, на будь-якому завданні *послідовність до послідовності* (яке також називається **трансдукцією речень**).

У випадку RNN, завдання "послідовність до послідовності" реалізується за допомогою двох рекурентних мереж, де одна мережа, **кодер**, стискає вхідну послідовність у прихований стан, а інша мережа, **декодер**, розгортає цей прихований стан у перекладений результат. Однак цей підхід має кілька проблем:

* Кінцевий стан мережі-кодера погано запам'ятовує початок речення, що призводить до низької якості моделі для довгих речень.
* Усі слова в послідовності мають однаковий вплив на результат. У реальності ж окремі слова у вхідній послідовності часто мають більший вплив на вихідні послідовності, ніж інші.

**Механізми уваги** забезпечують спосіб зважування контекстуального впливу кожного вхідного вектора на кожне передбачення виходу RNN. Це реалізується шляхом створення "коротких шляхів" між проміжними станами вхідної RNN та вихідної RNN. Таким чином, при генерації вихідного символу y<sub>t</sub>, ми враховуємо всі приховані стани h<sub>i</sub> вхідної мережі з різними коефіцієнтами ваги &alpha;<sub>t,i</sub>.

![Зображення моделі кодер/декодер з додатковим шаром уваги](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.uk.png)

> Модель кодер-декодер з механізмом додаткової уваги у [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитовано з [цього блогу](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Матриця уваги {&alpha;<sub>i,j</sub>} представляє ступінь, до якого певні слова у вході впливають на генерацію конкретного слова у вихідній послідовності. Нижче наведено приклад такої матриці:

![Зображення прикладу вирівнювання, знайденого RNNsearch-50, взято з Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.uk.png)

> Рисунок з [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Рис.3)

Механізми уваги є основою багатьох сучасних або майже сучасних досягнень у NLP. Однак додавання уваги значно збільшує кількість параметрів моделі, що призводить до проблем масштабування з RNN. Основним обмеженням масштабування RNN є те, що рекурентна природа моделей ускладнює пакетну обробку та паралелізацію навчання. У RNN кожен елемент послідовності потрібно обробляти в послідовному порядку, що ускладнює паралелізацію.

![Кодер-декодер з увагою](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Рисунок з [блогу Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Використання механізмів уваги у поєднанні з цим обмеженням призвело до створення сучасних моделей трансформерів, таких як BERT та Open-GPT3, які ми знаємо і використовуємо сьогодні.

## Моделі трансформерів

Одна з основних ідей трансформерів полягає в тому, щоб уникнути послідовної природи RNN і створити модель, яка може бути паралелізована під час навчання. Це досягається шляхом реалізації двох ідей:

* позиційне кодування
* використання механізму самоуваги для захоплення шаблонів замість RNN (або CNN) (саме тому стаття, яка представляє трансформери, називається *[Attention is all you need](https://arxiv.org/abs/1706.03762)*).

### Позиційне кодування/вбудовування

Ідея позиційного кодування полягає в наступному. 
1. У випадку RNN, відносна позиція токенів представлена кількістю кроків, і тому не потребує явного представлення. 
2. Однак, коли ми переходимо до уваги, нам потрібно знати відносні позиції токенів у послідовності. 
3. Для отримання позиційного кодування ми доповнюємо нашу послідовність токенів послідовністю позицій токенів у послідовності (тобто послідовністю чисел 0,1, ...).
4. Потім ми змішуємо позицію токена з вектором вбудовування токена. Для перетворення позиції (цілого числа) у вектор можна використовувати різні підходи:

* Навчуване вбудовування, схоже на вбудовування токенів. Це підхід, який ми розглядаємо тут. Ми застосовуємо шари вбудовування як до токенів, так і до їх позицій, отримуючи вектори вбудовування однакових розмірів, які потім додаємо.
* Фіксована функція позиційного кодування, як запропоновано в оригінальній статті.

<img src="images/pos-embedding.png" width="50%"/>

> Зображення автора

Результат, який ми отримуємо з позиційним вбудовуванням, включає як оригінальний токен, так і його позицію в послідовності.

### Мультиголовна самоувага

Далі нам потрібно захопити певні шаблони в нашій послідовності. Для цього трансформери використовують механізм **самоуваги**, який, по суті, є увагою, застосованою до тієї ж послідовності як до входу, так і до виходу. Застосування самоуваги дозволяє враховувати **контекст** у реченні та бачити, які слова взаємопов'язані. Наприклад, це дозволяє бачити, до яких слів відносяться кореференції, такі як *це*, а також враховувати контекст:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.uk.png)

> Зображення з [блогу Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

У трансформерах ми використовуємо **мультиголовну увагу**, щоб надати мережі можливість захоплювати кілька різних типів залежностей, наприклад, довгострокові та короткострокові відносини між словами, кореференції тощо.

[Ноутбук TensorFlow](TransformersTF.ipynb) містить більше деталей щодо реалізації шарів трансформера.

### Увага кодера-декодера

У трансформерах увага використовується у двох місцях:

* Для захоплення шаблонів у вхідному тексті за допомогою самоуваги.
* Для виконання перекладу послідовностей - це шар уваги між кодером і декодером.

Увага кодера-декодера дуже схожа на механізм уваги, який використовується в RNN, як описано на початку цього розділу. Ця анімована діаграма пояснює роль уваги кодера-декодера.

![Анімований GIF, що показує, як виконуються оцінки в моделях трансформерів.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Оскільки кожна позиція входу незалежно відображається на кожну позицію виходу, трансформери можуть краще паралелізуватися, ніж RNN, що дозволяє створювати набагато більші та більш виразні мовні моделі. Кожна голова уваги може використовуватися для вивчення різних відносин між словами, що покращує завдання обробки природної мови.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) — це дуже велика багатошарова трансформерна мережа з 12 шарами для *BERT-base* і 24 для *BERT-large*. Модель спочатку проходить попереднє навчання на великому корпусі текстових даних (WikiPedia + книги) за допомогою ненаглядуваного навчання (передбачення замаскованих слів у реченні). Під час попереднього навчання модель засвоює значний рівень розуміння мови, який потім можна використовувати з іншими наборами даних за допомогою тонкого налаштування. Цей процес називається **трансферним навчанням**.

![зображення з http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.uk.png)

> Зображення [джерело](http://jalammar.github.io/illustrated-bert/)

## ✍️ Вправи: Трансформери

Продовжуйте навчання у наступних ноутбуках:

* [Трансформери у PyTorch](TransformersPyTorch.ipynb)
* [Трансформери у TensorFlow](TransformersTF.ipynb)

## Висновок

У цьому уроці ви дізналися про трансформери та механізми уваги, які є важливими інструментами в арсеналі NLP. Існує багато варіацій архітектури трансформерів, включаючи BERT, DistilBERT, BigBird, OpenGPT3 та інші, які можна тонко налаштовувати. Пакет [HuggingFace](https://github.com/huggingface/) надає репозиторій для навчання багатьох з цих архітектур як з PyTorch, так і з TensorFlow.

## 🚀 Виклик

## [Тест після лекції](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## Огляд та самостійне навчання

* [Блог](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), що пояснює класичну статтю [Attention is all you need](https://arxiv.org/abs/1706.03762) про трансформери.
* [Серія блогів](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) про трансформери, що детально пояснює архітектуру.

## [Завдання](assignment.md)

---

