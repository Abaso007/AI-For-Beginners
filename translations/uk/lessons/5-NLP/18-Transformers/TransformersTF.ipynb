{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Механізми уваги та трансформери\n",
    "\n",
    "Одним із головних недоліків рекурентних мереж є те, що всі слова в послідовності мають однаковий вплив на результат. Це призводить до субоптимальної продуктивності стандартних моделей LSTM-енкодер-декодер для задач послідовність-у-послідовність, таких як розпізнавання іменованих сутностей та машинний переклад. Насправді, окремі слова у вхідній послідовності часто мають більший вплив на вихідні результати, ніж інші.\n",
    "\n",
    "Розглянемо модель послідовність-у-послідовність, наприклад, машинний переклад. Вона реалізується за допомогою двох рекурентних мереж, де одна мережа (**енкодер**) стискає вхідну послідовність у прихований стан, а інша, **декодер**, розгортає цей прихований стан у перекладений результат. Проблема цього підходу полягає в тому, що фінальному стану мережі важко запам'ятати початок речення, що призводить до низької якості моделі для довгих речень.\n",
    "\n",
    "**Механізми уваги** забезпечують спосіб зважування контекстуального впливу кожного вхідного вектора на кожне передбачення виходу RNN. Це реалізується шляхом створення \"ярликів\" між проміжними станами вхідної RNN та вихідної RNN. Таким чином, при генерації вихідного символу $y_t$ ми враховуємо всі приховані стани входу $h_i$ з різними ваговими коефіцієнтами $\\alpha_{t,i}$.\n",
    "\n",
    "![Зображення моделі енкодер/декодер з додатковим шаром уваги](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.uk.png)\n",
    "*Модель енкодер-декодер з механізмом додаткової уваги у [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), цитовано з [цього блогу](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Матриця уваги $\\{\\alpha_{i,j}\\}$ представляє ступінь, до якого певні вхідні слова впливають на генерацію конкретного слова у вихідній послідовності. Нижче наведено приклад такої матриці:\n",
    "\n",
    "![Зображення прикладу вирівнювання, знайденого RNNsearch-50, взято з Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.uk.png)\n",
    "\n",
    "*Зображення взято з [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Рис.3)*\n",
    "\n",
    "Механізми уваги є основою багатьох сучасних або близьких до сучасних досягнень у галузі обробки природної мови. Однак додавання уваги значно збільшує кількість параметрів моделі, що призводить до проблем масштабування з RNN. Ключовим обмеженням масштабування RNN є те, що рекурентна природа моделей ускладнює пакетну обробку та паралелізацію навчання. У RNN кожен елемент послідовності потрібно обробляти в послідовному порядку, що унеможливлює легку паралелізацію.\n",
    "\n",
    "Використання механізмів уваги разом із цим обмеженням призвело до створення сучасних трансформерних моделей, які ми сьогодні знаємо та використовуємо, таких як BERT і OpenGPT3.\n",
    "\n",
    "## Трансформерні моделі\n",
    "\n",
    "Замість передачі контексту кожного попереднього передбачення на наступний етап оцінки, **трансформерні моделі** використовують **позиційні кодування** та **увагу**, щоб захопити контекст заданого входу в межах наданого текстового вікна. Зображення нижче показує, як позиційні кодування разом із увагою можуть захоплювати контекст у межах заданого вікна.\n",
    "\n",
    "![Анімований GIF, що показує, як виконуються оцінки в трансформерних моделях.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Оскільки кожна вхідна позиція незалежно відображається на кожну вихідну позицію, трансформери можуть краще паралелізуватися, ніж RNN, що дозволяє створювати значно більші та виразніші мовні моделі. Кожна голова уваги може використовуватися для вивчення різних взаємозв’язків між словами, що покращує виконання задач обробки природної мови.\n",
    "\n",
    "## Створення простої трансформерної моделі\n",
    "\n",
    "Keras не містить вбудованого шару трансформера, але ми можемо створити його самостійно. Як і раніше, ми зосередимося на класифікації тексту з використанням набору даних AG News, але варто зазначити, що трансформерні моделі демонструють найкращі результати у складніших задачах обробки природної мови.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нові шари в Keras повинні успадковувати клас `Layer` і реалізовувати метод `call`. Почнемо з шару **Positional Embedding**. Ми використаємо [деякий код з офіційної документації Keras](https://keras.io/examples/nlp/text_classification_with_transformer/). Ми припустимо, що всі вхідні послідовності заповнені до довжини `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цей шар складається з двох шарів `Embedding`: для вбудовування токенів (у спосіб, який ми обговорювали раніше) і позицій токенів. Позиції токенів створюються як послідовність натуральних чисел від 0 до `maxlen` за допомогою `tf.range`, а потім передаються через шар вбудовування. Два отримані вектори вбудовування додаються, утворюючи позиційно-вбудоване представлення вхідних даних форми `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Тепер давайте реалізуємо блок трансформера. Він прийматиме вихід раніше визначеного шару вбудовування:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформер застосовує `MultiHeadAttention` до вхідних даних із позиційним кодуванням, щоб отримати вектор уваги розмірності `maxlen`$\\times$`embed_dim`, який потім змішується з вхідними даними та нормалізується за допомогою `LayerNormalization`.\n",
    "\n",
    "> **Примітка**: `LayerNormalization` схожа на `BatchNormalization`, яку ми розглядали в розділі *Комп'ютерне бачення* цього навчального курсу, але вона нормалізує виходи попереднього шару для кожного навчального зразка незалежно, приводячи їх до діапазону [-1..1].\n",
    "\n",
    "Вихід цього шару потім передається через `Dense` мережу (у нашому випадку - двошаровий перцептрон), і результат додається до фінального виходу (який знову проходить нормалізацію).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Моделі трансформерів BERT\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) — це велика багатошарова трансформерна мережа з 12 шарами для *BERT-base* і 24 для *BERT-large*. Модель спочатку проходить попереднє навчання на великому корпусі текстових даних (WikiPedia + книги) за допомогою неконтрольованого навчання (прогнозування замаскованих слів у реченні). Під час попереднього навчання модель засвоює значний рівень розуміння мови, який потім можна використовувати з іншими наборами даних за допомогою тонкого налаштування. Цей процес називається **трансферним навчанням**.\n",
    "\n",
    "![зображення з http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.uk.png)\n",
    "\n",
    "Існує багато варіацій архітектур трансформерів, включаючи BERT, DistilBERT, BigBird, OpenGPT3 та інші, які можна налаштовувати.\n",
    "\n",
    "Давайте подивимося, як ми можемо використати попередньо навчений BERT для вирішення нашої традиційної задачі класифікації послідовностей. Ми запозичимо ідею та трохи коду з [офіційної документації](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Для завантаження попередньо навчених моделей ми будемо використовувати **Tensorflow hub**. Спочатку завантажимо векторизатор, специфічний для BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важливо використовувати той самий векторизатор, на якому була навчена початкова мережа. Крім того, векторизатор BERT повертає три компоненти:\n",
    "* `input_word_ids`, що є послідовністю номерів токенів для вхідного речення\n",
    "* `input_mask`, який показує, яка частина послідовності містить фактичний вхід, а яка є заповненням. Це схоже на маску, створену шаром `Masking`\n",
    "* `input_type_ids` використовується для завдань мовного моделювання і дозволяє вказати два вхідні речення в одній послідовності.\n",
    "\n",
    "Після цього ми можемо створити екстрактор ознак BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отже, шар BERT повертає кілька корисних результатів:\n",
    "* `pooled_output` — це результат усереднення всіх токенів у послідовності. Його можна розглядати як інтелектуальне семантичне представлення всієї мережі. Це еквівалент виходу шару `GlobalAveragePooling1D` у нашій попередній моделі.\n",
    "* `sequence_output` — це вихід останнього шару трансформера (відповідає виходу `TransformerBlock` у нашій моделі вище).\n",
    "* `encoder_outputs` — це виходи всіх шарів трансформера. Оскільки ми завантажили 4-шарову модель BERT (як ви, мабуть, здогадалися з назви, яка містить `4_H`), вона має 4 тензори. Останній з них збігається з `sequence_output`.\n",
    "\n",
    "Тепер ми визначимо наскрізну модель класифікації. Ми будемо використовувати *функціональне визначення моделі*, коли ми визначаємо вхід моделі, а потім надаємо серію виразів для обчислення її виходу. Ми також зробимо ваги моделі BERT незмінними (не тренованими) і будемо тренувати лише фінальний класифікатор:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попри те, що кількість параметрів для навчання невелика, процес досить повільний, оскільки екстрактор ознак BERT є обчислювально важким. Схоже, нам не вдалося досягти прийнятної точності, або через недостатнє навчання, або через недостатню кількість параметрів моделі.\n",
    "\n",
    "Спробуймо розморозити ваги BERT і навчити його також. Це вимагає дуже маленької швидкості навчання, а також більш обережної стратегії навчання з **warmup**, використовуючи оптимізатор **AdamW**. Ми будемо використовувати пакет `tf-models-official` для створення оптимізатора:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як ви можете помітити, навчання проходить досить повільно — але ви можете спробувати експериментувати та тренувати модель протягом кількох епох (5-10), щоб побачити, чи вдасться отримати кращий результат у порівнянні з підходами, які ми використовували раніше.\n",
    "\n",
    "## Бібліотека Huggingface Transformers\n",
    "\n",
    "Ще один дуже поширений (і трохи простіший) спосіб використання моделей Transformer — це [пакет HuggingFace](https://github.com/huggingface/), який надає прості будівельні блоки для різних завдань обробки природної мови (NLP). Він доступний як для Tensorflow, так і для PyTorch — ще однієї дуже популярної бібліотеки для нейронних мереж.\n",
    "\n",
    "> **Примітка**: Якщо вам не цікаво побачити, як працює бібліотека Transformers, ви можете пропустити до кінця цього ноутбука, оскільки ви не побачите нічого принципово нового порівняно з тим, що ми робили вище. Ми повторимо ті самі кроки навчання моделі BERT, використовуючи іншу бібліотеку та значно більшу модель. Таким чином, процес включає досить тривале навчання, тому ви можете просто переглянути код.\n",
    "\n",
    "Давайте подивимося, як можна вирішити нашу задачу за допомогою [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перше, що нам потрібно зробити, це обрати модель, яку ми будемо використовувати. Окрім деяких вбудованих моделей, Huggingface має [онлайн-репозиторій моделей](https://huggingface.co/models), де ви можете знайти багато інших попередньо навчених моделей, створених спільнотою. Усі ці моделі можна завантажити та використовувати, просто вказавши назву моделі. Усі необхідні бінарні файли для моделі будуть автоматично завантажені.\n",
    "\n",
    "У деяких випадках вам може знадобитися завантажити власні моделі. У такому разі ви можете вказати директорію, яка містить усі відповідні файли, включаючи параметри для токенайзера, файл `config.json` із параметрами моделі, бінарні ваги тощо.\n",
    "\n",
    "З назви моделі ми можемо створити як саму модель, так і токенайзер. Почнемо з токенайзера:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Об'єкт `tokenizer` містить функцію `encode`, яку можна безпосередньо використовувати для кодування тексту:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми також можемо використовувати токенізатор для кодування послідовності у спосіб, придатний для передачі моделі, тобто включаючи поля `token_ids`, `input_mask` тощо. Ми також можемо вказати, що хочемо тензори Tensorflow, надавши аргумент `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нашому випадку ми будемо використовувати попередньо навчений BERT-модель під назвою `bert-base-uncased`. *Uncased* означає, що модель не чутлива до регістру.\n",
    "\n",
    "Під час навчання моделі нам потрібно надати токенізовану послідовність як вхідні дані, тому ми створимо конвеєр обробки даних. Оскільки `tokenizer.encode` є функцією Python, ми використаємо той самий підхід, що й у попередньому розділі, викликаючи її за допомогою `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми можемо завантажити фактичну модель, використовуючи пакет `BertForSequenceClassfication`. Це гарантує, що наша модель вже має необхідну архітектуру для класифікації, включаючи фінальний класифікатор. Ви побачите попередження, яке повідомляє, що ваги фінального класифікатора не ініціалізовані, і модель потребуватиме попереднього навчання - це абсолютно нормально, адже саме це ми збираємося зробити!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як видно з `summary()`, модель містить майже 110 мільйонів параметрів! Ймовірно, якщо ми хочемо виконати просте завдання класифікації на відносно невеликому наборі даних, ми не хочемо тренувати базовий шар BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми готові розпочати навчання!\n",
    "\n",
    "> **Примітка**: Навчання повномасштабної моделі BERT може зайняти дуже багато часу! Тому ми будемо тренувати її лише на перших 32 батчах. Це лише для демонстрації налаштування процесу навчання моделі. Якщо ви хочете спробувати повномасштабне навчання – просто видаліть параметри `steps_per_epoch` та `validation_steps`, і приготуйтеся чекати!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Якщо збільшити кількість ітерацій, зачекати достатньо часу та тренувати модель протягом кількох епох, можна очікувати, що класифікація за допомогою BERT забезпечить найкращу точність! Це тому, що BERT уже досить добре розуміє структуру мови, і нам потрібно лише доопрацювати фінальний класифікатор. Однак, через те, що BERT є великою моделлю, увесь процес навчання займає багато часу та потребує значних обчислювальних ресурсів! (GPU, і бажано більше ніж один).\n",
    "\n",
    "> **Примітка:** У нашому прикладі ми використовували одну з найменших попередньо натренованих моделей BERT. Існують більші моделі, які, ймовірно, дадуть кращі результати.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основні висновки\n",
    "\n",
    "У цьому розділі ми розглянули найсучасніші архітектури моделей, засновані на **трансформерах**. Ми застосували їх для завдання класифікації тексту, але аналогічно моделі BERT можуть використовуватися для вилучення сутностей, відповіді на запитання та інших завдань обробки природної мови.\n",
    "\n",
    "Моделі на основі трансформерів представляють сучасний рівень розвитку в NLP, і в більшості випадків це має бути перше рішення, з якого ви починаєте експериментувати при впровадженні власних NLP-рішень. Однак розуміння базових принципів рекурентних нейронних мереж, які обговорювалися в цьому модулі, є надзвичайно важливим, якщо ви хочете створювати просунуті нейронні моделі.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Відмова від відповідальності**:  \nЦей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-30T10:22:19+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "uk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}