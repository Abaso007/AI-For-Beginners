<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-25T21:47:02+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "uk"
}
-->
# Генеративні мережі

## [Тест перед лекцією](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Рекурентні нейронні мережі (RNN) та їх варіанти з керованими комірками, такі як комірки довготривалої пам'яті (LSTM) і керовані рекурентні блоки (GRU), забезпечують механізм моделювання мови, оскільки вони можуть навчатися порядку слів і передбачати наступне слово в послідовності. Це дозволяє використовувати RNN для **генеративних завдань**, таких як звичайне генерування тексту, машинний переклад і навіть створення підписів до зображень.

> ✅ Згадайте всі випадки, коли ви отримували користь від генеративних завдань, таких як автозаповнення тексту під час введення. Проведіть дослідження ваших улюблених додатків, щоб дізнатися, чи використовували вони RNN.

У архітектурі RNN, яку ми обговорювали в попередньому модулі, кожен блок RNN створював наступний прихований стан як вихід. Однак ми також можемо додати ще один вихід до кожного рекурентного блоку, що дозволить нам створювати **послідовність** (яка дорівнює довжині початкової послідовності). Більше того, ми можемо використовувати блоки RNN, які не приймають вхідні дані на кожному кроці, а лише беруть початковий вектор стану і створюють послідовність виходів.

Це дозволяє створювати різні нейронні архітектури, які показані на зображенні нижче:

![Зображення, що показує поширені шаблони рекурентних нейронних мереж.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.uk.jpg)

> Зображення з блогу [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) авторства [Андрея Карпаті](http://karpathy.github.io/)

* **Один до одного** — це традиційна нейронна мережа з одним входом і одним виходом.
* **Один до багатьох** — це генеративна архітектура, яка приймає одне вхідне значення і генерує послідовність вихідних значень. Наприклад, якщо ми хочемо навчити мережу для **створення підписів до зображень**, яка створюватиме текстовий опис картинки, ми можемо взяти зображення як вхід, пропустити його через CNN для отримання прихованого стану, а потім використати рекурентний ланцюг для генерації підпису слово за словом.
* **Багато до одного** відповідає архітектурам RNN, які ми описували в попередньому модулі, наприклад, класифікація тексту.
* **Багато до багатьох**, або **послідовність до послідовності**, відповідає завданням, таким як **машинний переклад**, де перший RNN збирає всю інформацію з вхідної послідовності в прихований стан, а інший ланцюг RNN розгортає цей стан у вихідну послідовність.

У цьому модулі ми зосередимося на простих генеративних моделях, які допомагають нам генерувати текст. Для простоти ми використовуватимемо токенізацію на рівні символів.

Ми навчимо RNN генерувати текст крок за кроком. На кожному кроці ми братимемо послідовність символів довжиною `nchars` і проситимемо мережу створити наступний вихідний символ для кожного вхідного символу:

![Зображення, що показує приклад генерації слова 'HELLO' за допомогою RNN.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.uk.png)

Під час генерації тексту (під час інференсу) ми починаємо з певного **запиту**, який передається через блоки RNN для створення проміжного стану, а потім з цього стану починається генерація. Ми генеруємо один символ за раз, передаємо стан і створений символ до іншого блоку RNN для генерації наступного, поки не буде створено достатню кількість символів.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Зображення авторства автора

## ✍️ Вправи: Генеративні мережі

Продовжуйте навчання у наступних ноутбуках:

* [Генеративні мережі з PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Генеративні мережі з TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## М'яке генерування тексту та температура

Вихід кожного блоку RNN — це розподіл ймовірностей символів. Якщо ми завжди обиратимемо символ із найвищою ймовірністю як наступний символ у створеному тексті, текст часто може "зациклюватися" між одними й тими ж послідовностями символів, як у цьому прикладі:

```
today of the second the company and a second the company ...
```

Однак, якщо ми подивимося на розподіл ймовірностей для наступного символу, може виявитися, що різниця між кількома найвищими ймовірностями незначна, наприклад, один символ може мати ймовірність 0.2, а інший — 0.19 тощо. Наприклад, коли ми шукаємо наступний символ у послідовності '*play*', наступним символом може однаково бути пробіл або **e** (як у слові *player*).

Це приводить нас до висновку, що не завжди "справедливо" обирати символ із найвищою ймовірністю, оскільки вибір другого за ймовірністю також може привести до осмисленого тексту. Більш розумно **вибирати** символи з розподілу ймовірностей, який надає мережа. Ми також можемо використовувати параметр **температура**, який згладжує розподіл ймовірностей, якщо ми хочемо додати більше випадковості, або робить його більш крутим, якщо ми хочемо більше дотримуватися символів із найвищою ймовірністю.

Досліджуйте, як це м'яке генерування тексту реалізовано в ноутбуках, посилання на які наведено вище.

## Висновок

Хоча генерування тексту може бути корисним саме по собі, основні переваги полягають у здатності генерувати текст за допомогою RNN із початкового вектора ознак. Наприклад, генерування тексту використовується як частина машинного перекладу (послідовність до послідовності, у цьому випадку вектор стану з *кодера* використовується для створення або *декодування* перекладеного повідомлення) або створення текстового опису зображення (у цьому випадку вектор ознак надходить від CNN-екстрактора).

## 🚀 Виклик

Пройдіть кілька уроків на Microsoft Learn на цю тему:

* Генерація тексту з [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Тест після лекції](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Огляд і самостійне навчання

Ось кілька статей для розширення ваших знань:

* Різні підходи до генерування тексту з використанням Марковського ланцюга, LSTM і GPT-2: [блог](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Приклад генерування тексту в [документації Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Завдання](lab/README.md)

Ми побачили, як генерувати текст символ за символом. У лабораторній роботі ви дослідите генерування тексту на рівні слів.

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, звертаємо вашу увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.