<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-25T22:09:12+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "uk"
}
-->
# Попередньо навчені великі мовні моделі

У всіх попередніх завданнях ми навчали нейронну мережу виконувати певне завдання, використовуючи розмічений набір даних. З великими трансформерними моделями, такими як BERT, ми використовуємо моделювання мови в самонавчальному режимі для створення мовної моделі, яка потім спеціалізується на конкретному завданні за допомогою подальшого навчання в певній галузі. Однак було доведено, що великі мовні моделі можуть вирішувати багато завдань без будь-якого навчання, специфічного для галузі. Сімейство моделей, здатних на це, називається **GPT**: Генеративний Попередньо Навчений Трансформер.

## [Тест перед лекцією](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/120)

## Генерація тексту та перплексія

Ідея про те, що нейронна мережа може виконувати загальні завдання без додаткового навчання, представлена в статті [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Основна ідея полягає в тому, що багато інших завдань можна моделювати за допомогою **генерації тексту**, оскільки розуміння тексту фактично означає здатність його створювати. Оскільки модель навчена на величезній кількості текстів, які охоплюють людські знання, вона також стає обізнаною в широкому спектрі тем.

> Розуміння тексту та здатність його створювати також передбачає знання про навколишній світ. Люди також значною мірою навчаються через читання, і мережа GPT схожа на них у цьому аспекті.

Мережі генерації тексту працюють, прогнозуючи ймовірність наступного слова $$P(w_N)$$. Однак безумовна ймовірність наступного слова дорівнює частоті цього слова в корпусі тексту. GPT здатна надати нам **умовну ймовірність** наступного слова, враховуючи попередні: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Ви можете дізнатися більше про ймовірності в нашому [курсі "Data Science для початківців"](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Якість моделі генерації тексту можна визначити за допомогою **перплексії**. Це внутрішня метрика, яка дозволяє оцінити якість моделі без використання специфічного для завдання набору даних. Вона базується на понятті *ймовірності речення* — модель присвоює високу ймовірність реченням, які, ймовірно, є реальними (тобто модель не **збентежена** ними), і низьку ймовірність реченням, які мають менше сенсу (наприклад, *Чи може це робити що?*). Коли ми подаємо моделі речення з реального корпусу тексту, ми очікуємо, що вони матимуть високу ймовірність і низьку **перплексію**. Математично це визначається як нормалізована обернена ймовірність тестового набору:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Ви можете експериментувати з генерацією тексту за допомогою [редактора тексту на основі GPT від Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. У цьому редакторі ви починаєте писати текст, і натискання **[TAB]** запропонує кілька варіантів завершення. Якщо вони занадто короткі або вас не задовольняють — натисніть [TAB] ще раз, і ви отримаєте більше варіантів, включаючи довші фрагменти тексту.

## GPT — це сімейство моделей

GPT — це не одна модель, а скоріше колекція моделей, розроблених і навчальних [OpenAI](https://openai.com).

Серед моделей GPT є:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| Мовна модель з до 1,5 мільярда параметрів. | Мовна модель з до 175 мільярдів параметрів. | 100 трильйонів параметрів, приймає як зображення, так і текст на вході, а на виході видає текст. |

Моделі GPT-3 та GPT-4 доступні [як когнітивна служба від Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) і через [API OpenAI](https://openai.com/api/).

## Інженерія запитів

Оскільки GPT навчена на величезних обсягах даних для розуміння мови та коду, вона надає відповіді у відповідь на введення (запити). Запити — це введення або запитання до GPT, де ви надаєте інструкції моделям щодо завдань, які вони мають виконати. Щоб отримати бажаний результат, потрібно створити найефективніший запит, що включає вибір правильних слів, форматів, фраз або навіть символів. Цей підхід називається [інженерією запитів](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Ця документація](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) надає більше інформації про інженерію запитів.

## ✍️ Приклад ноутбука: [Гра з OpenAI-GPT](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

Продовжуйте навчання за допомогою наступних ноутбуків:

* [Генерація тексту з OpenAI-GPT та Hugging Face Transformers](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## Висновок

Нові загальні попередньо навчені мовні моделі не лише моделюють структуру мови, але й містять величезний обсяг природної мови. Таким чином, їх можна ефективно використовувати для вирішення деяких завдань обробки природної мови в умовах нульового або обмеженого навчання.

## [Тест після лекції](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/220)

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.