<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T15:38:20+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "uk"
}
-->
# Попередньо навчені великі мовні моделі

У всіх попередніх завданнях ми навчали нейронну мережу виконувати певне завдання, використовуючи позначений набір даних. З великими трансформерними моделями, такими як BERT, ми використовуємо моделювання мови в самонавчальному режимі для створення мовної моделі, яка потім спеціалізується на конкретному завданні за допомогою додаткового навчання для певної галузі. Однак було продемонстровано, що великі мовні моделі також можуть вирішувати багато завдань без будь-якого навчання для конкретної галузі. Сімейство моделей, здатних це робити, називається **GPT**: Генеративний Попередньо Навчений Трансформер.

## [Тест перед лекцією](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## Генерація тексту та заплутаність

Ідея нейронної мережі, яка може виконувати загальні завдання без додаткового навчання, представлена в статті [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Основна ідея полягає в тому, що багато інших завдань можна моделювати за допомогою **генерації тексту**, оскільки розуміння тексту фактично означає здатність його створювати. Оскільки модель навчена на величезній кількості тексту, що охоплює людські знання, вона також стає обізнаною в широкому спектрі тем.

> Розуміння та здатність створювати текст також передбачає знання про навколишній світ. Люди значною мірою навчаються через читання, і мережа GPT схожа в цьому аспекті.

Мережі генерації тексту працюють, прогнозуючи ймовірність наступного слова $$P(w_N)$$. Однак безумовна ймовірність наступного слова дорівнює частоті цього слова в текстовому корпусі. GPT здатна надати нам **умовну ймовірність** наступного слова, враховуючи попередні: $$P(w_N | w_{n-1}, ..., w_0)$$.

> Ви можете дізнатися більше про ймовірності в нашій [Програмі для початківців у сфері Data Science](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

Якість моделі генерації тексту можна визначити за допомогою **заплутаності**. Це внутрішній показник, який дозволяє оцінити якість моделі без використання набору даних для конкретного завдання. Він базується на понятті *ймовірності речення* - модель присвоює високу ймовірність реченню, яке, ймовірно, є реальним (тобто модель не **заплутана** ним), і низьку ймовірність реченням, які мають менший сенс (наприклад, *Чи може це робити що?*). Коли ми даємо нашій моделі речення з реального текстового корпусу, ми очікуємо, що вони матимуть високу ймовірність і низьку **заплутаність**. Математично це визначається як нормалізована обернена ймовірність тестового набору:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Ви можете експериментувати з генерацією тексту, використовуючи [текстовий редактор на основі GPT від Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. У цьому редакторі ви починаєте писати текст, і натискання **[TAB]** запропонує вам кілька варіантів завершення. Якщо вони занадто короткі або вас не задовольняють - натисніть [TAB] ще раз, і ви отримаєте більше варіантів, включаючи довші фрагменти тексту.

## GPT - це сімейство

GPT - це не одна модель, а колекція моделей, розроблених і навчених [OpenAI](https://openai.com).

Серед моделей GPT є:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| Мовна модель з до 1,5 мільярда параметрів. | Мовна модель з до 175 мільярдів параметрів. | 100T параметрів, приймає як зображення, так і текстові входи, а виходом є текст. |

Моделі GPT-3 та GPT-4 доступні [як когнітивна служба від Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) та через [API OpenAI](https://openai.com/api/).

## Інженерія запитів

Оскільки GPT навчена на величезних обсягах даних для розуміння мови та коду, вона надає відповіді у відповідь на введення (запити). Запити - це введення або запити до GPT, де ви надаєте моделі інструкції щодо завдань, які вона має виконати. Щоб отримати бажаний результат, необхідно створити найефективніший запит, що включає вибір правильних слів, форматів, фраз або навіть символів. Цей підхід називається [Інженерією запитів](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[Ця документація](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) надає більше інформації про інженерію запитів.

## ✍️ Приклад блокнота: [Гра з OpenAI-GPT](GPT-PyTorch.ipynb)

Продовжуйте навчання за допомогою наступних блокнотів:

* [Генерація тексту з OpenAI-GPT та Hugging Face Transformers](GPT-PyTorch.ipynb)

## Висновок

Нові загальні попередньо навчені мовні моделі не лише моделюють структуру мови, але й містять величезну кількість природної мови. Таким чином, їх можна ефективно використовувати для вирішення деяких завдань обробки природної мови в режимах zero-shot або few-shot.

## [Тест після лекції](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

