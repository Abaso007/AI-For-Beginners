<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "088837b42b7d99198bf62db8a42411e0",
  "translation_date": "2025-08-26T09:26:19+00:00",
  "source_file": "lessons/4-ComputerVision/07-ConvNets/README.md",
  "language_code": "ar"
}
-->
# الشبكات العصبية الالتفافية

لقد رأينا سابقًا أن الشبكات العصبية جيدة جدًا في التعامل مع الصور، وحتى الشبكة العصبية ذات الطبقة الواحدة قادرة على التعرف على الأرقام المكتوبة بخط اليد من مجموعة بيانات MNIST بدقة معقولة. ومع ذلك، فإن مجموعة بيانات MNIST مميزة جدًا، حيث يتمركز جميع الأرقام داخل الصورة، مما يجعل المهمة أسهل.

## [اختبار ما قبل المحاضرة](https://ff-quizzes.netlify.app/en/ai/quiz/13)

في الحياة الواقعية، نريد أن نكون قادرين على التعرف على الأشياء في الصورة بغض النظر عن موقعها الدقيق داخل الصورة. رؤية الكمبيوتر تختلف عن التصنيف العام، لأنه عندما نحاول العثور على كائن معين في الصورة، فإننا نقوم بمسح الصورة بحثًا عن **أنماط** محددة وتراكيبها. على سبيل المثال، عند البحث عن قطة، قد نبدأ بالبحث عن خطوط أفقية، والتي يمكن أن تشكل الشوارب، ثم يمكن لتكوين معين من الشوارب أن يخبرنا بأنها صورة لقطة بالفعل. الموقع النسبي ووجود أنماط معينة مهم، وليس موقعها الدقيق في الصورة.

لاستخراج الأنماط، سنستخدم مفهوم **مرشحات الالتفاف**. كما تعلم، يتم تمثيل الصورة بمصفوفة ثنائية الأبعاد، أو موتر ثلاثي الأبعاد مع عمق اللون. تطبيق المرشح يعني أننا نأخذ مصفوفة **نواة المرشح** صغيرة نسبيًا، ولكل بكسل في الصورة الأصلية نحسب المتوسط المرجح مع النقاط المجاورة. يمكننا تصور ذلك كنافذة صغيرة تنزلق عبر الصورة بأكملها، وتقوم بتوسيط جميع البكسلات وفقًا للأوزان في مصفوفة نواة المرشح.

![مرشح الحواف العمودية](../../../../../translated_images/filter-vert.b7148390ca0bc356ddc7e55555d2481819c1e86ddde9dce4db5e71a69d6f887f.ar.png) | ![مرشح الحواف الأفقية](../../../../../translated_images/filter-horiz.59b80ed4feb946efbe201a7fe3ca95abb3364e266e6fd90820cb893b4d3a6dda.ar.png)
----|----

> الصورة بواسطة دميتري سوشنيكوف

على سبيل المثال، إذا قمنا بتطبيق مرشحات الحواف العمودية والأفقية بحجم 3x3 على أرقام MNIST، يمكننا الحصول على إبرازات (مثل القيم العالية) حيث توجد حواف عمودية وأفقية في صورتنا الأصلية. وبالتالي يمكن استخدام هذين المرشحين "للبحث عن" الحواف. وبالمثل، يمكننا تصميم مرشحات مختلفة للبحث عن أنماط منخفضة المستوى أخرى:

> صورة [مجموعة مرشحات ليونغ-مالك](https://www.robots.ox.ac.uk/~vgg/research/texclass/filters.html)

ومع ذلك، بينما يمكننا تصميم المرشحات لاستخراج بعض الأنماط يدويًا، يمكننا أيضًا تصميم الشبكة بطريقة تجعلها تتعلم الأنماط تلقائيًا. هذه واحدة من الأفكار الرئيسية وراء الشبكات العصبية الالتفافية.

## الأفكار الرئيسية وراء الشبكات العصبية الالتفافية

طريقة عمل الشبكات العصبية الالتفافية تعتمد على الأفكار المهمة التالية:

* مرشحات الالتفاف يمكنها استخراج الأنماط
* يمكننا تصميم الشبكة بطريقة تجعل المرشحات تُدرب تلقائيًا
* يمكننا استخدام نفس النهج للعثور على أنماط في الميزات عالية المستوى، وليس فقط في الصورة الأصلية. وبالتالي، تعمل عملية استخراج الميزات في الشبكات العصبية الالتفافية على تسلسل هرمي للميزات، بدءًا من تركيبات البكسلات منخفضة المستوى وصولًا إلى تركيبات أعلى مستوى لأجزاء الصورة.

![استخراج الميزات الهرمي](../../../../../translated_images/FeatureExtractionCNN.d9b456cbdae7cb643fde3032b81b2940e3cf8be842e29afac3f482725ba7f95c.ar.png)

> الصورة من [ورقة بحثية بواسطة هيسلوب-لينش](https://www.semanticscholar.org/paper/Computer-vision-based-pedestrian-trajectory-Hislop-Lynch/26e6f74853fc9bbb7487b06dc2cf095d36c9021d)، بناءً على [بحثهم](https://dl.acm.org/doi/abs/10.1145/1553374.1553453)

## ✍️ تمارين: الشبكات العصبية الالتفافية

لنواصل استكشاف كيفية عمل الشبكات العصبية الالتفافية، وكيف يمكننا تحقيق مرشحات قابلة للتدريب، من خلال العمل على دفاتر الملاحظات التالية:

* [الشبكات العصبية الالتفافية - PyTorch](../../../../../lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb)
* [الشبكات العصبية الالتفافية - TensorFlow](../../../../../lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb)

## بنية الهرم

معظم الشبكات العصبية الالتفافية المستخدمة في معالجة الصور تتبع ما يسمى بنية الهرم. الطبقة الالتفافية الأولى المطبقة على الصور الأصلية تحتوي عادةً على عدد قليل نسبيًا من المرشحات (8-16)، والتي تتوافق مع تركيبات البكسلات المختلفة، مثل الخطوط الأفقية/العمودية. في المستوى التالي، نقوم بتقليل الأبعاد المكانية للشبكة، وزيادة عدد المرشحات، مما يتوافق مع المزيد من تركيبات الميزات البسيطة. مع كل طبقة، بينما نتحرك نحو المصنف النهائي، تقل الأبعاد المكانية للصورة، ويزداد عدد المرشحات.

كمثال، دعونا نلقي نظرة على بنية VGG-16، وهي شبكة حققت دقة 92.7% في تصنيف ImageNet ضمن أفضل 5 في عام 2014:

![طبقات ImageNet](../../../../../translated_images/vgg-16-arch1.d901a5583b3a51baeaab3e768567d921e5d54befa46e1e642616c5458c934028.ar.jpg)

![هرم ImageNet](../../../../../translated_images/vgg-16-arch.64ff2137f50dd49fdaa786e3f3a975b3f22615efd13efb19c5d22f12e01451a1.ar.jpg)

> الصورة من [Researchgate](https://www.researchgate.net/figure/Vgg16-model-structure-To-get-the-VGG-NIN-model-we-replace-the-2-nd-4-th-6-th-7-th_fig2_335194493)

## أشهر بنى الشبكات العصبية الالتفافية

[تابع دراستك حول أشهر بنى الشبكات العصبية الالتفافية](CNN_Architectures.md)

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حساسة أو هامة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.