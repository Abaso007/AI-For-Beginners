<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-26T09:03:25+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "ar"
}
-->
# المشفرات التلقائية

عند تدريب الشبكات العصبية الالتفافية (CNNs)، واحدة من المشاكل هي أننا نحتاج إلى الكثير من البيانات الموصوفة. في حالة تصنيف الصور، نحتاج إلى فصل الصور إلى فئات مختلفة، وهو جهد يدوي.

## [اختبار ما قبل المحاضرة](https://ff-quizzes.netlify.app/en/ai/quiz/17)

ومع ذلك، قد نرغب في استخدام البيانات الخام (غير الموصوفة) لتدريب مستخلصات ميزات الشبكات العصبية الالتفافية، وهو ما يسمى **التعلم الذاتي الإشراف**. بدلاً من استخدام التصنيفات، سنستخدم صور التدريب كمدخلات ومخرجات للشبكة. الفكرة الرئيسية لـ **المشفر التلقائي** هي أننا سنمتلك **شبكة مشفرة** تقوم بتحويل الصورة المدخلة إلى **فضاء كامن** (عادةً ما يكون مجرد متجه بحجم أصغر)، ثم **شبكة فك التشفير**، التي يكون هدفها إعادة بناء الصورة الأصلية.

> ✅ [المشفر التلقائي](https://wikipedia.org/wiki/Autoencoder) هو "نوع من الشبكات العصبية الاصطناعية يُستخدم لتعلم ترميز فعال للبيانات غير الموصوفة."

بما أننا نقوم بتدريب المشفر التلقائي لالتقاط أكبر قدر ممكن من المعلومات من الصورة الأصلية لإعادة البناء بدقة، تحاول الشبكة العثور على أفضل **تمثيل** للصور المدخلة لالتقاط المعنى.

![رسم توضيحي للمشفر التلقائي](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.ar.jpg)

> الصورة من [مدونة Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## سيناريوهات استخدام المشفرات التلقائية

بينما قد لا يبدو إعادة بناء الصور الأصلية مفيدًا بحد ذاته، هناك بعض السيناريوهات التي تكون فيها المشفرات التلقائية مفيدة بشكل خاص:

* **تقليل أبعاد الصور للتصور** أو **تدريب تمثيلات الصور**. عادةً ما تقدم المشفرات التلقائية نتائج أفضل من تحليل المكونات الرئيسية (PCA)، لأنها تأخذ في الاعتبار الطبيعة المكانية للصور والميزات الهرمية.
* **إزالة الضوضاء**، أي إزالة الضوضاء من الصورة. نظرًا لأن الضوضاء تحمل الكثير من المعلومات غير المفيدة، لا يمكن للمشفر التلقائي تضمينها كلها في الفضاء الكامن الصغير نسبيًا، وبالتالي يلتقط فقط الجزء المهم من الصورة. عند تدريب أدوات إزالة الضوضاء، نبدأ بالصور الأصلية، ونستخدم الصور التي أُضيفت إليها ضوضاء بشكل اصطناعي كمدخلات للمشفر التلقائي.
* **زيادة الدقة**، أي تحسين دقة الصورة. نبدأ بالصور عالية الدقة، ونستخدم الصورة ذات الدقة المنخفضة كمدخلات للمشفر التلقائي.
* **النماذج التوليدية**. بمجرد تدريب المشفر التلقائي، يمكن استخدام جزء فك التشفير لإنشاء كائنات جديدة بدءًا من متجهات كامنة عشوائية.

## المشفرات التلقائية التباينية (VAE)

المشفرات التلقائية التقليدية تقلل أبعاد بيانات الإدخال بطريقة ما، وتكتشف الميزات المهمة للصور المدخلة. ومع ذلك، غالبًا ما تكون المتجهات الكامنة غير مفهومة. بمعنى آخر، إذا أخذنا مجموعة بيانات MNIST كمثال، فإن اكتشاف الأرقام التي تتوافق مع المتجهات الكامنة المختلفة ليس مهمة سهلة، لأن المتجهات الكامنة القريبة قد لا تتوافق بالضرورة مع نفس الأرقام.

من ناحية أخرى، لتدريب النماذج التوليدية، من الأفضل أن يكون لدينا فهم للفضاء الكامن. هذه الفكرة تقودنا إلى **المشفر التلقائي التبايني** (VAE).

VAE هو مشفر تلقائي يتعلم التنبؤ بـ *التوزيع الإحصائي* للمعلمات الكامنة، ما يسمى بـ **التوزيع الكامن**. على سبيل المثال، قد نرغب في أن تكون المتجهات الكامنة موزعة بشكل طبيعي مع متوسط z<sub>mean</sub> وانحراف معياري z<sub>sigma</sub> (كلاهما متوسط وانحراف معياري هما متجهان بأبعاد معينة d). يتعلم المشفر في VAE التنبؤ بهذه المعلمات، ثم يأخذ فك التشفير متجهًا عشوائيًا من هذا التوزيع لإعادة بناء الكائن.

لتلخيص:

* من المتجه المدخل، نتنبأ بـ `z_mean` و `z_log_sigma` (بدلاً من التنبؤ بالانحراف المعياري نفسه، نتنبأ باللوغاريتم الخاص به)
* نأخذ عينة `sample` من التوزيع N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
* يحاول فك التشفير إعادة بناء الصورة الأصلية باستخدام `sample` كمتجه مدخل

<img src="images/vae.png" width="50%">

> الصورة من [هذه المدونة](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) بواسطة Isaak Dykeman

تستخدم المشفرات التلقائية التباينية وظيفة خسارة معقدة تتكون من جزئين:

* **خسارة إعادة البناء** هي وظيفة الخسارة التي تظهر مدى قرب الصورة المعاد بناؤها من الهدف (يمكن أن تكون متوسط الخطأ التربيعي، أو MSE). وهي نفس وظيفة الخسارة كما في المشفرات التلقائية العادية.
* **خسارة KL**، التي تضمن أن توزيعات المتغيرات الكامنة تبقى قريبة من التوزيع الطبيعي. وهي تعتمد على مفهوم [تباعد كولباك-ليبلر](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - وهو مقياس لتقدير مدى تشابه توزيعين إحصائيين.

ميزة مهمة للمشفرات التلقائية التباينية هي أنها تسمح لنا بتوليد صور جديدة بسهولة نسبية، لأننا نعرف التوزيع الذي نأخذ منه عينات المتجهات الكامنة. على سبيل المثال، إذا قمنا بتدريب VAE مع متجه كامن ثنائي الأبعاد على MNIST، يمكننا بعد ذلك تغيير مكونات المتجه الكامن للحصول على أرقام مختلفة:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> الصورة بواسطة [Dmitry Soshnikov](http://soshnikov.com)

لاحظ كيف تمتزج الصور مع بعضها البعض، حيث نبدأ في الحصول على متجهات كامنة من أجزاء مختلفة من فضاء المعلمات الكامنة. يمكننا أيضًا تصور هذا الفضاء في بعدين:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> الصورة بواسطة [Dmitry Soshnikov](http://soshnikov.com)

## ✍️ تمارين: المشفرات التلقائية

تعرف على المزيد حول المشفرات التلقائية في دفاتر الملاحظات التالية:

* [المشفرات التلقائية باستخدام TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [المشفرات التلقائية باستخدام PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## خصائص المشفرات التلقائية

* **محددة البيانات** - تعمل بشكل جيد فقط مع نوع الصور التي تم تدريبها عليها. على سبيل المثال، إذا قمنا بتدريب شبكة زيادة الدقة على صور الزهور، فلن تعمل بشكل جيد على الصور الشخصية. وذلك لأن الشبكة يمكنها إنتاج صورة عالية الدقة من خلال أخذ التفاصيل الدقيقة من الميزات التي تعلمتها من مجموعة بيانات التدريب.
* **فقدان البيانات** - الصورة المعاد بناؤها ليست مطابقة للصورة الأصلية. طبيعة الفقد تُحدد بواسطة *وظيفة الخسارة* المستخدمة أثناء التدريب.
* تعمل على **البيانات غير الموصوفة**

## [اختبار ما بعد المحاضرة](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## الخاتمة

في هذه الدرس، تعلمت عن الأنواع المختلفة من المشفرات التلقائية المتاحة لعالم الذكاء الاصطناعي. تعلمت كيفية بنائها، وكيفية استخدامها لإعادة بناء الصور. كما تعلمت عن VAE وكيفية استخدامه لتوليد صور جديدة.

## 🚀 تحدي

في هذا الدرس، تعلمت عن استخدام المشفرات التلقائية للصور. ولكن يمكن استخدامها أيضًا للموسيقى! تحقق من مشروع Magenta [MusicVAE](https://magenta.tensorflow.org/music-vae)، الذي يستخدم المشفرات التلقائية لتعلم إعادة بناء الموسيقى. قم ببعض [التجارب](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) باستخدام هذه المكتبة لترى ما يمكنك إنشاؤه.

## [اختبار ما بعد المحاضرة](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## المراجعة والدراسة الذاتية

للمراجعة، اقرأ المزيد عن المشفرات التلقائية في هذه الموارد:

* [بناء المشفرات التلقائية باستخدام Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [مقالة في مدونة NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [شرح المشفرات التلقائية التباينية](https://kvfrans.com/variational-autoencoders-explained/)
* [المشفرات التلقائية التباينية الشرطية](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## الواجب

في نهاية [دفتر الملاحظات باستخدام TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)، ستجد "مهمة" - استخدمها كواجبك.

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حساسة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.