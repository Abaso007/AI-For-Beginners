<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9c592c26aca16ca085d268c732284187",
  "translation_date": "2025-08-26T10:17:26+00:00",
  "source_file": "lessons/X-Extras/X1-MultiModal/README.md",
  "language_code": "ar"
}
-->
# الشبكات متعددة الوسائط

بعد نجاح نماذج المحولات (Transformers) في حل مهام معالجة اللغة الطبيعية (NLP)، تم تطبيق نفس البنى أو مشابهة لها على مهام رؤية الحاسوب. هناك اهتمام متزايد ببناء نماذج تجمع بين قدرات الرؤية واللغة الطبيعية. إحدى هذه المحاولات قامت بها OpenAI، وتُعرف بـ CLIP وDALL.E.

## التدريب التبايني للصور (CLIP)

الفكرة الرئيسية لـ CLIP هي القدرة على مقارنة النصوص التوضيحية مع الصور وتحديد مدى تطابق الصورة مع النص.

![بنية CLIP](../../../../../translated_images/clip-arch.b3dbf20b4e8ed8be1c38e2bc6100fd3cc257c33cda4692b301be91f791b13ea7.ar.png)

> *الصورة مأخوذة من [هذه المقالة](https://openai.com/blog/clip/)*

يتم تدريب النموذج على صور مأخوذة من الإنترنت مع تسميات توضيحية لها. لكل دفعة، نأخذ N زوجًا من (صورة، نص)، ونحولها إلى تمثيلات متجهية I وT. 

تتم مطابقة هذه التمثيلات معًا. يتم تعريف دالة الخسارة بحيث تزيد من التشابه الكوني (cosine similarity) بين المتجهات التي تمثل زوجًا واحدًا (مثل I وT)، وتقلل التشابه الكوني بين جميع الأزواج الأخرى. ولهذا السبب يُطلق على هذا النهج اسم **التبايني**.

نموذج/مكتبة CLIP متاحة من [GitHub الخاص بـ OpenAI](https://github.com/openai/CLIP). تم وصف النهج في [هذه المقالة](https://openai.com/blog/clip/)، وبالتفصيل في [هذه الورقة البحثية](https://arxiv.org/pdf/2103.00020.pdf).

بمجرد تدريب هذا النموذج، يمكننا إعطاؤه دفعة من الصور ودفعة من النصوص التوضيحية، وسيعيد لنا مصفوفة احتمالات. يمكن استخدام CLIP في عدة مهام:

**تصنيف الصور**

لنفترض أننا بحاجة إلى تصنيف الصور بين، على سبيل المثال، القطط، الكلاب والبشر. في هذه الحالة، يمكننا إعطاء النموذج صورة وسلسلة من النصوص التوضيحية: "*صورة لقط*", "*صورة لكلب*", "*صورة لإنسان*". في المتجه الناتج الذي يحتوي على 3 احتمالات، نحتاج فقط إلى اختيار المؤشر الذي يحتوي على أعلى قيمة.

![CLIP لتصنيف الصور](../../../../../translated_images/clip-class.3af42ef0b2b19369a633df5f20ddf4f5a01d6c8ffa181e9d3a0572c19f919f72.ar.png)

> *الصورة مأخوذة من [هذه المقالة](https://openai.com/blog/clip/)*

**البحث عن الصور بناءً على النصوص**

يمكننا أيضًا القيام بالعكس. إذا كان لدينا مجموعة من الصور، يمكننا تمرير هذه المجموعة إلى النموذج مع نص توضيحي - وسيعيد لنا الصورة الأكثر تطابقًا مع النص المعطى.

## ✍️ مثال: [استخدام CLIP لتصنيف الصور والبحث عن الصور](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb)

افتح دفتر الملاحظات [Clip.ipynb](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb) لرؤية CLIP قيد التنفيذ.

## توليد الصور باستخدام VQGAN+CLIP

يمكن أيضًا استخدام CLIP لتوليد الصور من نصوص توضيحية. للقيام بذلك، نحتاج إلى **نموذج توليد** يمكنه إنشاء صور بناءً على مدخلات متجهية. أحد هذه النماذج يُعرف بـ [VQGAN](https://compvis.github.io/taming-transformers/) (شبكة GAN المكممة بالمتجهات).

الأفكار الرئيسية التي تميز VQGAN عن [GAN التقليدية](../../4-ComputerVision/10-GANs/README.md) هي كالتالي:
* استخدام بنية محول تلقائي لتوليد تسلسل من الأجزاء البصرية الغنية بالسياق التي تشكل الصورة. يتم تعلم هذه الأجزاء البصرية باستخدام [الشبكات العصبية الالتفافية (CNN)](../../4-ComputerVision/07-ConvNets/README.md).
* استخدام مميز فرعي للصورة يكتشف ما إذا كانت أجزاء الصورة "حقيقية" أو "مزيفة" (على عكس النهج "الكل أو لا شيء" في GAN التقليدية).

تعرف على المزيد حول VQGAN من خلال موقع [Taming Transformers](https://compvis.github.io/taming-transformers/).

أحد الفروقات المهمة بين VQGAN وGAN التقليدية هو أن الأخيرة يمكنها إنتاج صورة جيدة من أي متجه مدخلات، بينما من المحتمل أن ينتج VQGAN صورة غير متماسكة. لذلك، نحتاج إلى توجيه عملية إنشاء الصورة بشكل أكبر، ويمكن القيام بذلك باستخدام CLIP.

![بنية VQGAN+CLIP](../../../../../translated_images/vqgan.5027fe05051dfa3101950cfa930303f66e6478b9bd273e83766731796e462d9b.ar.png)

لإنشاء صورة تتوافق مع نص توضيحي، نبدأ بمتجه ترميز عشوائي يتم تمريره عبر VQGAN لإنتاج صورة. ثم يتم استخدام CLIP لإنتاج دالة خسارة تُظهر مدى تطابق الصورة مع النص التوضيحي. الهدف بعد ذلك هو تقليل هذه الخسارة باستخدام الانتشار العكسي لتعديل معلمات متجه المدخلات.

مكتبة رائعة تنفذ VQGAN+CLIP هي [Pixray](http://github.com/pixray/pixray).

![صورة تم إنتاجها بواسطة Pixray](../../../../../translated_images/a_closeup_watercolor_portrait_of_young_male_teacher_of_literature_with_a_book.2384968e9db8a0d09dc96de938b9f95bde8a7e1c721f48f286a7795bf16d56c7.ar.png) |  ![صورة تم إنتاجها بواسطة Pixray](../../../../../translated_images/a_closeup_oil_portrait_of_young_female_teacher_of_computer_science_with_a_computer.e0b6495f210a439077e1c32cc8afdf714e634fe24dc78dc5aa45fd2f560b0ed5.ar.png) | ![صورة تم إنتاجها بواسطة Pixray](../../../../../translated_images/a_closeup_oil_portrait_of_old_male_teacher_of_math.5362e67aa7fc2683b9d36a613b364deb7454760cd39205623fc1e3938fa133c0.ar.png)
----|----|----
صورة تم إنشاؤها من النص *صورة قريبة بالألوان المائية لمعلم شاب للأدب يحمل كتابًا* | صورة تم إنشاؤها من النص *صورة قريبة بالزيت لمُدرسة شابة لعلوم الحاسوب مع جهاز كمبيوتر* | صورة تم إنشاؤها من النص *صورة قريبة بالزيت لمعلم مسن للرياضيات أمام السبورة*

> الصور من مجموعة **المعلمون الاصطناعيون** بواسطة [ديمتري سوشنيكوف](http://soshnikov.com)

## DALL-E
### [DALL-E 1](https://openai.com/research/dall-e)
DALL-E هو نسخة من GPT-3 تم تدريبها لتوليد الصور من النصوص التوضيحية. تم تدريبه باستخدام 12 مليار معلمة.

على عكس CLIP، يتلقى DALL-E النص والصورة كتيار واحد من الرموز لكل من الصور والنصوص. وبالتالي، يمكن من خلال النصوص المختلفة توليد صور بناءً على النص.

### [DALL-E 2](https://openai.com/dall-e-2)
الفرق الرئيسي بين DALL-E 1 وDALL-E 2 هو أن الأخير يولد صورًا وفنونًا أكثر واقعية.

أمثلة على الصور التي تم إنشاؤها باستخدام DALL-E:
![صورة تم إنتاجها بواسطة Pixray](../../../../../translated_images/DALL·E%202023-06-20%2015.56.56%20-%20a%20closeup%20watercolor%20portrait%20of%20young%20male%20teacher%20of%20literature%20with%20a%20book.6c235e8271d9ed10ce985d86aeb241a58518958647973af136912116b9518fce.ar.png) |  ![صورة تم إنتاجها بواسطة Pixray](../../../../../translated_images/DALL·E%202023-06-20%2015.57.43%20-%20a%20closeup%20oil%20portrait%20of%20young%20female%20teacher%20of%20computer%20science%20with%20a%20computer.f21dc4166340b6c8b4d1cb57efd1e22127407f9b28c9ac7afe11344065369e64.ar.png) | ![صورة تم إنتاجها بواسطة Pixray](../../../../../translated_images/DALL·E%202023-06-20%2015.58.42%20-%20%20a%20closeup%20oil%20portrait%20of%20old%20male%20teacher%20of%20mathematics%20in%20front%20of%20blackboard.d331c2dfbdc3f7c46aa65c0809066f5e7ed4b49609cd259852e760df21051e4a.ar.png)
----|----|----
صورة تم إنشاؤها من النص *صورة قريبة بالألوان المائية لمعلم شاب للأدب يحمل كتابًا* | صورة تم إنشاؤها من النص *صورة قريبة بالزيت لمُدرسة شابة لعلوم الحاسوب مع جهاز كمبيوتر* | صورة تم إنشاؤها من النص *صورة قريبة بالزيت لمعلم مسن للرياضيات أمام السبورة*

## المراجع

* ورقة VQGAN: [Taming Transformers for High-Resolution Image Synthesis](https://compvis.github.io/taming-transformers/paper/paper.pdf)
* ورقة CLIP: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.