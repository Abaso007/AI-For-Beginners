{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## التضمينات\n",
    "\n",
    "في المثال السابق، عملنا على متجهات حقيبة الكلمات عالية الأبعاد بطول `vocab_size`، وقمنا بتحويل متجهات التمثيل الموضعي منخفضة الأبعاد بشكل صريح إلى تمثيل نادر باستخدام الترميز الواحد. هذا التمثيل الواحد ليس فعالاً من حيث الذاكرة. بالإضافة إلى ذلك، يتم التعامل مع كل كلمة بشكل مستقل عن الأخرى، لذا فإن المتجهات المشفرة باستخدام الترميز الواحد لا تعبر عن التشابهات الدلالية بين الكلمات.\n",
    "\n",
    "في هذه الوحدة، سنواصل استكشاف مجموعة بيانات **News AG**. للبدء، دعونا نقوم بتحميل البيانات ونسترجع بعض التعريفات من الوحدة السابقة.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ما هو التضمين؟\n",
    "\n",
    "فكرة **التضمين** هي تمثيل الكلمات باستخدام متجهات كثيفة ذات أبعاد أقل تعكس المعنى الدلالي للكلمة. سنتحدث لاحقًا عن كيفية بناء تضمينات كلمات ذات معنى، ولكن في الوقت الحالي دعونا نفكر في التضمينات كوسيلة لتقليل أبعاد متجه الكلمة.\n",
    "\n",
    "لذلك، تأخذ طبقة التضمين كلمة كمدخل، وتنتج متجهًا كخرج بحجم `embedding_size` المحدد. من ناحية ما، تشبه طبقة `Dense`، ولكن بدلاً من أخذ متجه مشفر بطريقة one-hot كمدخل، يمكنها أخذ رقم الكلمة.\n",
    "\n",
    "باستخدام طبقة التضمين كأول طبقة في شبكتنا، يمكننا الانتقال من نموذج حقيبة الكلمات إلى نموذج **حقيبة التضمينات**، حيث نقوم أولاً بتحويل كل كلمة في النص إلى التضمين المقابل لها، ثم نحسب دالة تجميع معينة على جميع تلك التضمينات، مثل `sum` أو `average` أو `max`.\n",
    "\n",
    "![صورة توضح مصنف تضمين لخمس كلمات متتالية.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ar.png)\n",
    "\n",
    "تتكون شبكة التصنيف العصبية الخاصة بنا من الطبقات التالية:\n",
    "\n",
    "* طبقة `TextVectorization`، التي تأخذ سلسلة نصية كمدخل، وتنتج موترًا يحتوي على أرقام الرموز. سنحدد حجم مفردات معقول `vocab_size`، ونتجاهل الكلمات الأقل استخدامًا. سيكون شكل المدخل 1، وشكل الخرج $n$، حيث سنحصل على $n$ رمزًا كنتيجة، كل منها يحتوي على أرقام من 0 إلى `vocab_size`.\n",
    "* طبقة `Embedding`، التي تأخذ $n$ أرقام، وتقلل كل رقم إلى متجه كثيف بطول معين (100 في مثالنا). وبالتالي، سيتم تحويل موتر المدخل الذي شكله $n$ إلى موتر شكله $n\\times 100$.\n",
    "* طبقة التجميع، التي تأخذ متوسط هذا الموتر على طول المحور الأول، أي أنها ستحسب متوسط جميع موترات المدخل $n$ التي تمثل كلمات مختلفة. لتنفيذ هذه الطبقة، سنستخدم طبقة `Lambda`، ونمرر إليها الدالة لحساب المتوسط. سيكون شكل الخرج 100، وسيكون التمثيل الرقمي للتسلسل النصي بالكامل.\n",
    "* المصنف الخطي النهائي `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         3000000   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,    \n",
    "    keras.layers.Embedding(vocab_size,100),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "في ملخص الطباعة، في عمود **شكل الإخراج**، يشير البُعد الأول للتنسور `None` إلى حجم الدفعة الصغيرة، بينما يشير البُعد الثاني إلى طول تسلسل الرموز. جميع تسلسلات الرموز في الدفعة الصغيرة لها أطوال مختلفة. سنتحدث عن كيفية التعامل مع ذلك في القسم التالي.\n",
    "\n",
    "الآن دعونا ندرب الشبكة:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 20ms/step - loss: 0.7891 - acc: 0.8155 - val_loss: 0.4470 - val_acc: 0.8642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22255515100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **ملاحظة** أننا نقوم ببناء المتجهات بناءً على مجموعة فرعية من البيانات. يتم ذلك لتسريع العملية، وقد يؤدي ذلك إلى حالة حيث لا تكون جميع الرموز من نصنا موجودة في المفردات. في هذه الحالة، سيتم تجاهل تلك الرموز، مما قد يؤدي إلى دقة أقل قليلاً. ومع ذلك، في الحياة الواقعية، غالبًا ما تعطي مجموعة فرعية من النص تقديرًا جيدًا للمفردات.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### التعامل مع أحجام تسلسل المتغيرات\n",
    "\n",
    "لنفهم كيف تتم عملية التدريب في مجموعات صغيرة. في المثال أعلاه، يكون للمدخلات شكل بعد واحد، ونستخدم مجموعات صغيرة بطول 128، بحيث يكون الحجم الفعلي للمدخلات هو $128 \\times 1$. ومع ذلك، يختلف عدد الرموز في كل جملة. إذا قمنا بتطبيق طبقة `TextVectorization` على مدخل واحد، فإن عدد الرموز الناتجة يختلف بناءً على كيفية تقسيم النص إلى رموز:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
      "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer('Hello, world!'))\n",
    "print(vectorizer('I am glad to meet you!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ومع ذلك، عندما نطبق المُوجِّه على عدة تسلسلات، يجب أن يُنتِج موترًا ذو شكل مستطيل، لذلك يملأ العناصر غير المستخدمة برمز PAD (والذي في حالتنا هو صفر):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[   1,   45,    0,    0,    0,    0],\n",
       "       [ 112, 1271,    1,    3, 1747,  158]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['Hello, world!','I am glad to meet you!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "هنا يمكننا رؤية التضمينات:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [ 2.57456154e-01,  2.79364467e-01, -2.03605562e-01, ...,\n",
       "         -2.07474351e-01,  8.31158683e-02, -2.03911960e-01],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02],\n",
       "        [ 3.98201384e-02, -8.03454965e-03,  2.39790026e-02, ...,\n",
       "         -7.18549127e-04,  2.66963355e-02, -4.30646613e-02]],\n",
       "\n",
       "       [[ 1.89674050e-01,  2.61548996e-01, -3.67433839e-02, ...,\n",
       "         -2.07366899e-01, -1.05442435e-01, -2.36952081e-01],\n",
       "        [ 6.16133213e-02,  1.80511594e-01,  9.77298319e-02, ...,\n",
       "         -5.46628237e-02, -1.07340455e-01, -1.06589928e-01],\n",
       "        [ 1.53059261e-02,  6.80514947e-02,  3.14026810e-02, ...,\n",
       "         -8.92002955e-02,  1.52911525e-04, -5.65562584e-02],\n",
       "        [-4.84890305e-02, -8.41715634e-02,  1.51529670e-01, ...,\n",
       "          1.28192469e-01, -7.77286515e-02,  1.26041949e-01],\n",
       "        [-4.17212099e-02, -5.60694858e-02,  4.08860669e-02, ...,\n",
       "          8.70475471e-02,  8.92383084e-02,  1.67974353e-01],\n",
       "        [ 2.85779923e-01,  4.57767487e-01,  4.52292450e-02, ...,\n",
       "         -1.97419018e-01, -2.04659685e-01, -2.79758364e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ملاحظة**: لتقليل كمية الحشو، في بعض الحالات يكون من المنطقي ترتيب جميع التسلسلات في مجموعة البيانات حسب زيادة الطول (أو بشكل أدق، حسب عدد الرموز). سيضمن ذلك أن يحتوي كل دفعة صغيرة على تسلسلات ذات أطوال متشابهة.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## التضمينات الدلالية: Word2Vec\n",
    "\n",
    "في المثال السابق، تعلمت طبقة التضمين كيفية تحويل الكلمات إلى تمثيلات متجهة، ولكن هذه التمثيلات لم تكن تحمل معنى دلاليًا. سيكون من الجيد تعلم تمثيل متجه بحيث تكون الكلمات المتشابهة أو المرادفات قريبة من بعضها البعض من حيث مسافة المتجه (على سبيل المثال، المسافة الإقليدية).\n",
    "\n",
    "للقيام بذلك، نحتاج إلى تدريب نموذج التضمين مسبقًا على مجموعة كبيرة من النصوص باستخدام تقنية مثل [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). تعتمد هذه التقنية على بنيتين رئيسيتين تُستخدمان لإنتاج تمثيل موزع للكلمات:\n",
    "\n",
    "- **الحقيبة المستمرة للكلمات** (CBoW)، حيث نقوم بتدريب النموذج للتنبؤ بكلمة من السياق المحيط. بالنظر إلى النغرام $(W_{-2},W_{-1},W_0,W_1,W_2)$، هدف النموذج هو التنبؤ بـ $W_0$ من $(W_{-2},W_{-1},W_1,W_2)$.\n",
    "- **التخطي المستمر للكلمات** هو عكس CBoW. يستخدم النموذج نافذة الكلمات المحيطة بالسياق للتنبؤ بالكلمة الحالية.\n",
    "\n",
    "CBoW أسرع، بينما التخطي المستمر أبطأ ولكنه يقوم بتمثيل الكلمات النادرة بشكل أفضل.\n",
    "\n",
    "![صورة توضح كلا من خوارزميات CBoW و Skip-Gram لتحويل الكلمات إلى متجهات.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ar.png)\n",
    "\n",
    "لتجربة تضمين Word2Vec المدرب مسبقًا على مجموعة بيانات أخبار Google، يمكننا استخدام مكتبة **gensim**. أدناه نجد الكلمات الأكثر تشابهًا مع كلمة 'neural'.\n",
    "\n",
    "> **ملاحظة:** عند إنشاء متجهات الكلمات لأول مرة، قد يستغرق تنزيلها بعض الوقت!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يمكننا أيضًا استخراج تضمين المتجه من الكلمة، لاستخدامه في تدريب نموذج التصنيف. يحتوي التضمين على 300 مكون، ولكن هنا نعرض فقط أول 20 مكونًا من المتجه للتوضيح:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['play'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الشيء الرائع بشأن التضمينات الدلالية هو أنه يمكنك التلاعب بترميز المتجه بناءً على الدلالات. على سبيل المثال، يمكننا أن نطلب العثور على كلمة يكون تمثيلها المتجهي قريبًا قدر الإمكان من الكلمات *ملك* و *امرأة*، وبعيدًا قدر الإمكان عن كلمة *رجل*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "يستخدم المثال أعلاه بعض السحر الداخلي لـ GenSym، لكن المنطق الأساسي بسيط جدًا في الواقع. الشيء المثير للاهتمام حول التضمينات هو أنه يمكنك إجراء عمليات المتجه العادية على متجهات التضمين، وسيعكس ذلك العمليات على **معاني** الكلمات. يمكن التعبير عن المثال أعلاه من حيث عمليات المتجه: نحسب المتجه المقابل لـ **ملك-رجل+امرأة** (تُجرى العمليات `+` و `-` على تمثيلات المتجهات للكلمات المقابلة)، ثم نجد الكلمة الأقرب في القاموس لذلك المتجه:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
    "# find the index of the closest embedding vector \n",
    "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
    "min_idx = np.argmin(d)\n",
    "# find the corresponding word\n",
    "w2v.index_to_key[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ملاحظة**: اضطررنا إلى إضافة معاملات صغيرة إلى متجهات *man* و *woman* - حاول إزالة هذه المعاملات لترى ما سيحدث.\n",
    "\n",
    "للعثور على أقرب متجه، نستخدم أدوات TensorFlow لحساب متجه المسافات بين متجهنا وجميع المتجهات في المفردات، ثم نحدد مؤشر الكلمة الأدنى باستخدام `argmin`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "بينما يبدو أن Word2Vec طريقة رائعة للتعبير عن دلالات الكلمات، إلا أنه يعاني من العديد من العيوب، بما في ذلك ما يلي:\n",
    "\n",
    "* كلا النموذجين CBoW و skip-gram هما **تضمينات تنبؤية**، ويأخذان في الاعتبار السياق المحلي فقط. Word2Vec لا يستفيد من السياق العام.\n",
    "* Word2Vec لا يأخذ في الاعتبار **مورفولوجيا** الكلمة، أي حقيقة أن معنى الكلمة يمكن أن يعتمد على أجزاء مختلفة منها، مثل الجذر.\n",
    "\n",
    "**FastText** يحاول التغلب على القيد الثاني، ويبني على Word2Vec من خلال تعلم تمثيلات المتجهات لكل كلمة والن-غرامات الحرفية الموجودة داخل كل كلمة. يتم بعد ذلك حساب متوسط قيم التمثيلات في متجه واحد في كل خطوة تدريب. على الرغم من أن هذا يضيف الكثير من العمليات الحسابية الإضافية أثناء التدريب المسبق، إلا أنه يمكّن تضمينات الكلمات من تشفير معلومات الأجزاء الفرعية للكلمة.\n",
    "\n",
    "طريقة أخرى، **GloVe**، تستخدم نهجًا مختلفًا لتضمينات الكلمات، يعتمد على تحليل مصفوفة سياق الكلمات. أولاً، يقوم ببناء مصفوفة كبيرة تحصي عدد مرات ظهور الكلمات في سياقات مختلفة، ثم يحاول تمثيل هذه المصفوفة في أبعاد أقل بطريقة تقلل من خسارة إعادة البناء.\n",
    "\n",
    "مكتبة gensim تدعم هذه التضمينات، ويمكنك تجربتها عن طريق تغيير كود تحميل النموذج أعلاه.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## استخدام التضمينات المدربة مسبقًا في Keras\n",
    "\n",
    "يمكننا تعديل المثال أعلاه لملء المصفوفة في طبقة التضمين الخاصة بنا بتضمينات دلالية، مثل Word2Vec. من المحتمل أن لا تتطابق مفردات التضمين المدرب مسبقًا مع مفردات النص، لذا علينا اختيار واحدة. هنا نستكشف الخيارين الممكنين: استخدام مفردات الـ tokenizer، واستخدام المفردات من تضمينات Word2Vec.\n",
    "\n",
    "### استخدام مفردات الـ tokenizer\n",
    "\n",
    "عند استخدام مفردات الـ tokenizer، بعض الكلمات من المفردات سيكون لها تضمينات Word2Vec المقابلة، وبعضها سيكون مفقودًا. بالنظر إلى أن حجم المفردات لدينا هو `vocab_size`، وطول متجه تضمين Word2Vec هو `embed_size`، فإن طبقة التضمين سيتم تمثيلها بمصفوفة أوزان ذات شكل `vocab_size`$\\times$`embed_size`. سنقوم بملء هذه المصفوفة من خلال المرور عبر المفردات:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size,embed_size))\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found+=1\n",
    "    except:\n",
    "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "بالنسبة للكلمات التي لا توجد في مفردات Word2Vec، يمكننا إما تركها كأصفار، أو إنشاء متجه عشوائي.\n",
    "\n",
    "الآن يمكننا تعريف طبقة التضمين باستخدام الأوزان المدربة مسبقًا:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1075 - acc: 0.7822 - val_loss: 0.9134 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220226ef10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ملاحظة**: لاحظ أننا قمنا بتعيين `trainable=False` عند إنشاء `Embedding`، مما يعني أننا لن نقوم بإعادة تدريب طبقة الـ Embedding. قد يؤدي ذلك إلى انخفاض طفيف في الدقة، ولكنه يسرّع عملية التدريب.\n",
    "\n",
    "### استخدام مفردات التضمين\n",
    "\n",
    "إحدى المشكلات في النهج السابق هي أن المفردات المستخدمة في `TextVectorization` و `Embedding` مختلفة. للتغلب على هذه المشكلة، يمكننا استخدام أحد الحلول التالية:\n",
    "* إعادة تدريب نموذج Word2Vec على مفرداتنا.\n",
    "* تحميل بياناتنا باستخدام المفردات من نموذج Word2Vec المدرب مسبقًا. يمكن تحديد المفردات المستخدمة لتحميل البيانات أثناء عملية التحميل.\n",
    "\n",
    "يبدو أن النهج الأخير أسهل، لذا دعونا نقوم بتنفيذه. أولاً، سنقوم بإنشاء طبقة `TextVectorization` مع المفردات المحددة، المأخوذة من تضمينات Word2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.vocab.keys())\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تحتوي مكتبة تضمين الكلمات gensim على وظيفة مريحة، `get_keras_embeddings`، والتي ستقوم تلقائيًا بإنشاء طبقة تضمين Keras المقابلة لك.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 20s 14ms/step - loss: 1.3377 - acc: 0.4978 - val_loss: 1.2995 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.2587 - acc: 0.5722 - val_loss: 1.2339 - val_acc: 0.5842\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 1.1980 - acc: 0.5884 - val_loss: 1.1826 - val_acc: 0.5954\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 1.1503 - acc: 0.6002 - val_loss: 1.1417 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.1120 - acc: 0.6097 - val_loss: 1.1083 - val_acc: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2220ccb81c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer, \n",
    "    w2v.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "أحد الأسباب التي تجعلنا لا نرى دقة أعلى هو أن بعض الكلمات من مجموعة البيانات الخاصة بنا مفقودة في مفردات GloVe المدربة مسبقًا، وبالتالي يتم تجاهلها بشكل أساسي. للتغلب على ذلك، يمكننا تدريب التضمينات الخاصة بنا بناءً على مجموعة البيانات الخاصة بنا.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## التضمينات السياقية\n",
    "\n",
    "أحد القيود الرئيسية لتمثيلات التضمينات المدربة مسبقًا مثل Word2Vec هو أنها، على الرغم من قدرتها على التقاط بعض معاني الكلمة، لا تستطيع التمييز بين المعاني المختلفة. يمكن أن يتسبب هذا في مشاكل في النماذج اللاحقة.\n",
    "\n",
    "على سبيل المثال، كلمة \"play\" لها معانٍ مختلفة في الجملتين التاليتين:\n",
    "- ذهبت إلى **عرض مسرحي** في المسرح.\n",
    "- جون يريد أن **يلعب** مع أصدقائه.\n",
    "\n",
    "التضمينات المدربة مسبقًا التي تحدثنا عنها تمثل كلا المعنيين لكلمة \"play\" بنفس التضمين. للتغلب على هذا القيد، نحتاج إلى بناء تضمينات تعتمد على **نموذج اللغة**، الذي يتم تدريبه على مجموعة كبيرة من النصوص، و*يعرف* كيف يمكن للكلمات أن تتجمع معًا في سياقات مختلفة. مناقشة التضمينات السياقية خارج نطاق هذا الدرس، لكننا سنعود إليها عند الحديث عن نماذج اللغة في الوحدة التالية.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**إخلاء المسؤولية**:  \nتم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "b859482be7f61d1eadc2c6a2720a37e4",
   "translation_date": "2025-08-28T04:29:06+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb",
   "language_code": "ar"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}