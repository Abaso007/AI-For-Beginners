<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-26T08:41:52+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "ar"
}
-->
# نماذج اللغة الكبيرة المدربة مسبقًا

في جميع المهام السابقة، كنا نقوم بتدريب شبكة عصبية لأداء مهمة معينة باستخدام مجموعة بيانات مرفقة بعلامات. مع النماذج الكبيرة القائمة على المحولات، مثل BERT، نستخدم نمذجة اللغة بطريقة ذاتية الإشراف لبناء نموذج لغة، يتم تخصيصه لاحقًا لمهام محددة باستخدام تدريب إضافي خاص بالمجال. ومع ذلك، فقد تم إثبات أن النماذج الكبيرة للغة يمكنها أيضًا حل العديد من المهام دون أي تدريب خاص بالمجال. عائلة النماذج القادرة على القيام بذلك تُسمى **GPT**: المحول المدرب مسبقًا التوليدي.

## [اختبار ما قبل المحاضرة](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/120)

## توليد النصوص ومقياس الحيرة

فكرة أن الشبكة العصبية يمكنها أداء مهام عامة دون تدريب إضافي تم تقديمها في ورقة بحثية بعنوان [نماذج اللغة هي متعلمون متعددون غير مشرف عليهم](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). الفكرة الرئيسية هي أن العديد من المهام الأخرى يمكن نمذجتها باستخدام **توليد النصوص**، لأن فهم النص يعني بشكل أساسي القدرة على إنتاجه. وبما أن النموذج يتم تدريبه على كمية هائلة من النصوص التي تشمل المعرفة البشرية، فإنه يصبح أيضًا على دراية بمجموعة واسعة من المواضيع.

> فهم النصوص والقدرة على إنتاجها يتطلب أيضًا معرفة شيء عن العالم من حولنا. الناس يتعلمون أيضًا إلى حد كبير من خلال القراءة، وشبكة GPT مشابهة في هذا الجانب.

شبكات توليد النصوص تعمل عن طريق توقع احتمال الكلمة التالية $$P(w_N)$$ ومع ذلك، فإن الاحتمال غير المشروط للكلمة التالية يساوي تكرار هذه الكلمة في مجموعة النصوص. GPT يمكنه إعطاؤنا **الاحتمال الشرطي** للكلمة التالية، بناءً على الكلمات السابقة: $$P(w_N | w_{n-1}, ..., w_0)$$

> يمكنك قراءة المزيد عن الاحتمالات في [منهج علم البيانات للمبتدئين](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability)

جودة نموذج توليد النصوص يمكن تعريفها باستخدام **مقياس الحيرة**. وهو مقياس داخلي يسمح لنا بقياس جودة النموذج دون الحاجة إلى مجموعة بيانات خاصة بالمهمة. يعتمد على مفهوم *احتمال الجملة* - النموذج يعطي احتمالًا عاليًا للجمل التي من المحتمل أن تكون حقيقية (أي أن النموذج ليس **مرتبكًا** بسببها)، واحتمالًا منخفضًا للجمل التي تكون أقل منطقية (مثل *هل يمكنه يفعل ماذا؟*). عندما نعطي النموذج جملًا من مجموعة نصوص حقيقية، نتوقع أن تكون احتمالاتها عالية، ومقياس الحيرة منخفض. رياضيًا، يتم تعريفه كمعكوس الاحتمال الطبيعي لمجموعة الاختبار:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**يمكنك تجربة توليد النصوص باستخدام [محرر النصوص المدعوم بـ GPT من Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. في هذا المحرر، تبدأ بكتابة نصك، وعند الضغط على **[TAB]** ستظهر لك عدة خيارات للإكمال. إذا كانت الخيارات قصيرة جدًا، أو لم تكن راضيًا عنها - اضغط [TAB] مرة أخرى، وستحصل على المزيد من الخيارات، بما في ذلك نصوص أطول.

## GPT عائلة من النماذج

GPT ليس نموذجًا واحدًا، بل هو مجموعة من النماذج التي تم تطويرها وتدريبها بواسطة [OpenAI](https://openai.com). 

ضمن نماذج GPT، لدينا:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|نموذج لغة يحتوي على ما يصل إلى 1.5 مليار معلمة. | نموذج لغة يحتوي على ما يصل إلى 175 مليار معلمة | 100 تريليون معلمة ويقبل مدخلات نصوص وصور ويخرج نصوصًا. |

نماذج GPT-3 و GPT-4 متوفرة [كخدمة معرفية من Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste)، وأيضًا كـ [واجهة برمجية من OpenAI](https://openai.com/api/).

## هندسة التوجيهات

نظرًا لأن GPT تم تدريبه على كميات هائلة من البيانات لفهم اللغة والرموز، فإنه يقدم مخرجات استجابة للمدخلات (التوجيهات). التوجيهات هي مدخلات أو استفسارات لـ GPT حيث يقدم المستخدم تعليمات للنماذج حول المهام التي يجب إكمالها. للحصول على النتيجة المطلوبة، تحتاج إلى التوجيه الأكثر فعالية والذي يتضمن اختيار الكلمات الصحيحة، التنسيقات، العبارات أو حتى الرموز. هذا النهج يُعرف بـ [هندسة التوجيهات](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum)

[هذا التوثيق](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) يقدم لك المزيد من المعلومات حول هندسة التوجيهات.

## ✍️ دفتر ملاحظات مثال: [اللعب مع OpenAI-GPT](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

واصل تعلمك في دفاتر الملاحظات التالية:

* [توليد النصوص باستخدام OpenAI-GPT و Hugging Face Transformers](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## الخاتمة

النماذج العامة المدربة مسبقًا للغة لا تقوم فقط بنمذجة هيكل اللغة، بل تحتوي أيضًا على كمية هائلة من اللغة الطبيعية. وبالتالي، يمكن استخدامها بشكل فعال لحل بعض مهام معالجة اللغة الطبيعية في إعدادات بدون تدريب إضافي أو تدريب محدود.

## [اختبار ما بعد المحاضرة](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/220)

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.