<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T06:37:59+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "ar"
}
-->
# نماذج اللغة الكبيرة المدربة مسبقًا

في جميع المهام السابقة، كنا نقوم بتدريب شبكة عصبية لأداء مهمة معينة باستخدام مجموعة بيانات مُعلمة. مع النماذج الكبيرة القائمة على المحولات، مثل BERT، نستخدم نمذجة اللغة بطريقة ذاتية الإشراف لبناء نموذج لغة، يتم تخصيصه لاحقًا لمهام محددة من خلال تدريب إضافي خاص بالمجال. ومع ذلك، فقد تم إثبات أن النماذج الكبيرة للغة يمكنها أيضًا حل العديد من المهام دون أي تدريب خاص بالمجال. عائلة النماذج القادرة على القيام بذلك تُسمى **GPT**: المحول المدرب مسبقًا التوليدي.

## [اختبار ما قبل المحاضرة](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## توليد النصوص ومقياس الحيرة

تم تقديم فكرة أن الشبكة العصبية يمكنها أداء مهام عامة دون تدريب إضافي في الورقة البحثية [نماذج اللغة هي متعلمون متعددون غير مشرف عليهم](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). الفكرة الرئيسية هي أن العديد من المهام الأخرى يمكن نمذجتها باستخدام **توليد النصوص**، لأن فهم النص يعني بشكل أساسي القدرة على إنتاجه. وبما أن النموذج يتم تدريبه على كمية هائلة من النصوص التي تشمل المعرفة البشرية، فإنه يصبح أيضًا على دراية بمجموعة واسعة من المواضيع.

> فهم النصوص والقدرة على إنتاجها يتطلب أيضًا معرفة شيء عن العالم من حولنا. الناس يتعلمون أيضًا إلى حد كبير من خلال القراءة، وشبكة GPT مشابهة في هذا الجانب.

تعمل شبكات توليد النصوص من خلال توقع احتمال الكلمة التالية $$P(w_N)$$. ومع ذلك، فإن الاحتمال غير المشروط للكلمة التالية يساوي تكرار هذه الكلمة في مجموعة النصوص. يمكن لـ GPT أن يقدم لنا **الاحتمال المشروط** للكلمة التالية، بناءً على الكلمات السابقة: $$P(w_N | w_{n-1}, ..., w_0)$$.

> يمكنك قراءة المزيد عن الاحتمالات في [منهج علم البيانات للمبتدئين](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability).

يمكن تعريف جودة نموذج توليد اللغة باستخدام **مقياس الحيرة**. وهو مقياس داخلي يسمح لنا بقياس جودة النموذج دون الحاجة إلى مجموعة بيانات خاصة بالمهمة. يعتمد على مفهوم *احتمالية الجملة* - حيث يقوم النموذج بتعيين احتمال مرتفع للجمل التي من المحتمل أن تكون حقيقية (أي أن النموذج ليس **مرتبكًا** بسببها)، واحتمال منخفض للجمل التي تكون أقل منطقية (مثل: *هل يمكنه يفعل ماذا؟*). عندما نقدم للنموذج جملًا من مجموعة نصوص حقيقية، نتوقع أن تكون احتمالاتها مرتفعة، وحيرتها منخفضة. رياضيًا، يتم تعريفه كمعكوس الاحتمال الطبيعي لمجموعة الاختبار:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**يمكنك تجربة توليد النصوص باستخدام [محرر النصوص المدعوم بـ GPT من Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. في هذا المحرر، تبدأ بكتابة نصك، وعند الضغط على **[TAB]** ستظهر لك عدة خيارات للإكمال. إذا كانت الخيارات قصيرة جدًا أو لم تكن راضيًا عنها - اضغط [TAB] مرة أخرى، وستحصل على المزيد من الخيارات، بما في ذلك نصوص أطول.

## GPT عائلة من النماذج

GPT ليس نموذجًا واحدًا، بل هو مجموعة من النماذج التي تم تطويرها وتدريبها بواسطة [OpenAI](https://openai.com).

ضمن نماذج GPT، لدينا:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|نموذج لغة يحتوي على ما يصل إلى 1.5 مليار معلمة. | نموذج لغة يحتوي على ما يصل إلى 175 مليار معلمة. | 100 تريليون معلمة ويقبل مدخلات نصوص وصور ويُخرج نصوصًا. |

نماذج GPT-3 و GPT-4 متوفرة [كخدمة معرفية من Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste)، وأيضًا عبر [واجهة برمجة التطبيقات من OpenAI](https://openai.com/api/).

## هندسة التوجيه

نظرًا لأن GPT تم تدريبه على كميات هائلة من البيانات لفهم اللغة والرموز، فإنه يقدم مخرجات استجابةً للمدخلات (التوجيهات). التوجيهات هي مدخلات أو استفسارات GPT حيث يقدم المستخدم تعليمات للنماذج حول المهام التي يجب إكمالها. للحصول على النتيجة المطلوبة، تحتاج إلى التوجيه الأكثر فعالية، والذي يتضمن اختيار الكلمات الصحيحة، التنسيقات، العبارات أو حتى الرموز. هذه الطريقة تُعرف بـ [هندسة التوجيه](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum).

[هذا التوثيق](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) يوفر لك المزيد من المعلومات حول هندسة التوجيه.

## ✍️ دفتر ملاحظات مثال: [التجربة مع OpenAI-GPT](GPT-PyTorch.ipynb)

واصل التعلم من خلال دفاتر الملاحظات التالية:

* [توليد النصوص باستخدام OpenAI-GPT و Hugging Face Transformers](GPT-PyTorch.ipynb)

## الخاتمة

النماذج العامة المدربة مسبقًا للغة لا تقوم فقط بنمذجة هيكل اللغة، بل تحتوي أيضًا على كميات هائلة من اللغة الطبيعية. وبالتالي، يمكن استخدامها بفعالية لحل بعض مهام معالجة اللغة الطبيعية في إعدادات بدون تدريب إضافي أو تدريب محدود.

## [اختبار ما بعد المحاضرة](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

