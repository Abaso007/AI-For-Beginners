<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-26T08:33:34+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "ar"
}
-->
# آليات الانتباه ونماذج المحولات

## [اختبار ما قبل المحاضرة](https://ff-quizzes.netlify.app/en/ai/quiz/35)

إحدى أهم المشكلات في مجال معالجة اللغة الطبيعية (NLP) هي **الترجمة الآلية**، وهي مهمة أساسية تعتمد عليها أدوات مثل Google Translate. في هذا القسم، سنركز على الترجمة الآلية، أو بشكل أعم، على أي مهمة *تحويل تسلسل إلى تسلسل* (والتي تُعرف أيضًا بـ **تحويل الجمل**).

مع الشبكات العصبية المتكررة (RNNs)، يتم تنفيذ تحويل التسلسل إلى تسلسل باستخدام شبكتين متكررتين، حيث تقوم إحدى الشبكات، وهي **المشفّر**، بضغط تسلسل الإدخال إلى حالة مخفية، بينما تقوم الشبكة الأخرى، وهي **المفسّر**، بفك هذه الحالة المخفية إلى نتيجة مترجمة. هناك بعض المشكلات في هذا النهج:

* الحالة النهائية لشبكة المشفّر تواجه صعوبة في تذكر بداية الجملة، مما يؤدي إلى ضعف جودة النموذج مع الجمل الطويلة.
* جميع الكلمات في التسلسل لها نفس التأثير على النتيجة. ولكن في الواقع، بعض الكلمات في تسلسل الإدخال غالبًا ما يكون لها تأثير أكبر على المخرجات المتتابعة من غيرها.

**آليات الانتباه** توفر وسيلة لوزن التأثير السياقي لكل متجه إدخال على كل توقع مخرجات للشبكة العصبية المتكررة. يتم تنفيذ ذلك من خلال إنشاء اختصارات بين الحالات الوسيطة لشبكة الإدخال وشبكة الإخراج. بهذه الطريقة، عند توليد رمز الإخراج y<sub>t</sub>، سنأخذ في الاعتبار جميع الحالات المخفية للإدخال h<sub>i</sub>، مع معاملات وزن مختلفة α<sub>t,i</sub>.

![صورة توضح نموذج مشفّر/مفسّر مع طبقة انتباه إضافية](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.ar.png)

> نموذج المشفّر-المفسّر مع آلية الانتباه الإضافية في [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)، مقتبس من [هذا المقال](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

مصفوفة الانتباه {α<sub>i,j</sub>} تمثل الدرجة التي تلعبها كلمات الإدخال المحددة في توليد كلمة معينة في تسلسل الإخراج. أدناه مثال على مثل هذه المصفوفة:

![صورة توضح محاذاة عينة تم العثور عليها بواسطة RNNsearch-50، مأخوذة من Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.ar.png)

> الشكل من [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (الشكل 3)

آليات الانتباه مسؤولة عن الكثير من التقدم الحالي أو القريب من الحالي في معالجة اللغة الطبيعية. ومع ذلك، فإن إضافة الانتباه تزيد بشكل كبير من عدد معلمات النموذج، مما أدى إلى مشكلات في التوسيع مع الشبكات العصبية المتكررة. أحد القيود الرئيسية لتوسيع الشبكات العصبية المتكررة هو أن الطبيعة المتكررة للنماذج تجعل من الصعب تجميع وتوازي التدريب. في الشبكات العصبية المتكررة، يجب معالجة كل عنصر من عناصر التسلسل بترتيب متسلسل، مما يعني أنه لا يمكن توازيته بسهولة.

![مشفّر مفسّر مع الانتباه](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> الشكل من [مدونة Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

اعتماد آليات الانتباه مع هذا القيد أدى إلى إنشاء نماذج المحولات الحديثة التي نعرفها ونستخدمها اليوم مثل BERT وOpen-GPT3.

## نماذج المحولات

إحدى الأفكار الرئيسية وراء المحولات هي تجنب الطبيعة التسلسلية للشبكات العصبية المتكررة وإنشاء نموذج يمكن توازيته أثناء التدريب. يتم تحقيق ذلك من خلال تنفيذ فكرتين:

* الترميز الموضعي
* استخدام آلية الانتباه الذاتي لالتقاط الأنماط بدلاً من الشبكات العصبية المتكررة (أو الشبكات العصبية الالتفافية)، ولهذا السبب تم تسمية الورقة التي قدمت المحولات بـ *[Attention is all you need](https://arxiv.org/abs/1706.03762)*.

### الترميز/التضمين الموضعي

فكرة الترميز الموضعي هي كالتالي:
1. عند استخدام الشبكات العصبية المتكررة، يتم تمثيل الموضع النسبي للرموز بعدد الخطوات، وبالتالي لا يحتاج إلى تمثيل صريح.
2. ومع ذلك، بمجرد التحول إلى الانتباه، نحتاج إلى معرفة المواضع النسبية للرموز داخل التسلسل.
3. للحصول على الترميز الموضعي، نقوم بإضافة تسلسل من مواضع الرموز إلى تسلسل الرموز (أي تسلسل الأرقام 0، 1، ...).
4. ثم نمزج موضع الرمز مع متجه تضمين الرمز. لتحويل الموضع (عدد صحيح) إلى متجه، يمكننا استخدام طرق مختلفة:

* تضمين قابل للتدريب، مشابه لتضمين الرموز. هذا هو النهج الذي نعتبره هنا. نطبق طبقات التضمين على كل من الرموز ومواضعها، مما ينتج متجهات تضمين بنفس الأبعاد، ثم نجمعها معًا.
* دالة ترميز موضعي ثابتة، كما هو مقترح في الورقة الأصلية.

<img src="images/pos-embedding.png" width="50%"/>

> الصورة من المؤلف

النتيجة التي نحصل عليها مع التضمين الموضعي تضمّن كل من الرمز الأصلي وموضعه داخل التسلسل.

### الانتباه الذاتي متعدد الرؤوس

بعد ذلك، نحتاج إلى التقاط بعض الأنماط داخل تسلسلنا. لتحقيق ذلك، تستخدم المحولات آلية **الانتباه الذاتي**، وهي في الأساس انتباه يتم تطبيقه على نفس التسلسل كمدخلات ومخرجات. تطبيق الانتباه الذاتي يسمح لنا بأخذ **السياق** داخل الجملة بعين الاعتبار، ورؤية الكلمات المترابطة. على سبيل المثال، يسمح لنا برؤية الكلمات التي تشير إليها الضمائر مثل *it*، وأيضًا أخذ السياق في الاعتبار:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.ar.png)

> الصورة من [مدونة Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

في المحولات، نستخدم **الانتباه متعدد الرؤوس** لمنح الشبكة القدرة على التقاط عدة أنواع مختلفة من العلاقات، مثل العلاقات طويلة المدى مقابل قصيرة المدى بين الكلمات، أو الإشارات المشتركة مقابل شيء آخر، إلخ.

[دفتر TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) يحتوي على مزيد من التفاصيل حول تنفيذ طبقات المحولات.

### الانتباه بين المشفّر والمفسّر

في المحولات، يتم استخدام الانتباه في مكانين:

* لالتقاط الأنماط داخل النص المدخل باستخدام الانتباه الذاتي.
* لتنفيذ ترجمة التسلسل - وهو طبقة الانتباه بين المشفّر والمفسّر.

الانتباه بين المشفّر والمفسّر مشابه جدًا لآلية الانتباه المستخدمة في الشبكات العصبية المتكررة، كما هو موضح في بداية هذا القسم. يوضح هذا الرسم المتحرك دور الانتباه بين المشفّر والمفسّر.

![رسم متحرك يوضح كيفية إجراء التقييمات في نماذج المحولات.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

نظرًا لأن كل موضع إدخال يتم تعيينه بشكل مستقل إلى كل موضع إخراج، يمكن للمحولات التوازي بشكل أفضل من الشبكات العصبية المتكررة، مما يتيح نماذج لغوية أكبر وأكثر تعبيرًا. يمكن استخدام كل رأس انتباه لتعلم علاقات مختلفة بين الكلمات، مما يحسن مهام معالجة اللغة الطبيعية.

## BERT

**BERT** (تمثيلات المشفّر ثنائية الاتجاه من المحولات) هو شبكة محولات متعددة الطبقات كبيرة جدًا تحتوي على 12 طبقة لـ *BERT-base*، و24 طبقة لـ *BERT-large*. يتم تدريب النموذج أولاً على مجموعة كبيرة من بيانات النصوص (ويكيبيديا + الكتب) باستخدام تدريب غير خاضع للإشراف (توقع الكلمات المحجوبة في الجملة). أثناء التدريب الأولي، يكتسب النموذج مستويات كبيرة من فهم اللغة، والتي يمكن استخدامها لاحقًا مع مجموعات بيانات أخرى باستخدام التخصيص. تُعرف هذه العملية بـ **التعلم بالنقل**.

![صورة من http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.ar.png)

> الصورة [المصدر](http://jalammar.github.io/illustrated-bert/)

## ✍️ تمارين: المحولات

واصل التعلم في الدفاتر التالية:

* [المحولات في PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [المحولات في TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## الخاتمة

في هذه الدرس، تعلمت عن المحولات وآليات الانتباه، وهي أدوات أساسية في صندوق أدوات معالجة اللغة الطبيعية. هناك العديد من التعديلات على بنية المحولات، بما في ذلك BERT، وDistilBERT، وBigBird، وOpenGPT3، والمزيد التي يمكن تخصيصها. حزمة [HuggingFace](https://github.com/huggingface/) توفر مستودعًا لتدريب العديد من هذه البنى باستخدام كل من PyTorch وTensorFlow.

## 🚀 التحدي

## [اختبار ما بعد المحاضرة](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## المراجعة والدراسة الذاتية

* [مقالة](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/) تشرح الورقة الكلاسيكية [Attention is all you need](https://arxiv.org/abs/1706.03762) حول المحولات.
* [سلسلة مقالات](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) حول المحولات، تشرح البنية بالتفصيل.

## [التكليف](assignment.md)

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.