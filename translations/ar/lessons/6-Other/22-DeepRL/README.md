<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-26T10:11:47+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "ar"
}
-->
# التعلم العميق بالتعزيز

يُعتبر التعلم بالتعزيز (RL) أحد الأسس الرئيسية في تعلم الآلة، إلى جانب التعلم الموجّه والتعلم غير الموجّه. بينما نعتمد في التعلم الموجّه على مجموعة بيانات ذات نتائج معروفة، فإن التعلم بالتعزيز يعتمد على **التعلم من خلال التجربة**. على سبيل المثال، عندما نرى لعبة فيديو لأول مرة، نبدأ باللعب دون معرفة القواعد، وسرعان ما نتمكن من تحسين مهاراتنا فقط من خلال عملية اللعب وتعديل سلوكنا.

## [اختبار ما قبل المحاضرة](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

لتنفيذ التعلم بالتعزيز، نحتاج إلى:

* **بيئة** أو **محاكي** يحدد قواعد اللعبة. يجب أن نتمكن من إجراء التجارب في المحاكي ومراقبة النتائج.
* **دالة مكافأة**، تشير إلى مدى نجاح تجربتنا. في حالة تعلم لعب لعبة فيديو، ستكون المكافأة هي النتيجة النهائية التي نحققها.

بناءً على دالة المكافأة، يجب أن نتمكن من تعديل سلوكنا وتحسين مهاراتنا، بحيث نلعب بشكل أفضل في المرة القادمة. الفرق الرئيسي بين أنواع التعلم الأخرى والتعلم بالتعزيز هو أنه في التعلم بالتعزيز لا نعرف عادةً ما إذا كنا سنفوز أو نخسر حتى ننهي اللعبة. وبالتالي، لا يمكننا تحديد ما إذا كانت حركة معينة جيدة أم لا - نحصل على المكافأة فقط في نهاية اللعبة.

خلال التعلم بالتعزيز، نقوم عادةً بإجراء العديد من التجارب. في كل تجربة، نحتاج إلى تحقيق توازن بين اتباع الاستراتيجية المثلى التي تعلمناها حتى الآن (**الاستغلال**) واستكشاف حالات جديدة محتملة (**الاستكشاف**).

## OpenAI Gym

أداة رائعة للتعلم بالتعزيز هي [OpenAI Gym](https://gym.openai.com/) - وهي **بيئة محاكاة** يمكنها محاكاة العديد من البيئات المختلفة بدءًا من ألعاب Atari إلى الفيزياء وراء توازن الأعمدة. تُعد واحدة من أكثر بيئات المحاكاة شيوعًا لتدريب خوارزميات التعلم بالتعزيز، ويتم صيانتها بواسطة [OpenAI](https://openai.com/).

> **Note**: يمكنك رؤية جميع البيئات المتاحة من OpenAI Gym [هنا](https://gym.openai.com/envs/#classic_control).

## توازن العربة والعمود (CartPole)

ربما رأيتم جميعًا أجهزة التوازن الحديثة مثل *Segway* أو *Gyroscooters*. يمكنها التوازن تلقائيًا عن طريق تعديل عجلاتها استجابةً لإشارة من مقياس التسارع أو الجيروسكوب. في هذا القسم، سنتعلم كيفية حل مشكلة مشابهة - توازن عمود. يشبه ذلك الوضع عندما يحتاج لاعب سيرك إلى موازنة عمود على يده - لكن هذا التوازن يحدث فقط في بُعد واحد.

نسخة مبسطة من التوازن تُعرف بمشكلة **CartPole**. في عالم CartPole، لدينا شريط أفقي يمكنه التحرك إلى اليسار أو اليمين، والهدف هو موازنة عمود عمودي فوق الشريط أثناء حركته.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

لإنشاء واستخدام هذه البيئة، نحتاج إلى بضعة أسطر من كود Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

يمكن الوصول إلى كل بيئة بنفس الطريقة:
* `env.reset` يبدأ تجربة جديدة.
* `env.step` ينفذ خطوة محاكاة. يتلقى **إجراء** من **مساحة الإجراءات**، ويعيد **ملاحظة** (من مساحة الملاحظات)، بالإضافة إلى مكافأة وعلم انتهاء.

في المثال أعلاه، نقوم بتنفيذ إجراء عشوائي في كل خطوة، ولهذا السبب تكون حياة التجربة قصيرة جدًا:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

هدف خوارزمية التعلم بالتعزيز هو تدريب نموذج - يُسمى **السياسة** π - والذي سيعيد الإجراء استجابةً لحالة معينة. يمكننا أيضًا اعتبار السياسة احتمالية، أي أنه لكل حالة *s* وإجراء *a* ستعيد السياسة الاحتمال π(*a*|*s*) الذي يجب أن نتخذ فيه الإجراء *a* في الحالة *s*.

## خوارزمية تدرجات السياسة (Policy Gradients)

الطريقة الأكثر وضوحًا لنمذجة السياسة هي إنشاء شبكة عصبية تأخذ الحالات كمدخلات وتعيد الإجراءات المقابلة (أو بالأحرى احتمالات جميع الإجراءات). بمعنى ما، ستكون مشابهة لمهمة تصنيف عادية، مع اختلاف رئيسي - لا نعرف مسبقًا أي الإجراءات يجب أن نتخذها في كل خطوة.

الفكرة هنا هي تقدير تلك الاحتمالات. نبني متجهًا من **المكافآت التراكمية** التي تُظهر إجمالي المكافأة في كل خطوة من التجربة. كما نطبق **خصم المكافآت** بضرب المكافآت السابقة بمعامل γ=0.99، لتقليل دور المكافآت السابقة. ثم نعزز تلك الخطوات على طول مسار التجربة التي تحقق مكافآت أكبر.

> تعرف على المزيد حول خوارزمية تدرجات السياسة وشاهدها قيد التنفيذ في [دفتر الملاحظات المثال](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## خوارزمية الممثل-الناقد (Actor-Critic)

نسخة محسنة من نهج تدرجات السياسة تُسمى **الممثل-الناقد**. الفكرة الرئيسية وراءها هي أن الشبكة العصبية ستُدرب لإعادة شيئين:

* السياسة، التي تحدد الإجراء الذي يجب اتخاذه. يُطلق على هذا الجزء **الممثل**.
* تقدير إجمالي المكافأة التي يمكننا توقع الحصول عليها في هذه الحالة - يُطلق على هذا الجزء **الناقد**.

بمعنى ما، تشبه هذه البنية [GAN](../../4-ComputerVision/10-GANs/README.md)، حيث لدينا شبكتان يتم تدريبهما ضد بعضهما البعض. في نموذج الممثل-الناقد، يقترح الممثل الإجراء الذي نحتاج إلى اتخاذه، ويحاول الناقد أن يكون ناقدًا ويقدر النتيجة. ومع ذلك، هدفنا هو تدريب تلك الشبكات معًا.

نظرًا لأننا نعرف كلًا من المكافآت التراكمية الحقيقية والنتائج التي يعيدها الناقد أثناء التجربة، فمن السهل نسبيًا بناء دالة خسارة تقلل الفرق بينهما. سيعطينا ذلك **خسارة الناقد**. يمكننا حساب **خسارة الممثل** باستخدام نفس النهج كما في خوارزمية تدرجات السياسة.

بعد تشغيل إحدى هذه الخوارزميات، يمكننا توقع أن يتصرف CartPole لدينا كما يلي:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ تمارين: تدرجات السياسة وخوارزمية الممثل-الناقد

واصل التعلم في دفاتر الملاحظات التالية:

* [التعلم بالتعزيز باستخدام TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [التعلم بالتعزيز باستخدام PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## مهام أخرى للتعلم بالتعزيز

يُعد التعلم بالتعزيز اليوم مجالًا سريع النمو في البحث. بعض الأمثلة المثيرة للاهتمام للتعلم بالتعزيز هي:

* تعليم الكمبيوتر لعب **ألعاب Atari**. الجزء الصعب في هذه المشكلة هو أنه ليس لدينا حالة بسيطة ممثلة كمتجه، بل لقطة شاشة - ونحتاج إلى استخدام CNN لتحويل صورة الشاشة إلى متجه ميزات، أو لاستخراج معلومات المكافأة. ألعاب Atari متوفرة في Gym.
* تعليم الكمبيوتر لعب ألعاب الطاولة، مثل الشطرنج وGo. مؤخرًا، تم تدريب برامج متقدمة مثل **Alpha Zero** من الصفر بواسطة وكيلين يلعبان ضد بعضهما البعض ويتحسنان في كل خطوة.
* في الصناعة، يُستخدم التعلم بالتعزيز لإنشاء أنظمة تحكم من المحاكاة. خدمة مثل [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) مصممة خصيصًا لذلك.

## الخلاصة

لقد تعلمنا الآن كيفية تدريب الوكلاء لتحقيق نتائج جيدة فقط من خلال تزويدهم بدالة مكافأة تحدد الحالة المرغوبة للعبة، ومنحهم فرصة لاستكشاف مساحة البحث بذكاء. لقد جربنا بنجاح خوارزميتين، وحققنا نتيجة جيدة في فترة زمنية قصيرة نسبيًا. ومع ذلك، هذه مجرد بداية رحلتك في التعلم بالتعزيز، ويجب أن تفكر بالتأكيد في أخذ دورة منفصلة إذا كنت ترغب في التعمق أكثر.

## 🚀 تحدي

استكشف التطبيقات المدرجة في قسم "مهام أخرى للتعلم بالتعزيز" وحاول تنفيذ أحدها!

## [اختبار ما بعد المحاضرة](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## المراجعة والدراسة الذاتية

تعرف على المزيد حول التعلم بالتعزيز الكلاسيكي في [منهج تعلم الآلة للمبتدئين](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

شاهد [هذا الفيديو الرائع](https://www.youtube.com/watch?v=qv6UVOQ0F44) الذي يتحدث عن كيفية تعلم الكمبيوتر لعب Super Mario.

## المهمة: [تدريب سيارة جبلية](lab/README.md)

هدفك خلال هذه المهمة هو تدريب بيئة Gym مختلفة - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.