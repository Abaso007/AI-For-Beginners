{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# تدريب التعلم المعزز (RL) على موازنة عمود على عربة\n",
    "\n",
    "هذا الدفتر هو جزء من [منهج الذكاء الاصطناعي للمبتدئين](http://aka.ms/ai-beginners). وقد استُلهم من [البرنامج التعليمي الرسمي لـ PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) و[تنفيذ Cartpole باستخدام PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "في هذا المثال، سنستخدم التعلم المعزز (RL) لتدريب نموذج على موازنة عمود على عربة يمكنها التحرك إلى اليسار واليمين على محور أفقي. سنستخدم بيئة [OpenAI Gym](https://www.gymlibrary.ml/) لمحاكاة العمود.\n",
    "\n",
    "> **ملاحظة**: يمكنك تشغيل كود هذا الدرس محليًا (على سبيل المثال، من خلال Visual Studio Code)، وفي هذه الحالة ستُفتح المحاكاة في نافذة جديدة. عند تشغيل الكود عبر الإنترنت، قد تحتاج إلى إجراء بعض التعديلات على الكود، كما هو موضح [هنا](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "سنبدأ بالتأكد من تثبيت Gym:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن دعونا ننشئ بيئة CartPole ونرى كيفية العمل عليها. تمتلك البيئة الخصائص التالية:\n",
    "\n",
    "* **مجال الحركة** هو مجموعة الحركات الممكنة التي يمكننا تنفيذها في كل خطوة من خطوات المحاكاة  \n",
    "* **مجال الملاحظات** هو مجال الملاحظات التي يمكننا القيام بها  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "دعونا نرى كيف تعمل المحاكاة. الحلقة التالية تشغل المحاكاة حتى تعيد `env.step` العلم الخاص بإنهاء العملية `done`. سنختار الإجراءات بشكل عشوائي باستخدام `env.action_space.sample()`، مما يعني أن التجربة ستفشل على الأرجح بسرعة كبيرة (بيئة CartPole تنتهي عندما تكون سرعة العربة، أو موقعها، أو زاويتها خارج حدود معينة).\n",
    "\n",
    "> ستفتح المحاكاة في نافذة جديدة. يمكنك تشغيل الكود عدة مرات ومراقبة كيفية تصرفه.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يمكنك ملاحظة أن الملاحظات تحتوي على 4 أرقام. وهي:\n",
    "- موقع العربة\n",
    "- سرعة العربة\n",
    "- زاوية العصا\n",
    "- معدل دوران العصا\n",
    "\n",
    "`rew` هو المكافأة التي نحصل عليها في كل خطوة. يمكنك أن ترى أنه في بيئة CartPole يتم منحك نقطة واحدة لكل خطوة محاكاة، والهدف هو تعظيم إجمالي المكافآت، أي الوقت الذي تستطيع فيه CartPole الحفاظ على التوازن دون السقوط.\n",
    "\n",
    "أثناء التعلم التعزيزي، هدفنا هو تدريب **سياسة** $\\pi$، التي ستخبرنا لكل حالة $s$ بالإجراء $a$ الذي يجب اتخاذه، بمعنى آخر $a = \\pi(s)$.\n",
    "\n",
    "إذا كنت تريد حلاً احتماليًا، يمكنك اعتبار السياسة كإرجاع مجموعة من الاحتمالات لكل إجراء، أي أن $\\pi(a|s)$ تعني احتمال أن نتخذ الإجراء $a$ في الحالة $s$.\n",
    "\n",
    "## طريقة تدرج السياسة\n",
    "\n",
    "في أبسط خوارزمية للتعلم التعزيزي، والتي تُسمى **تدرج السياسة**، سنقوم بتدريب شبكة عصبية لتوقع الإجراء التالي.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "سوف نقوم بتدريب الشبكة عن طريق إجراء العديد من التجارب وتحديث شبكتنا بعد كل تشغيل. دعنا نحدد وظيفة تقوم بتشغيل التجربة وتعيد النتائج (ما يسمى **التتبع**) - جميع الحالات، الإجراءات (واحتمالاتها الموصى بها)، والمكافآت:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يمكنك تشغيل حلقة واحدة بشبكة غير مدربة وملاحظة أن إجمالي المكافأة (أي طول الحلقة) منخفض جدًا:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "أحد الجوانب الصعبة في خوارزمية تدرج السياسة هو استخدام **المكافآت المخصومة**. الفكرة هي أننا نحسب متجه المكافآت الإجمالية في كل خطوة من خطوات اللعبة، وخلال هذه العملية نقوم بخصم المكافآت المبكرة باستخدام معامل $gamma$. كما نقوم أيضًا بتطبيع المتجه الناتج، لأننا سنستخدمه كوزن للتأثير على تدريبنا:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن دعونا نبدأ التدريب الفعلي! سنقوم بتشغيل 300 حلقة، وفي كل حلقة سنقوم بما يلي:\n",
    "\n",
    "1. تشغيل التجربة وجمع المسار.\n",
    "2. حساب الفرق (`gradients`) بين الإجراءات المتخذة والاحتمالات المتوقعة. كلما كان الفرق أقل، كلما كنا أكثر تأكدًا من أننا اتخذنا الإجراء الصحيح.\n",
    "3. حساب المكافآت المخصومة وضرب التدرجات بالمكافآت المخصومة - هذا سيضمن أن الخطوات ذات المكافآت الأعلى سيكون لها تأثير أكبر على النتيجة النهائية مقارنة بالخطوات ذات المكافآت الأقل.\n",
    "4. الإجراءات المستهدفة المتوقعة لشبكتنا العصبية سيتم أخذها جزئيًا من الاحتمالات المتوقعة أثناء التشغيل، وجزئيًا من التدرجات المحسوبة. سنستخدم معامل `alpha` لتحديد إلى أي مدى يتم أخذ التدرجات والمكافآت في الاعتبار - وهذا ما يسمى *معدل التعلم* لخوارزمية التعزيز.\n",
    "5. وأخيرًا، نقوم بتدريب شبكتنا على الحالات والإجراءات المتوقعة، ونكرر العملية.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن دعنا نشغل الحلقة مع العرض لنرى النتيجة:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نأمل أن تتمكن الآن من رؤية أن العمود يمكنه التوازن بشكل جيد!\n",
    "\n",
    "## نموذج الممثل-الناقد\n",
    "\n",
    "نموذج الممثل-الناقد هو تطوير إضافي لنموذج تدرجات السياسات، حيث نقوم ببناء شبكة عصبية لتعلم كل من السياسة والمكافآت المقدرة. تحتوي الشبكة على مخرجاتين (أو يمكنك اعتبارها شبكتين منفصلتين):\n",
    "* **الممثل** سيقترح الإجراء الذي يجب اتخاذه من خلال إعطائنا توزيع احتمالات الحالة، كما هو الحال في نموذج تدرجات السياسات.\n",
    "* **الناقد** سيقدر ما ستكون المكافأة الناتجة عن تلك الإجراءات. يقوم بإرجاع إجمالي المكافآت المقدرة في المستقبل عند الحالة المعطاة.\n",
    "\n",
    "لنقم بتعريف مثل هذا النموذج:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نحتاج إلى تعديل وظائف `discounted_rewards` و `run_episode` قليلاً:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن سنقوم بتشغيل الحلقة الرئيسية للتدريب. سنستخدم عملية تدريب الشبكة اليدوية من خلال حساب دوال الخسارة المناسبة وتحديث معلمات الشبكة:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## الخلاصة\n",
    "\n",
    "لقد رأينا في هذا العرض التوضيحي خوارزميتين للتعلم المعزز: خوارزمية بسيطة تعتمد على تدرج السياسة، وخوارزمية أكثر تطورًا تعتمد على الممثل-الناقد. يمكنك ملاحظة أن هذه الخوارزميات تعمل بمفاهيم مجردة مثل الحالة، الإجراء والمكافأة - مما يعني أنه يمكن تطبيقها على بيئات مختلفة تمامًا.\n",
    "\n",
    "يتيح لنا التعلم المعزز اكتشاف أفضل استراتيجية لحل المشكلة بمجرد النظر إلى المكافأة النهائية. حقيقة أننا لا نحتاج إلى مجموعات بيانات معنونة تتيح لنا تكرار المحاكاة عدة مرات لتحسين نماذجنا. ومع ذلك، لا تزال هناك العديد من التحديات في مجال التعلم المعزز، والتي يمكنك التعرف عليها إذا قررت التركيز أكثر على هذا المجال المثير للاهتمام في الذكاء الاصطناعي.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**إخلاء المسؤولية**:  \nتم ترجمة هذه الوثيقة باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار الوثيقة الأصلية بلغتها الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T02:52:30+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "ar"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}