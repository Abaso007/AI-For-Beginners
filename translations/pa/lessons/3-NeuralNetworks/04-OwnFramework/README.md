<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-26T10:27:42+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "pa"
}
-->
# ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਦਾ ਪਰਿਚਯ। ਮਲਟੀ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ

ਪਿਛਲੇ ਭਾਗ ਵਿੱਚ, ਤੁਸੀਂ ਸਭ ਤੋਂ ਸਧਾਰਣ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਮਾਡਲ - ਇੱਕ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ, ਜੋ ਕਿ ਇੱਕ ਰੇਖੀ ਦੋ-ਵਰਗੀਕਰਨ ਮਾਡਲ ਹੈ, ਬਾਰੇ ਸਿੱਖਿਆ।

ਇਸ ਭਾਗ ਵਿੱਚ ਅਸੀਂ ਇਸ ਮਾਡਲ ਨੂੰ ਇੱਕ ਹੋਰ ਲਚਕੀਲੇ ਢਾਂਚੇ ਵਿੱਚ ਵਧਾਉਂਦੇ ਹਾਂ, ਜੋ ਸਾਨੂੰ ਇਹ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ:

* ਦੋ-ਵਰਗੀਕਰਨ ਤੋਂ ਇਲਾਵਾ **ਮਲਟੀ-ਵਰਗੀਕਰਨ** ਕਰਨਾ  
* ਵਰਗੀਕਰਨ ਤੋਂ ਇਲਾਵਾ **ਰੈਗ੍ਰੈਸ਼ਨ ਸਮੱਸਿਆਵਾਂ** ਦਾ ਹੱਲ ਕਰਨਾ  
* ਉਹ ਵਰਗ ਵੱਖਰੇ ਕਰਨਾ ਜੋ ਰੇਖੀ ਤੌਰ 'ਤੇ ਵੱਖਰੇ ਨਹੀਂ ਕੀਤੇ ਜਾ ਸਕਦੇ  

ਅਸੀਂ ਪਾਇਥਨ ਵਿੱਚ ਆਪਣਾ ਮੋਡੀਊਲਰ ਢਾਂਚਾ ਵੀ ਵਿਕਸਿਤ ਕਰਾਂਗੇ ਜੋ ਸਾਨੂੰ ਵੱਖ-ਵੱਖ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਆਰਕੀਟੈਕਚਰ ਬਣਾਉਣ ਦੀ ਆਗਿਆ ਦੇਵੇਗਾ।

## [ਪ੍ਰੀ-ਲੈਕਚਰ ਕਵਿਜ਼](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਦੀ ਫਾਰਮਲਾਈਜ਼ੇਸ਼ਨ

ਆਓ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਸਮੱਸਿਆ ਨੂੰ ਫਾਰਮਲ ਕਰਕੇ ਸ਼ੁਰੂ ਕਰੀਏ। ਮੰਨ ਲਓ ਸਾਡੇ ਕੋਲ ਇੱਕ ਟ੍ਰੇਨਿੰਗ ਡੇਟਾਸੈੱਟ **X** ਹੈ ਜਿਸ ਦੇ ਲੇਬਲ **Y** ਹਨ, ਅਤੇ ਸਾਨੂੰ ਇੱਕ ਮਾਡਲ *f* ਬਣਾਉਣਾ ਹੈ ਜੋ ਸਭ ਤੋਂ ਸਹੀ ਅਨੁਮਾਨ ਲਗਾ ਸਕੇ। ਅਨੁਮਾਨਾਂ ਦੀ ਗੁਣਵੱਤਾ ਨੂੰ **ਲਾਸ ਫੰਕਸ਼ਨ** ℒ ਦੁਆਰਾ ਮਾਪਿਆ ਜਾਂਦਾ ਹੈ। ਹੇਠਾਂ ਦਿੱਤੇ ਲਾਸ ਫੰਕਸ਼ਨ ਅਕਸਰ ਵਰਤੇ ਜਾਂਦੇ ਹਨ:

* ਰੈਗ੍ਰੈਸ਼ਨ ਸਮੱਸਿਆ ਲਈ, ਜਦੋਂ ਸਾਨੂੰ ਇੱਕ ਨੰਬਰ ਦੀ ਭਵਿੱਖਵਾਣੀ ਕਰਨੀ ਹੁੰਦੀ ਹੈ, ਅਸੀਂ **ਐਬਸੋਲਿਊਟ ਐਰਰ** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| ਜਾਂ **ਸਕਵੇਅਰਡ ਐਰਰ** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> ਵਰਤ ਸਕਦੇ ਹਾਂ।  
* ਵਰਗੀਕਰਨ ਲਈ, ਅਸੀਂ **0-1 ਲਾਸ** (ਜੋ ਮੂਲ ਤੌਰ 'ਤੇ ਮਾਡਲ ਦੀ **ਸਹੀਤਾ** ਦੇ ਬਰਾਬਰ ਹੈ) ਜਾਂ **ਲੌਜਿਸਟਿਕ ਲਾਸ** ਵਰਤਦੇ ਹਾਂ।  

ਇੱਕ-ਪੱਧਰੀ ਪਰਸੈਪਟ੍ਰਾਨ ਲਈ, ਫੰਕਸ਼ਨ *f* ਨੂੰ ਇੱਕ ਰੇਖੀ ਫੰਕਸ਼ਨ ਵਜੋਂ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤਾ ਗਿਆ ਸੀ *f(x)=wx+b* (ਇੱਥੇ *w* ਭਾਰ ਮੈਟ੍ਰਿਕਸ ਹੈ, *x* ਇਨਪੁਟ ਫੀਚਰਜ਼ ਦਾ ਵੇਕਟਰ ਹੈ, ਅਤੇ *b* ਬਾਇਸ ਵੇਕਟਰ ਹੈ)। ਵੱਖ-ਵੱਖ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਆਰਕੀਟੈਕਚਰਾਂ ਲਈ, ਇਹ ਫੰਕਸ਼ਨ ਹੋਰ ਜਟਿਲ ਰੂਪ ਲੈ ਸਕਦਾ ਹੈ।

> ਵਰਗੀਕਰਨ ਦੇ ਮਾਮਲੇ ਵਿੱਚ, ਅਕਸਰ ਇਹ ਚਾਹੁੰਦੇ ਹਾਂ ਕਿ ਨੈੱਟਵਰਕ ਆਉਟਪੁਟ ਵਜੋਂ ਸੰਬੰਧਿਤ ਵਰਗਾਂ ਦੀ ਸੰਭਾਵਨਾ ਮਿਲੇ। ਅੰਕਾਂ ਨੂੰ ਸੰਭਾਵਨਾਵਾਂ ਵਿੱਚ ਬਦਲਣ ਲਈ (ਜਿਵੇਂ ਆਉਟਪੁਟ ਨੂੰ ਨਾਰਮਲਾਈਜ਼ ਕਰਨ ਲਈ), ਅਸੀਂ ਅਕਸਰ **ਸੌਫਟਮੈਕਸ** ਫੰਕਸ਼ਨ σ ਵਰਤਦੇ ਹਾਂ, ਅਤੇ ਫੰਕਸ਼ਨ *f* ਬਣ ਜਾਂਦਾ ਹੈ *f(x)=σ(wx+b)*।

ਉਪਰੋਕਤ *f* ਦੀ ਪਰਿਭਾਸ਼ਾ ਵਿੱਚ, *w* ਅਤੇ *b* ਨੂੰ **ਪੈਰਾਮੀਟਰਜ਼** θ=⟨*w,b*⟩ ਕਿਹਾ ਜਾਂਦਾ ਹੈ। ਦਿੱਤੇ ਗਏ ਡੇਟਾਸੈੱਟ ⟨**X**,**Y**⟩ ਦੇ ਆਧਾਰ 'ਤੇ, ਅਸੀਂ ਪੂਰੇ ਡੇਟਾਸੈੱਟ 'ਤੇ ਕੁੱਲ ਗਲਤੀ ਨੂੰ ਪੈਰਾਮੀਟਰਜ਼ θ ਦੇ ਫੰਕਸ਼ਨ ਵਜੋਂ ਗਣਨਾ ਕਰ ਸਕਦੇ ਹਾਂ।

> ✅ **ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਟ੍ਰੇਨਿੰਗ ਦਾ ਮਕਸਦ ਪੈਰਾਮੀਟਰਜ਼ θ ਨੂੰ ਬਦਲ ਕੇ ਗਲਤੀ ਨੂੰ ਘਟਾਉਣਾ ਹੈ।**

## ਗ੍ਰੇਡੀਅੰਟ ਡਿਸੈਂਟ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ

ਫੰਕਸ਼ਨ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਦਾ ਇੱਕ ਪ੍ਰਸਿੱਧ ਤਰੀਕਾ **ਗ੍ਰੇਡੀਅੰਟ ਡਿਸੈਂਟ** ਹੈ। ਇਸ ਵਿਚਾਰਧਾਰਾ ਇਹ ਹੈ ਕਿ ਅਸੀਂ ਲਾਸ ਫੰਕਸ਼ਨ ਦਾ ਡੈਰੀਵੇਟਿਵ (ਬਹੁ-ਪੱਖੀ ਮਾਮਲੇ ਵਿੱਚ **ਗ੍ਰੇਡੀਅੰਟ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ) ਪੈਰਾਮੀਟਰਜ਼ ਦੇ ਸਬੰਧ ਵਿੱਚ ਗਣਨਾ ਕਰ ਸਕਦੇ ਹਾਂ, ਅਤੇ ਪੈਰਾਮੀਟਰਜ਼ ਨੂੰ ਇਸ ਤਰੀਕੇ ਨਾਲ ਬਦਲ ਸਕਦੇ ਹਾਂ ਕਿ ਗਲਤੀ ਘਟੇ। ਇਸ ਨੂੰ ਹੇਠਾਂ ਦਿੱਤੇ ਤਰੀਕੇ ਨਾਲ ਫਾਰਮਲ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ:

* ਪੈਰਾਮੀਟਰਜ਼ ਨੂੰ ਕੁਝ ਰੈਂਡਮ ਮੁੱਲਾਂ w<sup>(0)</sup>, b<sup>(0)</sup> ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ।  
* ਹੇਠਾਂ ਦਿੱਤੇ ਕਦਮ ਕਈ ਵਾਰ ਦੁਹਰਾਓ:  
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w  
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b  

ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ, ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਕਦਮ ਪੂਰੇ ਡੇਟਾਸੈੱਟ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖ ਕੇ ਗਣਨਾ ਕੀਤੇ ਜਾਣੇ ਚਾਹੀਦੇ ਹਨ (ਯਾਦ ਰੱਖੋ ਕਿ ਲਾਸ ਸਾਰੇ ਟ੍ਰੇਨਿੰਗ ਨਮੂਨਿਆਂ ਦੇ ਜੋੜ ਵਜੋਂ ਗਣਨਾ ਕੀਤੀ ਜਾਂਦੀ ਹੈ)। ਹਾਲਾਂਕਿ, ਅਸਲ ਜ਼ਿੰਦਗੀ ਵਿੱਚ ਅਸੀਂ ਡੇਟਾਸੈੱਟ ਦੇ ਛੋਟੇ ਹਿੱਸੇ ਲੈਂਦੇ ਹਾਂ, ਜਿਨ੍ਹਾਂ ਨੂੰ **ਮਿਨੀਬੈਚਜ਼** ਕਿਹਾ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਡੇਟਾ ਦੇ ਇੱਕ ਉਪਸੈੱਟ ਦੇ ਆਧਾਰ 'ਤੇ ਗ੍ਰੇਡੀਅੰਟ ਗਣਨਾ ਕਰਦੇ ਹਾਂ। ਕਿਉਂਕਿ ਹਰ ਵਾਰ ਉਪਸੈੱਟ ਰੈਂਡਮ ਤੌਰ 'ਤੇ ਲਿਆ ਜਾਂਦਾ ਹੈ, ਇਸ ਤਰੀਕੇ ਨੂੰ **ਸਟੋਕੈਸਟਿਕ ਗ੍ਰੇਡੀਅੰਟ ਡਿਸੈਂਟ** (SGD) ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

## ਮਲਟੀ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ ਅਤੇ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ

ਜਿਵੇਂ ਕਿ ਅਸੀਂ ਉਪਰ ਦੇਖਿਆ, ਇੱਕ-ਪੱਧਰੀ ਨੈੱਟਵਰਕ ਰੇਖੀ ਤੌਰ 'ਤੇ ਵੱਖਰੇ ਕੀਤੇ ਜਾ ਸਕਣ ਵਾਲੇ ਵਰਗਾਂ ਦਾ ਵਰਗੀਕਰਨ ਕਰਨ ਦੇ ਯੋਗ ਹੈ। ਇੱਕ ਹੋਰ ਧਨਾਢ ਮਾਡਲ ਬਣਾਉਣ ਲਈ, ਅਸੀਂ ਨੈੱਟਵਰਕ ਦੇ ਕਈ ਪੱਧਰ ਜੋੜ ਸਕਦੇ ਹਾਂ। ਗਣਿਤਕ ਤੌਰ 'ਤੇ ਇਸਦਾ ਅਰਥ ਇਹ ਹੋਵੇਗਾ ਕਿ ਫੰਕਸ਼ਨ *f* ਹੋਰ ਜਟਿਲ ਰੂਪ ਲਵੇਗਾ, ਅਤੇ ਕਈ ਕਦਮਾਂ ਵਿੱਚ ਗਣਨਾ ਕੀਤੀ ਜਾਵੇਗੀ:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>  
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>  
* f = σ(z<sub>2</sub>)  

ਇੱਥੇ, α ਇੱਕ **ਗੈਰ-ਰੇਖੀ ਐਕਟੀਵੇਸ਼ਨ ਫੰਕਸ਼ਨ** ਹੈ, σ ਇੱਕ ਸੌਫਟਮੈਕਸ ਫੰਕਸ਼ਨ ਹੈ, ਅਤੇ ਪੈਰਾਮੀਟਰਜ਼ θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> ਹਨ।

ਗ੍ਰੇਡੀਅੰਟ ਡਿਸੈਂਟ ਐਲਗੋਰਿਦਮ ਉਹੀ ਰਹੇਗਾ, ਪਰ ਗ੍ਰੇਡੀਅੰਟ ਦੀ ਗਣਨਾ ਕਰਨਾ ਹੋਰ ਮੁਸ਼ਕਲ ਹੋਵੇਗਾ। ਚੇਨ ਡਿਫਰੈਂਸ਼ੀਏਸ਼ਨ ਨਿਯਮ ਦੇ ਆਧਾਰ 'ਤੇ, ਅਸੀਂ ਡੈਰੀਵੇਟਿਵਜ਼ ਨੂੰ ਹੇਠਾਂ ਦਿੱਤੇ ਤਰੀਕੇ ਨਾਲ ਗਣਨਾ ਕਰ ਸਕਦੇ ਹਾਂ:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)  
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)  

> ✅ ਚੇਨ ਡਿਫਰੈਂਸ਼ੀਏਸ਼ਨ ਨਿਯਮ ਨੂੰ ਲਾਸ ਫੰਕਸ਼ਨ ਦੇ ਪੈਰਾਮੀਟਰਜ਼ ਦੇ ਸਬੰਧ ਵਿੱਚ ਡੈਰੀਵੇਟਿਵਜ਼ ਦੀ ਗਣਨਾ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ।  

ਯਾਦ ਰੱਖੋ ਕਿ ਸਾਰੇ ਹਿਸਾਬਾਂ ਦੇ ਖੱਬੇ ਪਾਸੇ ਦਾ ਹਿੱਸਾ ਇੱਕੋ ਜਿਹਾ ਹੈ, ਅਤੇ ਇਸ ਤਰ੍ਹਾਂ ਅਸੀਂ ਲਾਸ ਫੰਕਸ਼ਨ ਤੋਂ ਸ਼ੁਰੂ ਕਰਕੇ ਅਤੇ ਗਣਨਾ ਗ੍ਰਾਫ ਦੇ "ਪਿੱਛੇ" ਜਾ ਕੇ ਡੈਰੀਵੇਟਿਵਜ਼ ਨੂੰ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਤਰੀਕੇ ਨਾਲ ਗਣਨਾ ਕਰ ਸਕਦੇ ਹਾਂ। ਇਸ ਤਰ੍ਹਾਂ, ਮਲਟੀ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨ ਦੇ ਤਰੀਕੇ ਨੂੰ **ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ**, ਜਾਂ 'ਬੈਕਪ੍ਰੋਪ' ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

<img alt="ਕੰਪਿਊਟ ਗ੍ਰਾਫ" src="images/ComputeGraphGrad.png"/>

> TODO: ਚਿੱਤਰ ਦਾ ਹਵਾਲਾ

> ✅ ਅਸੀਂ ਆਪਣੇ ਨੋਟਬੁੱਕ ਉਦਾਹਰਨ ਵਿੱਚ ਬੈਕਪ੍ਰੋਪ ਬਾਰੇ ਹੋਰ ਵਿਸਥਾਰ ਨਾਲ ਕਵਰ ਕਰਾਂਗੇ।  

## ਨਿਸ਼ਕਰਸ਼

ਇਸ ਪਾਠ ਵਿੱਚ, ਅਸੀਂ ਆਪਣੀ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਲਾਇਬ੍ਰੇਰੀ ਬਣਾਈ ਹੈ, ਅਤੇ ਅਸੀਂ ਇਸਨੂੰ ਇੱਕ ਸਧਾਰਣ ਦੋ-ਪੱਖੀ ਵਰਗੀਕਰਨ ਕੰਮ ਲਈ ਵਰਤਿਆ ਹੈ।

## 🚀 ਚੁਣੌਤੀ

ਸੰਲਗਨ ਨੋਟਬੁੱਕ ਵਿੱਚ, ਤੁਸੀਂ ਮਲਟੀ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ ਬਣਾਉਣ ਅਤੇ ਟ੍ਰੇਨ ਕਰਨ ਲਈ ਆਪਣਾ ਢਾਂਚਾ ਲਾਗੂ ਕਰੋਗੇ। ਤੁਸੀਂ ਵੇਖ ਸਕੋਗੇ ਕਿ ਆਧੁਨਿਕ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਕਿਵੇਂ ਕੰਮ ਕਰਦੇ ਹਨ।

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) ਨੋਟਬੁੱਕ 'ਤੇ ਜਾਓ ਅਤੇ ਇਸਨੂੰ ਪੂਰਾ ਕਰੋ।

## [ਪੋਸਟ-ਲੈਕਚਰ ਕਵਿਜ਼](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## ਸਮੀਖਿਆ ਅਤੇ ਸਵੈ ਅਧਿਐਨ

ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ AI ਅਤੇ ML ਵਿੱਚ ਵਰਤਿਆ ਜਾਣ ਵਾਲਾ ਇੱਕ ਆਮ ਐਲਗੋਰਿਦਮ ਹੈ, ਜਿਸਨੂੰ [ਹੋਰ ਵਿਸਥਾਰ ਨਾਲ](https://wikipedia.org/wiki/Backpropagation) ਪੜ੍ਹਨ ਯੋਗ ਹੈ।

## [ਅਸਾਈਨਮੈਂਟ](lab/README.md)

ਇਸ ਲੈਬ ਵਿੱਚ, ਤੁਹਾਨੂੰ ਇਸ ਪਾਠ ਵਿੱਚ ਬਣਾਏ ਢਾਂਚੇ ਨੂੰ ਵਰਤ ਕੇ MNIST ਹੱਥ ਨਾਲ ਲਿਖੇ ਅੰਕਾਂ ਦੇ ਵਰਗੀਕਰਨ ਦੀ ਸਮੱਸਿਆ ਹੱਲ ਕਰਨ ਲਈ ਕਿਹਾ ਗਿਆ ਹੈ।

* [ਹਦਾਇਤਾਂ](lab/README.md)  
* [ਨੋਟਬੁੱਕ](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)  

**ਅਸਵੀਕਰਤਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਇਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੌਜੂਦ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਪ੍ਰਮਾਣਿਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੇ ਪ੍ਰਯੋਗ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਮੀਆਂ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆਵਾਂ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।