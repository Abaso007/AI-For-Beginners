<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T07:40:28+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "pa"
}
-->
# ਪ੍ਰੀ-ਟ੍ਰੇਨਡ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲ

ਸਾਡੇ ਪਿਛਲੇ ਸਾਰੇ ਕੰਮਾਂ ਵਿੱਚ, ਅਸੀਂ ਲੇਬਲ ਕੀਤੇ ਡੇਟਾਸੈੱਟ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਨੂੰ ਇੱਕ ਖਾਸ ਕੰਮ ਕਰਨ ਲਈ ਟ੍ਰੇਨ ਕਰ ਰਹੇ ਸਾਂ। ਵੱਡੇ ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲਾਂ, ਜਿਵੇਂ ਕਿ BERT, ਵਿੱਚ ਅਸੀਂ ਸਵੈ-ਸੰਪਰਕਤਮਕ ਢੰਗ ਨਾਲ ਭਾਸ਼ਾ ਮਾਡਲਿੰਗ ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹਾਂ ਤਾਂ ਜੋ ਇੱਕ ਭਾਸ਼ਾ ਮਾਡਲ ਬਣਾਇਆ ਜਾ ਸਕੇ, ਜਿਸ ਨੂੰ ਫਿਰ ਖਾਸ ਡਾਊਨਸਟ੍ਰੀਮ ਕੰਮ ਲਈ ਖੇਤਰ-ਵਿਸ਼ੇਸ਼ ਟ੍ਰੇਨਿੰਗ ਨਾਲ ਵਿਸ਼ੇਸ਼ ਬਣਾਇਆ ਜਾਂਦਾ ਹੈ। ਹਾਲਾਂਕਿ, ਇਹ ਸਾਬਤ ਹੋਇਆ ਹੈ ਕਿ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲ ਬਿਨਾਂ ਕਿਸੇ ਖੇਤਰ-ਵਿਸ਼ੇਸ਼ ਟ੍ਰੇਨਿੰਗ ਦੇ ਵੀ ਕਈ ਕੰਮ ਕਰ ਸਕਦੇ ਹਨ। ਮਾਡਲਾਂ ਦੇ ਇੱਕ ਪਰਿਵਾਰ ਨੂੰ ਜੋ ਇਹ ਕਰਨ ਦੇ ਯੋਗ ਹਨ, **GPT** ਕਿਹਾ ਜਾਂਦਾ ਹੈ: ਜਨਰੇਟਿਵ ਪ੍ਰੀ-ਟ੍ਰੇਨਡ ਟ੍ਰਾਂਸਫਾਰਮਰ।

## [ਪ੍ਰੀ-ਲੈਕਚਰ ਕਵਿਜ਼](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਅਤੇ ਪੇਰਪਲੇਕਸਿਟੀ

ਇੱਕ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਦੇ ਡਾਊਨਸਟ੍ਰੀਮ ਟ੍ਰੇਨਿੰਗ ਤੋਂ ਬਿਨਾਂ ਜਨਰਲ ਕੰਮ ਕਰਨ ਦੇ ਯੋਗ ਹੋਣ ਦਾ ਵਿਚਾਰ [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ਪੇਪਰ ਵਿੱਚ ਪੇਸ਼ ਕੀਤਾ ਗਿਆ ਹੈ। ਮੁੱਖ ਵਿਚਾਰ ਇਹ ਹੈ ਕਿ ਕਈ ਹੋਰ ਕੰਮਾਂ ਨੂੰ **ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ** ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਮਾਡਲ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ, ਕਿਉਂਕਿ ਟੈਕਸਟ ਨੂੰ ਸਮਝਣਾ ਅਸਲ ਵਿੱਚ ਇਸਨੂੰ ਪੈਦਾ ਕਰਨ ਦੇ ਯੋਗ ਹੋਣਾ ਹੈ। ਕਿਉਂਕਿ ਮਾਡਲ ਨੂੰ ਮਨੁੱਖੀ ਗਿਆਨ ਨੂੰ ਸ਼ਾਮਲ ਕਰਦੇ ਹੋਏ ਬਹੁਤ ਵੱਡੇ ਮਾਤਰਾ ਦੇ ਟੈਕਸਟ 'ਤੇ ਟ੍ਰੇਨ ਕੀਤਾ ਜਾਂਦਾ ਹੈ, ਇਸ ਲਈ ਇਹ ਵੱਖ-ਵੱਖ ਵਿਸ਼ਿਆਂ ਬਾਰੇ ਜਾਣੂ ਵੀ ਬਣ ਜਾਂਦਾ ਹੈ।

> ਟੈਕਸਟ ਨੂੰ ਸਮਝਣਾ ਅਤੇ ਪੈਦਾ ਕਰਨ ਦੇ ਯੋਗ ਹੋਣਾ ਦੁਨੀਆ ਦੇ ਆਲੇ-ਦੁਆਲੇ ਕੁਝ ਜਾਣਨ ਨੂੰ ਵੀ ਸ਼ਾਮਲ ਕਰਦਾ ਹੈ। ਲੋਕ ਵੀ ਵੱਡੇ ਪੱਧਰ 'ਤੇ ਪੜ੍ਹ ਕੇ ਸਿੱਖਦੇ ਹਨ, ਅਤੇ GPT ਨੈੱਟਵਰਕ ਇਸ ਸੰਦਰਭ ਵਿੱਚ ਇਸੇ ਤਰ੍ਹਾਂ ਹੈ।

ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਨੈੱਟਵਰਕ ਅਗਲੇ ਸ਼ਬਦ ਦੀ ਸੰਭਾਵਨਾ ਦੀ ਪੇਸ਼ਗੋਈ ਕਰਕੇ ਕੰਮ ਕਰਦੇ ਹਨ $$P(w_N)$$। ਹਾਲਾਂਕਿ, ਅਗਲੇ ਸ਼ਬਦ ਦੀ ਬਿਨਾ ਸ਼ਰਤ ਸੰਭਾਵਨਾ ਟੈਕਸਟ ਕਾਰਪਸ ਵਿੱਚ ਇਸ ਸ਼ਬਦ ਦੀ ਆਵ੍ਰਿਤੀ ਦੇ ਬਰਾਬਰ ਹੁੰਦੀ ਹੈ। GPT ਸਾਨੂੰ ਪਿਛਲੇ ਸ਼ਬਦਾਂ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖਦੇ ਹੋਏ ਅਗਲੇ ਸ਼ਬਦ ਦੀ **ਸ਼ਰਤ ਸੰਭਾਵਨਾ** ਦੇ ਸਕਦਾ ਹੈ: $$P(w_N | w_{n-1}, ..., w_0)$$

> ਤੁਸੀਂ ਸਾਡੇ [ਡਾਟਾ ਸਾਇੰਸ ਫਾਰ ਬਿਗਿਨਰਜ਼ ਕਰਿਕੁਲਮ](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) ਵਿੱਚ ਸੰਭਾਵਨਾਵਾਂ ਬਾਰੇ ਹੋਰ ਪੜ੍ਹ ਸਕਦੇ ਹੋ।

ਭਾਸ਼ਾ ਜਨਰੇਸ਼ਨ ਮਾਡਲ ਦੀ ਗੁਣਵੱਤਾ ਨੂੰ **ਪੇਰਪਲੇਕਸਿਟੀ** ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ। ਇਹ ਇੱਕ ਅੰਦਰੂਨੀ ਮੈਟ੍ਰਿਕ ਹੈ ਜੋ ਸਾਨੂੰ ਕਿਸੇ ਟਾਸਕ-ਵਿਸ਼ੇਸ਼ ਡੇਟਾਸੈੱਟ ਤੋਂ ਬਿਨਾਂ ਮਾਡਲ ਦੀ ਗੁਣਵੱਤਾ ਨੂੰ ਮਾਪਣ ਦੀ ਆਗਿਆ ਦਿੰਦੀ ਹੈ। ਇਹ *ਵਾਕ ਦੀ ਸੰਭਾਵਨਾ* ਦੇ ਧਾਰਨਾ 'ਤੇ ਆਧਾਰਿਤ ਹੈ - ਮਾਡਲ ਇੱਕ ਵਾਕ ਨੂੰ ਜੋ ਸੰਭਾਵਨਾ ਨਾਲ ਅਸਲੀ ਹੋ ਸਕਦਾ ਹੈ, ਉੱਚ ਸੰਭਾਵਨਾ ਦਿੰਦਾ ਹੈ (ਅਰਥਾਤ ਮਾਡਲ ਇਸ ਨਾਲ **ਹੈਰਾਨ ਨਹੀਂ ਹੁੰਦਾ**), ਅਤੇ ਉਹ ਵਾਕ ਜੋ ਘੱਟ ਸਮਝਦਾਰ ਲੱਗਦੇ ਹਨ (ਜਿਵੇਂ ਕਿ *Can it does what?*) ਉਨ੍ਹਾਂ ਨੂੰ ਘੱਟ ਸੰਭਾਵਨਾ ਦਿੰਦਾ ਹੈ। ਜਦੋਂ ਅਸੀਂ ਆਪਣੇ ਮਾਡਲ ਨੂੰ ਅਸਲੀ ਟੈਕਸਟ ਕਾਰਪਸ ਤੋਂ ਵਾਕ ਦਿੰਦੇ ਹਾਂ, ਤਾਂ ਅਸੀਂ ਉਨ੍ਹਾਂ ਤੋਂ ਉੱਚ ਸੰਭਾਵਨਾ ਅਤੇ ਘੱਟ **ਪੇਰਪਲੇਕਸਿਟੀ** ਦੀ ਉਮੀਦ ਕਰਦੇ ਹਾਂ। ਗਣਿਤਕ ਤੌਰ 'ਤੇ, ਇਹ ਟੈਸਟ ਸੈੱਟ ਦੀ ਨਾਰਮਲਾਈਜ਼ਡ ਇਨਵਰਸ ਸੰਭਾਵਨਾ ਵਜੋਂ ਪਰਿਭਾਸ਼ਿਤ ਹੈ:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**ਤੁਸੀਂ [Hugging Face ਤੋਂ GPT-ਚਲਾਏ ਗਏ ਟੈਕਸਟ ਐਡੀਟਰ](https://transformer.huggingface.co/doc/gpt2-large)** ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਨਾਲ ਪ੍ਰਯੋਗ ਕਰ ਸਕਦੇ ਹੋ। ਇਸ ਐਡੀਟਰ ਵਿੱਚ, ਤੁਸੀਂ ਆਪਣਾ ਟੈਕਸਟ ਲਿਖਣਾ ਸ਼ੁਰੂ ਕਰਦੇ ਹੋ, ਅਤੇ **[TAB]** ਦਬਾਉਣ ਨਾਲ ਤੁਹਾਨੂੰ ਕਈ ਪੂਰਨ ਵਿਕਲਪ ਮਿਲਦੇ ਹਨ। ਜੇ ਉਹ ਬਹੁਤ ਛੋਟੇ ਹਨ, ਜਾਂ ਤੁਸੀਂ ਉਨ੍ਹਾਂ ਨਾਲ ਸੰਤੁਸ਼ਟ ਨਹੀਂ ਹੋ - [TAB] ਨੂੰ ਮੁੜ ਦਬਾਓ, ਅਤੇ ਤੁਹਾਨੂੰ ਹੋਰ ਵਿਕਲਪ ਮਿਲਣਗੇ, ਜਿਨ੍ਹਾਂ ਵਿੱਚ ਲੰਬੇ ਟੁਕੜੇ ਸ਼ਾਮਲ ਹਨ।

## GPT ਇੱਕ ਪਰਿਵਾਰ ਹੈ

GPT ਇੱਕ ਇਕੱਲਾ ਮਾਡਲ ਨਹੀਂ ਹੈ, ਬਲਕਿ ਇਹ [OpenAI](https://openai.com) ਦੁਆਰਾ ਵਿਕਸਿਤ ਅਤੇ ਟ੍ਰੇਨ ਕੀਤੇ ਮਾਡਲਾਂ ਦਾ ਇੱਕ ਸੰਗ੍ਰਹਿ ਹੈ। 

GPT ਮਾਡਲਾਂ ਦੇ ਅਧੀਨ, ਸਾਡੇ ਕੋਲ ਹਨ:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|ਭਾਸ਼ਾ ਮਾਡਲ ਜਿਸ ਵਿੱਚ 1.5 ਬਿਲੀਅਨ ਪੈਰਾਮੀਟਰ ਹਨ। | ਭਾਸ਼ਾ ਮਾਡਲ ਜਿਸ ਵਿੱਚ 175 ਬਿਲੀਅਨ ਪੈਰਾਮੀਟਰ ਹਨ। | 100 ਟ੍ਰਿਲੀਅਨ ਪੈਰਾਮੀਟਰ ਅਤੇ ਇਹ ਟੈਕਸਟ ਅਤੇ ਚਿੱਤਰ ਦੋਹਾਂ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਸਵੀਕਾਰ ਕਰਦਾ ਹੈ ਅਤੇ ਟੈਕਸਟ ਆਉਟਪੁਟ ਕਰਦਾ ਹੈ। |

GPT-3 ਅਤੇ GPT-4 ਮਾਡਲ [ਮਾਈਕਰੋਸਾਫਟ ਐਜ਼ਿਊਰ ਤੋਂ ਇੱਕ ਕੌਗਨਿਟਿਵ ਸਰਵਿਸ ਵਜੋਂ](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) ਅਤੇ [OpenAI API](https://openai.com/api/) ਵਜੋਂ ਉਪਲਬਧ ਹਨ।

## ਪ੍ਰੌੰਪਟ ਇੰਜੀਨੀਅਰਿੰਗ

ਕਿਉਂਕਿ GPT ਨੂੰ ਭਾਸ਼ਾ ਅਤੇ ਕੋਡ ਨੂੰ ਸਮਝਣ ਲਈ ਵੱਡੇ ਪੱਧਰ ਦੇ ਡਾਟੇ 'ਤੇ ਟ੍ਰੇਨ ਕੀਤਾ ਗਿਆ ਹੈ, ਇਹ ਇਨਪੁਟ (ਪ੍ਰੌੰਪਟ) ਦੇ ਜਵਾਬ ਵਿੱਚ ਆਉਟਪੁਟ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਪ੍ਰੌੰਪਟ GPT ਦੇ ਇਨਪੁਟ ਜਾਂ ਕਵੈਰੀ ਹੁੰਦੇ ਹਨ ਜਿੱਥੇ ਕੋਈ ਮਾਡਲਾਂ ਨੂੰ ਕੰਮਾਂ ਦੀਆਂ ਹਦਾਇਤਾਂ ਦਿੰਦਾ ਹੈ ਜੋ ਉਹ ਅਗਲੇ ਪੂਰੇ ਕਰਦੇ ਹਨ। ਇੱਛਿਤ ਨਤੀਜੇ ਨੂੰ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ, ਤੁਹਾਨੂੰ ਸਭ ਤੋਂ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਪ੍ਰੌੰਪਟ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ, ਜਿਸ ਵਿੱਚ ਸਹੀ ਸ਼ਬਦਾਂ, ਫਾਰਮੈਟਾਂ, ਵਾਕਾਂਸ਼ਾਂ ਜਾਂ ਇੱਥੋਂ ਤੱਕ ਕਿ ਚਿੰਨ੍ਹਾਂ ਦੀ ਚੋਣ ਸ਼ਾਮਲ ਹੁੰਦੀ ਹੈ। ਇਸ ਪਹੁੰਚ ਨੂੰ [ਪ੍ਰੌੰਪਟ ਇੰਜੀਨੀਅਰਿੰਗ](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

[ਇਹ ਦਸਤਾਵੇਜ਼](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) ਤੁਹਾਨੂੰ ਪ੍ਰੌੰਪਟ ਇੰਜੀਨੀਅਰਿੰਗ ਬਾਰੇ ਹੋਰ ਜਾਣਕਾਰੀ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

## ✍️ ਉਦਾਹਰਣ ਨੋਟਬੁੱਕ: [OpenAI-GPT ਨਾਲ ਖੇਡਣਾ](GPT-PyTorch.ipynb)

ਹੇਠਾਂ ਦਿੱਤੇ ਨੋਟਬੁੱਕਾਂ ਵਿੱਚ ਆਪਣੀ ਸਿੱਖਿਆ ਜਾਰੀ ਰੱਖੋ:

* [OpenAI-GPT ਅਤੇ Hugging Face ਟ੍ਰਾਂਸਫਾਰਮਰਜ਼ ਨਾਲ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ](GPT-PyTorch.ipynb)

## ਨਿਸਕਰਸ਼

ਨਵੇਂ ਜਨਰਲ ਪ੍ਰੀ-ਟ੍ਰੇਨਡ ਭਾਸ਼ਾ ਮਾਡਲ ਸਿਰਫ ਭਾਸ਼ਾ ਦੀ ਬਣਤਰ ਨੂੰ ਹੀ ਮਾਡਲ ਨਹੀਂ ਕਰਦੇ, ਬਲਕਿ ਇਹ ਪ੍ਰਾਕ੍ਰਿਤਿਕ ਭਾਸ਼ਾ ਦੀ ਵੱਡੀ ਮਾਤਰਾ ਨੂੰ ਵੀ ਸ਼ਾਮਲ ਕਰਦੇ ਹਨ। ਇਸ ਲਈ, ਇਹ ਕੁਝ NLP ਕੰਮਾਂ ਨੂੰ ਜ਼ੀਰੋ-ਸ਼ੌਟ ਜਾਂ ਫਿਊ-ਸ਼ੌਟ ਸੈਟਿੰਗਜ਼ ਵਿੱਚ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਢੰਗ ਨਾਲ ਹੱਲ ਕਰਨ ਲਈ ਵਰਤੇ ਜਾ ਸਕਦੇ ਹਨ।

## [ਪੋਸਟ-ਲੈਕਚਰ ਕਵਿਜ਼](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

