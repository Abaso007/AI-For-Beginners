<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-26T08:44:23+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "pa"
}
-->
# ਪ੍ਰੀ-ਟ੍ਰੇਨਡ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲ

ਸਾਡੇ ਪਿਛਲੇ ਸਾਰੇ ਕੰਮਾਂ ਵਿੱਚ, ਅਸੀਂ ਲੇਬਲ ਕੀਤੇ ਡਾਟਾਸੈਟ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ ਨਿਊਰਲ ਨੈਟਵਰਕ ਨੂੰ ਇੱਕ ਨਿਰਧਾਰਤ ਕੰਮ ਕਰਨ ਲਈ ਟ੍ਰੇਨ ਕਰ ਰਹੇ ਸੀ। ਵੱਡੇ ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲਾਂ, ਜਿਵੇਂ ਕਿ BERT, ਨਾਲ ਅਸੀਂ ਸਵੈ-ਸੁਪਰਵਾਈਜ਼ਡ ਢੰਗ ਵਿੱਚ ਭਾਸ਼ਾ ਮਾਡਲਿੰਗ ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹਾਂ ਤਾਂ ਜੋ ਇੱਕ ਭਾਸ਼ਾ ਮਾਡਲ ਬਣਾਇਆ ਜਾ ਸਕੇ, ਜਿਸਨੂੰ ਫਿਰ ਖਾਸ ਡਾਊਨਸਟ੍ਰੀਮ ਕੰਮ ਲਈ ਅਗਲੇ ਡੋਮੇਨ-ਵਿਸ਼ੇਸ਼ ਟ੍ਰੇਨਿੰਗ ਨਾਲ ਵਿਸ਼ੇਸ਼ ਬਣਾਇਆ ਜਾਂਦਾ ਹੈ। ਹਾਲਾਂਕਿ, ਇਹ ਸਾਬਤ ਹੋਇਆ ਹੈ ਕਿ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲ ਬਿਨਾਂ ਕਿਸੇ ਡੋਮੇਨ-ਵਿਸ਼ੇਸ਼ ਟ੍ਰੇਨਿੰਗ ਦੇ ਕਈ ਕੰਮਾਂ ਨੂੰ ਹੱਲ ਕਰ ਸਕਦੇ ਹਨ। ਮਾਡਲਾਂ ਦਾ ਇੱਕ ਪਰਿਵਾਰ ਜੋ ਇਹ ਕਰਨ ਦੇ ਯੋਗ ਹੈ, **GPT**: ਜਨਰੇਟਿਵ ਪ੍ਰੀ-ਟ੍ਰੇਨਡ ਟ੍ਰਾਂਸਫਾਰਮਰ ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

## [ਪ੍ਰੀ-ਲੈਕਚਰ ਕਵਿਜ਼](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/120)

## ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਅਤੇ ਪਰਪਲੈਕਸਿਟੀ

ਇੱਕ ਨਿਊਰਲ ਨੈਟਵਰਕ ਦੇ ਸਮਰੱਥ ਹੋਣ ਦਾ ਵਿਚਾਰ ਕਿ ਉਹ ਡਾਊਨਸਟ੍ਰੀਮ ਟ੍ਰੇਨਿੰਗ ਤੋਂ ਬਿਨਾਂ ਜਨਰਲ ਕੰਮ ਕਰ ਸਕਦਾ ਹੈ, [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ਪੇਪਰ ਵਿੱਚ ਪੇਸ਼ ਕੀਤਾ ਗਿਆ ਹੈ। ਮੁੱਖ ਵਿਚਾਰ ਇਹ ਹੈ ਕਿ ਕਈ ਹੋਰ ਕੰਮਾਂ ਨੂੰ **ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ** ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਮਾਡਲ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ, ਕਿਉਂਕਿ ਟੈਕਸਟ ਨੂੰ ਸਮਝਣਾ ਅਸਲ ਵਿੱਚ ਇਸਨੂੰ ਪੈਦਾ ਕਰਨ ਦੇ ਯੋਗ ਹੋਣਾ ਹੈ। ਕਿਉਂਕਿ ਮਾਡਲ ਨੂੰ ਬਹੁਤ ਵੱਡੀ ਮਾਤਰਾ ਦੇ ਟੈਕਸਟ 'ਤੇ ਟ੍ਰੇਨ ਕੀਤਾ ਗਿਆ ਹੈ ਜੋ ਮਨੁੱਖੀ ਗਿਆਨ ਨੂੰ ਸ਼ਾਮਲ ਕਰਦਾ ਹੈ, ਇਹ ਵੱਖ-ਵੱਖ ਵਿਸ਼ਿਆਂ ਬਾਰੇ ਜਾਣਕਾਰੀ ਵਾਲਾ ਵੀ ਬਣ ਜਾਂਦਾ ਹੈ।

> ਟੈਕਸਟ ਨੂੰ ਸਮਝਣਾ ਅਤੇ ਪੈਦਾ ਕਰਨ ਦੇ ਯੋਗ ਹੋਣਾ ਇਸਦਾ ਮਤਲਬ ਹੈ ਕਿ ਦੁਨੀਆ ਦੇ ਆਸ-ਪਾਸ ਕੁਝ ਜਾਣਨਾ। ਲੋਕ ਵੀ ਵੱਡੇ ਪੱਧਰ 'ਤੇ ਪੜ੍ਹ ਕੇ ਸਿੱਖਦੇ ਹਨ, ਅਤੇ GPT ਨੈਟਵਰਕ ਇਸ ਮਾਮਲੇ ਵਿੱਚ ਇਸਦੇ ਸਮਾਨ ਹੈ।

ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਨੈਟਵਰਕ ਅਗਲੇ ਸ਼ਬਦ $$P(w_N)$$ ਦੀ ਸੰਭਾਵਨਾ ਦੀ ਪੇਸ਼ਕਸ਼ ਕਰਕੇ ਕੰਮ ਕਰਦੇ ਹਨ। ਹਾਲਾਂਕਿ, ਅਗਲੇ ਸ਼ਬਦ ਦੀ ਬਿਨਾਂ ਸ਼ਰਤਾਂ ਵਾਲੀ ਸੰਭਾਵਨਾ ਟੈਕਸਟ ਕੋਰਪਸ ਵਿੱਚ ਇਸ ਸ਼ਬਦ ਦੀ ਆਵ੍ਰਿਤੀ ਦੇ ਬਰਾਬਰ ਹੁੰਦੀ ਹੈ। GPT ਸਾਨੂੰ ਪਿਛਲੇ ਸ਼ਬਦਾਂ ਦੇ ਆਧਾਰ 'ਤੇ ਅਗਲੇ ਸ਼ਬਦ ਦੀ **ਸ਼ਰਤਾਂ ਵਾਲੀ ਸੰਭਾਵਨਾ** ਦੇਣ ਦੇ ਯੋਗ ਹੈ: $$P(w_N | w_{n-1}, ..., w_0)$$

> ਤੁਸੀਂ ਸੰਭਾਵਨਾਵਾਂ ਬਾਰੇ ਹੋਰ ਪੜ੍ਹ ਸਕਦੇ ਹੋ ਸਾਡੇ [Data Science for Beginners Curriculum](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) ਵਿੱਚ।

ਭਾਸ਼ਾ ਜਨਰੇਸ਼ਨ ਮਾਡਲ ਦੀ ਗੁਣਵੱਤਾ ਨੂੰ **ਪਰਪਲੈਕਸਿਟੀ** ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ। ਇਹ ਇੱਕ ਅੰਦਰੂਨੀ ਮਾਪ ਹੈ ਜੋ ਸਾਨੂੰ ਕਿਸੇ ਟਾਸਕ-ਵਿਸ਼ੇਸ਼ ਡਾਟਾਸੈਟ ਤੋਂ ਬਿਨਾਂ ਮਾਡਲ ਦੀ ਗੁਣਵੱਤਾ ਨੂੰ ਮਾਪਣ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ। ਇਹ *ਵਾਕ ਦੀ ਸੰਭਾਵਨਾ* ਦੇ ਧਾਰਨਾ 'ਤੇ ਆਧਾਰਿਤ ਹੈ - ਮਾਡਲ ਉੱਚ ਸੰਭਾਵਨਾ ਉਸ ਵਾਕ ਨੂੰ ਦਿੰਦਾ ਹੈ ਜੋ ਹਕੀਕਤ ਵਿੱਚ ਹੋਣ ਦੀ ਸੰਭਾਵਨਾ ਹੈ (ਅਰਥਾਤ ਮਾਡਲ ਇਸਨੂੰ **ਹੈਰਾਨ** ਨਹੀਂ ਕਰਦਾ), ਅਤੇ ਘੱਟ ਸੰਭਾਵਨਾ ਉਹਨਾਂ ਵਾਕਾਂ ਨੂੰ ਦਿੰਦਾ ਹੈ ਜੋ ਘੱਟ ਅਰਥਵਾਨ ਹੁੰਦੇ ਹਨ (ਜਿਵੇਂ *Can it does what?*)। ਜਦੋਂ ਅਸੀਂ ਆਪਣੇ ਮਾਡਲ ਨੂੰ ਅਸਲ ਟੈਕਸਟ ਕੋਰਪਸ ਤੋਂ ਵਾਕ ਦਿੰਦੇ ਹਾਂ, ਤਾਂ ਅਸੀਂ ਉਨ੍ਹਾਂ ਤੋਂ ਉੱਚ ਸੰਭਾਵਨਾ ਅਤੇ ਘੱਟ **ਪਰਪਲੈਕਸਿਟੀ** ਦੀ ਉਮੀਦ ਕਰਦੇ ਹਾਂ। ਗਣਿਤਕ ਤੌਰ 'ਤੇ, ਇਹ ਟੈਸਟ ਸੈਟ ਦੀ ਨਾਰਮਲਾਈਜ਼ਡ ਇਨਵਰਸ ਸੰਭਾਵਨਾ ਵਜੋਂ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤਾ ਜਾਂਦਾ ਹੈ:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**ਤੁਸੀਂ [GPT-powered text editor from Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)** ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਨਾਲ ਪ੍ਰਯੋਗ ਕਰ ਸਕਦੇ ਹੋ। ਇਸ ਐਡੀਟਰ ਵਿੱਚ, ਤੁਸੀਂ ਆਪਣਾ ਟੈਕਸਟ ਲਿਖਣਾ ਸ਼ੁਰੂ ਕਰਦੇ ਹੋ, ਅਤੇ **[TAB]** ਦਬਾਉਣ ਨਾਲ ਤੁਹਾਨੂੰ ਕਈ ਪੂਰਨ ਵਿਕਲਪ ਮਿਲਦੇ ਹਨ। ਜੇ ਉਹ ਬਹੁਤ ਛੋਟੇ ਹਨ, ਜਾਂ ਤੁਸੀਂ ਉਨ੍ਹਾਂ ਨਾਲ ਸੰਤੁਸ਼ਟ ਨਹੀਂ ਹੋ - [TAB] ਨੂੰ ਫਿਰ ਦਬਾਓ, ਅਤੇ ਤੁਹਾਨੂੰ ਹੋਰ ਵਿਕਲਪ ਮਿਲਣਗੇ, ਜਿਨ੍ਹਾਂ ਵਿੱਚ ਲੰਬੇ ਟੈਕਸਟ ਦੇ ਟੁਕੜੇ ਸ਼ਾਮਲ ਹਨ।

## GPT ਇੱਕ ਪਰਿਵਾਰ ਹੈ

GPT ਇੱਕ ਇਕੱਲਾ ਮਾਡਲ ਨਹੀਂ ਹੈ, ਬਲਕਿ [OpenAI](https://openai.com) ਦੁਆਰਾ ਵਿਕਸਿਤ ਅਤੇ ਟ੍ਰੇਨ ਕੀਤੇ ਮਾਡਲਾਂ ਦਾ ਇੱਕ ਸੰਗ੍ਰਹਿ ਹੈ।

GPT ਮਾਡਲਾਂ ਦੇ ਅਧੀਨ, ਸਾਡੇ ਕੋਲ ਹਨ:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|ਭਾਸ਼ਾ ਮਾਡਲ ਜਿਸ ਵਿੱਚ 1.5 ਬਿਲੀਅਨ ਪੈਰਾਮੀਟਰ ਹਨ। | ਭਾਸ਼ਾ ਮਾਡਲ ਜਿਸ ਵਿੱਚ 175 ਬਿਲੀਅਨ ਪੈਰਾਮੀਟਰ ਹਨ। | 100T ਪੈਰਾਮੀਟਰ ਅਤੇ ਇਹ ਦੋਵੇਂ ਚਿੱਤਰ ਅਤੇ ਟੈਕਸਟ ਇਨਪੁਟ ਲੈਂਦਾ ਹੈ ਅਤੇ ਟੈਕਸਟ ਆਉਟਪੁਟ ਕਰਦਾ ਹੈ। |

GPT-3 ਅਤੇ GPT-4 ਮਾਡਲ [Microsoft Azure ਤੋਂ ਇੱਕ ਕਾਗਨੀਟਿਵ ਸੇਵਾ](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) ਵਜੋਂ, ਅਤੇ [OpenAI API](https://openai.com/api/) ਵਜੋਂ ਉਪਲਬਧ ਹਨ।

## ਪ੍ਰਾਂਪਟ ਇੰਜੀਨੀਅਰਿੰਗ

ਕਿਉਂਕਿ GPT ਨੂੰ ਭਾਸ਼ਾ ਅਤੇ ਕੋਡ ਨੂੰ ਸਮਝਣ ਲਈ ਵੱਡੀ ਮਾਤਰਾ ਦੇ ਡਾਟਾ 'ਤੇ ਟ੍ਰੇਨ ਕੀਤਾ ਗਿਆ ਹੈ, ਇਹ ਇਨਪੁਟ (ਪ੍ਰਾਂਪਟ) ਦੇ ਜਵਾਬ ਵਿੱਚ ਆਉਟਪੁਟ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਪ੍ਰਾਂਪਟ GPT ਇਨਪੁਟ ਜਾਂ ਕਵੈਰੀ ਹੁੰਦੇ ਹਨ ਜਿਨ੍ਹਾਂ ਵਿੱਚ ਕੋਈ ਮਾਡਲਾਂ ਨੂੰ ਅਗਲੇ ਕੰਮਾਂ ਦੀਆਂ ਹਦਾਇਤਾਂ ਦਿੰਦਾ ਹੈ। ਇੱਛਿਤ ਨਤੀਜੇ ਨੂੰ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ, ਤੁਹਾਨੂੰ ਸਭ ਤੋਂ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਪ੍ਰਾਂਪਟ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ, ਜਿਸ ਵਿੱਚ ਸਹੀ ਸ਼ਬਦ, ਫਾਰਮੈਟ, ਵਾਕਾਂਸ਼ ਜਾਂ ਇੱਥੋਂ ਤੱਕ ਕਿ ਚਿੰਨ੍ਹ ਚੁਣਨਾ ਸ਼ਾਮਲ ਹੁੰਦਾ ਹੈ। ਇਸ ਪਹੁੰਚ ਨੂੰ [Prompt Engineering](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

[ਇਹ ਦਸਤਾਵੇਜ਼](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) ਤੁਹਾਨੂੰ ਪ੍ਰਾਂਪਟ ਇੰਜੀਨੀਅਰਿੰਗ ਬਾਰੇ ਹੋਰ ਜਾਣਕਾਰੀ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

## ✍️ ਉਦਾਹਰਨ ਨੋਟਬੁੱਕ: [OpenAI-GPT ਨਾਲ ਖੇਡਣਾ](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

ਹੇਠਾਂ ਦਿੱਤੇ ਨੋਟਬੁੱਕਾਂ ਵਿੱਚ ਆਪਣਾ ਸਿੱਖਣਾ ਜਾਰੀ ਰੱਖੋ:

* [OpenAI-GPT ਅਤੇ Hugging Face Transformers ਨਾਲ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## ਨਿਸਕਰਸ਼

ਨਵੇਂ ਜਨਰਲ ਪ੍ਰੀ-ਟ੍ਰੇਨਡ ਭਾਸ਼ਾ ਮਾਡਲ ਸਿਰਫ਼ ਭਾਸ਼ਾ ਦੀ ਬਣਤਰ ਨੂੰ ਹੀ ਮਾਡਲ ਨਹੀਂ ਕਰਦੇ, ਬਲਕਿ ਬਹੁਤ ਵੱਡੀ ਮਾਤਰਾ ਵਿੱਚ ਕੁਦਰਤੀ ਭਾਸ਼ਾ ਨੂੰ ਵੀ ਸ਼ਾਮਲ ਕਰਦੇ ਹਨ। ਇਸ ਲਈ, ਉਹ ਕੁਝ NLP ਕੰਮਾਂ ਨੂੰ ਜ਼ੀਰੋ-ਸ਼ਾਪ ਜਾਂ ਫਿਊ-ਸ਼ਾਪ ਸੈਟਿੰਗ ਵਿੱਚ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਢੰਗ ਨਾਲ ਹੱਲ ਕਰਨ ਲਈ ਵਰਤੇ ਜਾ ਸਕਦੇ ਹਨ।

## [ਪੋਸਟ-ਲੈਕਚਰ ਕਵਿਜ਼](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/220)

**ਅਸਵੀਕਾਰਨਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚੱਜੇਪਣ ਹੋ ਸਕਦੇ ਹਨ। ਇਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੌਜੂਦ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਅਧਿਕਾਰਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੇ ਉਪਯੋਗ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।