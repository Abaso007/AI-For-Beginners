{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਟੈਕਸਟ ਵਰਗੀਕਰਨ ਟਾਸਕ\n",
    "\n",
    "ਇਸ ਮੋਡੀਊਲ ਵਿੱਚ, ਅਸੀਂ **[AG_NEWS](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)** ਡੇਟਾਸੈੱਟ ਦੇ ਆਧਾਰ 'ਤੇ ਇੱਕ ਸਧਾਰਨ ਟੈਕਸਟ ਵਰਗੀਕਰਨ ਟਾਸਕ ਸ਼ੁਰੂ ਕਰਾਂਗੇ: ਅਸੀਂ ਖਬਰਾਂ ਦੇ ਸਿਰਲੇਖਾਂ ਨੂੰ 4 ਸ਼੍ਰੇਣੀਆਂ ਵਿੱਚ ਵਰਗਬੱਧ ਕਰਾਂਗੇ: ਵਿਸ਼ਵ, ਖੇਡਾਂ, ਕਾਰੋਬਾਰ ਅਤੇ ਵਿਗਿਆਨ/ਤਕਨਾਲੋਜੀ।\n",
    "\n",
    "## ਡੇਟਾਸੈੱਟ\n",
    "\n",
    "ਡੇਟਾਸੈੱਟ ਨੂੰ ਲੋਡ ਕਰਨ ਲਈ, ਅਸੀਂ **[TensorFlow Datasets](https://www.tensorflow.org/datasets)** API ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# In this tutorial, we will be training a lot of models. In order to use GPU memory cautiously,\n",
    "# we will set tensorflow option to grow GPU memory allocation when required.\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(physical_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "dataset = tfds.load('ag_news_subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਹੁਣ `dataset['train']` ਅਤੇ `dataset['test']` ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਡੇਟਾਸੈੱਟ ਦੇ ਟ੍ਰੇਨਿੰਗ ਅਤੇ ਟੈਸਟ ਹਿੱਸਿਆਂ ਤੱਕ ਪਹੁੰਚ ਕਰ ਸਕਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 120000\n",
      "Length of test dataset = 7600\n"
     ]
    }
   ],
   "source": [
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "\n",
    "print(f\"Length of train dataset = {len(ds_train)}\")\n",
    "print(f\"Length of test dataset = {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਆਓ ਆਪਣੇ ਡੇਟਾਸੈੱਟ ਵਿੱਚੋਂ ਪਹਿਲੀਆਂ 10 ਨਵੀਆਂ ਸੁਰਖੀਆਂ ਪ੍ਰਿੰਟ ਕਰੀਏ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n",
      "3 (Sci/Tech) -> b\"'Halt science decline in schools'\" b'Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.'\n",
      "1 (Sports) -> b'Gerrard leaves practice' b'London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.'\n"
     ]
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "for i,x in zip(range(5),ds_train):\n",
    "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਟੈਕਸਟ ਵੇਕਟਰਾਈਜ਼ੇਸ਼ਨ\n",
    "\n",
    "ਹੁਣ ਸਾਨੂੰ ਟੈਕਸਟ ਨੂੰ **ਨੰਬਰਾਂ** ਵਿੱਚ ਬਦਲਣਾ ਪਵੇਗਾ ਜੋ ਟੈਂਸਰ ਦੇ ਰੂਪ ਵਿੱਚ ਦਰਸਾਏ ਜਾ ਸਕਣ। ਜੇਕਰ ਅਸੀਂ ਸ਼ਬਦ-ਪੱਧਰ ਦੀ ਪ੍ਰਤੀਨਿਧਤਾ ਚਾਹੁੰਦੇ ਹਾਂ, ਤਾਂ ਸਾਨੂੰ ਦੋ ਗੱਲਾਂ ਕਰਨੀਆਂ ਪੈਣਗੀਆਂ:\n",
    "\n",
    "* ਇੱਕ **ਟੋਕਨਾਈਜ਼ਰ** ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟੈਕਸਟ ਨੂੰ **ਟੋਕਨਜ਼** ਵਿੱਚ ਵੰਡੋ।\n",
    "* ਉਹਨਾਂ ਟੋਕਨਜ਼ ਦੀ ਇੱਕ **ਸ਼ਬਦਾਵਲੀ** ਤਿਆਰ ਕਰੋ।\n",
    "\n",
    "### ਸ਼ਬਦਾਵਲੀ ਦੇ ਆਕਾਰ ਨੂੰ ਸੀਮਿਤ ਕਰਨਾ\n",
    "\n",
    "AG News ਡੇਟਾਸੈੱਟ ਦੇ ਉਦਾਹਰਨ ਵਿੱਚ, ਸ਼ਬਦਾਵਲੀ ਦਾ ਆਕਾਰ ਕਾਫ਼ੀ ਵੱਡਾ ਹੈ, 100,000 ਤੋਂ ਵੱਧ ਸ਼ਬਦ। ਆਮ ਤੌਰ 'ਤੇ, ਸਾਨੂੰ ਉਹ ਸ਼ਬਦਾਂ ਦੀ ਲੋੜ ਨਹੀਂ ਹੁੰਦੀ ਜੋ ਟੈਕਸਟ ਵਿੱਚ ਘੱਟ ਹੀ ਮਿਲਦੇ ਹਨ — ਸਿਰਫ ਕੁਝ ਹੀ ਵਾਕਾਂ ਵਿੱਚ ਉਹ ਸ਼ਬਦ ਹੋਣਗੇ, ਅਤੇ ਮਾਡਲ ਉਹਨਾਂ ਤੋਂ ਕੁਝ ਨਹੀਂ ਸਿੱਖੇਗਾ। ਇਸ ਲਈ, ਸ਼ਬਦਾਵਲੀ ਦੇ ਆਕਾਰ ਨੂੰ ਛੋਟੇ ਨੰਬਰ ਤੱਕ ਸੀਮਿਤ ਕਰਨਾ ਸਮਝਦਾਰੀ ਹੈ, ਜੋ ਕਿ ਵੇਕਟਰਾਈਜ਼ਰ ਕੰਸਟਰਕਟਰ ਨੂੰ ਇੱਕ ਆਰਗੂਮੈਂਟ ਦੇ ਕੇ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ:\n",
    "\n",
    "ਇਹ ਦੋਵੇਂ ਕਦਮ **TextVectorization** ਲੇਅਰ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਸੰਭਾਲੇ ਜਾ ਸਕਦੇ ਹਨ। ਆਓ ਵੇਕਟਰਾਈਜ਼ਰ ਆਬਜੈਕਟ ਬਣਾਈਏ ਅਤੇ ਫਿਰ `adapt` ਮੈਥਡ ਨੂੰ ਕਾਲ ਕਰਕੇ ਸਾਰੇ ਟੈਕਸਟ ਵਿੱਚੋਂ ਗੁਜ਼ਰ ਕੇ ਇੱਕ ਸ਼ਬਦਾਵਲੀ ਤਿਆਰ ਕਰੀਏ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title']+' '+x['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ਨੋਟ** ਕਿ ਅਸੀਂ ਸਿਰਫ਼ ਪੂਰੇ ਡੇਟਾਸੈੱਟ ਦੇ ਇੱਕ ਹਿੱਸੇ ਨੂੰ ਵਰਤ ਰਹੇ ਹਾਂ ਤਾਂ ਜੋ ਇੱਕ ਸ਼ਬਦਾਵਲੀ ਤਿਆਰ ਕੀਤੀ ਜਾ ਸਕੇ। ਅਸੀਂ ਇਹ ਇਸ ਲਈ ਕਰਦੇ ਹਾਂ ਤਾਂ ਕਿ ਕਾਰਜਕਾਲ ਦਾ ਸਮਾਂ ਤੇਜ਼ ਹੋਵੇ ਅਤੇ ਤੁਹਾਨੂੰ ਉਡੀਕ ਨਾ ਕਰਨੀ ਪਵੇ। ਹਾਲਾਂਕਿ, ਅਸੀਂ ਇਹ ਜੋਖਮ ਲੈ ਰਹੇ ਹਾਂ ਕਿ ਪੂਰੇ ਡੇਟਾਸੈੱਟ ਵਿੱਚੋਂ ਕੁਝ ਸ਼ਬਦ ਸ਼ਬਦਾਵਲੀ ਵਿੱਚ ਸ਼ਾਮਲ ਨਹੀਂ ਹੋਣਗੇ ਅਤੇ ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਅਣਡਿੱਠੇ ਰਹਿ ਜਾਣਗੇ। ਇਸ ਤਰ੍ਹਾਂ, ਪੂਰੀ ਸ਼ਬਦਾਵਲੀ ਦਾ ਆਕਾਰ ਵਰਤਣਾ ਅਤੇ `adapt` ਦੌਰਾਨ ਸਾਰੇ ਡੇਟਾਸੈੱਟ ਵਿੱਚੋਂ ਗੁਜ਼ਰਨਾ ਅੰਤਿਮ ਸਹੀਤਾ ਨੂੰ ਵਧਾ ਸਕਦਾ ਹੈ, ਪਰ ਇਹ ਵਾਧਾ ਬਹੁਤ ਵੱਡਾ ਨਹੀਂ ਹੋਵੇਗਾ।\n",
    "\n",
    "ਹੁਣ ਅਸੀਂ ਅਸਲ ਸ਼ਬਦਾਵਲੀ ਤੱਕ ਪਹੁੰਚ ਸਕਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for']\n",
      "Length of vocabulary: 5335\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "print(vocab[:10])\n",
    "print(f\"Length of vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਵੇਕਟੋਰਾਈਜ਼ਰ ਦੀ ਵਰਤੋਂ ਕਰਕੇ, ਅਸੀਂ ਆਸਾਨੀ ਨਾਲ ਕਿਸੇ ਵੀ ਟੈਕਸਟ ਨੂੰ ਨੰਬਰਾਂ ਦੇ ਸੈੱਟ ਵਿੱਚ ਕੋਡ ਕਰ ਸਕਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 112, 3695,    3,  304,   11, 1041,    1], dtype=int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਬੈਗ-ਆਫ-ਵਰਡਸ ਟੈਕਸਟ ਪ੍ਰਤੀਨਿਧਤਾ\n",
    "\n",
    "ਕਿਉਂਕਿ ਸ਼ਬਦ ਅਰਥ ਨੂੰ ਦਰਸਾਉਂਦੇ ਹਨ, ਕਈ ਵਾਰ ਅਸੀਂ ਸਿਰਫ਼ ਵੱਖ-ਵੱਖ ਸ਼ਬਦਾਂ ਨੂੰ ਦੇਖ ਕੇ ਟੈਕਸਟ ਦੇ ਅਰਥ ਨੂੰ ਸਮਝ ਸਕਦੇ ਹਾਂ, ਭਾਵੇਂ ਉਹ ਵਾਕ ਵਿੱਚ ਕਿਸੇ ਵੀ ਕ੍ਰਮ ਵਿੱਚ ਹੋਣ। ਉਦਾਹਰਣ ਵਜੋਂ, ਜਦੋਂ ਅਸੀਂ ਖ਼ਬਰਾਂ ਦੀ ਵਰਗੀਕਰਨ ਕਰਦੇ ਹਾਂ, ਤਾਂ *weather* ਅਤੇ *snow* ਵਰਗੇ ਸ਼ਬਦ *weather forecast* ਵੱਲ ਇਸ਼ਾਰਾ ਕਰਦੇ ਹਨ, ਜਦਕਿ *stocks* ਅਤੇ *dollar* ਵਰਗੇ ਸ਼ਬਦ *financial news* ਵੱਲ ਗਿਣੇ ਜਾਂਦੇ ਹਨ।\n",
    "\n",
    "**ਬੈਗ-ਆਫ-ਵਰਡਸ** (BoW) ਵੇਕਟਰ ਪ੍ਰਤੀਨਿਧਤਾ ਸਭ ਤੋਂ ਸਧਾਰਨ ਅਤੇ ਸਮਝਣ ਵਿੱਚ ਆਸਾਨ ਰਵਾਇਤੀ ਵੇਕਟਰ ਪ੍ਰਤੀਨਿਧਤਾ ਹੈ। ਹਰ ਸ਼ਬਦ ਨੂੰ ਇੱਕ ਵੇਕਟਰ ਇੰਡੈਕਸ ਨਾਲ ਜੋੜਿਆ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਇੱਕ ਵੇਕਟਰ ਤੱਤ ਵਿੱਚ ਦਿੱਤੇ ਗਏ ਦਸਤਾਵੇਜ਼ ਵਿੱਚ ਹਰ ਸ਼ਬਦ ਦੇ ਆਵਿਰਤੀ ਦੀ ਗਿਣਤੀ ਹੁੰਦੀ ਹੈ।\n",
    "\n",
    "![ਇਮੇਜ ਦਿਖਾ ਰਿਹਾ ਹੈ ਕਿ ਬੈਗ-ਆਫ-ਵਰਡਸ ਵੇਕਟਰ ਪ੍ਰਤੀਨਿਧਤਾ ਮੈਮੋਰੀ ਵਿੱਚ ਕਿਵੇਂ ਦਰਸਾਈ ਜਾਂਦੀ ਹੈ।](../../../../../translated_images/bag-of-words-example.606fc1738f1d7ba98a9d693e3bcd706c6e83fa7bf8221e6e90d1a206d82f2ea4.pa.png)\n",
    "\n",
    "> **Note**: ਤੁਸੀਂ BoW ਨੂੰ ਟੈਕਸਟ ਵਿੱਚ ਵੱਖ-ਵੱਖ ਸ਼ਬਦਾਂ ਲਈ ਇੱਕ-ਹਾਟ-ਐਨਕੋਡਡ ਵੇਕਟਰਾਂ ਦੇ ਜੋੜ ਵਜੋਂ ਵੀ ਸੋਚ ਸਕਦੇ ਹੋ।\n",
    "\n",
    "ਹੇਠਾਂ Scikit Learn ਪਾਇਥਨ ਲਾਇਬ੍ਰੇਰੀ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਬੈਗ-ਆਫ-ਵਰਡਸ ਪ੍ਰਤੀਨਿਧਤਾ ਬਣਾਉਣ ਦਾ ਇੱਕ ਉਦਾਹਰਣ ਦਿੱਤਾ ਗਿਆ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sc_vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "sc_vectorizer.fit_transform(corpus)\n",
    "sc_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਉਪਰ ਪਰिभਾਸ਼ਿਤ ਕੀਤਾ ਕੈਰਾਸ ਵੈਕਟਰਾਈਜ਼ਰ ਵੀ ਵਰਤ ਸਕਦੇ ਹਾਂ, ਹਰ ਸ਼ਬਦ ਦੀ ਗਿਣਤੀ ਨੂੰ ਇੱਕ-ਹਾਟ ਕੋਡਿੰਗ ਵਿੱਚ ਰੂਪਾਂਤਰਿਤ ਕਰਦੇ ਹੋਏ ਅਤੇ ਉਹਨਾਂ ਸਾਰੇ ਵੈਕਟਰਾਂ ਨੂੰ ਜੋੜਦੇ ਹੋਏ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 5., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_bow(text):\n",
    "    return tf.reduce_sum(tf.one_hot(vectorizer(text),vocab_size),axis=0)\n",
    "\n",
    "to_bow('My dog likes hot dogs on a hot day.').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ਨੋਟ**: ਤੁਸੀਂ ਹੈਰਾਨ ਹੋ ਸਕਦੇ ਹੋ ਕਿ ਨਤੀਜਾ ਪਿਛਲੇ ਉਦਾਹਰਨ ਤੋਂ ਵੱਖਰਾ ਹੈ। ਇਸ ਦਾ ਕਾਰਨ ਇਹ ਹੈ ਕਿ Keras ਦੇ ਉਦਾਹਰਨ ਵਿੱਚ ਵੇਕਟਰ ਦੀ ਲੰਬਾਈ ਸ਼ਬਦਾਵਲੀ ਦੇ ਆਕਾਰ ਦੇ ਬਰਾਬਰ ਹੈ, ਜੋ ਪੂਰੇ AG News ਡੇਟਾਸੈੱਟ ਤੋਂ ਬਣਾਈ ਗਈ ਸੀ, ਜਦਕਿ Scikit Learn ਦੇ ਉਦਾਹਰਨ ਵਿੱਚ ਅਸੀਂ ਸ਼ਬਦਾਵਲੀ ਨੂੰ ਸੈਂਪਲ ਟੈਕਸਟ ਤੋਂ ਤੁਰੰਤ ਬਣਾਇਆ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW ਕਲਾਸੀਫਾਇਰ ਦੀ ਟ੍ਰੇਨਿੰਗ\n",
    "\n",
    "ਹੁਣ ਜਦੋਂ ਅਸੀਂ ਸਿੱਖ ਲਿਆ ਹੈ ਕਿ ਆਪਣੇ ਟੈਕਸਟ ਦਾ ਬੈਗ-ਆਫ-ਵਰਡਸ ਪ੍ਰਤੀਨਿਧੀ ਕਿਵੇਂ ਬਣਾਉਣਾ ਹੈ, ਆਓ ਇੱਕ ਕਲਾਸੀਫਾਇਰ ਟ੍ਰੇਨ ਕਰੀਏ ਜੋ ਇਸਨੂੰ ਵਰਤਦਾ ਹੈ। ਸਭ ਤੋਂ ਪਹਿਲਾਂ, ਸਾਨੂੰ ਆਪਣਾ ਡੇਟਾਸੈਟ ਬੈਗ-ਆਫ-ਵਰਡਸ ਪ੍ਰਤੀਨਿਧੀ ਵਿੱਚ ਤਬਦੀਲ ਕਰਨਾ ਪਵੇਗਾ। ਇਹ `map` ਫੰਕਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਹਾਸਲ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "ds_train_bow = ds_train.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)\n",
    "ds_test_bow = ds_test.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਆਓ ਇੱਕ ਸਧਾਰਨ ਕਲਾਸੀਫਾਇਰ ਨਿਊਰਲ ਨੈਟਵਰਕ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰੀਏ ਜੋ ਇੱਕ ਲਿਨੀਅਰ ਲੇਅਰ ਸ਼ਾਮਲ ਕਰਦਾ ਹੈ। ਇਨਪੁਟ ਸਾਈਜ਼ `vocab_size` ਹੈ, ਅਤੇ ਆਉਟਪੁਟ ਸਾਈਜ਼ ਕਲਾਸਾਂ ਦੀ ਗਿਣਤੀ (4) ਦੇ ਅਨੁਸਾਰ ਹੈ। ਕਿਉਂਕਿ ਅਸੀਂ ਇੱਕ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਟਾਸਕ ਹੱਲ ਕਰ ਰਹੇ ਹਾਂ, ਅੰਤਮ ਐਕਟੀਵੇਸ਼ਨ ਫੰਕਸ਼ਨ **softmax** ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 66s 70ms/step - loss: 0.6144 - acc: 0.8427 - val_loss: 0.4416 - val_acc: 0.8697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c70a947f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4,activation='softmax',input_shape=(vocab_size,))\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train_bow,validation_data=ds_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਚਾਰ ਕਲਾਸਾਂ ਹੋਣ ਕਰਕੇ, 80% ਤੋਂ ਵੱਧ ਦੀ ਸਹੀਤਾ ਇੱਕ ਵਧੀਆ ਨਤੀਜਾ ਹੈ।\n",
    "\n",
    "## ਇੱਕ ਨੈੱਟਵਰਕ ਵਜੋਂ ਕਲਾਸੀਫਾਇਰ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ\n",
    "\n",
    "ਕਿਉਂਕਿ ਵੈਕਟੋਰਾਈਜ਼ਰ ਵੀ ਇੱਕ Keras ਲੇਅਰ ਹੈ, ਅਸੀਂ ਇੱਕ ਨੈੱਟਵਰਕ ਪਰਿਭਾਸ਼ਿਤ ਕਰ ਸਕਦੇ ਹਾਂ ਜੋ ਇਸਨੂੰ ਸ਼ਾਮਲ ਕਰਦਾ ਹੈ ਅਤੇ ਇਸਨੂੰ ਐਂਡ-ਟੂ-ਐਂਡ ਟ੍ਰੇਨ ਕਰ ਸਕਦੇ ਹਾਂ। ਇਸ ਤਰੀਕੇ ਨਾਲ ਸਾਨੂੰ ਡਾਟਾਸੈਟ ਨੂੰ `map` ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਵੈਕਟੋਰਾਈਜ਼ ਕਰਨ ਦੀ ਲੋੜ ਨਹੀਂ ਹੈ, ਅਸੀਂ ਸਿਰਫ਼ ਅਸਲ ਡਾਟਾਸੈਟ ਨੂੰ ਨੈੱਟਵਰਕ ਦੇ ਇਨਪੁਟ 'ਤੇ ਪਾਸ ਕਰ ਸਕਦੇ ਹਾਂ।\n",
    "\n",
    "> **Note**: ਸਾਨੂੰ ਫਿਰ ਵੀ ਆਪਣੇ ਡਾਟਾਸੈਟ 'ਤੇ ਮੈਪ ਲਾਗੂ ਕਰਨ ਦੀ ਲੋੜ ਹੋਵੇਗੀ ਤਾਂ ਜੋ ਡਿਕਸ਼ਨਰੀਜ਼ (ਜਿਵੇਂ ਕਿ `title`, `description` ਅਤੇ `label`) ਤੋਂ ਫੀਲਡ ਨੂੰ ਟਿਊਪਲ ਵਿੱਚ ਬਦਲਿਆ ਜਾ ਸਕੇ। ਹਾਲਾਂਕਿ, ਜਦੋਂ ਅਸੀਂ ਡਿਸਕ ਤੋਂ ਡਾਟਾ ਲੋਡ ਕਰਦੇ ਹਾਂ, ਅਸੀਂ ਪਹਿਲੇ ਹੀ ਸਹੀ ਸਟ੍ਰਕਚਰ ਵਾਲਾ ਡਾਟਾਸੈਟ ਤਿਆਰ ਕਰ ਸਕਦੇ ਹਾਂ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, None, 5335)        0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum (TFOpLam  (None, 5335)             0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 21344     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,344\n",
      "Trainable params: 21,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "938/938 [==============================] - 73s 77ms/step - loss: 0.6057 - acc: 0.8414 - val_loss: 0.4202 - val_acc: 0.8736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c721521f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "inp = keras.Input(shape=(1,),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = tf.reduce_sum(tf.one_hot(x,vocab_size),axis=1)\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਬਾਈਗ੍ਰਾਮ, ਟ੍ਰਾਈਗ੍ਰਾਮ ਅਤੇ ਐਨ-ਗ੍ਰਾਮ\n",
    "\n",
    "ਬੈਗ-ਆਫ-ਵਰਡਸ ਪਹੁੰਚ ਦੀ ਇੱਕ ਸੀਮਿਤਤਾ ਇਹ ਹੈ ਕਿ ਕੁਝ ਸ਼ਬਦ ਬਹੁ-ਸ਼ਬਦੀ ਅਭਿਵਿਅੰਜਨਾਂ ਦਾ ਹਿੱਸਾ ਹੁੰਦੇ ਹਨ। ਉਦਾਹਰਨ ਵਜੋਂ, 'ਹਾਟ ਡੌਗ' ਸ਼ਬਦ ਦਾ ਅਰਥ 'ਹਾਟ' ਅਤੇ 'ਡੌਗ' ਸ਼ਬਦਾਂ ਦੇ ਹੋਰ ਸੰਦਰਭਾਂ ਵਿੱਚੋਂ ਬਿਲਕੁਲ ਵੱਖਰਾ ਹੈ। ਜੇਕਰ ਅਸੀਂ 'ਹਾਟ' ਅਤੇ 'ਡੌਗ' ਸ਼ਬਦਾਂ ਨੂੰ ਹਮੇਸ਼ਾ ਇੱਕੋ ਜਿਹੇ ਵੇਕਟਰਾਂ ਨਾਲ ਦਰਸਾਉਂਦੇ ਹਾਂ, ਤਾਂ ਇਹ ਸਾਡੇ ਮਾਡਲ ਨੂੰ ਗੁੰਝਲ ਵਿੱਚ ਪਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਇਸ ਸਮੱਸਿਆ ਨੂੰ ਹੱਲ ਕਰਨ ਲਈ, **ਐਨ-ਗ੍ਰਾਮ ਪ੍ਰਤੀਨਿਧੀਆਂ** ਨੂੰ ਦਸਤਾਵੇਜ਼ ਵਰਗੀਕਰਨ ਦੇ ਤਰੀਕਿਆਂ ਵਿੱਚ ਅਕਸਰ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ, ਜਿੱਥੇ ਹਰ ਸ਼ਬਦ, ਦੋ-ਸ਼ਬਦ ਜਾਂ ਤਿੰਨ-ਸ਼ਬਦ ਦੀ ਆਵ੍ਰਿਤੀ ਵਰਗੀਕਰਨ ਮਾਡਲਾਂ ਨੂੰ ਸਿਖਾਉਣ ਲਈ ਇੱਕ ਲਾਭਦਾਇਕ ਵਿਸ਼ੇਸ਼ਤਾ ਹੁੰਦੀ ਹੈ। ਬਾਈਗ੍ਰਾਮ ਪ੍ਰਤੀਨਿਧੀਆਂ ਵਿੱਚ, ਉਦਾਹਰਨ ਵਜੋਂ, ਅਸੀਂ ਮੂਲ ਸ਼ਬਦਾਂ ਦੇ ਨਾਲ ਨਾਲ ਸਾਰੇ ਸ਼ਬਦ ਜੋੜਾਂ ਨੂੰ ਸ਼ਬਦਾਵਲੀ ਵਿੱਚ ਸ਼ਾਮਲ ਕਰਾਂਗੇ।\n",
    "\n",
    "ਹੇਠਾਂ ਦਿੱਤਾ ਇੱਕ ਉਦਾਹਰਨ ਹੈ ਕਿ ਕਿਵੇਂ Scikit Learn ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਬਾਈਗ੍ਰਾਮ ਬੈਗ-ਆਫ-ਵਰਡ ਪ੍ਰਤੀਨਿਧੀ ਬਣਾਈ ਜਾ ਸਕਦੀ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram ਪਹੁੰਚ ਦਾ ਮੁੱਖ ਨੁਕਸਾਨ ਇਹ ਹੈ ਕਿ ਸ਼ਬਦਾਵਲੀ ਦਾ ਆਕਾਰ ਬਹੁਤ ਤੇਜ਼ੀ ਨਾਲ ਵਧਣਾ ਸ਼ੁਰੂ ਹੋ ਜਾਂਦਾ ਹੈ। ਅਮਲ ਵਿੱਚ, ਸਾਨੂੰ n-gram ਪ੍ਰਸਤੁਤੀ ਨੂੰ ਇੱਕ ਮਾਪ ਘਟਾਉਣ ਦੀ ਤਕਨੀਕ ਨਾਲ ਜੋੜਨਾ ਪੈਂਦਾ ਹੈ, ਜਿਵੇਂ ਕਿ *embeddings*, ਜਿਸ ਬਾਰੇ ਅਸੀਂ ਅਗਲੇ ਯੂਨਿਟ ਵਿੱਚ ਚਰਚਾ ਕਰਾਂਗੇ।\n",
    "\n",
    "**AG News** ਡੇਟਾਸੈੱਟ ਵਿੱਚ n-gram ਪ੍ਰਸਤੁਤੀ ਨੂੰ ਵਰਤਣ ਲਈ, ਸਾਨੂੰ `TextVectorization` ਕੰਸਟਰਕਟਰ ਨੂੰ `ngrams` ਪੈਰਾਮੀਟਰ ਪਾਸ ਕਰਨਾ ਪਵੇਗਾ। ਇੱਕ bigram ਸ਼ਬਦਾਵਲੀ ਦੀ ਲੰਬਾਈ **ਕਾਫ਼ੀ ਵੱਡੀ** ਹੁੰਦੀ ਹੈ, ਸਾਡੇ ਕੇਸ ਵਿੱਚ ਇਹ 1.3 ਮਿਲੀਅਨ ਤੋਂ ਵੱਧ ਟੋਕਨ ਹਨ! ਇਸ ਲਈ, ਇਹ ਸਮਝਦਾਰੀ ਵਾਲੀ ਗੱਲ ਹੈ ਕਿ bigram ਟੋਕਨ ਨੂੰ ਵੀ ਕਿਸੇ ਵਾਜਬ ਗਿਣਤੀ ਤੱਕ ਸੀਮਿਤ ਕੀਤਾ ਜਾਵੇ।\n",
    "\n",
    "ਅਸੀਂ ਉਪਰ ਦਿੱਤੇ ਗਏ ਕੋਡ ਨੂੰ ਵਰਤ ਕੇ ਕਲਾਸੀਫਾਇਰ ਨੂੰ ਟ੍ਰੇਨ ਕਰ ਸਕਦੇ ਹਾਂ, ਪਰ ਇਹ ਯਾਦਦਾਸ਼ਤ ਦੀ ਦ੍ਰਿਸ਼ਟੀ ਤੋਂ ਬਹੁਤ ਅਪਰਭਾਵਸ਼ਾਲੀ ਹੋਵੇਗਾ। ਅਗਲੇ ਯੂਨਿਟ ਵਿੱਚ, ਅਸੀਂ embeddings ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹੋਏ bigram ਕਲਾਸੀਫਾਇਰ ਨੂੰ ਟ੍ਰੇਨ ਕਰਾਂਗੇ। ਇਸ ਦੌਰਾਨ, ਤੁਸੀਂ ਇਸ ਨੋਟਬੁੱਕ ਵਿੱਚ bigram ਕਲਾਸੀਫਾਇਰ ਟ੍ਰੇਨਿੰਗ ਨਾਲ ਪ੍ਰਯੋਗ ਕਰ ਸਕਦੇ ਹੋ ਅਤੇ ਵੇਖ ਸਕਦੇ ਹੋ ਕਿ ਕੀ ਤੁਸੀਂ ਵਧੇਰੇ ਸਹੀ ਨਤੀਜੇ ਪ੍ਰਾਪਤ ਕਰ ਸਕਦੇ ਹੋ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਬੋਵ ਵੈਕਟਰਾਂ ਦੀ ਆਟੋਮੈਟਿਕ ਗਿਣਤੀ\n",
    "\n",
    "ਉਪਰੋਕਤ ਉਦਾਹਰਨ ਵਿੱਚ ਅਸੀਂ ਬੋਵ ਵੈਕਟਰਾਂ ਨੂੰ ਹੱਥੋਂ ਗਿਣਿਆ ਸੀ, ਜਿੱਥੇ ਅਸੀਂ ਵਿਅਕਤੀਗਤ ਸ਼ਬਦਾਂ ਦੇ ਇੱਕ-ਹਾਟ ਐਨਕੋਡਿੰਗ ਨੂੰ ਜੋੜਿਆ ਸੀ। ਹਾਲਾਂਕਿ, TensorFlow ਦੇ ਨਵੇਂ ਵਰਜਨ ਵਿੱਚ ਅਸੀਂ ਬੋਵ ਵੈਕਟਰਾਂ ਨੂੰ ਆਟੋਮੈਟਿਕ ਤੌਰ 'ਤੇ ਗਿਣ ਸਕਦੇ ਹਾਂ, ਜਦੋਂ ਅਸੀਂ `output_mode='count` ਪੈਰਾਮੀਟਰ ਨੂੰ ਵੈਕਟੋਰਾਈਜ਼ਰ ਕੰਸਟਰੱਕਟਰ ਵਿੱਚ ਪਾਸ ਕਰਦੇ ਹਾਂ। ਇਸ ਨਾਲ ਸਾਡਾ ਮਾਡਲ ਡਿਫਾਈਨ ਕਰਨਾ ਅਤੇ ਟ੍ਰੇਨ ਕਰਨਾ ਕਾਫ਼ੀ ਆਸਾਨ ਹੋ ਜਾਂਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.5929 - acc: 0.8486 - val_loss: 0.4168 - val_acc: 0.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c725217c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='count'),\n",
    "    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਟਰਮ ਫ੍ਰੀਕਵੈਂਸੀ - ਇਨਵਰਸ ਡੌਕਯੂਮੈਂਟ ਫ੍ਰੀਕਵੈਂਸੀ (TF-IDF)\n",
    "\n",
    "BoW ਪ੍ਰਸਤੁਤੀ ਵਿੱਚ, ਸ਼ਬਦਾਂ ਦੀ ਗਿਣਤੀ ਨੂੰ ਇੱਕੋ ਤਰੀਕੇ ਨਾਲ ਵਜ਼ਨ ਦਿੱਤਾ ਜਾਂਦਾ ਹੈ, ਭਾਵੇਂ ਸ਼ਬਦ ਦੀ ਕਿਸਮ ਜੋ ਮਰਜ਼ੀ ਹੋਵੇ। ਹਾਲਾਂਕਿ, ਇਹ ਸਪਸ਼ਟ ਹੈ ਕਿ ਆਮ ਸ਼ਬਦ ਜਿਵੇਂ ਕਿ *a* ਅਤੇ *in* ਵਰਗੇ ਸ਼ਬਦ ਵਰਤੋਂ ਲਈ ਵਿਸ਼ੇਸ਼ ਸ਼ਬਦਾਂ ਦੇ ਮੁਕਾਬਲੇ ਘੱਟ ਮਹੱਤਵਪੂਰਨ ਹੁੰਦੇ ਹਨ। ਜ਼ਿਆਦਾਤਰ NLP ਕੰਮਾਂ ਵਿੱਚ ਕੁਝ ਸ਼ਬਦ ਹੋਰਾਂ ਨਾਲੋਂ ਜ਼ਿਆਦਾ ਮਹੱਤਵਪੂਰਨ ਹੁੰਦੇ ਹਨ।\n",
    "\n",
    "**TF-IDF** ਦਾ ਮਤਲਬ ਹੈ **ਟਰਮ ਫ੍ਰੀਕਵੈਂਸੀ - ਇਨਵਰਸ ਡੌਕਯੂਮੈਂਟ ਫ੍ਰੀਕਵੈਂਸੀ**। ਇਹ ਬੈਗ-ਆਫ-ਵਰਡਜ਼ ਦੀ ਇੱਕ ਵੈਰੀਏਸ਼ਨ ਹੈ, ਜਿਸ ਵਿੱਚ ਦਸਤਾਵੇਜ਼ ਵਿੱਚ ਸ਼ਬਦ ਦੀ ਮੌਜੂਦਗੀ ਨੂੰ ਦਰਸਾਉਣ ਵਾਲੇ ਬਾਈਨਰੀ 0/1 ਮੁੱਲ ਦੀ ਥਾਂ ਇੱਕ ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਮੁੱਲ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ, ਜੋ ਸ਼ਬਦ ਦੀ ਕੌਰਪਸ ਵਿੱਚ ਆਵਿਰਤੀ ਨਾਲ ਸੰਬੰਧਿਤ ਹੁੰਦਾ ਹੈ।\n",
    "\n",
    "ਵਧੇਰੇ ਰਸਮੀ ਤੌਰ 'ਤੇ, ਦਸਤਾਵੇਜ਼ $j$ ਵਿੱਚ ਸ਼ਬਦ $i$ ਦਾ ਵਜ਼ਨ $w_{ij}$ ਇਸ ਤਰ੍ਹਾਂ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤਾ ਜਾਂਦਾ ਹੈ:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "ਜਿੱਥੇ\n",
    "* $tf_{ij}$ $j$ ਵਿੱਚ $i$ ਦੀ ਗਿਣਤੀ ਹੈ, ਅਰਥਾਤ ਉਹ BoW ਮੁੱਲ ਜੋ ਅਸੀਂ ਪਹਿਲਾਂ ਵੇਖਿਆ ਸੀ\n",
    "* $N$ ਕਲੈਕਸ਼ਨ ਵਿੱਚ ਦਸਤਾਵੇਜ਼ਾਂ ਦੀ ਗਿਣਤੀ ਹੈ\n",
    "* $df_i$ ਪੂਰੀ ਕਲੈਕਸ਼ਨ ਵਿੱਚ ਉਹ ਦਸਤਾਵੇਜ਼ਾਂ ਦੀ ਗਿਣਤੀ ਹੈ ਜਿਨ੍ਹਾਂ ਵਿੱਚ ਸ਼ਬਦ $i$ ਸ਼ਾਮਲ ਹੈ\n",
    "\n",
    "TF-IDF ਮੁੱਲ $w_{ij}$ ਦਸਤਾਵੇਜ਼ ਵਿੱਚ ਸ਼ਬਦ ਦੀ ਗਿਣਤੀ ਦੇ ਅਨੁਪਾਤ ਵਿੱਚ ਵਧਦਾ ਹੈ ਅਤੇ ਕੌਰਪਸ ਵਿੱਚ ਉਹ ਦਸਤਾਵੇਜ਼ਾਂ ਦੀ ਗਿਣਤੀ ਨਾਲ ਸੰਬੰਧਿਤ ਹੁੰਦਾ ਹੈ ਜਿਨ੍ਹਾਂ ਵਿੱਚ ਉਹ ਸ਼ਬਦ ਸ਼ਾਮਲ ਹੈ। ਇਹ ਇਸ ਗੱਲ ਨੂੰ ਠੀਕ ਕਰਨ ਵਿੱਚ ਮਦਦ ਕਰਦਾ ਹੈ ਕਿ ਕੁਝ ਸ਼ਬਦ ਹੋਰਾਂ ਨਾਲੋਂ ਵੱਧ ਵਾਰ ਆਉਂਦੇ ਹਨ। ਉਦਾਹਰਣ ਲਈ, ਜੇਕਰ ਸ਼ਬਦ ਕਲੈਕਸ਼ਨ ਵਿੱਚ *ਹਰ* ਦਸਤਾਵੇਜ਼ ਵਿੱਚ ਆਉਂਦਾ ਹੈ, ਤਾਂ $df_i=N$, ਅਤੇ $w_{ij}=0$, ਅਤੇ ਉਹ ਸ਼ਬਦ ਪੂਰੀ ਤਰ੍ਹਾਂ ਨਜ਼ਰਅੰਦਾਜ਼ ਕਰ ਦਿੱਤੇ ਜਾਣਗੇ।\n",
    "\n",
    "ਤੁਸੀਂ ਆਸਾਨੀ ਨਾਲ Scikit Learn ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟੈਕਸਟ ਦੀ TF-IDF ਵੈਕਟਰਾਈਜ਼ੇਸ਼ਨ ਬਣਾ ਸਕਦੇ ਹੋ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਕੇਰਾਸ ਵਿੱਚ, `TextVectorization` ਲੇਅਰ `output_mode='tf-idf'` ਪੈਰਾਮੀਟਰ ਪਾਸ ਕਰਕੇ ਸਵੈਚਾਲਿਤ ਤੌਰ 'ਤੇ TF-IDF ਫ੍ਰੀਕਵੈਂਸੀ ਦੀ ਗਣਨਾ ਕਰ ਸਕਦੀ ਹੈ। ਆਓ, ਉਪਰੋਕਤ ਵਰਤੇ ਗਏ ਕੋਡ ਨੂੰ ਦੁਹਰਾਈਏ ਤਾਂ ਜੋ ਦੇਖ ਸਕੀਏ ਕਿ TF-IDF ਵਰਤਣ ਨਾਲ ਸ਼ੁੱਧਤਾ ਵਿੱਚ ਵਾਧਾ ਹੁੰਦਾ ਹੈ ਜਾਂ ਨਹੀਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 12s 12ms/step - loss: 0.4197 - acc: 0.8662 - val_loss: 0.3432 - val_acc: 0.8849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c729dfd30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='tf-idf'),\n",
    "    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਨਤੀਜਾ\n",
    "\n",
    "ਹਾਲਾਂਕਿ TF-IDF ਪ੍ਰਸਤੁਤੀਆਂ ਵੱਖ-ਵੱਖ ਸ਼ਬਦਾਂ ਨੂੰ ਆਵ੍ਰਿਤੀ ਵਜ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦੀਆਂ ਹਨ, ਇਹ ਅਰਥ ਜਾਂ ਕ੍ਰਮ ਨੂੰ ਦਰਸਾਉਣ ਵਿੱਚ ਅਸਮਰਥ ਹਨ। ਪ੍ਰਸਿੱਧ ਭਾਸ਼ਾ ਵਿਗਿਆਨੀ J. R. Firth ਨੇ 1935 ਵਿੱਚ ਕਿਹਾ ਸੀ, \"ਸ਼ਬਦ ਦਾ ਪੂਰਾ ਅਰਥ ਹਮੇਸ਼ਾ ਸੰਦਰਭਕ ਹੁੰਦਾ ਹੈ, ਅਤੇ ਸੰਦਰਭ ਤੋਂ ਬਿਨਾਂ ਅਰਥ ਦਾ ਕੋਈ ਅਧਿਐਨ ਗੰਭੀਰਤਾ ਨਾਲ ਨਹੀਂ ਲਿਆ ਜਾ ਸਕਦਾ।\" ਅਸੀਂ ਇਸ ਕੋਰਸ ਵਿੱਚ ਅਗਲੇ ਹਿੱਸੇ ਵਿੱਚ ਭਾਸ਼ਾ ਮਾਡਲਿੰਗ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟੈਕਸਟ ਤੋਂ ਸੰਦਰਭਕ ਜਾਣਕਾਰੀ ਕਿਵੇਂ ਕੈਪਚਰ ਕਰਨੀ ਹੈ, ਇਹ ਸਿੱਖਾਂਗੇ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਾਰਨਾ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚੱਜੇਪਣ ਹੋ ਸਕਦੇ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼, ਜੋ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਹੈ, ਨੂੰ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernel_info": {
   "name": "conda-env-py37_tensorflow-py"
  },
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "19b43951d55b377a76209c24c1f017e4",
   "translation_date": "2025-08-28T09:56:45+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}