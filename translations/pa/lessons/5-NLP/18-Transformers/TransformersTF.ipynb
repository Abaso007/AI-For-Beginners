{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਧਿਆਨ ਮਕੈਨਿਜ਼ਮ ਅਤੇ ਟ੍ਰਾਂਸਫਾਰਮਰ\n",
    "\n",
    "ਰੀਕਰਨਟ ਨੈਟਵਰਕਸ ਦਾ ਇੱਕ ਵੱਡਾ ਨੁਕਸਾਨ ਇਹ ਹੈ ਕਿ ਕ੍ਰਮ ਵਿੱਚ ਸਾਰੇ ਸ਼ਬਦ ਨਤੀਜੇ 'ਤੇ ਇੱਕੋ ਜਿਹਾ ਪ੍ਰਭਾਵ ਪਾਉਂਦੇ ਹਨ। ਇਸ ਕਾਰਨ ਸੈਕਵੈਂਸ-ਟੂ-ਸੈਕਵੈਂਸ ਕੰਮਾਂ ਲਈ ਸਧਾਰਨ LSTM ਐਨਕੋਡਰ-ਡਿਕੋਡਰ ਮਾਡਲਾਂ ਨਾਲ ਸਬ-ਆਪਟੀਮਲ ਪ੍ਰਦਰਸ਼ਨ ਹੁੰਦਾ ਹੈ, ਜਿਵੇਂ ਕਿ Named Entity Recognition ਅਤੇ Machine Translation। ਹਕੀਕਤ ਵਿੱਚ, ਇਨਪੁਟ ਕ੍ਰਮ ਵਿੱਚ ਕੁਝ ਵਿਸ਼ੇਸ਼ ਸ਼ਬਦ ਅਕਸਰ ਕ੍ਰਮਵਾਰ ਆਉਟਪੁੱਟ 'ਤੇ ਹੋਰਾਂ ਨਾਲੋਂ ਵੱਧ ਪ੍ਰਭਾਵ ਪਾਉਂਦੇ ਹਨ।\n",
    "\n",
    "ਸੈਕਵੈਂਸ-ਟੂ-ਸੈਕਵੈਂਸ ਮਾਡਲ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖੋ, ਜਿਵੇਂ ਕਿ ਮਸ਼ੀਨ ਅਨੁਵਾਦ। ਇਹ ਦੋ ਰੀਕਰਨਟ ਨੈਟਵਰਕਸ ਦੁਆਰਾ ਲਾਗੂ ਕੀਤਾ ਜਾਂਦਾ ਹੈ, ਜਿੱਥੇ ਇੱਕ ਨੈਟਵਰਕ (**ਐਨਕੋਡਰ**) ਇਨਪੁਟ ਕ੍ਰਮ ਨੂੰ ਹਿਡਨ ਸਟੇਟ ਵਿੱਚ ਸੰਗ੍ਰਹਿਤ ਕਰੇਗਾ, ਅਤੇ ਦੂਜਾ (**ਡਿਕੋਡਰ**) ਇਸ ਹਿਡਨ ਸਟੇਟ ਨੂੰ ਅਨੁਵਾਦਿਤ ਨਤੀਜੇ ਵਿੱਚ ਬਦਲ ਦੇਵੇਗਾ। ਇਸ ਪਹੁੰਚ ਨਾਲ ਸਮੱਸਿਆ ਇਹ ਹੈ ਕਿ ਨੈਟਵਰਕ ਦੀ ਅੰਤਲੀ ਸਟੇਟ ਨੂੰ ਵਾਕ ਦੇ ਸ਼ੁਰੂਆਤ ਨੂੰ ਯਾਦ ਰੱਖਣ ਵਿੱਚ ਮੁਸ਼ਕਲ ਹੋਵੇਗੀ, ਜਿਸ ਕਾਰਨ ਲੰਬੇ ਵਾਕਾਂ 'ਤੇ ਮਾਡਲ ਦੀ ਗੁਣਵੱਤਾ ਘਟ ਜਾਂਦੀ ਹੈ।\n",
    "\n",
    "**ਧਿਆਨ ਮਕੈਨਿਜ਼ਮ** RNN ਦੇ ਹਰ ਆਉਟਪੁੱਟ ਅਨੁਮਾਨ 'ਤੇ ਹਰ ਇਨਪੁਟ ਵੇਕਟਰ ਦੇ ਸੰਦਰਭਕ ਪ੍ਰਭਾਵ ਨੂੰ ਵਜਨ ਦੇਣ ਦਾ ਇੱਕ ਢੰਗ ਪ੍ਰਦਾਨ ਕਰਦੇ ਹਨ। ਇਹ ਇਸ ਤਰੀਕੇ ਨਾਲ ਲਾਗੂ ਕੀਤਾ ਜਾਂਦਾ ਹੈ ਕਿ ਇਨਪੁਟ RNN ਦੇ ਮੱਧਵਰਤੀ ਸਟੇਟਸ ਅਤੇ ਆਉਟਪੁੱਟ RNN ਦੇ ਵਿਚਕਾਰ ਸ਼ਾਰਟਕਟ ਬਣਾਏ ਜਾਂਦੇ ਹਨ। ਇਸ ਤਰੀਕੇ ਨਾਲ, ਜਦੋਂ ਆਉਟਪੁੱਟ ਚਿੰਨ੍ਹ $y_t$ ਬਣਾਇਆ ਜਾ ਰਿਹਾ ਹੈ, ਅਸੀਂ ਸਾਰੇ ਇਨਪੁਟ ਹਿਡਨ ਸਟੇਟਸ $h_i$ ਨੂੰ ਵੱਖ-ਵੱਖ ਵਜਨ ਗੁਣਾਂ $\\alpha_{t,i}$ ਦੇ ਨਾਲ ਧਿਆਨ ਵਿੱਚ ਲਵਾਂਗੇ।\n",
    "\n",
    "![ਇੱਕ ਐਨਕੋਡਰ/ਡਿਕੋਡਰ ਮਾਡਲ ਨੂੰ additive attention layer ਦੇ ਨਾਲ ਦਿਖਾਉਂਦੀ ਚਿੱਤਰ](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.pa.png)\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) ਵਿੱਚ additive attention ਮਕੈਨਿਜ਼ਮ ਵਾਲਾ ਐਨਕੋਡਰ-ਡਿਕੋਡਰ ਮਾਡਲ, [ਇਸ ਬਲੌਗ ਪੋਸਟ](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) ਤੋਂ ਲਿਆ ਗਿਆ।]*\n",
    "\n",
    "Attention ਮੈਟ੍ਰਿਕਸ $\\{\\alpha_{i,j}\\}$ ਇਹ ਦਰਸਾਏਗੀ ਕਿ ਕਿਸ ਹੱਦ ਤੱਕ ਕੁਝ ਇਨਪੁਟ ਸ਼ਬਦ ਆਉਟਪੁੱਟ ਕ੍ਰਮ ਵਿੱਚ ਦਿੱਤੇ ਗਏ ਸ਼ਬਦ ਦੇ ਜਨਰੇਸ਼ਨ ਵਿੱਚ ਭੂਮਿਕਾ ਨਿਭਾਉਂਦੇ ਹਨ। ਹੇਠਾਂ ਇਸ ਮੈਟ੍ਰਿਕਸ ਦਾ ਇੱਕ ਉਦਾਹਰਨ ਦਿੱਤਾ ਗਿਆ ਹੈ:\n",
    "\n",
    "![Bahdanau - arviz.org ਤੋਂ ਲਿਆ ਗਿਆ RNNsearch-50 ਦੁਆਰਾ ਪਾਈ ਗਈ ਇੱਕ ਨਮੂਨਾ ਅਲਾਈਨਮੈਂਟ ਦਿਖਾਉਂਦੀ ਚਿੱਤਰ](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.pa.png)\n",
    "\n",
    "*[Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) ਤੋਂ ਲਿਆ ਗਿਆ ਚਿੱਤਰ]*\n",
    "\n",
    "ਧਿਆਨ ਮਕੈਨਿਜ਼ਮ ਆਧੁਨਿਕ ਜਾਂ ਆਧੁਨਿਕ-ਨਿਕਟ Natural Language Processing ਵਿੱਚ ਸਟੇਟ-ਆਫ-ਦ-ਆਰਟ ਪ੍ਰਦਰਸ਼ਨ ਲਈ ਜ਼ਿੰਮੇਵਾਰ ਹਨ। ਧਿਆਨ ਸ਼ਾਮਲ ਕਰਨ ਨਾਲ ਮਾਡਲ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਗਿਣਤੀ ਵਿੱਚ ਕਾਫ਼ੀ ਵਾਧਾ ਹੁੰਦਾ ਹੈ, ਜਿਸ ਕਾਰਨ RNNs ਨਾਲ ਸਕੇਲਿੰਗ ਸਮੱਸਿਆਵਾਂ ਆਈਆਂ। RNNs ਨੂੰ ਸਕੇਲ ਕਰਨ ਦੀ ਇੱਕ ਮੁੱਖ ਪਾਬੰਦੀ ਇਹ ਹੈ ਕਿ ਮਾਡਲਾਂ ਦੀ ਰੀਕਰਨਟ ਪ੍ਰਕਿਰਤੀ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਬੈਚ ਅਤੇ ਪੈਰਲਲਾਈਜ਼ ਕਰਨਾ ਮੁਸ਼ਕਲ ਬਣਾ ਦਿੰਦੀ ਹੈ। RNN ਵਿੱਚ ਕ੍ਰਮ ਦੇ ਹਰ ਤੱਤ ਨੂੰ ਕ੍ਰਮਵਾਰ ਪ੍ਰਕਿਰਿਆਵਧੀ ਕਰਨਾ ਪੈਂਦਾ ਹੈ, ਜਿਸਦਾ ਮਤਲਬ ਹੈ ਕਿ ਇਸਨੂੰ ਆਸਾਨੀ ਨਾਲ ਪੈਰਲਲਾਈਜ਼ ਨਹੀਂ ਕੀਤਾ ਜਾ ਸਕਦਾ।\n",
    "\n",
    "ਧਿਆਨ ਮਕੈਨਿਜ਼ਮ ਦੀ ਅਪਨਾਉਣ ਅਤੇ ਇਸ ਪਾਬੰਦੀ ਨੇ ਅੱਜ ਦੇ ਸਟੇਟ-ਆਫ-ਦ-ਆਰਟ ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲਾਂ ਦੀ ਰਚਨਾ ਲਈ ਰਾਹ ਹਮਵਾਰ ਕੀਤਾ, ਜਿਨ੍ਹਾਂ ਨੂੰ ਅਸੀਂ BERT ਤੋਂ OpenGPT3 ਤੱਕ ਵਰਤਦੇ ਹਾਂ।\n",
    "\n",
    "## ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ\n",
    "\n",
    "ਪਿਛਲੇ ਅਨੁਮਾਨ ਦੇ ਸੰਦਰਭ ਨੂੰ ਅਗਲੇ ਮੁਲਾਂਕਣ ਕਦਮ ਵਿੱਚ ਅੱਗੇ ਭੇਜਣ ਦੀ ਬਜਾਏ, **ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ** **ਪੋਜ਼ੀਸ਼ਨਲ ਐਨਕੋਡਿੰਗ** ਅਤੇ **ਧਿਆਨ** ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹਨ, ਜੋ ਦਿੱਤੇ ਗਏ ਇਨਪੁਟ ਦਾ ਸੰਦਰਭ ਇੱਕ ਪ੍ਰਦਾਨ ਕੀਤੇ ਟੈਕਸਟ ਵਿੰਡੋ ਵਿੱਚ ਕੈਪਚਰ ਕਰਦੇ ਹਨ। ਹੇਠਾਂ ਦਿੱਤੀ ਚਿੱਤਰ ਦਿਖਾਉਂਦੀ ਹੈ ਕਿ ਪੋਜ਼ੀਸ਼ਨਲ ਐਨਕੋਡਿੰਗ ਅਤੇ ਧਿਆਨ ਕਿਸ ਤਰੀਕੇ ਨਾਲ ਇੱਕ ਦਿੱਤੇ ਗਏ ਵਿੰਡੋ ਵਿੱਚ ਸੰਦਰਭ ਕੈਪਚਰ ਕਰ ਸਕਦੇ ਹਨ।\n",
    "\n",
    "![Animated GIF ਦਿਖਾਉਂਦੀ ਹੈ ਕਿ ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲਾਂ ਵਿੱਚ ਮੁਲਾਂਕਣ ਕਿਵੇਂ ਕੀਤੇ ਜਾਂਦੇ ਹਨ।](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "ਜਦੋਂ ਕਿ ਹਰ ਇਨਪੁਟ ਸਥਿਤੀ ਨੂੰ ਹਰ ਆਉਟਪੁੱਟ ਸਥਿਤੀ ਨਾਲ ਸਵਤੰਤਰ ਤੌਰ 'ਤੇ ਮੈਪ ਕੀਤਾ ਜਾਂਦਾ ਹੈ, ਟ੍ਰਾਂਸਫਾਰਮਰ RNNs ਨਾਲੋਂ ਵਧੀਆ ਪੈਰਲਲਾਈਜ਼ ਕਰ ਸਕਦੇ ਹਨ, ਜਿਸ ਨਾਲ ਕਾਫ਼ੀ ਵੱਡੇ ਅਤੇ ਹੋਰ ਪ੍ਰਗਟਵਾਦੀ ਭਾਸ਼ਾ ਮਾਡਲ ਬਣਾਉਣ ਦੀ ਸਮਰੱਥਾ ਮਿਲਦੀ ਹੈ। ਹਰ attention head ਨੂੰ ਸ਼ਬਦਾਂ ਦੇ ਵਿਚਕਾਰ ਵੱਖ-ਵੱਖ ਸੰਬੰਧ ਸਿੱਖਣ ਲਈ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ, ਜੋ ਡਾਊਨਸਟ੍ਰੀਮ Natural Language Processing ਕੰਮਾਂ ਨੂੰ ਸੁਧਾਰਦਾ ਹੈ।\n",
    "\n",
    "## ਸਧਾਰਨ ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ ਬਣਾਉਣਾ\n",
    "\n",
    "Keras ਵਿੱਚ ਬਣਿਆ-ਬਣਾਇਆ ਟ੍ਰਾਂਸਫਾਰਮਰ layer ਨਹੀਂ ਹੈ, ਪਰ ਅਸੀਂ ਆਪਣਾ ਬਣਾਉਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰ ਸਕਦੇ ਹਾਂ। ਪਹਿਲਾਂ ਦੀ ਤਰ੍ਹਾਂ, ਅਸੀਂ AG News ਡੇਟਾਸੈਟ ਦੇ ਟੈਕਸਟ ਕਲਾਸੀਫਿਕੇਸ਼ਨ 'ਤੇ ਧਿਆਨ ਕੇਂਦਰਿਤ ਕਰਾਂਗੇ, ਪਰ ਇਹ ਜ਼ਿਕਰ ਕਰਨਾ ਯੋਗ ਹੈ ਕਿ ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ ਸਭ ਤੋਂ ਵਧੀਆ ਨਤੀਜੇ ਮੁਸ਼ਕਲ NLP ਕੰਮਾਂ 'ਤੇ ਦਿਖਾਉਂਦੇ ਹਨ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਨਵੀਆਂ ਲੇਅਰਾਂ Keras ਵਿੱਚ `Layer` ਕਲਾਸ ਨੂੰ ਸਬਕਲਾਸ ਕਰਨੀ ਚਾਹੀਦੀ ਹੈ, ਅਤੇ `call` ਵਿਧੀ ਨੂੰ ਲਾਗੂ ਕਰਨਾ ਚਾਹੀਦਾ ਹੈ। ਆਓ **Positional Embedding** ਲੇਅਰ ਨਾਲ ਸ਼ੁਰੂ ਕਰੀਏ। ਅਸੀਂ [ਅਧਿਕਾਰਿਕ Keras ਦਸਤਾਵੇਜ਼ੀ](https://keras.io/examples/nlp/text_classification_with_transformer/) ਤੋਂ ਕੁਝ ਕੋਡ ਵਰਤਾਂਗੇ। ਅਸੀਂ ਮੰਨ ਲੈਂਦੇ ਹਾਂ ਕਿ ਅਸੀਂ ਸਾਰੀਆਂ ਇਨਪੁਟ ਲੜੀਆਂ ਨੂੰ `maxlen` ਦੀ ਲੰਬਾਈ ਤੱਕ ਪੈਡ ਕਰਦੇ ਹਾਂ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਇਹ ਲੇਅਰ ਦੋ `Embedding` ਲੇਅਰਾਂ 'ਤੇ ਆਧਾਰਿਤ ਹੈ: ਟੋਕਨਜ਼ ਨੂੰ ਐਮਬੈੱਡ ਕਰਨ ਲਈ (ਜਿਵੇਂ ਕਿ ਅਸੀਂ ਪਹਿਲਾਂ ਚਰਚਾ ਕੀਤੀ ਹੈ) ਅਤੇ ਟੋਕਨ ਪੋਜ਼ੀਸ਼ਨਜ਼ ਲਈ। ਟੋਕਨ ਪੋਜ਼ੀਸ਼ਨਜ਼ ਨੂੰ 0 ਤੋਂ `maxlen` ਤੱਕ ਦੇ ਕੁਦਰਤੀ ਨੰਬਰਾਂ ਦੀ ਲੜੀ ਵਜੋਂ `tf.range` ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਬਣਾਇਆ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਫਿਰ ਇਸਨੂੰ ਐਮਬੈੱਡਿੰਗ ਲੇਅਰ ਵਿੱਚ ਪਾਸ ਕੀਤਾ ਜਾਂਦਾ ਹੈ। ਦੋ resulting embedding ਵੇਕਟਰਾਂ ਨੂੰ ਫਿਰ ਜੋੜਿਆ ਜਾਂਦਾ ਹੈ, ਜਿਸ ਨਾਲ ਇਨਪੁਟ ਦਾ ਪੋਜ਼ੀਸ਼ਨਲੀ-ਐਮਬੈੱਡ ਕੀਤਾ ਪ੍ਰਦਰਸ਼ਨ ਬਣਦਾ ਹੈ ਜਿਸਦੀ ਸ਼ਕਲ `maxlen`$\\times$`embed_dim` ਹੁੰਦੀ ਹੈ।\n",
    "\n",
    "ਹੁਣ, ਆਓ ਟ੍ਰਾਂਸਫਾਰਮਰ ਬਲਾਕ ਨੂੰ ਇੰਪਲੀਮੈਂਟ ਕਰੀਏ। ਇਹ ਪਹਿਲਾਂ ਪਰਿਭਾਸ਼ਿਤ ਐਮਬੈੱਡਿੰਗ ਲੇਅਰ ਦੇ ਆਉਟਪੁਟ ਨੂੰ ਲਵੇਗਾ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ, ਅਸੀਂ ਪੂਰੇ ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਨ ਲਈ ਤਿਆਰ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) ਇੱਕ ਬਹੁਤ ਵੱਡਾ ਬਹੁ-ਪਰਤ ਟ੍ਰਾਂਸਫਾਰਮਰ ਨੈਟਵਰਕ ਹੈ ਜਿਸ ਵਿੱਚ *BERT-base* ਲਈ 12 ਪਰਤਾਂ ਹਨ ਅਤੇ *BERT-large* ਲਈ 24 ਪਰਤਾਂ। ਮਾਡਲ ਨੂੰ ਪਹਿਲਾਂ ਵੱਡੇ ਟੈਕਸਟ ਡਾਟਾ (WikiPedia + ਕਿਤਾਬਾਂ) ਦੇ ਕੋਰਪਸ 'ਤੇ ਅਨਸੁਪਰਵਾਈਜ਼ਡ ਟ੍ਰੇਨਿੰਗ (ਵਾਕ ਵਿੱਚ ਮਾਸਕ ਕੀਤੇ ਸ਼ਬਦਾਂ ਦੀ ਪੇਸ਼ਕਸ਼) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤਾ ਜਾਂਦਾ ਹੈ। ਪ੍ਰੀ-ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ, ਮਾਡਲ ਭਾਸ਼ਾ ਦੀ ਸਮਝ ਦੇ ਇੱਕ ਮਹੱਤਵਪੂਰਨ ਪੱਧਰ ਨੂੰ ਅਪਣਾਉਂਦਾ ਹੈ, ਜਿਸਨੂੰ ਫਿਰ ਫਾਈਨ ਟਿਊਨਿੰਗ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਹੋਰ ਡਾਟਾਸੈਟਾਂ ਨਾਲ ਲਾਗੂ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ। ਇਸ ਪ੍ਰਕਿਰਿਆ ਨੂੰ **ਟ੍ਰਾਂਸਫਰ ਲਰਨਿੰਗ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ।\n",
    "\n",
    "![http://jalammar.github.io/illustrated-bert/ ਤੋਂ ਤਸਵੀਰ](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.pa.png)\n",
    "\n",
    "ਟ੍ਰਾਂਸਫਾਰਮਰ ਆਰਕੀਟੈਕਚਰ ਦੇ ਕਈ ਰੂਪ ਹਨ ਜਿਵੇਂ ਕਿ BERT, DistilBERT, BigBird, OpenGPT3 ਅਤੇ ਹੋਰ, ਜਿਨ੍ਹਾਂ ਨੂੰ ਫਾਈਨ ਟਿਊਨ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਆਓ ਵੇਖੀਏ ਕਿ ਅਸੀਂ ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੇ BERT ਮਾਡਲ ਨੂੰ ਆਪਣੀ ਰਵਾਇਤੀ ਸੀਕਵੈਂਸ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਸਮੱਸਿਆ ਨੂੰ ਹੱਲ ਕਰਨ ਲਈ ਕਿਵੇਂ ਵਰਤ ਸਕਦੇ ਹਾਂ। ਅਸੀਂ [ਆਧਿਕਾਰਿਕ ਦਸਤਾਵੇਜ਼](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) ਤੋਂ ਵਿਚਾਰ ਅਤੇ ਕੁਝ ਕੋਡ ਲੈਣ ਜਾ ਰਹੇ ਹਾਂ।\n",
    "\n",
    "ਪ੍ਰੀ-ਟ੍ਰੇਨ ਕੀਤੇ ਮਾਡਲਾਂ ਨੂੰ ਲੋਡ ਕਰਨ ਲਈ, ਅਸੀਂ **Tensorflow hub** ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ। ਪਹਿਲਾਂ, ਆਓ BERT-ਵਿਸ਼ੇਸ਼ ਵੈਕਟਰਾਈਜ਼ਰ ਨੂੰ ਲੋਡ ਕਰੀਏ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਇਹ ਮਹੱਤਵਪੂਰਨ ਹੈ ਕਿ ਤੁਸੀਂ ਉਹੀ ਵੈਕਟੋਰਾਈਜ਼ਰ ਵਰਤੋ ਜੋ ਮੂਲ ਨੈੱਟਵਰਕ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨ ਲਈ ਵਰਤਿਆ ਗਿਆ ਸੀ। ਇਸ ਤੋਂ ਇਲਾਵਾ, BERT ਵੈਕਟੋਰਾਈਜ਼ਰ ਤਿੰਨ ਹਿੱਸੇ ਵਾਪਸ ਕਰਦਾ ਹੈ:\n",
    "* `input_word_ids`, ਜੋ ਇਨਪੁਟ ਵਾਕ ਲਈ ਟੋਕਨ ਨੰਬਰਾਂ ਦੀ ਲੜੀ ਹੈ\n",
    "* `input_mask`, ਜੋ ਦਿਖਾਉਂਦਾ ਹੈ ਕਿ ਲੜੀ ਦਾ ਕਿਹੜਾ ਹਿੱਸਾ ਅਸਲ ਇਨਪੁਟ ਹੈ, ਅਤੇ ਕਿਹੜਾ ਪੈਡਿੰਗ ਹੈ। ਇਹ `Masking` ਲੇਅਰ ਦੁਆਰਾ ਤਿਆਰ ਕੀਤੇ ਮਾਸਕ ਦੇ ਸਮਾਨ ਹੈ\n",
    "* `input_type_ids` ਭਾਸ਼ਾ ਮਾਡਲਿੰਗ ਕੰਮਾਂ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਇੱਕ ਲੜੀ ਵਿੱਚ ਦੋ ਇਨਪੁਟ ਵਾਕਾਂ ਨੂੰ ਨਿਰਧਾਰਤ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ।\n",
    "\n",
    "ਫਿਰ, ਅਸੀਂ BERT ਫੀਚਰ ਐਕਸਟ੍ਰੈਕਟਰ ਨੂੰ ਇੰਸਟੈਂਸ਼ੀਏਟ ਕਰ ਸਕਦੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਇਸ ਤਰ੍ਹਾਂ, BERT ਲੇਅਰ ਕਈ ਲਾਭਦਾਇਕ ਨਤੀਜੇ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ:\n",
    "* `pooled_output` ਸਾਰੇ ਟੋਕਨਜ਼ ਨੂੰ ਕ੍ਰਮ ਵਿੱਚ ਔਸਤ ਕਰਕੇ ਪ੍ਰਾਪਤ ਨਤੀਜਾ ਹੈ। ਤੁਸੀਂ ਇਸਨੂੰ ਪੂਰੇ ਨੈੱਟਵਰਕ ਦੀ ਸਮਝਦਾਰ ਸੈਮੈਂਟਿਕ ਐਮਬੈਡਿੰਗ ਵਜੋਂ ਦੇਖ ਸਕਦੇ ਹੋ। ਇਹ ਸਾਡੇ ਪਿਛਲੇ ਮਾਡਲ ਵਿੱਚ `GlobalAveragePooling1D` ਲੇਅਰ ਦੇ ਆਉਟਪੁੱਟ ਦੇ ਬਰਾਬਰ ਹੈ।\n",
    "* `sequence_output` ਆਖਰੀ ਟ੍ਰਾਂਸਫਾਰਮਰ ਲੇਅਰ ਦਾ ਆਉਟਪੁੱਟ ਹੈ (ਜੋ ਉੱਪਰ ਦਿੱਤੇ ਸਾਡੇ ਮਾਡਲ ਵਿੱਚ `TransformerBlock` ਦੇ ਆਉਟਪੁੱਟ ਦੇ ਸਮਾਨ ਹੈ)।\n",
    "* `encoder_outputs` ਸਾਰੇ ਟ੍ਰਾਂਸਫਾਰਮਰ ਲੇਅਰਾਂ ਦੇ ਆਉਟਪੁੱਟ ਹਨ। ਕਿਉਂਕਿ ਅਸੀਂ 4-ਲੇਅਰ BERT ਮਾਡਲ ਲੋਡ ਕੀਤਾ ਹੈ (ਜਿਵੇਂ ਤੁਸੀਂ ਸ਼ਾਇਦ ਨਾਮ ਤੋਂ ਅਨੁਮਾਨ ਲਗਾ ਸਕਦੇ ਹੋ, ਜਿਸ ਵਿੱਚ `4_H` ਸ਼ਾਮਲ ਹੈ), ਇਸ ਵਿੱਚ 4 ਟੈਂਸਰ ਹਨ। ਆਖਰੀ ਟੈਂਸਰ `sequence_output` ਦੇ ਬਰਾਬਰ ਹੈ।\n",
    "\n",
    "ਹੁਣ ਅਸੀਂ ਐਂਡ-ਟੂ-ਐਂਡ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਮਾਡਲ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਾਂਗੇ। ਅਸੀਂ *ਫੰਕਸ਼ਨਲ ਮਾਡਲ ਡਿਫਿਨੀਸ਼ਨ* ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ, ਜਿਸ ਵਿੱਚ ਅਸੀਂ ਮਾਡਲ ਇਨਪੁੱਟ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦੇ ਹਾਂ, ਅਤੇ ਫਿਰ ਇਸਦੇ ਆਉਟਪੁੱਟ ਦੀ ਗਣਨਾ ਕਰਨ ਲਈ ਕਈ ਐਕਸਪ੍ਰੈਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦੇ ਹਾਂ। ਅਸੀਂ BERT ਮਾਡਲ ਦੇ ਵਜ਼ਨ ਨੂੰ ਗੈਰ-ਟ੍ਰੇਨ ਕਰਨਯੋਗ ਬਣਾਵਾਂਗੇ, ਅਤੇ ਸਿਰਫ਼ ਆਖਰੀ ਕਲਾਸੀਫਾਇਰ ਨੂੰ ਟ੍ਰੇਨ ਕਰਾਂਗੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹਾਲਾਂਕਿ ਟ੍ਰੇਨ ਕਰਨ ਯੋਗ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਗਿਣਤੀ ਘੱਟ ਹੈ, ਪਰ ਪ੍ਰਕਿਰਿਆ ਕਾਫ਼ੀ ਹੌਲੀ ਹੈ, ਕਿਉਂਕਿ BERT ਫੀਚਰ ਐਕਸਟ੍ਰੈਕਟਰ ਗਣਨਾਤਮਕ ਤੌਰ 'ਤੇ ਭਾਰੀ ਹੈ। ਇਹ ਲੱਗਦਾ ਹੈ ਕਿ ਅਸੀਂ ਯਥੋਚਿਤ ਸਹੀਤਾ ਪ੍ਰਾਪਤ ਕਰਨ ਵਿੱਚ ਅਸਫਲ ਰਹੇ, ਜਾਂ ਤਾਂ ਟ੍ਰੇਨਿੰਗ ਦੀ ਘਾਟ ਕਾਰਨ, ਜਾਂ ਮਾਡਲ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਘਾਟ ਕਾਰਨ।\n",
    "\n",
    "ਆਓ BERT ਦੇ ਵਜ਼ਨ ਅਨਫ੍ਰੀਜ਼ ਕਰਕੇ ਇਸਨੂੰ ਵੀ ਟ੍ਰੇਨ ਕਰਨ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰੀਏ। ਇਸ ਲਈ ਬਹੁਤ ਛੋਟੀ ਲਰਨਿੰਗ ਰੇਟ ਦੀ ਲੋੜ ਹੈ, ਅਤੇ **warmup** ਦੇ ਨਾਲ ਇੱਕ ਹੋਰ ਧਿਆਨਪੂਰਵਕ ਟ੍ਰੇਨਿੰਗ ਰਣਨੀਤੀ ਦੀ ਵੀ ਲੋੜ ਹੈ, ਜਿਸ ਵਿੱਚ **AdamW** ਓਪਟੀਮਾਈਜ਼ਰ ਦੀ ਵਰਤੋਂ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਅਸੀਂ `tf-models-official` ਪੈਕੇਜ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਓਪਟੀਮਾਈਜ਼ਰ ਬਣਾਵਾਂਗੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਜਿਵੇਂ ਤੁਸੀਂ ਦੇਖ ਸਕਦੇ ਹੋ, ਟ੍ਰੇਨਿੰਗ ਕਾਫੀ ਹੌਲੀ ਚੱਲਦੀ ਹੈ - ਪਰ ਤੁਸੀਂ ਕੁਝ ਐਕਸਪੇਰੀਮੈਂਟ ਕਰਕੇ ਮਾਡਲ ਨੂੰ ਕੁਝ ਈਪੋਕਸ (5-10) ਲਈ ਟ੍ਰੇਨ ਕਰ ਸਕਦੇ ਹੋ ਅਤੇ ਪਿਛਲੇ ਤਰੀਕਿਆਂ ਨਾਲ ਤੁਲਨਾ ਕਰਕੇ ਸਭ ਤੋਂ ਵਧੀਆ ਨਤੀਜਾ ਪ੍ਰਾਪਤ ਕਰਨ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰ ਸਕਦੇ ਹੋ।\n",
    "\n",
    "## Huggingface Transformers ਲਾਇਬ੍ਰੇਰੀ\n",
    "\n",
    "ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲਾਂ ਨੂੰ ਵਰਤਣ ਦਾ ਇੱਕ ਹੋਰ ਆਮ (ਅਤੇ ਕੁਝ ਹੱਦ ਤੱਕ ਸਧਾਰਨ) ਤਰੀਕਾ [HuggingFace ਪੈਕੇਜ](https://github.com/huggingface/) ਹੈ, ਜੋ ਵੱਖ-ਵੱਖ NLP ਟਾਸਕਾਂ ਲਈ ਸਧਾਰਨ ਬਿਲਡਿੰਗ ਬਲਾਕ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਇਹ Tensorflow ਅਤੇ PyTorch, ਜੋ ਕਿ ਇੱਕ ਹੋਰ ਬਹੁਤ ਹੀ ਪ੍ਰਸਿੱਧ ਨਿਊਰਲ ਨੈਟਵਰਕ ਫਰੇਮਵਰਕ ਹੈ, ਦੋਵਾਂ ਲਈ ਉਪਲਬਧ ਹੈ।\n",
    "\n",
    "> **Note**: ਜੇ ਤੁਸੀਂ ਨਹੀਂ ਚਾਹੁੰਦੇ ਕਿ Transformers ਲਾਇਬ੍ਰੇਰੀ ਕਿਵੇਂ ਕੰਮ ਕਰਦੀ ਹੈ - ਤਾਂ ਤੁਸੀਂ ਇਸ ਨੋਟਬੁੱਕ ਦੇ ਅੰਤ ਤੱਕ ਜਾ ਸਕਦੇ ਹੋ, ਕਿਉਂਕਿ ਤੁਸੀਂ ਉੱਥੇ ਕੁਝ ਵੀ ਬਹੁਤ ਵੱਖਰਾ ਨਹੀਂ ਦੇਖੋਗੇ ਜੋ ਅਸੀਂ ਉੱਪਰ ਕੀਤਾ ਹੈ। ਅਸੀਂ BERT ਮਾਡਲ ਨੂੰ ਵੱਖਰੀ ਲਾਇਬ੍ਰੇਰੀ ਅਤੇ ਕਾਫੀ ਵੱਡੇ ਮਾਡਲ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਟ੍ਰੇਨ ਕਰਨ ਦੇ ਉਹੀ ਕਦਮ ਦੁਹਰਾਉਣ ਜਾ ਰਹੇ ਹਾਂ। ਇਸ ਲਈ, ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ ਕੁਝ ਲੰਬੀ ਟ੍ਰੇਨਿੰਗ ਸ਼ਾਮਲ ਹੈ, ਤਾਂ ਤੁਸੀਂ ਸਿਰਫ ਕੋਡ ਨੂੰ ਦੇਖਣ ਦੀ ਚਾਹਤ ਰੱਖ ਸਕਦੇ ਹੋ।\n",
    "\n",
    "ਆਓ ਵੇਖੀਏ ਕਿ ਸਾਡੀ ਸਮੱਸਿਆ ਨੂੰ [Huggingface Transformers](http://huggingface.co) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕਿਵੇਂ ਹੱਲ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਸਭ ਤੋਂ ਪਹਿਲਾਂ ਸਾਨੂੰ ਮਾਡਲ ਚੁਣਨਾ ਹੈ ਜੋ ਅਸੀਂ ਵਰਤਣ ਜਾ ਰਹੇ ਹਾਂ। ਕੁਝ ਬਿਲਟ-ਇਨ ਮਾਡਲਾਂ ਦੇ ਇਲਾਵਾ, Huggingface ਵਿੱਚ ਇੱਕ [ਆਨਲਾਈਨ ਮਾਡਲ ਰਿਪੋਜ਼ਟਰੀ](https://huggingface.co/models) ਹੈ, ਜਿੱਥੇ ਤੁਸੀਂ ਕਮਿਊਨਿਟੀ ਦੁਆਰਾ ਬਣਾਏ ਹੋਏ ਹੋਰ ਬਹੁਤ ਸਾਰੇ ਪ੍ਰੀ-ਟ੍ਰੇਨਡ ਮਾਡਲ ਲੱਭ ਸਕਦੇ ਹੋ। ਇਹ ਸਾਰੇ ਮਾਡਲ ਸਿਰਫ ਮਾਡਲ ਦਾ ਨਾਮ ਪ੍ਰਦਾਨ ਕਰਕੇ ਲੋਡ ਅਤੇ ਵਰਤੇ ਜਾ ਸਕਦੇ ਹਨ। ਮਾਡਲ ਲਈ ਸਾਰੇ ਲੋੜੀਂਦੇ ਬਾਈਨਰੀ ਫਾਈਲਾਂ ਆਟੋਮੈਟਿਕ ਤੌਰ 'ਤੇ ਡਾਊਨਲੋਡ ਹੋ ਜਾਣਗੀਆਂ।\n",
    "\n",
    "ਕਈ ਵਾਰ ਤੁਹਾਨੂੰ ਆਪਣੇ ਮਾਡਲ ਲੋਡ ਕਰਨ ਦੀ ਲੋੜ ਹੋ ਸਕਦੀ ਹੈ, ਇਸ ਸਥਿਤੀ ਵਿੱਚ ਤੁਸੀਂ ਉਹ ਡਾਇਰੈਕਟਰੀ ਸਪਸ਼ਟ ਕਰ ਸਕਦੇ ਹੋ ਜਿਸ ਵਿੱਚ ਸਾਰੇ ਸੰਬੰਧਿਤ ਫਾਈਲਾਂ ਸ਼ਾਮਲ ਹਨ, ਜਿਵੇਂ ਕਿ ਟੋਕਨਾਈਜ਼ਰ ਲਈ ਪੈਰਾਮੀਟਰ, `config.json` ਫਾਈਲ ਜਿਸ ਵਿੱਚ ਮਾਡਲ ਪੈਰਾਮੀਟਰ ਹਨ, ਬਾਈਨਰੀ ਵਜ਼ਨ ਆਦਿ।\n",
    "\n",
    "ਮਾਡਲ ਦੇ ਨਾਮ ਤੋਂ, ਅਸੀਂ ਮਾਡਲ ਅਤੇ ਟੋਕਨਾਈਜ਼ਰ ਦੋਨੋ ਨੂੰ ਇੰਸਟੈਂਸ਼ੀਏਟ ਕਰ ਸਕਦੇ ਹਾਂ। ਆਓ ਟੋਕਨਾਈਜ਼ਰ ਨਾਲ ਸ਼ੁਰੂ ਕਰੀਏ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` ਆਬਜੈਕਟ ਵਿੱਚ `encode` ਫੰਕਸ਼ਨ ਸ਼ਾਮਲ ਹੈ ਜੋ ਟੈਕਸਟ ਨੂੰ encode ਕਰਨ ਲਈ ਸਿੱਧੇ ਤੌਰ 'ਤੇ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਟੋਕਨਾਈਜ਼ਰ ਨੂੰ ਇਸ ਤਰ੍ਹਾਂ ਇੱਕ ਕ੍ਰਮ ਨੂੰ ਐਨਕੋਡ ਕਰਨ ਲਈ ਵੀ ਵਰਤ ਸਕਦੇ ਹਾਂ ਜੋ ਮਾਡਲ ਨੂੰ ਪਾਸ ਕਰਨ ਲਈ ਉਚਿਤ ਹੋਵੇ, ਜਿਵੇਂ ਕਿ `token_ids`, `input_mask` ਫੀਲਡ ਆਦਿ ਸ਼ਾਮਲ ਕਰਨਾ। ਅਸੀਂ ਇਹ ਵੀ ਨਿਰਧਾਰਤ ਕਰ ਸਕਦੇ ਹਾਂ ਕਿ ਅਸੀਂ ਟੈਂਸਰਫਲੋ ਟੈਂਸਰ ਚਾਹੁੰਦੇ ਹਾਂ `return_tensors='tf'` ਦਲੀਲ ਪ੍ਰਦਾਨ ਕਰਕੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਸਾਡੇ ਕੇਸ ਵਿੱਚ, ਅਸੀਂ ਪਹਿਲਾਂ ਤੋਂ ਪ੍ਰਸ਼ਿਕਸ਼ਿਤ BERT ਮਾਡਲ ਵਰਤ ਰਹੇ ਹਾਂ ਜਿਸਨੂੰ `bert-base-uncased` ਕਿਹਾ ਜਾਂਦਾ ਹੈ। *Uncased* ਦਾ ਮਤਲਬ ਹੈ ਕਿ ਇਹ ਮਾਡਲ ਅੱਖਰਾਂ ਦੇ ਕੇਸ (ਵੱਡੇ ਜਾਂ ਛੋਟੇ) ਪ੍ਰਤੀ ਸੰਵੇਦਨਸ਼ੀਲ ਨਹੀਂ ਹੈ।\n",
    "\n",
    "ਜਦੋਂ ਮਾਡਲ ਨੂੰ ਪ੍ਰਸ਼ਿਕਸ਼ਿਤ ਕਰਦੇ ਹਾਂ, ਸਾਨੂੰ ਟੋਕਨਾਈਜ਼ ਕੀਤੀ ਗਈ ਲੜੀ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਦੇਣ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ, ਅਤੇ ਇਸ ਲਈ ਅਸੀਂ ਡਾਟਾ ਪ੍ਰੋਸੈਸਿੰਗ ਪਾਈਪਲਾਈਨ ਤਿਆਰ ਕਰਾਂਗੇ। ਕਿਉਂਕਿ `tokenizer.encode` ਇੱਕ Python ਫੰਕਸ਼ਨ ਹੈ, ਅਸੀਂ ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ ਵਰਤੇ ਗਏ ਹੀ ਤਰੀਕੇ ਨੂੰ ਅਪਨਾਵਾਂਗੇ, ਜਿਸ ਵਿੱਚ ਇਸਨੂੰ `py_function` ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕਾਲ ਕੀਤਾ ਜਾਵੇਗਾ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਅਸੀਂ `BertForSequenceClassification` ਪੈਕੇਜ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਸਲ ਮਾਡਲ ਨੂੰ ਲੋਡ ਕਰ ਸਕਦੇ ਹਾਂ। ਇਹ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ ਕਿ ਸਾਡੇ ਮਾਡਲ ਵਿੱਚ ਪਹਿਲਾਂ ਹੀ ਵਰਗੀਕਰਨ ਲਈ ਲੋੜੀਂਦਾ ਆਰਕੀਟੈਕਚਰ ਹੈ, ਜਿਸ ਵਿੱਚ ਅੰਤਮ ਵਰਗੀਕਰਨ ਸ਼ਾਮਲ ਹੈ। ਤੁਹਾਨੂੰ ਚੇਤਾਵਨੀ ਸੁਨੇਹਾ ਵੇਖਣ ਨੂੰ ਮਿਲੇਗਾ ਕਿ ਅੰਤਮ ਵਰਗੀਕਰਨ ਦੇ ਵਜ਼ਨ ਸ਼ੁਰੂਆਤ ਨਹੀਂ ਕੀਤੇ ਗਏ ਹਨ, ਅਤੇ ਮਾਡਲ ਨੂੰ ਪਹਿਲਾਂ ਸਿਖਲਾਈ ਦੀ ਲੋੜ ਹੋਵੇਗੀ - ਇਹ ਬਿਲਕੁਲ ਠੀਕ ਹੈ, ਕਿਉਂਕਿ ਇਹੀ ਹੈ ਜੋ ਅਸੀਂ ਕਰਨ ਵਾਲੇ ਹਾਂ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਜਿਵੇਂ ਤੁਸੀਂ `summary()` ਤੋਂ ਦੇਖ ਸਕਦੇ ਹੋ, ਮਾਡਲ ਵਿੱਚ ਲਗਭਗ 110 ਮਿਲੀਅਨ ਪੈਰਾਮੀਟਰ ਹਨ! ਸੰਭਵਤ: ਜੇਕਰ ਅਸੀਂ ਛੋਟੇ ਡਾਟਾਸੈਟ 'ਤੇ ਸਧਾਰਣ ਵਰਗੀਕਰਨ ਦਾ ਕੰਮ ਕਰਨਾ ਚਾਹੁੰਦੇ ਹਾਂ, ਤਾਂ ਅਸੀਂ BERT ਬੇਸ ਲੇਅਰ ਨੂੰ ਟ੍ਰੇਨ ਨਹੀਂ ਕਰਨਾ ਚਾਹੁੰਦੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਅਸੀਂ ਟ੍ਰੇਨਿੰਗ ਸ਼ੁਰੂ ਕਰਨ ਲਈ ਤਿਆਰ ਹਾਂ!\n",
    "\n",
    "> **ਨੋਟ**: ਪੂਰੇ ਪੱਧਰ ਦੇ BERT ਮਾਡਲ ਦੀ ਟ੍ਰੇਨਿੰਗ ਕਰਨਾ ਬਹੁਤ ਸਮਾਂ ਲੈ ਸਕਦਾ ਹੈ! ਇਸ ਲਈ ਅਸੀਂ ਸਿਰਫ ਪਹਿਲੀਆਂ 32 ਬੈਚਾਂ ਲਈ ਇਸਦੀ ਟ੍ਰੇਨਿੰਗ ਕਰਾਂਗੇ। ਇਹ ਸਿਰਫ ਇਹ ਦਿਖਾਉਣ ਲਈ ਹੈ ਕਿ ਮਾਡਲ ਟ੍ਰੇਨਿੰਗ ਕਿਵੇਂ ਸੈਟ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਜੇ ਤੁਸੀਂ ਪੂਰੇ ਪੱਧਰ ਦੀ ਟ੍ਰੇਨਿੰਗ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਨ ਵਿੱਚ ਰੁਚੀ ਰੱਖਦੇ ਹੋ - ਤਾਂ `steps_per_epoch` ਅਤੇ `validation_steps` ਪੈਰਾਮੀਟਰ ਹਟਾ ਦਿਓ, ਅਤੇ ਇੰਤਜ਼ਾਰ ਕਰਨ ਲਈ ਤਿਆਰ ਰਹੋ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਜੇ ਤੁਸੀਂ ਇਟਰੇਸ਼ਨ ਦੀ ਗਿਣਤੀ ਵਧਾਉਂਦੇ ਹੋ ਅਤੇ ਕਾਫ਼ੀ ਸਮਾਂ ਇੰਤਜ਼ਾਰ ਕਰਦੇ ਹੋ, ਅਤੇ ਕਈ epochs ਲਈ ਟ੍ਰੇਨਿੰਗ ਕਰਦੇ ਹੋ, ਤਾਂ ਤੁਸੀਂ ਉਮੀਦ ਕਰ ਸਕਦੇ ਹੋ ਕਿ BERT ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਸਾਨੂੰ ਸਭ ਤੋਂ ਵਧੀਆ ਸਹੀਤਾ ਦੇਵੇਗਾ! ਇਸ ਦਾ ਕਾਰਨ ਇਹ ਹੈ ਕਿ BERT ਪਹਿਲਾਂ ਹੀ ਭਾਸ਼ਾ ਦੀ ਬਣਤਰ ਨੂੰ ਕਾਫ਼ੀ ਚੰਗੀ ਤਰ੍ਹਾਂ ਸਮਝਦਾ ਹੈ, ਅਤੇ ਸਾਨੂੰ ਸਿਰਫ਼ ਅੰਤਮ ਕਲਾਸੀਫਾਇਰ ਨੂੰ fine-tune ਕਰਨ ਦੀ ਲੋੜ ਹੈ। ਹਾਲਾਂਕਿ, ਕਿਉਂਕਿ BERT ਇੱਕ ਵੱਡਾ ਮਾਡਲ ਹੈ, ਪੂਰਾ ਟ੍ਰੇਨਿੰਗ ਪ੍ਰਕਿਰਿਆ ਕਾਫ਼ੀ ਸਮਾਂ ਲੈਂਦੀ ਹੈ, ਅਤੇ ਗੰਭੀਰ ਗਣਨਾਤਮਕ ਸ਼ਕਤੀ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ! (GPU, ਅਤੇ ਵਧੀਆ ਹੋਵੇ ਤਾਂ ਇੱਕ ਤੋਂ ਵੱਧ).\n",
    "\n",
    "> **Note:** ਸਾਡੇ ਉਦਾਹਰਨ ਵਿੱਚ, ਅਸੀਂ ਸਭ ਤੋਂ ਛੋਟੇ pre-trained BERT ਮਾਡਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਦੀ ਵਰਤੋਂ ਕਰ ਰਹੇ ਹਾਂ। ਵੱਡੇ ਮਾਡਲ ਹਨ ਜੋ ਸੰਭਵ ਹੈ ਕਿ ਵਧੀਆ ਨਤੀਜੇ ਦੇਣ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮੁੱਖ ਗੱਲ\n",
    "\n",
    "ਇਸ ਯੂਨਿਟ ਵਿੱਚ, ਅਸੀਂ **ਟ੍ਰਾਂਸਫਾਰਮਰਜ਼** ਅਧਾਰਿਤ ਬਹੁਤ ਹੀ ਨਵੇਂ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰ ਦੇਖੇ ਹਨ। ਅਸੀਂ ਇਨ੍ਹਾਂ ਨੂੰ ਆਪਣੇ ਟੈਕਸਟ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਟਾਸਕ ਲਈ ਲਾਗੂ ਕੀਤਾ ਹੈ, ਪਰ ਇਸੇ ਤਰ੍ਹਾਂ, BERT ਮਾਡਲ ਨੂੰ ਐਂਟਿਟੀ ਐਕਸਟ੍ਰੈਕਸ਼ਨ, ਪ੍ਰਸ਼ਨ ਉੱਤਰ ਦੇਣ ਅਤੇ ਹੋਰ NLP ਟਾਸਕ ਲਈ ਵੀ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲ NLP ਵਿੱਚ ਮੌਜੂਦਾ ਸਥਿਤੀ ਦੇ ਸਭ ਤੋਂ ਅੱਗੇ ਹਨ, ਅਤੇ ਜ਼ਿਆਦਾਤਰ ਮਾਮਲਿਆਂ ਵਿੱਚ ਇਹ ਪਹਿਲਾ ਹੱਲ ਹੋਣਾ ਚਾਹੀਦਾ ਹੈ ਜਿਸ ਨਾਲ ਤੁਸੀਂ ਕਸਟਮ NLP ਹੱਲਾਂ ਨੂੰ ਲਾਗੂ ਕਰਦੇ ਸਮੇਂ ਪ੍ਰਯੋਗ ਕਰਨਾ ਸ਼ੁਰੂ ਕਰਦੇ ਹੋ। ਹਾਲਾਂਕਿ, ਜੇ ਤੁਸੀਂ ਉੱਚ-ਪੱਧਰੀ ਨਿਊਰਲ ਮਾਡਲ ਬਣਾਉਣਾ ਚਾਹੁੰਦੇ ਹੋ, ਤਾਂ ਇਸ ਮੋਡੀਊਲ ਵਿੱਚ ਚਰਚਾ ਕੀਤੇ ਗਏ ਰੀਕਰਨਟ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਦੇ ਬੁਨਿਆਦੀ ਅਧਾਰਭੂਤ ਸਿਧਾਂਤਾਂ ਨੂੰ ਸਮਝਣਾ ਬਹੁਤ ਜ਼ਰੂਰੀ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਰਤੀ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਛਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਇਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੌਜੂਦ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਪ੍ਰਮਾਣਿਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੇ ਪ੍ਰਯੋਗ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।  \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-28T09:29:50+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}