<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-26T08:11:11+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "pa"
}
-->
# ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕਸ

## [ਪ੍ਰੀ-ਲੈਕਚਰ ਕਵਿਜ਼](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

ਪਿਛਲੇ ਸੈਕਸ਼ਨਾਂ ਵਿੱਚ, ਅਸੀਂ ਟੈਕਸਟ ਦੇ ਸਮਰੱਥ ਸੈਮਾਂਟਿਕ ਪ੍ਰਤੀਨਿਧੀਆਂ ਅਤੇ ਐਮਬੈਡਿੰਗ ਦੇ ਉੱਪਰ ਇੱਕ ਸਧਾਰਨ ਲੀਨੀਅਰ ਕਲਾਸੀਫਾਇਰ ਦੀ ਵਰਤੋਂ ਕਰ ਰਹੇ ਸਾਂ। ਇਹ ਆਰਕੀਟੈਕਚਰ ਵਾਕਾਂਸ਼ ਵਿੱਚ ਸ਼ਬਦਾਂ ਦੇ ਸਮੁੱਚਿਤ ਅਰਥ ਨੂੰ ਕੈਪਚਰ ਕਰਦਾ ਹੈ, ਪਰ ਇਹ ਸ਼ਬਦਾਂ ਦੇ **ਕ੍ਰਮ** ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਨਹੀਂ ਰੱਖਦਾ, ਕਿਉਂਕਿ ਐਮਬੈਡਿੰਗ ਦੇ ਉੱਪਰ ਅਗਰੀਗੇਸ਼ਨ ਓਪਰੇਸ਼ਨ ਨੇ ਮੂਲ ਟੈਕਸਟ ਤੋਂ ਇਹ ਜਾਣਕਾਰੀ ਹਟਾ ਦਿੱਤੀ। ਕਿਉਂਕਿ ਇਹ ਮਾਡਲ ਸ਼ਬਦ ਕ੍ਰਮਬੱਧਤਾ ਨੂੰ ਮਾਡਲ ਨਹੀਂ ਕਰ ਸਕਦੇ, ਇਹ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਜਾਂ ਪ੍ਰਸ਼ਨ ਉੱਤਰ ਦੇਣ ਵਰਗੇ ਜਟਿਲ ਜਾਂ ਅਸਪਸ਼ਟ ਕੰਮਾਂ ਨੂੰ ਹੱਲ ਨਹੀਂ ਕਰ ਸਕਦੇ।

ਟੈਕਸਟ ਕ੍ਰਮ ਦਾ ਅਰਥ ਕੈਪਚਰ ਕਰਨ ਲਈ, ਸਾਨੂੰ ਇੱਕ ਹੋਰ ਨਿਊਰਲ ਨੈਟਵਰਕ ਆਰਕੀਟੈਕਚਰ ਦੀ ਵਰਤੋਂ ਕਰਨ ਦੀ ਲੋੜ ਹੈ, ਜਿਸਨੂੰ **ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕ**, ਜਾਂ RNN ਕਿਹਾ ਜਾਂਦਾ ਹੈ। RNN ਵਿੱਚ, ਅਸੀਂ ਆਪਣਾ ਵਾਕਾਂਸ਼ ਨੈਟਵਰਕ ਵਿੱਚ ਇੱਕ ਸਮੇਂ ਵਿੱਚ ਇੱਕ ਚਿੰਨ੍ਹ ਦੇ ਕੇ ਪਾਸ ਕਰਦੇ ਹਾਂ, ਅਤੇ ਨੈਟਵਰਕ ਕੁਝ **ਸਟੇਟ** ਪੈਦਾ ਕਰਦਾ ਹੈ, ਜਿਸਨੂੰ ਅਸੀਂ ਅਗਲੇ ਚਿੰਨ੍ਹ ਦੇ ਨਾਲ ਮੁੜ ਨੈਟਵਰਕ ਵਿੱਚ ਪਾਸ ਕਰਦੇ ਹਾਂ।

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.pa.png)

> ਲੇਖਕ ਦੁਆਰਾ ਚਿੱਤਰ

ਟੋਕਨ ਦੇ ਇਨਪੁਟ ਕ੍ਰਮ X<sub>0</sub>,...,X<sub>n</sub> ਨੂੰ ਦੇਖਦੇ ਹੋਏ, RNN ਨਿਊਰਲ ਨੈਟਵਰਕ ਬਲਾਕਾਂ ਦੀ ਇੱਕ ਲੜੀ ਬਣਾਉਂਦਾ ਹੈ, ਅਤੇ ਇਸ ਲੜੀ ਨੂੰ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅੰਤ-ਤੱਕ ਸਿਖਾਉਂਦਾ ਹੈ। ਹਰ ਨੈਟਵਰਕ ਬਲਾਕ (X<sub>i</sub>,S<sub>i</sub>) ਦੀ ਜੋੜੀ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਲੈਂਦਾ ਹੈ, ਅਤੇ ਨਤੀਜੇ ਵਜੋਂ S<sub>i+1</sub> ਪੈਦਾ ਕਰਦਾ ਹੈ। ਅੰਤਮ ਸਟੇਟ S<sub>n</sub> ਜਾਂ (ਆਉਟਪੁਟ Y<sub>n</sub>) ਨੂੰ ਨਤੀਜਾ ਪੈਦਾ ਕਰਨ ਲਈ ਲੀਨੀਅਰ ਕਲਾਸੀਫਾਇਰ ਵਿੱਚ ਪਾਸ ਕੀਤਾ ਜਾਂਦਾ ਹੈ। ਸਾਰੇ ਨੈਟਵਰਕ ਬਲਾਕ ਇੱਕੋ ਜਿਹੇ ਵਜ਼ਨ ਸਾਂਝੇ ਕਰਦੇ ਹਨ, ਅਤੇ ਇੱਕ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ ਪਾਸ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅੰਤ-ਤੱਕ ਸਿਖਾਏ ਜਾਂਦੇ ਹਨ।

ਕਿਉਂਕਿ ਸਟੇਟ ਵੈਕਟਰ S<sub>0</sub>,...,S<sub>n</sub> ਨੈਟਵਰਕ ਵਿੱਚ ਪਾਸ ਕੀਤੇ ਜਾਂਦੇ ਹਨ, ਇਹ ਸ਼ਬਦਾਂ ਦੇ ਕ੍ਰਮਬੱਧ ਨਿਰਭਰਤਾਵਾਂ ਨੂੰ ਸਿੱਖਣ ਦੇ ਯੋਗ ਹੁੰਦਾ ਹੈ। ਉਦਾਹਰਨ ਲਈ, ਜਦੋਂ ਸ਼ਬਦ *not* ਕ੍ਰਮ ਵਿੱਚ ਕਿਤੇ ਵੀ ਆਉਂਦਾ ਹੈ, ਇਹ ਸਟੇਟ ਵੈਕਟਰ ਦੇ ਕੁਝ ਤੱਤਾਂ ਨੂੰ ਨਕਾਰਤਮਕ ਕਰਨ ਲਈ ਸਿੱਖ ਸਕਦਾ ਹੈ, ਜਿਸ ਨਾਲ ਨਕਾਰਾਤਮਕਤਾ ਹੁੰਦੀ ਹੈ।

> ✅ ਕਿਉਂਕਿ ਉੱਪਰ ਦਿੱਤੇ ਚਿੱਤਰ ਵਿੱਚ ਸਾਰੇ RNN ਬਲਾਕਾਂ ਦੇ ਵਜ਼ਨ ਸਾਂਝੇ ਹਨ, ਇੱਕੋ ਚਿੱਤਰ ਨੂੰ ਇੱਕ ਬਲਾਕ (ਸੱਜੇ ਪਾਸੇ) ਵਜੋਂ ਦਰਸਾਇਆ ਜਾ ਸਕਦਾ ਹੈ ਜਿਸ ਵਿੱਚ ਇੱਕ ਰਿਕਰੰਟ ਫੀਡਬੈਕ ਲੂਪ ਹੁੰਦਾ ਹੈ, ਜੋ ਨੈਟਵਰਕ ਦੇ ਆਉਟਪੁਟ ਸਟੇਟ ਨੂੰ ਮੁੜ ਇਨਪੁਟ ਵਿੱਚ ਪਾਸ ਕਰਦਾ ਹੈ।

## RNN ਸੈਲ ਦੀ ਬਣਤਰ

ਆਓ ਵੇਖੀਏ ਕਿ ਇੱਕ ਸਧਾਰਨ RNN ਸੈਲ ਕਿਵੇਂ ਸੰਗਠਿਤ ਹੈ। ਇਹ ਪਿਛਲੇ ਸਟੇਟ S<sub>i-1</sub> ਅਤੇ ਮੌਜੂਦਾ ਚਿੰਨ੍ਹ X<sub>i</sub> ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਸਵੀਕਾਰ ਕਰਦਾ ਹੈ, ਅਤੇ S<sub>i</sub> (ਕਈ ਵਾਰ, ਅਸੀਂ ਕੁਝ ਹੋਰ ਆਉਟਪੁਟ Y<sub>i</sub> ਵਿੱਚ ਵੀ ਦਿਲਚਸਪੀ ਰੱਖਦੇ ਹਾਂ, ਜਿਵੇਂ ਕਿ ਜਨਰੇਟਿਵ ਨੈਟਵਰਕਸ ਦੇ ਮਾਮਲੇ ਵਿੱਚ) ਪੈਦਾ ਕਰਨਾ ਹੁੰਦਾ ਹੈ।

ਇੱਕ ਸਧਾਰਨ RNN ਸੈਲ ਵਿੱਚ ਦੋ ਵਜ਼ਨ ਮੈਟ੍ਰਿਕਸ ਹੁੰਦੀਆਂ ਹਨ: ਇੱਕ ਇਨਪੁਟ ਚਿੰਨ੍ਹ ਨੂੰ ਰੂਪਾਂਤਰਿਤ ਕਰਦੀ ਹੈ (ਇਸਨੂੰ W ਕਹੋ), ਅਤੇ ਦੂਜੀ ਇਨਪੁਟ ਸਟੇਟ ਨੂੰ ਰੂਪਾਂਤਰਿਤ ਕਰਦੀ ਹੈ (H)। ਇਸ ਮਾਮਲੇ ਵਿੱਚ ਨੈਟਵਰਕ ਦਾ ਆਉਟਪੁਟ σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b) ਵਜੋਂ ਗਣਨਾ ਕੀਤਾ ਜਾਂਦਾ ਹੈ, ਜਿੱਥੇ σ ਐਕਟੀਵੇਸ਼ਨ ਫੰਕਸ਼ਨ ਹੈ ਅਤੇ b ਵਾਧੂ ਬਾਇਸ ਹੈ।

<img alt="RNN Cell Anatomy" src="images/rnn-anatomy.png" width="50%"/>

> ਲੇਖਕ ਦੁਆਰਾ ਚਿੱਤਰ

ਕਈ ਮਾਮਲਿਆਂ ਵਿੱਚ, ਇਨਪੁਟ ਟੋਕਨ RNN ਵਿੱਚ ਦਾਖਲ ਹੋਣ ਤੋਂ ਪਹਿਲਾਂ ਐਮਬੈਡਿੰਗ ਲੇਅਰ ਵਿੱਚ ਪਾਸ ਕੀਤੇ ਜਾਂਦੇ ਹਨ ਤਾਂ ਜੋ ਡਾਇਮੈਂਸ਼ਨਲਿਟੀ ਘਟਾਈ ਜਾ ਸਕੇ। ਇਸ ਮਾਮਲੇ ਵਿੱਚ, ਜੇਕਰ ਇਨਪੁਟ ਵੈਕਟਰ ਦਾ ਡਾਇਮੈਂਸ਼ਨ *emb_size* ਹੈ, ਅਤੇ ਸਟੇਟ ਵੈਕਟਰ *hid_size* ਹੈ - ਤਾਂ W ਦਾ ਆਕਾਰ *emb_size*×*hid_size* ਹੈ, ਅਤੇ H ਦਾ ਆਕਾਰ *hid_size*×*hid_size* ਹੈ।

## ਲਾਂਗ ਸ਼ਾਰਟ ਟਰਮ ਮੈਮਰੀ (LSTM)

ਕਲਾਸੀਕਲ RNNs ਦੀ ਇੱਕ ਮੁੱਖ ਸਮੱਸਿਆ **ਵੈਨਿਸ਼ਿੰਗ ਗ੍ਰੇਡੀਅੰਟਸ** ਸਮੱਸਿਆ ਹੈ। ਕਿਉਂਕਿ RNNs ਨੂੰ ਇੱਕ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ ਪਾਸ ਵਿੱਚ ਅੰਤ-ਤੱਕ ਸਿਖਾਇਆ ਜਾਂਦਾ ਹੈ, ਇਹ ਨੈਟਵਰਕ ਦੇ ਪਹਿਲੇ ਲੇਅਰਾਂ ਵਿੱਚ ਗਲਤੀ ਨੂੰ ਪ੍ਰਚਾਰਿਤ ਕਰਨ ਵਿੱਚ ਮੁਸ਼ਕਲ ਹੁੰਦੀ ਹੈ, ਅਤੇ ਇਸ ਲਈ ਨੈਟਵਰਕ ਦੂਰ-ਦੂਰ ਦੇ ਟੋਕਨ ਦੇ ਰਿਸ਼ਤਿਆਂ ਨੂੰ ਸਿੱਖ ਨਹੀਂ ਸਕਦਾ। ਇਸ ਸਮੱਸਿਆ ਤੋਂ ਬਚਣ ਦੇ ਤਰੀਕਿਆਂ ਵਿੱਚੋਂ ਇੱਕ **ਗੇਟਸ** ਦੀ ਵਰਤੋਂ ਕਰਕੇ **ਸਪਸ਼ਟ ਸਟੇਟ ਪ੍ਰਬੰਧਨ** ਪੇਸ਼ ਕਰਨਾ ਹੈ। ਇਸ ਕਿਸਮ ਦੀਆਂ ਦੋ ਪ੍ਰਸਿੱਧ ਆਰਕੀਟੈਕਚਰ ਹਨ: **ਲਾਂਗ ਸ਼ਾਰਟ ਟਰਮ ਮੈਮਰੀ** (LSTM) ਅਤੇ **ਗੇਟਡ ਰੀਲੇਅ ਯੂਨਿਟ** (GRU)।

![Image showing an example long short term memory cell](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> ਚਿੱਤਰ ਸਰੋਤ TBD

LSTM ਨੈਟਵਰਕ RNN ਦੇ ਸਮਾਨ ਢੰਗ ਨਾਲ ਸੰਗਠਿਤ ਹੈ, ਪਰ ਦੋ ਸਟੇਟ ਹਨ ਜੋ ਲੇਅਰ ਤੋਂ ਲੇਅਰ ਵਿੱਚ ਪਾਸ ਕੀਤੇ ਜਾਂਦੇ ਹਨ: ਅਸਲ ਸਟੇਟ C, ਅਤੇ ਲੁਕਿਆ ਹੋਇਆ ਵੈਕਟਰ H। ਹਰ ਯੂਨਿਟ ਵਿੱਚ, ਲੁਕਿਆ ਹੋਇਆ ਵੈਕਟਰ H<sub>i</sub> ਇਨਪੁਟ X<sub>i</sub> ਨਾਲ ਜੋੜਿਆ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਇਹ **ਗੇਟਸ** ਰਾਹੀਂ ਸਟੇਟ C ਵਿੱਚ ਕੀ ਹੁੰਦਾ ਹੈ ਇਸਨੂੰ ਨਿਯੰਤਰਿਤ ਕਰਦੇ ਹਨ। ਹਰ ਗੇਟ ਇੱਕ ਨਿਊਰਲ ਨੈਟਵਰਕ ਹੁੰਦਾ ਹੈ ਜਿਸ ਵਿੱਚ ਸਿਗਮਾਇਡ ਐਕਟੀਵੇਸ਼ਨ ਹੁੰਦੀ ਹੈ (ਆਉਟਪੁਟ [0,1] ਦੀ ਰੇਂਜ ਵਿੱਚ), ਜਿਸਨੂੰ ਸਟੇਟ ਵੈਕਟਰ ਨਾਲ ਗੁਣਾ ਕਰਨ ਸਮੇਂ ਬਿਟਵਾਈਜ਼ ਮਾਸਕ ਵਜੋਂ ਸੋਚਿਆ ਜਾ ਸਕਦਾ ਹੈ। ਹੇਠਾਂ ਦਿੱਤੇ ਗੇਟ ਹਨ (ਚਿੱਤਰ ਵਿੱਚ ਖੱਬੇ ਤੋਂ ਸੱਜੇ ਤੱਕ):

* **ਫੋਰਗੇਟ ਗੇਟ** ਲੁਕਿਆ ਹੋਇਆ ਵੈਕਟਰ ਲੈਂਦਾ ਹੈ ਅਤੇ ਨਿਰਧਾਰਤ ਕਰਦਾ ਹੈ ਕਿ ਸਟੇਟ C ਦੇ ਕਿਹੜੇ ਤੱਤਾਂ ਨੂੰ ਭੁੱਲਣਾ ਹੈ, ਅਤੇ ਕਿਹੜੇ ਪਾਸ ਕਰਨਾ ਹੈ।
* **ਇਨਪੁਟ ਗੇਟ** ਇਨਪੁਟ ਅਤੇ ਲੁਕਿਆ ਹੋਏ ਵੈਕਟਰਾਂ ਤੋਂ ਕੁਝ ਜਾਣਕਾਰੀ ਲੈਂਦਾ ਹੈ ਅਤੇ ਇਸਨੂੰ ਸਟੇਟ ਵਿੱਚ ਸ਼ਾਮਲ ਕਰਦਾ ਹੈ।
* **ਆਉਟਪੁਟ ਗੇਟ** ਸਟੇਟ ਨੂੰ *tanh* ਐਕਟੀਵੇਸ਼ਨ ਨਾਲ ਲੀਨੀਅਰ ਲੇਅਰ ਰਾਹੀਂ ਰੂਪਾਂਤਰਿਤ ਕਰਦਾ ਹੈ, ਫਿਰ H<sub>i</sub> ਲੁਕਿਆ ਹੋਏ ਵੈਕਟਰ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇਸਦੇ ਕੁਝ ਤੱਤਾਂ ਨੂੰ ਚੁਣਦਾ ਹੈ ਤਾਂ ਜੋ ਨਵਾਂ ਸਟੇਟ C<sub>i+1</sub> ਪੈਦਾ ਕੀਤਾ ਜਾ ਸਕੇ।

ਸਟੇਟ C ਦੇ ਤੱਤਾਂ ਨੂੰ ਕੁਝ ਝੰਡਿਆਂ ਵਜੋਂ ਸੋਚਿਆ ਜਾ ਸਕਦਾ ਹੈ ਜੋ ਚਾਲੂ ਅਤੇ ਬੰਦ ਕੀਤੇ ਜਾ ਸਕਦੇ ਹਨ। ਉਦਾਹਰਨ ਲਈ, ਜਦੋਂ ਅਸੀਂ ਕ੍ਰਮ ਵਿੱਚ *Alice* ਨਾਮ ਨੂੰ ਮਿਲਦੇ ਹਾਂ, ਅਸੀਂ ਮੰਨ ਸਕਦੇ ਹਾਂ ਕਿ ਇਹ ਇੱਕ ਮਹਿਲਾ ਪਾਤਰ ਨੂੰ ਦਰਸਾਉਂਦਾ ਹੈ, ਅਤੇ ਸਟੇਟ ਵਿੱਚ ਝੰਡਾ ਚੁੱਕਦੇ ਹਾਂ ਕਿ ਸਾਡੇ ਕੋਲ ਵਾਕਾਂਸ਼ ਵਿੱਚ ਇੱਕ ਮਹਿਲਾ ਸੰਗਿਆ ਹੈ। ਜਦੋਂ ਅਸੀਂ ਅਗਲੇ ਵਾਕਾਂਸ਼ *and Tom* ਨੂੰ ਮਿਲਦੇ ਹਾਂ, ਅਸੀਂ ਝੰਡਾ ਚੁੱਕਦੇ ਹਾਂ ਕਿ ਸਾਡੇ ਕੋਲ ਬਹੁਵਚਨ ਸੰਗਿਆ ਹੈ। ਇਸ ਤਰ੍ਹਾਂ ਸਟੇਟ ਨੂੰ ਮੈਨਿਪੁਲੇਟ ਕਰਕੇ ਅਸੀਂ ਵਾਕਾਂਸ਼ ਦੇ ਹਿੱਸਿਆਂ ਦੇ ਵਿਆਕਰਣਕ ਗੁਣਾਂ ਦਾ ਟ੍ਰੈਕ ਰੱਖ ਸਕਦੇ ਹਾਂ।

> ✅ LSTM ਦੇ ਅੰਦਰੂਨੀ ਹਿੱਸਿਆਂ ਨੂੰ ਸਮਝਣ ਲਈ ਇੱਕ ਸ਼ਾਨਦਾਰ ਸਰੋਤ ਇਹ ਵਧੀਆ ਲੇਖ ਹੈ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) ਕ੍ਰਿਸਟੋਫਰ ਓਲਾਹ ਦੁਆਰਾ।

## ਬਾਈਡਾਇਰੈਕਸ਼ਨਲ ਅਤੇ ਮਲਟੀਲੇਅਰ RNNs

ਅਸੀਂ ਰਿਕਰੰਟ ਨੈਟਵਰਕਸ ਦੀ ਚਰਚਾ ਕੀਤੀ ਹੈ ਜੋ ਇੱਕ ਦਿਸ਼ਾ ਵਿੱਚ ਕੰਮ ਕਰਦੇ ਹਨ, ਕ੍ਰਮ ਦੀ ਸ਼ੁਰੂਆਤ ਤੋਂ ਅੰਤ ਤੱਕ। ਇਹ ਕੁਦਰਤੀ ਲੱਗਦਾ ਹੈ, ਕਿਉਂਕਿ ਇਹ ਉਸ ਤਰੀਕੇ ਨੂੰ ਦਰਸਾਉਂਦਾ ਹੈ ਜਿਸ ਤਰ੍ਹਾਂ ਅਸੀਂ ਪੜ੍ਹਦੇ ਅਤੇ ਬੋਲਦੇ ਹਾਂ। ਹਾਲਾਂਕਿ, ਕਈ ਪ੍ਰਯੋਗਿਕ ਮਾਮਲਿਆਂ ਵਿੱਚ ਸਾਡੇ ਕੋਲ ਇਨਪੁਟ ਕ੍ਰਮ ਤੱਕ ਰੈਂਡਮ ਪਹੁੰਚ ਹੁੰਦੀ ਹੈ, ਇਹ ਦੋਵੇਂ ਦਿਸ਼ਾਵਾਂ ਵਿੱਚ ਰਿਕਰੰਟ ਗਣਨਾ ਚਲਾਉਣ ਲਈ ਸਮਝਦਾਰ ਹੋ ਸਕਦਾ ਹੈ। ਇਸ ਤਰ੍ਹਾਂ ਦੇ ਨੈਟਵਰਕਸ ਨੂੰ **ਬਾਈਡਾਇਰੈਕਸ਼ਨਲ** RNNs ਕਿਹਾ ਜਾਂਦਾ ਹੈ। ਬਾਈਡਾਇਰੈਕਸ਼ਨਲ ਨੈਟਵਰਕ ਨਾਲ ਨਜਿੱਠਣ ਸਮੇਂ, ਸਾਨੂੰ ਦੋ ਲੁਕਿਆ ਹੋਏ ਸਟੇਟ ਵੈਕਟਰਾਂ ਦੀ ਲੋੜ ਹੋਵੇਗੀ, ਇੱਕ ਹਰ ਦਿਸ਼ਾ ਲਈ।

ਇੱਕ ਰਿਕਰੰਟ ਨੈਟਵਰਕ, ਚਾਹੇ ਇੱਕ-ਦਿਸ਼ਾ ਵਾਲਾ ਹੋਵੇ ਜਾਂ ਬਾਈਡਾਇਰੈਕਸ਼ਨਲ, ਕ੍ਰਮ ਵਿੱਚ ਕੁਝ ਪੈਟਰਨਾਂ ਨੂੰ ਕੈਪਚਰ ਕਰਦਾ ਹੈ, ਅਤੇ ਇਸਨੂੰ ਸਟੇਟ ਵੈਕਟਰ ਵਿੱਚ ਸਟੋਰ ਕਰਦਾ ਹੈ ਜਾਂ ਆਉਟਪੁਟ ਵਿੱਚ ਪਾਸ ਕਰਦਾ ਹੈ। ਕਨਵੋਲੂਸ਼ਨਲ ਨੈਟਵਰਕਸ ਦੀ ਤਰ੍ਹਾਂ, ਅਸੀਂ ਪਹਿਲੇ ਲੇਅਰ ਦੁਆਰਾ ਕੈਪਚਰ ਕੀਤੇ ਗਏ ਨੀਵੀਂ-ਸਤਰ ਦੇ ਪੈਟਰਨਾਂ ਤੋਂ ਉੱਚ-ਸਤਰ ਦੇ ਪੈਟਰਨਾਂ ਨੂੰ ਕੈਪਚਰ ਕਰਨ ਲਈ ਪਹਿਲੇ ਲੇਅਰ ਦੇ ਉੱਪਰ ਇੱਕ ਹੋਰ ਰਿਕਰੰਟ ਲੇਅਰ ਬਣਾਉਣ ਲਈ ਬਣਾਉਣ ਕਰ ਸਕਦੇ ਹਾਂ। ਇਸ ਨਾਲ ਸਾਨੂੰ **ਮਲਟੀ-ਲੇਅਰ RNN** ਦਾ ਧਾਰਨਾ ਮਿਲਦੀ ਹੈ ਜੋ ਦੋ ਜਾਂ ਵੱਧ ਰਿਕਰੰਟ ਨੈਟਵਰਕਸ ਤੋਂ ਬਣਦਾ ਹੈ, ਜਿੱਥੇ ਪਿਛਲੇ ਲੇਅਰ ਦਾ ਆਉਟਪੁਟ ਅਗਲੇ ਲੇਅਰ ਵਿੱਚ ਇਨਪੁਟ ਵਜੋਂ ਪਾਸ ਕੀਤਾ ਜਾਂਦਾ ਹੈ।

![Image showing a Multilayer long-short-term-memory- RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.pa.jpg)

*ਚਿੱਤਰ [ਇਸ ਸ਼ਾਨਦਾਰ ਪੋਸਟ](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) ਫਰਨਾਂਡੋ ਲੋਪੇਜ਼ ਦੁਆਰਾ*

## ✍️ ਅਭਿਆਸ: ਐਮਬੈਡਿੰਗਸ

ਹੇਠਾਂ ਦਿੱਤੇ ਨੋਟਬੁੱਕਸ ਵਿੱਚ ਆਪਣਾ ਸਿੱਖਣਾ ਜਾਰੀ ਰੱਖੋ:

* [PyTorch ਨਾਲ RNNs](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow ਨਾਲ RNNs](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## ਨਿਸਕਰਸ਼

ਇਸ ਯੂਨਿਟ ਵਿੱਚ, ਅਸੀਂ ਵੇਖਿਆ ਕਿ RNNs ਨੂੰ ਕ੍ਰਮ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਲਈ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ, ਪਰ ਅਸਲ ਵਿੱਚ, ਇਹ ਕਈ ਹੋਰ ਕੰਮਾਂ ਨੂੰ ਸੰਭਾਲ ਸਕਦੇ ਹਨ, ਜਿਵੇਂ ਕਿ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ, ਮਸ਼ੀਨ ਅਨੁਵਾਦ, ਅਤੇ ਹੋਰ। ਅਸੀਂ ਅਗਲੇ ਯੂਨਿਟ ਵਿੱਚ ਉਹ ਕੰਮਾਂ ਵਿਚਾਰਾਂਗੇ।

## 🚀 ਚੁਣੌਤੀ

LSTMs ਬਾਰੇ ਕੁਝ ਸਾਹਿਤ ਪੜ੍ਹੋ ਅਤੇ ਉਨ੍ਹਾਂ ਦੇ ਐਪਲੀਕੇਸ਼ਨਸ ਬਾਰੇ ਵਿਚਾਰ ਕਰੋ:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [ਪੋਸਟ-ਲੈਕਚਰ ਕਵਿਜ਼](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## ਸਮੀਖਾ ਅਤੇ ਸਵੈ-ਅਧਿਐਨ

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) ਕ੍ਰਿਸਟੋਫਰ ਓਲਾਹ ਦੁਆਰਾ।

## [ਅਸਾਈਨਮੈਂਟ: ਨੋਟਬੁੱਕਸ](assignment.md)

**ਅਸਵੀਕਾਰਨਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਤਾ ਲਈ ਯਤਨਸ਼ੀਲ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚੀਤਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।