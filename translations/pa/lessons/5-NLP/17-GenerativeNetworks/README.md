<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-26T08:21:27+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "pa"
}
-->
# ਜਨਰੇਟਿਵ ਨੈਟਵਰਕਸ

## [ਪ੍ਰੀ-ਲੈਕਚਰ ਕਵਿਜ਼](https://ff-quizzes.netlify.app/en/ai/quiz/33)

ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕਸ (RNNs) ਅਤੇ ਉਨ੍ਹਾਂ ਦੇ ਗੇਟਡ ਸੈੱਲ ਵੈਰੀਐਂਟਸ ਜਿਵੇਂ ਕਿ ਲਾਂਗ ਸ਼ਾਰਟ ਟਰਮ ਮੈਮੋਰੀ ਸੈੱਲਜ਼ (LSTMs) ਅਤੇ ਗੇਟਡ ਰਿਕਰੰਟ ਯੂਨਿਟਸ (GRUs) ਨੇ ਭਾਸ਼ਾ ਮਾਡਲਿੰਗ ਲਈ ਇੱਕ ਮਕੈਨਿਜ਼ਮ ਪ੍ਰਦਾਨ ਕੀਤਾ ਹੈ, ਕਿਉਂਕਿ ਇਹ ਸ਼ਬਦਾਂ ਦੀ ਕ੍ਰਮਵਾਰਤਾ ਸਿੱਖ ਸਕਦੇ ਹਨ ਅਤੇ ਕ੍ਰਮ ਵਿੱਚ ਅਗਲੇ ਸ਼ਬਦ ਦੀ ਭਵਿੱਖਵਾਣੀ ਕਰ ਸਕਦੇ ਹਨ। ਇਸ ਨਾਲ ਸਾਨੂੰ RNNs ਨੂੰ **ਜਨਰੇਟਿਵ ਕੰਮਾਂ** ਲਈ ਵਰਤਣ ਦੀ ਆਗਿਆ ਮਿਲਦੀ ਹੈ, ਜਿਵੇਂ ਕਿ ਆਮ ਪਾਠ ਜਨਰੇਸ਼ਨ, ਮਸ਼ੀਨ ਅਨੁਵਾਦ, ਅਤੇ ਇਮੇਜ ਕੈਪਸ਼ਨਿੰਗ।

> ✅ ਸੋਚੋ ਕਿ ਤੁਸੀਂ ਕਿੰਨੀ ਵਾਰ ਪਾਠ ਪੂਰਨ ਕਰਨ ਵਰਗੀਆਂ ਜਨਰੇਟਿਵ ਕੰਮਾਂ ਤੋਂ ਲਾਭ ਪ੍ਰਾਪਤ ਕੀਤਾ ਹੈ। ਆਪਣੀਆਂ ਮਨਪਸੰਦ ਐਪਲੀਕੇਸ਼ਨਾਂ ਬਾਰੇ ਖੋਜ ਕਰੋ ਅਤੇ ਵੇਖੋ ਕਿ ਕੀ ਉਨ੍ਹਾਂ ਨੇ RNNs ਦਾ ਸਹਾਰਾ ਲਿਆ ਸੀ।

ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ ਚਰਚਾ ਕੀਤੇ RNN ਆਰਕੀਟੈਕਚਰ ਵਿੱਚ, ਹਰ RNN ਯੂਨਿਟ ਨੇ ਆਉਟਪੁੱਟ ਵਜੋਂ ਅਗਲਾ ਹਿਡਨ ਸਟੇਟ ਤਿਆਰ ਕੀਤਾ। ਹਾਲਾਂਕਿ, ਅਸੀਂ ਹਰ ਰਿਕਰੰਟ ਯੂਨਿਟ ਵਿੱਚ ਇੱਕ ਹੋਰ ਆਉਟਪੁੱਟ ਸ਼ਾਮਲ ਕਰ ਸਕਦੇ ਹਾਂ, ਜੋ ਸਾਨੂੰ ਇੱਕ **ਕ੍ਰਮ** (ਜੋ ਮੂਲ ਕ੍ਰਮ ਦੇ ਬਰਾਬਰ ਲੰਬਾਈ ਦਾ ਹੈ) ਤਿਆਰ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ। ਇਸ ਤੋਂ ਇਲਾਵਾ, ਅਸੀਂ RNN ਯੂਨਿਟਸ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹਾਂ ਜੋ ਹਰ ਕਦਮ 'ਤੇ ਇਨਪੁਟ ਸਵੀਕਾਰ ਨਹੀਂ ਕਰਦੇ, ਸਿਰਫ਼ ਕੁਝ ਸ਼ੁਰੂਆਤੀ ਸਟੇਟ ਵੇਕਟਰ ਲੈਂਦੇ ਹਨ, ਅਤੇ ਫਿਰ ਆਉਟਪੁੱਟ ਦਾ ਕ੍ਰਮ ਤਿਆਰ ਕਰਦੇ ਹਨ।

ਇਹ ਵੱਖ-ਵੱਖ ਨਿਊਰਲ ਆਰਕੀਟੈਕਚਰਾਂ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ, ਜੋ ਹੇਠਾਂ ਦਿੱਤੇ ਚਿੱਤਰ ਵਿੱਚ ਦਿਖਾਏ ਗਏ ਹਨ:

![ਆਮ ਰਿਕਰੰਟ ਨਿਊਰਲ ਨੈਟਵਰਕ ਪੈਟਰਨ ਦਿਖਾਉਂਦਾ ਚਿੱਤਰ।](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.pa.jpg)

> ਚਿੱਤਰ ਬਲੌਗ ਪੋਸਟ [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) ਤੋਂ [Andrej Karpaty](http://karpathy.github.io/) ਦੁਆਰਾ

* **ਵਨ-ਟੂ-ਵਨ** ਇੱਕ ਪਰੰਪਰਾਗਤ ਨਿਊਰਲ ਨੈਟਵਰਕ ਹੈ ਜਿਸ ਵਿੱਚ ਇੱਕ ਇਨਪੁਟ ਅਤੇ ਇੱਕ ਆਉਟਪੁੱਟ ਹੁੰਦਾ ਹੈ।
* **ਵਨ-ਟੂ-ਮੇਨੀ** ਇੱਕ ਜਨਰੇਟਿਵ ਆਰਕੀਟੈਕਚਰ ਹੈ ਜੋ ਇੱਕ ਇਨਪੁਟ ਮੁੱਲ ਨੂੰ ਸਵੀਕਾਰ ਕਰਦਾ ਹੈ ਅਤੇ ਆਉਟਪੁੱਟ ਮੁੱਲਾਂ ਦਾ ਕ੍ਰਮ ਤਿਆਰ ਕਰਦਾ ਹੈ। ਉਦਾਹਰਣ ਲਈ, ਜੇ ਅਸੀਂ ਇੱਕ **ਇਮੇਜ ਕੈਪਸ਼ਨਿੰਗ** ਨੈਟਵਰਕ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ ਚਾਹੁੰਦੇ ਹਾਂ ਜੋ ਤਸਵੀਰ ਦਾ ਵਰਣਨ ਪਾਠ ਰੂਪ ਵਿੱਚ ਤਿਆਰ ਕਰੇ, ਤਾਂ ਅਸੀਂ ਇੱਕ ਤਸਵੀਰ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਲੈ ਸਕਦੇ ਹਾਂ, ਇਸਨੂੰ ਇੱਕ CNN ਰਾਹੀਂ ਪਾਸ ਕਰ ਸਕਦੇ ਹਾਂ ਤਾਂ ਜੋ ਇਸਦਾ ਹਿਡਨ ਸਟੇਟ ਪ੍ਰਾਪਤ ਹੋਵੇ, ਅਤੇ ਫਿਰ ਇੱਕ ਰਿਕਰੰਟ ਚੇਨ ਸ਼ਬਦ-ਦਰ-ਸ਼ਬਦ ਕੈਪਸ਼ਨ ਤਿਆਰ ਕਰ ਸਕੇ।
* **ਮੇਨੀ-ਟੂ-ਵਨ** ਉਹ RNN ਆਰਕੀਟੈਕਚਰਾਂ ਨਾਲ ਸਬੰਧਤ ਹੈ ਜਿਨ੍ਹਾਂ ਦੀ ਪਿਛਲੇ ਯੂਨਿਟ ਵਿੱਚ ਚਰਚਾ ਕੀਤੀ ਗਈ ਸੀ, ਜਿਵੇਂ ਕਿ ਪਾਠ ਵਰਗੀਕਰਨ।
* **ਮੇਨੀ-ਟੂ-ਮੇਨੀ**, ਜਾਂ **ਸਿਕਵੈਂਸ-ਟੂ-ਸਿਕਵੈਂਸ**, ਉਹ ਕੰਮਾਂ ਨਾਲ ਸਬੰਧਤ ਹੈ ਜਿਵੇਂ ਕਿ **ਮਸ਼ੀਨ ਅਨੁਵਾਦ**, ਜਿੱਥੇ ਪਹਿਲਾਂ RNN ਇਨਪੁਟ ਕ੍ਰਮ ਤੋਂ ਸਾਰੀ ਜਾਣਕਾਰੀ ਹਿਡਨ ਸਟੇਟ ਵਿੱਚ ਇਕੱਠੀ ਕਰਦਾ ਹੈ, ਅਤੇ ਦੂਜਾ RNN ਚੇਨ ਇਸ ਸਟੇਟ ਨੂੰ ਆਉਟਪੁੱਟ ਕ੍ਰਮ ਵਿੱਚ ਬਦਲਦਾ ਹੈ।

ਇਸ ਯੂਨਿਟ ਵਿੱਚ, ਅਸੀਂ ਸਧਾਰਣ ਜਨਰੇਟਿਵ ਮਾਡਲਾਂ 'ਤੇ ਧਿਆਨ ਕੇਂਦਰਿਤ ਕਰਾਂਗੇ ਜੋ ਸਾਨੂੰ ਪਾਠ ਤਿਆਰ ਕਰਨ ਵਿੱਚ ਮਦਦ ਕਰਦੇ ਹਨ। ਸਧਾਰਣਤਾ ਲਈ, ਅਸੀਂ ਕਿਰਦਾਰ-ਪੱਧਰ ਟੋਕਨਾਈਜ਼ੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ।

ਅਸੀਂ ਇਸ RNN ਨੂੰ ਪਾਠ ਕਦਮ-ਦਰ-ਕਦਮ ਤਿਆਰ ਕਰਨ ਲਈ ਟ੍ਰੇਨ ਕਰਾਂਗੇ। ਹਰ ਕਦਮ 'ਤੇ, ਅਸੀਂ `nchars` ਲੰਬਾਈ ਦੇ ਕਿਰਦਾਰਾਂ ਦੇ ਕ੍ਰਮ ਨੂੰ ਲਵਾਂਗੇ, ਅਤੇ ਨੈਟਵਰਕ ਨੂੰ ਹਰ ਇਨਪੁਟ ਕਿਰਦਾਰ ਲਈ ਅਗਲਾ ਆਉਟਪੁੱਟ ਕਿਰਦਾਰ ਤਿਆਰ ਕਰਨ ਲਈ ਕਹਾਂਗੇ:

![ਚਿੱਤਰ 'HELLO' ਸ਼ਬਦ ਦੇ RNN ਜਨਰੇਸ਼ਨ ਦਾ ਉਦਾਹਰਣ ਦਿਖਾਉਂਦਾ ਹੈ।](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.pa.png)

ਜਦੋਂ ਪਾਠ ਤਿਆਰ ਕਰਦੇ ਹਾਂ (ਇਨਫਰੈਂਸ ਦੌਰਾਨ), ਅਸੀਂ ਕੁਝ **ਪ੍ਰਾਂਪਟ** ਨਾਲ ਸ਼ੁਰੂ ਕਰਦੇ ਹਾਂ, ਜਿਸਨੂੰ RNN ਸੈੱਲਾਂ ਰਾਹੀਂ ਪਾਸ ਕੀਤਾ ਜਾਂਦਾ ਹੈ ਤਾਂ ਜੋ ਇਸਦਾ ਇੰਟਰਮੀਡੀਏਟ ਸਟੇਟ ਤਿਆਰ ਹੋਵੇ, ਅਤੇ ਫਿਰ ਇਸ ਸਟੇਟ ਤੋਂ ਜਨਰੇਸ਼ਨ ਸ਼ੁਰੂ ਹੁੰਦੀ ਹੈ। ਅਸੀਂ ਇੱਕ ਸਮੇਂ ਵਿੱਚ ਇੱਕ ਕਿਰਦਾਰ ਤਿਆਰ ਕਰਦੇ ਹਾਂ, ਅਤੇ ਸਟੇਟ ਅਤੇ ਤਿਆਰ ਕੀਤਾ ਕਿਰਦਾਰ ਦੂਜੇ RNN ਸੈੱਲ ਨੂੰ ਪਾਸ ਕਰਦੇ ਹਾਂ ਤਾਂ ਜੋ ਅਗਲਾ ਤਿਆਰ ਕੀਤਾ ਜਾ ਸਕੇ, ਜਦੋਂ ਤੱਕ ਅਸੀਂ ਕਾਫ਼ੀ ਕਿਰਦਾਰ ਤਿਆਰ ਨਾ ਕਰ ਲਵਾਂ।

<img src="images/rnn-generate-inf.png" width="60%"/>

> ਚਿੱਤਰ ਲੇਖਕ ਦੁਆਰਾ

## ✍️ ਅਭਿਆਸ: ਜਨਰੇਟਿਵ ਨੈਟਵਰਕਸ

ਹੇਠਾਂ ਦਿੱਤੇ ਨੋਟਬੁੱਕਸ ਵਿੱਚ ਆਪਣੀ ਸਿੱਖਿਆ ਜਾਰੀ ਰੱਖੋ:

* [PyTorch ਨਾਲ ਜਨਰੇਟਿਵ ਨੈਟਵਰਕਸ](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [TensorFlow ਨਾਲ ਜਨਰੇਟਿਵ ਨੈਟਵਰਕਸ](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## ਨਰਮ ਪਾਠ ਜਨਰੇਸ਼ਨ ਅਤੇ ਟੈਂਪਰੇਚਰ

ਹਰ RNN ਸੈੱਲ ਦਾ ਆਉਟਪੁੱਟ ਕਿਰਦਾਰਾਂ ਦਾ ਸੰਭਾਵਨਾ ਵੰਡ ਹੈ। ਜੇਕਰ ਅਸੀਂ ਹਮੇਸ਼ਾਂ ਜਨਰੇਟ ਕੀਤੇ ਗਏ ਪਾਠ ਵਿੱਚ ਅਗਲੇ ਕਿਰਦਾਰ ਵਜੋਂ ਸਭ ਤੋਂ ਉੱਚੀ ਸੰਭਾਵਨਾ ਵਾਲੇ ਕਿਰਦਾਰ ਨੂੰ ਲੈਂਦੇ ਹਾਂ, ਤਾਂ ਪਾਠ ਅਕਸਰ "ਚੱਕਰਵਾਦੀ" ਹੋ ਸਕਦਾ ਹੈ, ਜਿੱਥੇ ਉਹ ਵਾਰ-ਵਾਰ ਇੱਕੋ ਜਿਹੇ ਕਿਰਦਾਰ ਕ੍ਰਮਾਂ ਵਿੱਚ ਫਸ ਜਾਂਦਾ ਹੈ, ਜਿਵੇਂ ਇਸ ਉਦਾਹਰਣ ਵਿੱਚ:

```
today of the second the company and a second the company ...
```

ਹਾਲਾਂਕਿ, ਜੇ ਅਸੀਂ ਅਗਲੇ ਕਿਰਦਾਰ ਲਈ ਸੰਭਾਵਨਾ ਵੰਡ ਨੂੰ ਵੇਖਦੇ ਹਾਂ, ਤਾਂ ਇਹ ਹੋ ਸਕਦਾ ਹੈ ਕਿ ਕੁਝ ਸਭ ਤੋਂ ਉੱਚੀਆਂ ਸੰਭਾਵਨਾਵਾਂ ਵਿੱਚ ਵੱਡਾ ਅੰਤਰ ਨਾ ਹੋਵੇ, ਉਦਾਹਰਣ ਲਈ, ਇੱਕ ਕਿਰਦਾਰ ਦੀ ਸੰਭਾਵਨਾ 0.2 ਹੋ ਸਕਦੀ ਹੈ, ਅਤੇ ਦੂਜੇ ਦੀ 0.19। ਉਦਾਹਰਣ ਲਈ, ਜਦੋਂ ਕ੍ਰਮ '*play*' ਵਿੱਚ ਅਗਲੇ ਕਿਰਦਾਰ ਦੀ ਭਾਲ ਕਰਦੇ ਹਾਂ, ਤਾਂ ਅਗਲਾ ਕਿਰਦਾਰ ਜਿਵੇਂ **ਖਾਲੀ ਜਗ੍ਹਾ** ਜਾਂ **e** (ਜਿਵੇਂ ਸ਼ਬਦ *player* ਵਿੱਚ) ਹੋ ਸਕਦਾ ਹੈ।

ਇਸ ਨਾਲ ਸਾਨੂੰ ਇਹ ਨਤੀਜਾ ਨਿਕਲਦਾ ਹੈ ਕਿ ਹਮੇਸ਼ਾਂ ਸਭ ਤੋਂ ਉੱਚੀ ਸੰਭਾਵਨਾ ਵਾਲੇ ਕਿਰਦਾਰ ਨੂੰ ਚੁਣਨਾ "ਨਿਆਂਸੰਗਤ" ਨਹੀਂ ਹੈ, ਕਿਉਂਕਿ ਦੂਜੀ ਸਭ ਤੋਂ ਉੱਚੀ ਸੰਭਾਵਨਾ ਵਾਲੇ ਕਿਰਦਾਰ ਨੂੰ ਚੁਣਨਾ ਵੀ ਅਰਥਪੂਰਨ ਪਾਠ ਤਿਆਰ ਕਰ ਸਕਦਾ ਹੈ। ਇਹ ਜ਼ਿਆਦਾ ਸਮਝਦਾਰੀ ਵਾਲੀ ਗੱਲ ਹੈ ਕਿ ਨੈਟਵਰਕ ਆਉਟਪੁੱਟ ਦੁਆਰਾ ਦਿੱਤੀ ਗਈ ਸੰਭਾਵਨਾ ਵੰਡ ਤੋਂ ਕਿਰਦਾਰਾਂ ਨੂੰ **ਨਮੂਨਾ** ਬਣਾਇਆ ਜਾਵੇ। ਅਸੀਂ ਇੱਕ ਪੈਰਾਮੀਟਰ, **ਟੈਂਪਰੇਚਰ**, ਦੀ ਵੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹਾਂ ਜੋ ਸੰਭਾਵਨਾ ਵੰਡ ਨੂੰ ਚੌੜਾ ਕਰ ਸਕਦਾ ਹੈ, ਜੇ ਅਸੀਂ ਹੋਰ ਬੇਤਰਤੀਬੀ ਸ਼ਾਮਲ ਕਰਨੀ ਚਾਹੁੰਦੇ ਹਾਂ, ਜਾਂ ਇਸਨੂੰ ਹੋਰ ਖੜਾ ਕਰ ਸਕਦੇ ਹਾਂ, ਜੇ ਅਸੀਂ ਸਭ ਤੋਂ ਉੱਚੀ-ਸੰਭਾਵਨਾ ਵਾਲੇ ਕਿਰਦਾਰਾਂ ਨਾਲ ਜ਼ਿਆਦਾ ਚਿਪਕਣਾ ਚਾਹੁੰਦੇ ਹਾਂ।

ਉਪਰ ਦਿੱਤੇ ਨੋਟਬੁੱਕਸ ਵਿੱਚ ਵੇਖੋ ਕਿ ਇਹ ਨਰਮ ਪਾਠ ਜਨਰੇਸ਼ਨ ਕਿਵੇਂ ਲਾਗੂ ਕੀਤਾ ਗਿਆ ਹੈ।

## ਨਤੀਜਾ

ਜਦੋਂ ਕਿ ਪਾਠ ਜਨਰੇਸ਼ਨ ਆਪਣੇ ਆਪ ਵਿੱਚ ਲਾਭਦਾਇਕ ਹੋ ਸਕਦਾ ਹੈ, ਪਰ ਮੁੱਖ ਲਾਭ ਇਹ ਹੈ ਕਿ RNNs ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੁਝ ਸ਼ੁਰੂਆਤੀ ਫੀਚਰ ਵੇਕਟਰ ਤੋਂ ਪਾਠ ਤਿਆਰ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ। ਉਦਾਹਰਣ ਲਈ, ਪਾਠ ਜਨਰੇਸ਼ਨ ਨੂੰ ਮਸ਼ੀਨ ਅਨੁਵਾਦ (ਸਿਕਵੈਂਸ-ਟੂ-ਸਿਕਵੈਂਸ, ਇਸ ਮਾਮਲੇ ਵਿੱਚ *ਐਨਕੋਡਰ* ਤੋਂ ਸਟੇਟ ਵੇਕਟਰ ਨੂੰ ਵਰਤ ਕੇ ਅਨੁਵਾਦਿਤ ਸੁਨੇਹਾ *ਡਿਕੋਡ* ਕੀਤਾ ਜਾਂਦਾ ਹੈ) ਜਾਂ ਤਸਵੀਰ ਦਾ ਪਾਠ ਰੂਪ ਵਿੱਚ ਵਰਣਨ ਤਿਆਰ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ (ਜਿਸ ਮਾਮਲੇ ਵਿੱਚ ਫੀਚਰ ਵੇਕਟਰ CNN ਐਕਸਟ੍ਰੈਕਟਰ ਤੋਂ ਆਉਂਦਾ ਹੈ)।

## 🚀 ਚੁਣੌਤੀ

ਮਾਈਕਰੋਸੌਫਟ ਲਰਨ 'ਤੇ ਇਸ ਵਿਸ਼ੇ 'ਤੇ ਕੁਝ ਪਾਠ ਲਵੋ

* [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste) ਨਾਲ ਪਾਠ ਜਨਰੇਸ਼ਨ

## [ਪੋਸਟ-ਲੈਕਚਰ ਕਵਿਜ਼](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## ਸਮੀਖਿਆ ਅਤੇ ਸਵੈ ਅਧਿਐਨ

ਇਹ ਲੇਖ ਤੁਹਾਡੀ ਜਾਣਕਾਰੀ ਨੂੰ ਵਧਾਉਣ ਲਈ ਹਨ:

* ਮਾਰਕੋਵ ਚੇਨ, LSTM ਅਤੇ GPT-2 ਨਾਲ ਪਾਠ ਜਨਰੇਸ਼ਨ ਦੇ ਵੱਖ-ਵੱਖ ਤਰੀਕੇ: [ਬਲੌਗ ਪੋਸਟ](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Keras ਡੌਕਯੂਮੈਂਟੇਸ਼ਨ](https://keras.io/examples/generative/lstm_character_level_text_generation/) ਵਿੱਚ ਪਾਠ ਜਨਰੇਸ਼ਨ ਦਾ ਉਦਾਹਰਣ

## [ਅਸਾਈਨਮੈਂਟ](lab/README.md)

ਅਸੀਂ ਵੇਖਿਆ ਕਿ ਕਿਵੇਂ ਕਿਰਦਾਰ-ਦਰ-ਕਿਰਦਾਰ ਪਾਠ ਤਿਆਰ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ। ਲੈਬ ਵਿੱਚ, ਤੁਸੀਂ ਸ਼ਬਦ-ਪੱਧਰ ਪਾਠ ਜਨਰੇਸ਼ਨ ਦੀ ਪੜਚੋਲ ਕਰੋਗੇ।

**ਅਸਵੀਕਰਤਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚੱਜੇਪਣ ਹੋ ਸਕਦੇ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।