<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ae074cd940fc2f4dc24fc07b66ccbd99",
  "translation_date": "2025-08-26T09:50:45+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md",
  "language_code": "pa"
}
-->
# ਡੀਪ ਲਰਨਿੰਗ ਟ੍ਰੇਨਿੰਗ ਟ੍ਰਿਕਸ

ਜਿਵੇਂ ਨਿਊਰਲ ਨੈਟਵਰਕ ਗਹਿਰੇ ਹੁੰਦੇ ਹਨ, ਉਨ੍ਹਾਂ ਦੀ ਟ੍ਰੇਨਿੰਗ ਦੀ ਪ੍ਰਕਿਰਿਆ ਹੋਰ ਅਤੇ ਹੋਰ ਮੁਸ਼ਕਲ ਹੋ ਜਾਂਦੀ ਹੈ। ਇੱਕ ਵੱਡੀ ਸਮੱਸਿਆ [ਵੈਨਿਸਿੰਗ ਗ੍ਰੇਡੀਅੰਟ](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) ਜਾਂ [ਐਕਸਪਲੋਡਿੰਗ ਗ੍ਰੇਡੀਅੰਟ](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.) ਹੈ। [ਇਹ ਪੋਸਟ](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) ਉਨ੍ਹਾਂ ਸਮੱਸਿਆਵਾਂ ਬਾਰੇ ਚੰਗੀ ਜਾਣਕਾਰੀ ਦਿੰਦੀ ਹੈ।

ਡੀਪ ਨੈਟਵਰਕ ਦੀ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਹੋਰ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਬਣਾਉਣ ਲਈ ਕੁਝ ਤਕਨੀਕਾਂ ਵਰਤੀ ਜਾ ਸਕਦੀਆਂ ਹਨ।

## ਮੁੱਲਾਂ ਨੂੰ ਵਾਜਬ ਇੰਟਰਵਲ ਵਿੱਚ ਰੱਖਣਾ

ਸੰਖਿਆਤਮਕ ਗਣਨਾਵਾਂ ਨੂੰ ਹੋਰ ਸਥਿਰ ਬਣਾਉਣ ਲਈ, ਅਸੀਂ ਇਹ ਯਕੀਨੀ ਬਣਾਉਣਾ ਚਾਹੁੰਦੇ ਹਾਂ ਕਿ ਸਾਡੇ ਨਿਊਰਲ ਨੈਟਵਰਕ ਦੇ ਸਾਰੇ ਮੁੱਲ ਵਾਜਬ ਪੈਮਾਨੇ ਵਿੱਚ ਹਨ, ਆਮ ਤੌਰ 'ਤੇ [-1..1] ਜਾਂ [0..1]। ਇਹ ਕੋਈ ਬਹੁਤ ਸਖਤ ਜ਼ਰੂਰਤ ਨਹੀਂ ਹੈ, ਪਰ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਗਣਨਾਵਾਂ ਦੀ ਪ੍ਰਕਿਰਤੀ ਅਜਿਹੀ ਹੈ ਕਿ ਵੱਖ-ਵੱਖ ਮੈਗਨੀਟਿਊਡ ਦੇ ਮੁੱਲਾਂ ਨੂੰ ਇਕੱਠੇ ਸਹੀ ਤਰੀਕੇ ਨਾਲ ਮੈਨਿਪੁਲੇਟ ਨਹੀਂ ਕੀਤਾ ਜਾ ਸਕਦਾ। ਉਦਾਹਰਨ ਲਈ, ਜੇ ਅਸੀਂ 10<sup>-10</sup> ਅਤੇ 10<sup>10</sup> ਨੂੰ ਜੋੜਦੇ ਹਾਂ, ਤਾਂ ਸਾਨੂੰ ਸੰਭਾਵਨਾ ਹੈ ਕਿ 10<sup>10</sup> ਮਿਲੇਗਾ, ਕਿਉਂਕਿ ਛੋਟਾ ਮੁੱਲ "ਵੱਡੇ" ਦੇ ਆਰਡਰ ਵਿੱਚ ਬਦਲਿਆ ਜਾਵੇਗਾ, ਅਤੇ ਇਸ ਤਰ੍ਹਾਂ ਮਾਂਟਿਸਾ ਖਤਮ ਹੋ ਜਾਵੇਗੀ।

ਜਿਆਦਾਤਰ ਐਕਟੀਵੇਸ਼ਨ ਫੰਕਸ਼ਨ [-1..1] ਦੇ ਆਸ-ਪਾਸ ਗੈਰ-ਰੇਖਿਕਤਾ ਰੱਖਦੇ ਹਨ, ਇਸ ਲਈ ਸਾਰੇ ਇਨਪੁਟ ਡਾਟਾ ਨੂੰ [-1..1] ਜਾਂ [0..1] ਇੰਟਰਵਲ ਵਿੱਚ ਸਕੇਲ ਕਰਨਾ ਵਾਜਬ ਹੈ।

## ਸ਼ੁਰੂਆਤੀ ਵਜ਼ਨ ਇਨਿਸ਼ੀਅਲਾਈਜ਼ੇਸ਼ਨ

ਆਦਰਸ਼ ਤੌਰ 'ਤੇ, ਅਸੀਂ ਚਾਹੁੰਦੇ ਹਾਂ ਕਿ ਨੈਟਵਰਕ ਲੇਅਰਾਂ ਵਿੱਚੋਂ ਗੁਜ਼ਰਨ ਤੋਂ ਬਾਅਦ ਮੁੱਲ ਇੱਕੋ ਪੈਮਾਨੇ ਵਿੱਚ ਰਹਿਣ। ਇਸ ਲਈ, ਵਜ਼ਨਾਂ ਨੂੰ ਇਸ ਤਰੀਕੇ ਨਾਲ ਸ਼ੁਰੂ ਕਰਨਾ ਮਹੱਤਵਪੂਰਨ ਹੈ ਕਿ ਮੁੱਲਾਂ ਦੇ ਵੰਡਨ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਿਆ ਜਾ ਸਕੇ।

**N(0,1)** ਵਰਗਾ ਨਾਰਮਲ ਵੰਡਨ ਚੰਗਾ ਵਿਚਾਰ ਨਹੀਂ ਹੈ, ਕਿਉਂਕਿ ਜੇ ਸਾਡੇ ਕੋਲ *n* ਇਨਪੁਟ ਹਨ, ਤਾਂ ਆਉਟਪੁੱਟ ਦਾ ਸਟੈਂਡਰਡ ਡਿਵਿਏਸ਼ਨ *n* ਹੋਵੇਗਾ, ਅਤੇ ਮੁੱਲ [0..1] ਇੰਟਰਵਲ ਤੋਂ ਬਾਹਰ ਚਲੇ ਜਾਣ ਦੀ ਸੰਭਾਵਨਾ ਹੈ।

ਨਿਮਨਲਿਖਤ ਇਨਿਸ਼ੀਅਲਾਈਜ਼ੇਸ਼ਨ ਅਕਸਰ ਵਰਤੇ ਜਾਂਦੇ ਹਨ:

* ਯੂਨੀਫਾਰਮ ਵੰਡਨ -- `uniform`
* **N(0,1/n)** -- `gaussian`
* **N(0,1/√n_in)** ਇਹ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ ਕਿ ਜੇ ਇਨਪੁਟ ਦਾ ਮੀਨ ਜ਼ੀਰੋ ਹੈ ਅਤੇ ਸਟੈਂਡਰਡ ਡਿਵਿਏਸ਼ਨ 1 ਹੈ, ਤਾਂ ਉਹੀ ਮੀਨ/ਸਟੈਂਡਰਡ ਡਿਵਿਏਸ਼ਨ ਬਰਕਰਾਰ ਰਹੇਗਾ।
* **N(0,√2/(n_in+n_out))** -- **Xavier initialization** (`glorot`), ਇਹ ਸਿਗਨਲਾਂ ਨੂੰ ਫਾਰਵਰਡ ਅਤੇ ਬੈਕਵਰਡ ਪ੍ਰੋਪਾਗੇਸ਼ਨ ਦੌਰਾਨ ਪੈਮਾਨੇ ਵਿੱਚ ਰੱਖਣ ਵਿੱਚ ਮਦਦ ਕਰਦਾ ਹੈ।

## ਬੈਚ ਨਾਰਮਲਾਈਜ਼ੇਸ਼ਨ

ਸਹੀ ਵਜ਼ਨ ਇਨਿਸ਼ੀਅਲਾਈਜ਼ੇਸ਼ਨ ਦੇ ਨਾਲ ਵੀ, ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਵਜ਼ਨ ਬਹੁਤ ਵੱਡੇ ਜਾਂ ਛੋਟੇ ਹੋ ਸਕਦੇ ਹਨ, ਅਤੇ ਇਹ ਸਿਗਨਲਾਂ ਨੂੰ ਸਹੀ ਪੈਮਾਨੇ ਤੋਂ ਬਾਹਰ ਲੈ ਜਾ ਸਕਦੇ ਹਨ। ਅਸੀਂ **ਨਾਰਮਲਾਈਜ਼ੇਸ਼ਨ** ਤਕਨੀਕਾਂ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਸਿਗਨਲਾਂ ਨੂੰ ਵਾਪਸ ਸਹੀ ਪੈਮਾਨੇ ਵਿੱਚ ਲਿਆ ਸਕਦੇ ਹਾਂ। ਜਦਕਿ ਕਈ ਤਕਨੀਕਾਂ ਹਨ (Weight Normalization, Layer Normalization), ਸਭ ਤੋਂ ਵੱਧ ਵਰਤੀ ਜਾਣ ਵਾਲੀ ਤਕਨੀਕ ਬੈਚ ਨਾਰਮਲਾਈਜ਼ੇਸ਼ਨ ਹੈ।

**ਬੈਚ ਨਾਰਮਲਾਈਜ਼ੇਸ਼ਨ** ਦਾ ਵਿਚਾਰ ਇਹ ਹੈ ਕਿ ਮਿਨੀਬੈਚ ਦੇ ਸਾਰੇ ਮੁੱਲਾਂ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖਣਾ, ਅਤੇ ਉਨ੍ਹਾਂ ਮੁੱਲਾਂ ਦੇ ਆਧਾਰ 'ਤੇ ਨਾਰਮਲਾਈਜ਼ੇਸ਼ਨ ਕਰਨਾ (ਜਾਂ ਮੀਨ ਘਟਾਉਣਾ ਅਤੇ ਸਟੈਂਡਰਡ ਡਿਵਿਏਸ਼ਨ ਨਾਲ ਵੰਡਣਾ)। ਇਹ ਇੱਕ ਨੈਟਵਰਕ ਲੇਅਰ ਵਜੋਂ ਲਾਗੂ ਕੀਤਾ ਜਾਂਦਾ ਹੈ ਜੋ ਵਜ਼ਨਾਂ ਨੂੰ ਲਾਗੂ ਕਰਨ ਤੋਂ ਬਾਅਦ, ਪਰ ਐਕਟੀਵੇਸ਼ਨ ਫੰਕਸ਼ਨ ਤੋਂ ਪਹਿਲਾਂ ਇਹ ਨਾਰਮਲਾਈਜ਼ੇਸ਼ਨ ਕਰਦਾ ਹੈ। ਇਸ ਦੇ ਨਤੀਜੇ ਵਜੋਂ, ਅਸੀਂ ਆਖਰੀ ਸਹੀਤਾ ਵਿੱਚ ਵਾਧਾ ਅਤੇ ਤੇਜ਼ ਟ੍ਰੇਨਿੰਗ ਦੇਖ ਸਕਦੇ ਹਾਂ।

ਇਹ ਹੈ [ਅਸਲ ਪੇਪਰ](https://arxiv.org/pdf/1502.03167.pdf) ਬੈਚ ਨਾਰਮਲਾਈਜ਼ੇਸ਼ਨ 'ਤੇ, [ਵਿਕੀਪੀਡੀਆ 'ਤੇ ਵਿਆਖਿਆ](https://en.wikipedia.org/wiki/Batch_normalization), ਅਤੇ [ਇੱਕ ਚੰਗੀ ਸ਼ੁਰੂਆਤੀ ਬਲੌਗ ਪੋਸਟ](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (ਅਤੇ ਇੱਕ [ਰੂਸੀ ਵਿੱਚ](https://habrahabr.ru/post/309302/))।

## ਡ੍ਰੌਪਆਉਟ

**ਡ੍ਰੌਪਆਉਟ** ਇੱਕ ਦਿਲਚਸਪ ਤਕਨੀਕ ਹੈ ਜੋ ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਕੁਝ ਪ੍ਰਤੀਸ਼ਤ ਰੈਂਡਮ ਨਿਊਰੋਨ ਨੂੰ ਹਟਾ ਦਿੰਦੀ ਹੈ। ਇਹ ਇੱਕ ਲੇਅਰ ਵਜੋਂ ਲਾਗੂ ਕੀਤਾ ਜਾਂਦਾ ਹੈ ਜਿਸ ਵਿੱਚ ਇੱਕ ਪੈਰਾਮੀਟਰ ਹੁੰਦਾ ਹੈ (ਹਟਾਏ ਜਾਣ ਵਾਲੇ ਨਿਊਰੋਨ ਦਾ ਪ੍ਰਤੀਸ਼ਤ, ਆਮ ਤੌਰ 'ਤੇ 10%-50%), ਅਤੇ ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਇਹ ਇਨਪੁਟ ਵੇਕਟਰ ਦੇ ਰੈਂਡਮ ਤੱਤਾਂ ਨੂੰ ਜ਼ੀਰੋ ਕਰ ਦਿੰਦਾ ਹੈ, ਇਸਨੂੰ ਅਗਲੇ ਲੇਅਰ ਵਿੱਚ ਭੇਜਣ ਤੋਂ ਪਹਿਲਾਂ।

ਜਦਕਿ ਇਹ ਇੱਕ ਅਜੀਬ ਵਿਚਾਰ ਲੱਗ ਸਕਦਾ ਹੈ, ਤੁਸੀਂ MNIST ਡਿਜਿਟ ਕਲਾਸੀਫਾਇਰ ਦੀ ਟ੍ਰੇਨਿੰਗ 'ਤੇ ਡ੍ਰੌਪਆਉਟ ਦੇ ਪ੍ਰਭਾਵ ਨੂੰ [`Dropout.ipynb`](../../../../../lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb) ਨੋਟਬੁੱਕ ਵਿੱਚ ਦੇਖ ਸਕਦੇ ਹੋ। ਇਹ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਤੇਜ਼ ਕਰਦਾ ਹੈ ਅਤੇ ਘੱਟ ਟ੍ਰੇਨਿੰਗ ਇਪੋਕਸ ਵਿੱਚ ਉੱਚ ਸਹੀਤਾ ਪ੍ਰਾਪਤ ਕਰਨ ਵਿੱਚ ਸਹਾਇਕ ਹੈ।

ਇਸ ਪ੍ਰਭਾਵ ਨੂੰ ਕਈ ਤਰੀਕਿਆਂ ਨਾਲ ਸਮਝਾਇਆ ਜਾ ਸਕਦਾ ਹੈ:

* ਇਸਨੂੰ ਮਾਡਲ ਲਈ ਇੱਕ ਰੈਂਡਮ ਸ਼ਾਕਿੰਗ ਫੈਕਟਰ ਮੰਨਿਆ ਜਾ ਸਕਦਾ ਹੈ, ਜੋ ਸਥਾਨਕ ਮਿਨੀਮਮ ਤੋਂ ਬਾਹਰ ਆਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਲੈ ਜਾਂਦਾ ਹੈ।
* ਇਸਨੂੰ *implicit model averaging* ਵਜੋਂ ਮੰਨਿਆ ਜਾ ਸਕਦਾ ਹੈ, ਕਿਉਂਕਿ ਅਸੀਂ ਕਹਿ ਸਕਦੇ ਹਾਂ ਕਿ ਡ੍ਰੌਪਆਉਟ ਦੌਰਾਨ ਅਸੀਂ ਥੋੜ੍ਹਾ ਵੱਖਰਾ ਮਾਡਲ ਟ੍ਰੇਨ ਕਰ ਰਹੇ ਹਾਂ।

> *ਕੁਝ ਲੋਕ ਕਹਿੰਦੇ ਹਨ ਕਿ ਜਦੋਂ ਇੱਕ ਨਸ਼ੇ ਵਿੱਚ ਹੋਇਆ ਵਿਅਕਤੀ ਕੁਝ ਸਿੱਖਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦਾ ਹੈ, ਤਾਂ ਉਹ ਇਸਨੂੰ ਅਗਲੇ ਸਵੇਰੇ ਬਿਹਤਰ ਯਾਦ ਕਰਦਾ ਹੈ, ਮੁਕਾਬਲੇ ਇੱਕ ਸਧਾਰਨ ਵਿਅਕਤੀ ਦੇ, ਕਿਉਂਕਿ ਕੁਝ ਖਰਾਬ ਨਿਊਰੋਨ ਵਾਲਾ ਦਿਮਾਗ ਮਤਲਬ ਨੂੰ ਸਮਝਣ ਲਈ ਬਿਹਤਰ ਅਨੁਕੂਲ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦਾ ਹੈ। ਅਸੀਂ ਕਦੇ ਆਪਣੇ ਆਪ ਇਸਨੂੰ ਸੱਚ ਜਾਂ ਝੂਠ ਨਹੀਂ ਪਰਖਿਆ।*

## ਓਵਰਫਿਟਿੰਗ ਨੂੰ ਰੋਕਣਾ

ਡੀਪ ਲਰਨਿੰਗ ਦੇ ਬਹੁਤ ਮਹੱਤਵਪੂਰਨ ਪਹਲੂ ਵਿੱਚੋਂ ਇੱਕ ਹੈ [ਓਵਰਫਿਟਿੰਗ](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) ਨੂੰ ਰੋਕਣ ਦੀ ਯੋਗਤਾ। ਜਦਕਿ ਬਹੁਤ ਸ਼ਕਤੀਸ਼ਾਲੀ ਨਿਊਰਲ ਨੈਟਵਰਕ ਮਾਡਲ ਵਰਤਣ ਦਾ ਲਾਲਚ ਹੋ ਸਕਦਾ ਹੈ, ਅਸੀਂ ਹਮੇਸ਼ਾ ਮਾਡਲ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਗਿਣਤੀ ਨੂੰ ਟ੍ਰੇਨਿੰਗ ਨਮੂਨਿਆਂ ਦੀ ਗਿਣਤੀ ਨਾਲ ਸੰਤੁਲਿਤ ਕਰਨਾ ਚਾਹੀਦਾ ਹੈ।

> ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਤੁਸੀਂ [ਓਵਰਫਿਟਿੰਗ](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) ਦੇ ਸੰਕਲਪ ਨੂੰ ਸਮਝਦੇ ਹੋ ਜੋ ਅਸੀਂ ਪਹਿਲਾਂ ਪੇਸ਼ ਕੀਤਾ ਹੈ!

ਓਵਰਫਿਟਿੰਗ ਨੂੰ ਰੋਕਣ ਦੇ ਕਈ ਤਰੀਕੇ ਹਨ:

* ਅਗੇ ਰੋਕਣਾ -- ਵੈਲੀਡੇਸ਼ਨ ਸੈਟ 'ਤੇ ਗਲਤੀ ਨੂੰ ਲਗਾਤਾਰ ਮਾਨਟਰ ਕਰਨਾ ਅਤੇ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਰੋਕਣਾ ਜਦੋਂ ਵੈਲੀਡੇਸ਼ਨ ਗਲਤੀ ਵਧਣ ਲੱਗਦੀ ਹੈ।
* Explicit Weight Decay / Regularization -- ਲੌਸ ਫੰਕਸ਼ਨ ਵਿੱਚ ਵਜ਼ਨਾਂ ਦੇ ਉੱਚ ਅਬਸੋਲਿਊਟ ਮੁੱਲਾਂ ਲਈ ਇੱਕ ਵਾਧੂ ਜੁਰਮਾਨਾ ਸ਼ਾਮਲ ਕਰਨਾ, ਜੋ ਮਾਡਲ ਨੂੰ ਬਹੁਤ ਅਸਥਿਰ ਨਤੀਜੇ ਪ੍ਰਾਪਤ ਕਰਨ ਤੋਂ ਰੋਕਦਾ ਹੈ।
* ਮਾਡਲ ਐਵਰੇਜਿੰਗ -- ਕਈ ਮਾਡਲਾਂ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ ਅਤੇ ਫਿਰ ਨਤੀਜੇ ਨੂੰ ਐਵਰੇਜ ਕਰਨਾ। ਇਹ ਵੈਰੀਅੰਸ ਨੂੰ ਘਟਾਉਣ ਵਿੱਚ ਮਦਦ ਕਰਦਾ ਹੈ।
* ਡ੍ਰੌਪਆਉਟ (Implicit Model Averaging)

## ਓਪਟੀਮਾਈਜ਼ਰ / ਟ੍ਰੇਨਿੰਗ ਐਲਗੋਰਿਥਮ

ਟ੍ਰੇਨਿੰਗ ਦਾ ਇੱਕ ਹੋਰ ਮਹੱਤਵਪੂਰਨ ਪਹਲੂ ਚੰਗੇ ਟ੍ਰੇਨਿੰਗ ਐਲਗੋਰਿਥਮ ਦੀ ਚੋਣ ਕਰਨਾ ਹੈ। ਜਦਕਿ ਕਲਾਸਿਕ **gradient descent** ਇੱਕ ਵਾਜਬ ਚੋਣ ਹੈ, ਇਹ ਕਈ ਵਾਰ ਬਹੁਤ ਹੌਲੀ ਹੋ ਸਕਦੀ ਹੈ, ਜਾਂ ਹੋਰ ਸਮੱਸਿਆਵਾਂ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦੀ ਹੈ।

ਡੀਪ ਲਰਨਿੰਗ ਵਿੱਚ, ਅਸੀਂ **Stochastic Gradient Descent** (SGD) ਵਰਤਦੇ ਹਾਂ, ਜੋ ਕਿ ਮਿਨੀਬੈਚਾਂ 'ਤੇ ਲਾਗੂ ਕੀਤਾ ਗਿਆ gradient descent ਹੈ, ਜੋ ਟ੍ਰੇਨਿੰਗ ਸੈਟ ਤੋਂ ਰੈਂਡਮ ਤਰੀਕੇ ਨਾਲ ਚੁਣੇ ਜਾਂਦੇ ਹਨ। ਵਜ਼ਨਾਂ ਨੂੰ ਇਸ ਫਾਰਮੂਲੇ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਕੂਲਿਤ ਕੀਤਾ ਜਾਂਦਾ ਹੈ:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### ਮੋਮੈਂਟਮ

**momentum SGD** ਵਿੱਚ, ਅਸੀਂ ਪਿਛਲੇ ਕਦਮਾਂ ਤੋਂ ਗ੍ਰੇਡੀਅੰਟ ਦਾ ਇੱਕ ਹਿੱਸਾ ਰੱਖਦੇ ਹਾਂ। ਇਹ ਇਸ ਤਰ੍ਹਾਂ ਹੈ ਜਿਵੇਂ ਅਸੀਂ ਜਦੋਂ ਕਿਸੇ ਦਿਸ਼ਾ ਵਿੱਚ ਜਾ ਰਹੇ ਹੁੰਦੇ ਹਾਂ ਅਤੇ ਸਾਨੂੰ ਇੱਕ ਵੱਖਰੀ ਦਿਸ਼ਾ ਵਿੱਚ ਝਟਕਾ ਮਿਲਦਾ ਹੈ, ਤਾਂ ਸਾਡੀ ਰਾਹਦਾਰੀ ਤੁਰੰਤ ਨਹੀਂ ਬਦਲਦੀ, ਪਰ ਅਸਲ ਮੂਵਮੈਂਟ ਦਾ ਕੁਝ ਹਿੱਸਾ ਰੱਖਦੀ ਹੈ। ਇੱਥੇ ਅਸੀਂ *speed* ਨੂੰ ਦਰਸਾਉਣ ਲਈ ਇੱਕ ਹੋਰ ਵੇਕਟਰ v ਪੇਸ਼ ਕਰਦੇ ਹਾਂ:

* v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
* w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

ਇੱਥੇ ਪੈਰਾਮੀਟਰ γ ਦਰਸਾਉਂਦਾ ਹੈ ਕਿ ਅਸੀਂ ਜੜਤ ਨੂੰ ਕਿੰਨਾ ਧਿਆਨ ਵਿੱਚ ਰੱਖਦੇ ਹਾਂ: γ=0 ਕਲਾਸਿਕ SGD ਦੇ ਸਮਾਨ ਹੈ; γ=1 ਇੱਕ ਸ਼ੁੱਧ ਮੋਸ਼ਨ ਸਮੀਕਰਨ ਹੈ।

### Adam, Adagrad, ਆਦਿ

ਜਦੋਂ ਹਰ ਲੇਅਰ ਵਿੱਚ ਅਸੀਂ ਸਿਗਨਲਾਂ ਨੂੰ ਕੁਝ ਮੈਟ੍ਰਿਕਸ W<sub>i</sub> ਨਾਲ ਗੁਣਾ ਕਰਦੇ ਹਾਂ, ||W<sub>i</sub>|| ਦੇ ਆਧਾਰ 'ਤੇ, ਗ੍ਰੇਡੀਅੰਟ ਜਾਂ ਤਾਂ ਘਟ ਸਕਦਾ ਹੈ ਅਤੇ 0 ਦੇ ਨੇੜੇ ਹੋ ਸਕਦਾ ਹੈ, ਜਾਂ ਬੇਹਦ ਵਧ ਸਕਦਾ ਹੈ। ਇਹ Exploding/Vanishing Gradients ਸਮੱਸਿਆ ਦਾ ਮੂਲ ਹੈ।

ਇਸ ਸਮੱਸਿਆ ਦਾ ਇੱਕ ਹੱਲ ਇਹ ਹੈ ਕਿ ਸਮੀਕਰਨ ਵਿੱਚ ਗ੍ਰੇਡੀਅੰਟ ਦੀ ਸਿਰਫ ਦਿਸ਼ਾ ਦੀ ਵਰਤੋਂ ਕੀਤੀ ਜਾਵੇ, ਅਤੇ ਅਬਸੋਲਿਊਟ ਮੁੱਲ ਨੂੰ ਨਜ਼ਰਅੰਦਾਜ਼ ਕੀਤਾ ਜਾਵੇ, ਜਿਵੇਂ:

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), ਜਿੱਥੇ ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

ਇਸ ਐਲਗੋਰਿਥਮ ਨੂੰ **Adagrad** ਕਿਹਾ ਜਾਂਦਾ ਹੈ। ਹੋਰ ਐਲਗੋਰਿਥਮ ਜੋ ਇਹੀ ਵਿਚਾਰ ਵਰਤਦੇ ਹਨ: **RMSProp**, **Adam**

> **Adam** ਨੂੰ ਕਈ ਐਪਲੀਕੇਸ਼ਨਾਂ ਲਈ ਬਹੁਤ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਐਲਗੋਰਿਥਮ ਮੰਨਿਆ ਜਾਂਦਾ ਹੈ, ਇਸ ਲਈ ਜੇ ਤੁਸੀਂ ਇਹ ਨਹੀਂ ਜਾਣਦੇ ਕਿ ਕਿਹੜਾ ਵਰਤਣਾ ਹੈ - Adam ਵਰਤੋ।

### Gradient clipping

Gradient clipping ਉਪਰੋਕਤ ਵਿਚਾਰ ਦਾ ਇੱਕ ਵਧਾਉ ਹੈ। ਜਦੋਂ ||∇ℒ|| ≤ θ ਹੁੰਦਾ ਹੈ, ਅਸੀਂ ਵਜ਼ਨ ਅਨੁਕੂਲਤਾ ਵਿੱਚ ਅਸਲ ਗ੍ਰੇਡੀਅੰਟ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖਦੇ ਹਾਂ, ਅਤੇ ਜਦੋਂ ||∇ℒ|| > θ ਹੁੰਦਾ ਹੈ - ਅਸੀਂ ਗ੍ਰੇਡੀਅੰਟ ਨੂੰ ਇਸਦੇ ਨਾਰਮ ਨਾਲ ਵੰਡ ਦਿੰਦੇ ਹਾਂ। ਇੱਥੇ θ ਇੱਕ ਪੈਰਾਮੀਟਰ ਹੈ, ਜਿਆਦਾਤਰ ਮਾਮਲਿਆਂ ਵਿੱਚ ਅਸੀਂ θ=1 ਜਾਂ θ=10 ਲੈ ਸਕਦੇ ਹਾਂ।

### Learning rate decay

ਟ੍ਰੇਨਿੰਗ ਦੀ ਸਫਲਤਾ ਅਕਸਰ ਲਰਨਿੰਗ ਰੇਟ ਪੈਰਾਮੀਟਰ η 'ਤੇ ਨਿਰਭਰ ਕਰਦੀ ਹੈ। ਇਹ ਤਰਕਸੰਗਤ ਹੈ ਕਿ η ਦੇ ਵੱਡੇ ਮੁੱਲ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਤੇਜ਼ ਕਰਦੇ ਹਨ, ਜੋ ਕਿ ਅਸੀਂ ਆਮ ਤੌਰ 'ਤੇ ਟ੍ਰੇਨਿੰਗ ਦੇ ਸ਼ੁਰੂ ਵਿੱਚ ਚਾਹੁੰਦੇ ਹਾਂ, ਅਤੇ ਫਿਰ η ਦੇ ਛੋਟੇ ਮੁੱਲ ਨੈਟਵਰਕ ਨੂੰ ਫਾਈਨ-ਟਿਊਨ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦੇ ਹਨ। ਇਸ ਲਈ, ਜਿਆਦਾਤਰ ਮਾਮਲਿਆਂ ਵਿੱਚ ਅਸੀਂ ਟ੍ਰੇਨਿੰਗ ਦੀ ਪ੍ਰਕਿਰਿਆ ਦੌਰਾਨ η ਨੂੰ ਘਟਾਉਣਾ ਚਾਹੁੰਦੇ ਹਾਂ।

ਇਹ ਟ੍ਰੇਨਿੰਗ ਦੇ ਹਰ ਇਪੋਕ ਦੇ ਬਾਅਦ η ਨੂੰ ਕੁਝ ਸੰਖਿਆ (ਜਿਵੇਂ 0.98) ਨਾਲ ਗੁਣਾ ਕਰਕੇ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ, ਜਾਂ ਹੋਰ ਜਟਿਲ **learning rate schedule** ਦੀ ਵਰਤੋਂ ਕਰਕੇ।

## ਵੱਖ-ਵੱਖ ਨੈਟਵਰਕ ਆਰਕੀਟੈਕਚਰ

ਤੁਹਾਡੇ ਸਮੱਸਿਆ ਲਈ ਸਹੀ ਨੈਟਵਰਕ ਆਰਕੀਟੈਕਚਰ ਦੀ ਚੋਣ ਕਰਨਾ ਮੁਸ਼ਕਲ ਹੋ ਸਕਦਾ ਹੈ। ਆਮ ਤੌਰ 'ਤੇ, ਅਸੀਂ ਇੱਕ ਆਰਕੀਟੈਕਚਰ ਲੈਂਦੇ ਹਾਂ ਜੋ ਸਾਡੇ ਵਿਸ਼ੇਸ਼ ਕੰਮ (ਜਾਂ ਇਸਦੇ ਸਮਾਨ) ਲਈ ਸਫਲ ਸਾਬਤ ਹੋਇਆ ਹੈ। ਇੱਥੇ ਇੱਕ [ਚੰਗਾ ਜਾਇਜ਼ਾ](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) ਹੈ ਕੰਪਿਊਟਰ ਵਿਜ਼ਨ ਲਈ

**ਅਸਵੀਕਰਤੀ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਣਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਇਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੌਜੂਦ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਪ੍ਰਮਾਣਿਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੇ ਪ੍ਰਯੋਗ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।