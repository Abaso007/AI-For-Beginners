{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਕਾਰਟਪੋਲ ਬੈਲੈਂਸਿੰਗ ਲਈ RL ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ\n",
    "\n",
    "ਇਹ ਨੋਟਬੁੱਕ [AI for Beginners Curriculum](http://aka.ms/ai-beginners) ਦਾ ਹਿੱਸਾ ਹੈ। ਇਹ [ਅਧਿਕਾਰਕ PyTorch ਟਿਊਟੋਰਿਅਲ](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) ਅਤੇ [ਇਹ Cartpole PyTorch implementation](https://github.com/yc930401/Actor-Critic-pytorch) ਤੋਂ ਪ੍ਰੇਰਿਤ ਹੈ।\n",
    "\n",
    "ਇਸ ਉਦਾਹਰਨ ਵਿੱਚ, ਅਸੀਂ RL ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ ਮਾਡਲ ਨੂੰ ਟ੍ਰੇਨ ਕਰਾਂਗੇ ਜੋ ਇੱਕ ਕਾਰਟ 'ਤੇ ਪੋਲ ਨੂੰ ਸੰਤੁਲਿਤ ਕਰ ਸਕੇ। ਇਹ ਕਾਰਟ ਹੋਰਿਜ਼ਾਂਟਲ ਸਕੇਲ 'ਤੇ ਖੱਬੇ ਅਤੇ ਸੱਜੇ ਦੋਵਾਂ ਪਾਸੇ ਹਿਲ ਸਕਦਾ ਹੈ। ਅਸੀਂ [OpenAI Gym](https://www.gymlibrary.ml/) ਵਾਤਾਵਰਣ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਪੋਲ ਦੀ ਨਕਲ ਕਰਾਂਗੇ।\n",
    "\n",
    "> **Note**: ਤੁਸੀਂ ਇਸ ਪਾਠ ਦਾ ਕੋਡ ਲੋਕਲ (ਜਿਵੇਂ ਕਿ Visual Studio Code ਤੋਂ) ਚਲਾ ਸਕਦੇ ਹੋ, ਜਿਸ ਵਿੱਚ ਸਿਮੂਲੇਸ਼ਨ ਇੱਕ ਨਵੀਂ ਵਿੰਡੋ ਵਿੱਚ ਖੁੱਲੇਗਾ। ਜਦੋਂ ਤੁਸੀਂ ਕੋਡ ਆਨਲਾਈਨ ਚਲਾਉਂਦੇ ਹੋ, ਤਾਂ ਤੁਹਾਨੂੰ ਕੋਡ ਵਿੱਚ ਕੁਝ ਬਦਲਾਅ ਕਰਨ ਦੀ ਲੋੜ ਹੋ ਸਕਦੀ ਹੈ, ਜਿਵੇਂ ਕਿ [ਇੱਥੇ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) ਵਰਣਨ ਕੀਤਾ ਗਿਆ ਹੈ।\n",
    "\n",
    "ਅਸੀਂ ਸ਼ੁਰੂ ਕਰਾਂਗੇ ਇਹ ਯਕੀਨੀ ਬਣਾਉਣ ਨਾਲ ਕਿ Gym ਇੰਸਟਾਲ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਚਲੋ ਹੁਣ CartPole ਵਾਤਾਵਰਣ ਬਣਾਈਏ ਅਤੇ ਦੇਖੀਏ ਕਿ ਇਸ 'ਤੇ ਕਿਵੇਂ ਕੰਮ ਕਰਨਾ ਹੈ। ਇੱਕ ਵਾਤਾਵਰਣ ਵਿੱਚ ਹੇਠਾਂ ਦਿੱਤੀਆਂ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਹੁੰਦੀਆਂ ਹਨ:\n",
    "\n",
    "* **ਐਕਸ਼ਨ ਸਪੇਸ** ਉਹ ਸੰਭਾਵਿਤ ਕਾਰਵਾਈਆਂ ਦਾ ਸੈੱਟ ਹੈ ਜੋ ਅਸੀਂ ਸਿਮੂਲੇਸ਼ਨ ਦੇ ਹਰ ਕਦਮ 'ਤੇ ਕਰ ਸਕਦੇ ਹਾਂ।\n",
    "* **ਅਬਜ਼ਰਵੇਸ਼ਨ ਸਪੇਸ** ਉਹ ਸਪੇਸ ਹੈ ਜਿਸ ਵਿੱਚ ਅਸੀਂ ਕੀਤੇ ਜਾ ਸਕਣ ਵਾਲੇ ਅਬਜ਼ਰਵੇਸ਼ਨ ਨੂੰ ਦੇਖ ਸਕਦੇ ਹਾਂ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਆਓ ਵੇਖੀਏ ਕਿ ਸਿਮੂਲੇਸ਼ਨ ਕਿਵੇਂ ਕੰਮ ਕਰਦੀ ਹੈ। ਹੇਠਾਂ ਦਿੱਤਾ ਲੂਪ ਸਿਮੂਲੇਸ਼ਨ ਚਲਾਉਂਦਾ ਹੈ, ਜਦ ਤੱਕ ਕਿ `env.step` ਟਰਮੀਨੇਸ਼ਨ ਫਲੈਗ `done` ਵਾਪਸ ਨਹੀਂ ਕਰਦਾ। ਅਸੀਂ `env.action_space.sample()` ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਰੈਂਡਮ ਤਰੀਕੇ ਨਾਲ ਐਕਸ਼ਨ ਚੁਣਾਂਗੇ, ਜਿਸਦਾ ਮਤਲਬ ਹੈ ਕਿ ਪ੍ਰਯੋਗਸ਼ਾਲਾ ਸ਼ਾਇਦ ਬਹੁਤ ਜਲਦੀ ਫੇਲ ਹੋ ਜਾਵੇਗੀ (CartPole ਵਾਤਾਵਰਣ ਉਸ ਸਮੇਂ ਖਤਮ ਹੋ ਜਾਂਦਾ ਹੈ ਜਦੋਂ CartPole ਦੀ ਗਤੀ, ਇਸ ਦੀ ਸਥਿਤੀ ਜਾਂ ਕੋਣ ਨਿਰਧਾਰਿਤ ਸੀਮਾਵਾਂ ਤੋਂ ਬਾਹਰ ਹੋ ਜਾਂਦੇ ਹਨ)।\n",
    "\n",
    "> ਸਿਮੂਲੇਸ਼ਨ ਨਵੀਂ ਵਿੰਡੋ ਵਿੱਚ ਖੁਲ੍ਹੇਗੀ। ਤੁਸੀਂ ਕੋਡ ਕਈ ਵਾਰ ਚਲਾ ਸਕਦੇ ਹੋ ਅਤੇ ਦੇਖ ਸਕਦੇ ਹੋ ਕਿ ਇਹ ਕਿਵੇਂ ਵਰਤਾਅ ਕਰਦੀ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਤੁਸੀਂ ਨੋਟ ਕਰ ਸਕਦੇ ਹੋ ਕਿ ਅਬਜ਼ਰਵੇਸ਼ਨਜ਼ ਵਿੱਚ 4 ਨੰਬਰ ਸ਼ਾਮਲ ਹਨ। ਇਹ ਹਨ:  \n",
    "- ਕਾਰਟ ਦੀ ਸਥਿਤੀ  \n",
    "- ਕਾਰਟ ਦੀ ਗਤੀ  \n",
    "- ਪੋਲ ਦਾ ਕੋਣ  \n",
    "- ਪੋਲ ਦੀ ਘੁੰਮਣ ਦੀ ਦਰ  \n",
    "\n",
    "`rew` ਉਹ ਇਨਾਮ ਹੈ ਜੋ ਸਾਨੂੰ ਹਰ ਕਦਮ 'ਤੇ ਮਿਲਦਾ ਹੈ। ਤੁਸੀਂ ਦੇਖ ਸਕਦੇ ਹੋ ਕਿ CartPole ਵਾਤਾਵਰਣ ਵਿੱਚ ਹਰ ਸਿਮੂਲੇਸ਼ਨ ਕਦਮ ਲਈ ਤੁਹਾਨੂੰ 1 ਅੰਕ ਦਾ ਇਨਾਮ ਮਿਲਦਾ ਹੈ, ਅਤੇ ਉਦੇਸ਼ ਕੁੱਲ ਇਨਾਮ ਨੂੰ ਵਧਾਉਣਾ ਹੈ, ਅਰਥਾਤ CartPole ਨੂੰ ਬਿਨਾਂ ਡਿੱਗੇ ਸੰਤੁਲਿਤ ਰੱਖਣ ਦਾ ਸਮਾਂ ਵਧਾਉਣਾ।  \n",
    "\n",
    "ਰੀਇਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ ਦੌਰਾਨ, ਸਾਡਾ ਉਦੇਸ਼ ਇੱਕ **ਪਾਲਸੀ** $\\pi$ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ ਹੈ, ਜੋ ਹਰ ਰਾਜ $s$ ਲਈ ਸਾਨੂੰ ਦੱਸੇਗੀ ਕਿ ਕਿਹੜੀ ਕਾਰਵਾਈ $a$ ਕਰਨੀ ਹੈ, ਇਸ ਲਈ ਅਸਲ ਵਿੱਚ $a = \\pi(s)$।  \n",
    "\n",
    "ਜੇ ਤੁਸੀਂ ਸੰਭਾਵਨਾਤਮਕ ਹੱਲ ਚਾਹੁੰਦੇ ਹੋ, ਤਾਂ ਤੁਸੀਂ ਪਾਲਸੀ ਨੂੰ ਹਰ ਕਾਰਵਾਈ ਲਈ ਸੰਭਾਵਨਾਵਾਂ ਦੇ ਸੈੱਟ ਵਾਪਸ ਕਰਨ ਵਜੋਂ ਸੋਚ ਸਕਦੇ ਹੋ, ਅਰਥਾਤ $\\pi(a|s)$ ਦਾ ਮਤਲਬ ਹੋਵੇਗਾ ਕਿ ਰਾਜ $s$ 'ਤੇ ਕਾਰਵਾਈ $a$ ਕਰਨ ਦੀ ਸੰਭਾਵਨਾ ਕੀ ਹੈ।  \n",
    "\n",
    "## ਪਾਲਸੀ ਗ੍ਰੇਡੀਅੰਟ ਮੈਥਡ  \n",
    "\n",
    "ਸਭ ਤੋਂ ਸਧਾਰਣ RL ਐਲਗੋਰਿਦਮ, ਜਿਸਨੂੰ **ਪਾਲਸੀ ਗ੍ਰੇਡੀਅੰਟ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ, ਵਿੱਚ ਅਸੀਂ ਇੱਕ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਨੂੰ ਅਗਲੀ ਕਾਰਵਾਈ ਦੀ ਪੇਸ਼ਗੋਈ ਕਰਨ ਲਈ ਟ੍ਰੇਨ ਕਰਾਂਗੇ।  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਬਹੁਤ ਸਾਰੇ ਪ੍ਰਯੋਗ ਚਲਾ ਕੇ ਅਤੇ ਹਰ ਰਨ ਤੋਂ ਬਾਅਦ ਆਪਣੇ ਨੈਟਵਰਕ ਨੂੰ ਅਪਡੇਟ ਕਰਕੇ ਨੈਟਵਰਕ ਨੂੰ ਟ੍ਰੇਨ ਕਰਾਂਗੇ। ਆਓ ਇੱਕ ਫੰਕਸ਼ਨ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰੀਏ ਜੋ ਪ੍ਰਯੋਗ ਚਲਾਏਗਾ ਅਤੇ ਨਤੀਜੇ (ਜਿਨ੍ਹਾਂ ਨੂੰ **ਟ੍ਰੇਸ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ) - ਸਾਰੇ ਸਥਿਤੀਆਂ, ਕਾਰਵਾਈਆਂ (ਅਤੇ ਉਨ੍ਹਾਂ ਦੀਆਂ ਸਿਫਾਰਸ਼ੀ ਸੰਭਾਵਨਾਵਾਂ), ਅਤੇ ਇਨਾਮ ਵਾਪਸ ਕਰੇਗਾ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਤੁਸੀਂ ਬਿਨਾਂ ਸਿਖਲਾਈ ਹੋਈ ਨੈਟਵਰਕ ਨਾਲ ਇੱਕ ਐਪੀਸੋਡ ਚਲਾ ਸਕਦੇ ਹੋ ਅਤੇ ਦੇਖ ਸਕਦੇ ਹੋ ਕਿ ਕੁੱਲ ਇਨਾਮ (ਜਿਸਨੂੰ ਐਪੀਸੋਡ ਦੀ ਲੰਬਾਈ ਵੀ ਕਿਹਾ ਜਾਂਦਾ ਹੈ) ਬਹੁਤ ਘੱਟ ਹੈ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਪਾਲਿਸੀ ਗ੍ਰੇਡੀਅੰਟ ਐਲਗੋਰਿਥਮ ਦੇ ਮੁਸ਼ਕਲ ਪਹਲੂਆਂ ਵਿੱਚੋਂ ਇੱਕ **ਛੂਟ ਵਾਲੇ ਇਨਾਮ** ਦੀ ਵਰਤੋਂ ਕਰਨਾ ਹੈ। ਵਿਚਾਰ ਇਹ ਹੈ ਕਿ ਅਸੀਂ ਖੇਡ ਦੇ ਹਰ ਕਦਮ 'ਤੇ ਕੁੱਲ ਇਨਾਮਾਂ ਦੇ ਵੇਕਟਰ ਦੀ ਗਣਨਾ ਕਰਦੇ ਹਾਂ, ਅਤੇ ਇਸ ਪ੍ਰਕਿਰਿਆ ਦੌਰਾਨ ਅਸੀਂ ਕੁਝ ਗੁਣਕ $gamma$ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਸ਼ੁਰੂਆਤੀ ਇਨਾਮਾਂ ਨੂੰ ਛੂਟ ਦਿੰਦੇ ਹਾਂ। ਅਸੀਂ resulting ਵੇਕਟਰ ਨੂੰ ਸਧਾਰਨ ਕਰਦੇ ਹਾਂ, ਕਿਉਂਕਿ ਅਸੀਂ ਇਸਨੂੰ ਆਪਣੇ ਟ੍ਰੇਨਿੰਗ ਨੂੰ ਪ੍ਰਭਾਵਿਤ ਕਰਨ ਲਈ ਵਜ਼ਨ ਵਜੋਂ ਵਰਤਣ ਵਾਲੇ ਹਾਂ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਚਲੋ ਹੁਣ ਅਸਲ ਟ੍ਰੇਨਿੰਗ ਕਰਦੇ ਹਾਂ! ਅਸੀਂ 300 ਐਪੀਸੋਡ ਚਲਾਉਣਗੇ, ਅਤੇ ਹਰ ਐਪੀਸੋਡ ਵਿੱਚ ਅਸੀਂ ਇਹ ਕਰਾਂਗੇ:\n",
    "\n",
    "1. ਪ੍ਰਯੋਗ ਚਲਾਓ ਅਤੇ ਟ੍ਰੇਸ ਇਕੱਠਾ ਕਰੋ।\n",
    "1. ਕੀਤੇ ਗਏ ਐਕਸ਼ਨ ਅਤੇ ਅਨੁਮਾਨਿਤ ਸੰਭਾਵਨਾਵਾਂ ਦੇ ਵਿਚਕਾਰ ਅੰਤਰ (`gradients`) ਦੀ ਗਣਨਾ ਕਰੋ। ਜੇ ਅੰਤਰ ਘੱਟ ਹੈ, ਤਾਂ ਇਹ ਸਬੂਤ ਹੈ ਕਿ ਅਸੀਂ ਸਹੀ ਐਕਸ਼ਨ ਲਿਆ ਹੈ।\n",
    "1. ਡਿਸਕਾਊਂਟ ਕੀਤੇ ਗਏ ਇਨਾਮਾਂ ਦੀ ਗਣਨਾ ਕਰੋ ਅਤੇ gradients ਨੂੰ ਡਿਸਕਾਊਂਟ ਕੀਤੇ ਇਨਾਮਾਂ ਨਾਲ ਗੁਣਾ ਕਰੋ - ਇਹ ਯਕੀਨੀ ਬਣਾਏਗਾ ਕਿ ਉੱਚ ਇਨਾਮ ਵਾਲੇ ਕਦਮਾਂ ਦਾ ਅੰਤਮ ਨਤੀਜੇ 'ਤੇ ਵਧੇਰੇ ਪ੍ਰਭਾਵ ਪਵੇਗਾ ਬਜਾਏ ਘੱਟ ਇਨਾਮ ਵਾਲਿਆਂ ਦੇ।\n",
    "1. ਸਾਡੇ ਨਿਊਰਲ ਨੈਟਵਰਕ ਲਈ ਉਮੀਦ ਕੀਤੇ ਟਾਰਗਟ ਐਕਸ਼ਨ ਕੁਝ ਹੱਦ ਤੱਕ ਦੌਰਾਨ ਅਨੁਮਾਨਿਤ ਸੰਭਾਵਨਾਵਾਂ ਤੋਂ ਲਏ ਜਾਣਗੇ, ਅਤੇ ਕੁਝ ਹਿੱਸਾ ਗਣਨਾ ਕੀਤੇ ਗਏ gradients ਤੋਂ। ਅਸੀਂ `alpha` ਪੈਰਾਮੀਟਰ ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ ਇਹ ਨਿਰਧਾਰਤ ਕਰਨ ਲਈ ਕਿ gradients ਅਤੇ rewards ਨੂੰ ਕਿੰਨੀ ਹੱਦ ਤੱਕ ਧਿਆਨ ਵਿੱਚ ਲਿਆ ਜਾਵੇ - ਇਸਨੂੰ reinforcement algorithm ਦੀ *learning rate* ਕਿਹਾ ਜਾਂਦਾ ਹੈ।\n",
    "1. ਆਖਿਰਕਾਰ, ਅਸੀਂ ਸਾਡੇ ਨੈਟਵਰਕ ਨੂੰ states ਅਤੇ ਉਮੀਦ ਕੀਤੇ ਐਕਸ਼ਨ 'ਤੇ ਟ੍ਰੇਨ ਕਰਦੇ ਹਾਂ, ਅਤੇ ਇਸ ਪ੍ਰਕਿਰਿਆ ਨੂੰ ਦੁਹਰਾਉਂਦੇ ਹਾਂ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਚਲੋ ਐਪੀਸੋਡ ਨੂੰ ਰੈਂਡਰਿੰਗ ਨਾਲ ਚਲਾਈਏ ਤਾਂ ਜੋ ਨਤੀਜਾ ਦੇਖਿਆ ਜਾ ਸਕੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੋਪਫੁਲੀ, ਹੁਣ ਤੁਸੀਂ ਦੇਖ ਸਕਦੇ ਹੋ ਕਿ ਪੋਲ ਹੁਣ ਕਾਫ਼ੀ ਚੰਗੀ ਤਰ੍ਹਾਂ ਸੰਤੁਲਿਤ ਹੋ ਸਕਦਾ ਹੈ!\n",
    "\n",
    "## ਐਕਟਰ-ਕ੍ਰਿਟਿਕ ਮਾਡਲ\n",
    "\n",
    "ਐਕਟਰ-ਕ੍ਰਿਟਿਕ ਮਾਡਲ ਪਾਲਿਸੀ ਗ੍ਰੇਡੀਅੰਟਸ ਦਾ ਅੱਗੇ ਦਾ ਵਿਕਾਸ ਹੈ, ਜਿਸ ਵਿੱਚ ਅਸੀਂ ਇੱਕ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਬਣਾਉਂਦੇ ਹਾਂ ਜੋ ਪਾਲਿਸੀ ਅਤੇ ਅਨੁਮਾਨਿਤ ਇਨਾਮ ਦੋਵਾਂ ਨੂੰ ਸਿੱਖਦਾ ਹੈ। ਨੈੱਟਵਰਕ ਦੇ ਦੋ ਆਉਟਪੁਟ ਹੋਣਗੇ (ਜਾਂ ਤੁਸੀਂ ਇਸਨੂੰ ਦੋ ਵੱਖਰੇ ਨੈੱਟਵਰਕ ਵਜੋਂ ਦੇਖ ਸਕਦੇ ਹੋ):\n",
    "* **ਐਕਟਰ** ਸਾਨੂੰ ਸਟੇਟ ਪ੍ਰੋਬੈਬਿਲਿਟੀ ਡਿਸਟ੍ਰੀਬਿਊਸ਼ਨ ਦੇ ਕੇ ਕਾਰਵਾਈ ਦੀ ਸਿਫਾਰਸ਼ ਕਰੇਗਾ, ਜਿਵੇਂ ਕਿ ਪਾਲਿਸੀ ਗ੍ਰੇਡੀਅੰਟ ਮਾਡਲ ਵਿੱਚ।\n",
    "* **ਕ੍ਰਿਟਿਕ** ਅਨੁਮਾਨ ਲਗਾਏਗਾ ਕਿ ਉਹਨਾਂ ਕਾਰਵਾਈਆਂ ਤੋਂ ਇਨਾਮ ਕੀ ਹੋ ਸਕਦਾ ਹੈ। ਇਹ ਦਿੱਤੇ ਗਏ ਸਟੇਟ 'ਤੇ ਭਵਿੱਖ ਵਿੱਚ ਕੁੱਲ ਅਨੁਮਾਨਿਤ ਇਨਾਮ ਵਾਪਸ ਕਰਦਾ ਹੈ।\n",
    "\n",
    "ਆਓ ਅਜਿਹਾ ਮਾਡਲ ਪਰਿਭਾਸ਼ਿਤ ਕਰੀਏ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਅਸੀਂ ਆਪਣੀਆਂ `discounted_rewards` ਅਤੇ `run_episode` ਫੰਕਸ਼ਨਾਂ ਨੂੰ ਥੋੜ੍ਹਾ ਸੋਧਣ ਦੀ ਲੋੜ ਹੋਵੇਗੀ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਹੁਣ ਅਸੀਂ ਮੁੱਖ ਟ੍ਰੇਨਿੰਗ ਲੂਪ ਚਲਾਉਣਗੇ। ਅਸੀਂ ਸਹੀ ਨੁਕਸਾਨ ਫੰਕਸ਼ਨਾਂ ਦੀ ਗਣਨਾ ਕਰਕੇ ਅਤੇ ਨੈਟਵਰਕ ਪੈਰਾਮੀਟਰਾਂ ਨੂੰ ਅਪਡੇਟ ਕਰਕੇ ਮੈਨੁਅਲ ਨੈਟਵਰਕ ਟ੍ਰੇਨਿੰਗ ਪ੍ਰਕਿਰਿਆ ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮੁੱਖ ਗੱਲ\n",
    "\n",
    "ਅਸੀਂ ਇਸ ਡੈਮੋ ਵਿੱਚ ਦੋ RL ਐਲਗੋਰਿਦਮ ਦੇਖੇ ਹਨ: ਸਧਾਰਣ ਪਾਲਸੀ ਗ੍ਰੇਡੀਅੰਟ ਅਤੇ ਹੋਰ ਉੱਨਤ ਐਕਟਰ-ਕ੍ਰਿਟਿਕ। ਤੁਸੀਂ ਦੇਖ ਸਕਦੇ ਹੋ ਕਿ ਇਹ ਐਲਗੋਰਿਦਮ ਸਟੇਟ, ਐਕਸ਼ਨ ਅਤੇ ਰਿਵਾਰਡ ਦੇ ਅਬਸਟਰੈਕਟ ਧਾਰਨਾਵਾਂ ਨਾਲ ਕੰਮ ਕਰਦੇ ਹਨ - ਇਸ ਲਈ ਇਹ ਵੱਖ-ਵੱਖ ਤਰ੍ਹਾਂ ਦੇ ਵਾਤਾਵਰਣਾਂ ਵਿੱਚ ਲਾਗੂ ਕੀਤੇ ਜਾ ਸਕਦੇ ਹਨ।\n",
    "\n",
    "ਰਿਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ ਸਾਨੂੰ ਸਿਰਫ ਅੰਤਿਮ ਰਿਵਾਰਡ ਨੂੰ ਦੇਖ ਕੇ ਸਮੱਸਿਆ ਦਾ ਸਭ ਤੋਂ ਵਧੀਆ ਹੱਲ ਸਿੱਖਣ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ। ਇਹ ਗੱਲ ਕਿ ਸਾਨੂੰ ਲੇਬਲ ਕੀਤੇ ਡੇਟਾਸੈਟ ਦੀ ਲੋੜ ਨਹੀਂ ਹੁੰਦੀ, ਸਾਨੂੰ ਆਪਣੇ ਮਾਡਲਾਂ ਨੂੰ ਅਨੁਕੂਲਿਤ ਕਰਨ ਲਈ ਸਿਮੂਲੇਸ਼ਨ ਨੂੰ ਕਈ ਵਾਰ ਦੁਹਰਾਉਣ ਦੀ ਆਗਿਆ ਦਿੰਦੀ ਹੈ। ਹਾਲਾਂਕਿ, RL ਵਿੱਚ ਅਜੇ ਵੀ ਕਈ ਚੁਣੌਤੀਆਂ ਹਨ, ਜਿਨ੍ਹਾਂ ਬਾਰੇ ਤੁਸੀਂ ਸਿੱਖ ਸਕਦੇ ਹੋ ਜੇਕਰ ਤੁਸੀਂ AI ਦੇ ਇਸ ਦਿਲਚਸਪ ਖੇਤਰ 'ਤੇ ਹੋਰ ਧਿਆਨ ਕੇਂਦਰਿਤ ਕਰਨ ਦਾ ਫੈਸਲਾ ਕਰਦੇ ਹੋ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਰਤਾ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਹਾਲਾਂਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚਨਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤ ਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T07:34:09+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}