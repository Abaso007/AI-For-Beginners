<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T07:27:11+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "pa"
}
-->
# ਡੀਪ ਰੀਇਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ

ਰੀਇਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ (RL) ਨੂੰ ਸਿੱਖਿਆ ਪ੍ਰਾਪਤ ਕਰਨ ਦੇ ਤਿੰਨ ਮੁੱਖ ਪੈਰਾਗ੍ਰਾਮਾਂ ਵਿੱਚੋਂ ਇੱਕ ਮੰਨਿਆ ਜਾਂਦਾ ਹੈ, ਜਿਵੇਂ ਕਿ ਸਪਰਵਾਈਜ਼ਡ ਲਰਨਿੰਗ ਅਤੇ ਅਨਸਪਰਵਾਈਜ਼ਡ ਲਰਨਿੰਗ। ਜਦੋਂ ਸਪਰਵਾਈਜ਼ਡ ਲਰਨਿੰਗ ਵਿੱਚ ਅਸੀਂ ਜਾਣੇ-ਪਛਾਣੇ ਨਤੀਜਿਆਂ ਵਾਲੇ ਡਾਟਾਸੈਟ 'ਤੇ ਨਿਰਭਰ ਕਰਦੇ ਹਾਂ, RL **ਕਰ ਕੇ ਸਿੱਖਣ** 'ਤੇ ਅਧਾਰਿਤ ਹੈ। ਉਦਾਹਰਣ ਲਈ, ਜਦੋਂ ਅਸੀਂ ਪਹਿਲੀ ਵਾਰ ਕੋਈ ਕੰਪਿਊਟਰ ਗੇਮ ਦੇਖਦੇ ਹਾਂ, ਅਸੀਂ ਖੇਡਣਾ ਸ਼ੁਰੂ ਕਰਦੇ ਹਾਂ, ਭਾਵੇਂ ਸਾਨੂੰ ਨਿਯਮਾਂ ਦਾ ਪਤਾ ਨਹੀਂ ਹੁੰਦਾ, ਅਤੇ ਜਲਦੀ ਹੀ ਅਸੀਂ ਸਿਰਫ ਖੇਡਣ ਅਤੇ ਆਪਣੇ ਵਿਹਾਰ ਨੂੰ ਢਾਲ ਕੇ ਆਪਣੀਆਂ ਕੁਸ਼ਲਤਾਵਾਂ ਵਿੱਚ ਸੁਧਾਰ ਕਰਨ ਦੇ ਯੋਗ ਹੋ ਜਾਂਦੇ ਹਾਂ।

## [ਪ੍ਰੀ-ਲੈਕਚਰ ਕਵਿਜ਼](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL ਕਰਨ ਲਈ ਸਾਨੂੰ ਲੋੜ ਹੈ:

* ਇੱਕ **ਮਾਹੌਲ** ਜਾਂ **ਸਿਮੂਲੇਟਰ**, ਜੋ ਗੇਮ ਦੇ ਨਿਯਮ ਸਥਾਪਿਤ ਕਰਦਾ ਹੈ। ਸਾਨੂੰ ਸਿਮੂਲੇਟਰ ਵਿੱਚ ਪ੍ਰਯੋਗ ਕਰਨ ਅਤੇ ਨਤੀਜੇ ਦੇਖਣ ਦੇ ਯੋਗ ਹੋਣਾ ਚਾਹੀਦਾ ਹੈ।
* ਕੁਝ **ਰਿਵਾਰਡ ਫੰਕਸ਼ਨ**, ਜੋ ਦਰਸਾਉਂਦਾ ਹੈ ਕਿ ਸਾਡਾ ਪ੍ਰਯੋਗ ਕਿੰਨਾ ਸਫਲ ਰਿਹਾ। ਜੇਕਰ ਕੰਪਿਊਟਰ ਗੇਮ ਖੇਡਣ ਦੀ ਸਿੱਖਿਆ ਲਈ, ਰਿਵਾਰਡ ਸਾਡਾ ਅੰਤਮ ਸਕੋਰ ਹੋਵੇਗਾ।

ਰਿਵਾਰਡ ਫੰਕਸ਼ਨ ਦੇ ਆਧਾਰ 'ਤੇ, ਸਾਨੂੰ ਆਪਣੇ ਵਿਹਾਰ ਨੂੰ ਢਾਲਣ ਅਤੇ ਆਪਣੀਆਂ ਕੁਸ਼ਲਤਾਵਾਂ ਵਿੱਚ ਸੁਧਾਰ ਕਰਨ ਦੇ ਯੋਗ ਹੋਣਾ ਚਾਹੀਦਾ ਹੈ, ਤਾਂ ਜੋ ਅਗਲੀ ਵਾਰ ਅਸੀਂ ਬਿਹਤਰ ਖੇਡ ਸਕੀਏ। ਹੋਰ ਕਿਸਮਾਂ ਦੇ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਅਤੇ RL ਵਿੱਚ ਮੁੱਖ ਅੰਤਰ ਇਹ ਹੈ ਕਿ RL ਵਿੱਚ ਅਸੀਂ ਆਮ ਤੌਰ 'ਤੇ ਇਹ ਨਹੀਂ ਜਾਣਦੇ ਕਿ ਅਸੀਂ ਜਿੱਤੇ ਜਾਂ ਹਾਰੇ ਜਦ ਤੱਕ ਗੇਮ ਖਤਮ ਨਹੀਂ ਹੁੰਦੀ। ਇਸ ਲਈ, ਅਸੀਂ ਇਹ ਨਹੀਂ ਕਹਿ ਸਕਦੇ ਕਿ ਕੋਈ ਖਾਸ ਚਾਲ ਚੰਗੀ ਹੈ ਜਾਂ ਨਹੀਂ - ਸਾਨੂੰ ਸਿਰਫ ਗੇਮ ਦੇ ਅੰਤ ਵਿੱਚ ਰਿਵਾਰਡ ਮਿਲਦਾ ਹੈ।

RL ਦੌਰਾਨ, ਅਸੀਂ ਆਮ ਤੌਰ 'ਤੇ ਕਈ ਪ੍ਰਯੋਗ ਕਰਦੇ ਹਾਂ। ਹਰ ਪ੍ਰਯੋਗ ਦੌਰਾਨ, ਸਾਨੂੰ ਆਪਣੀ ਸਿੱਖੀ ਗਈ ਸਰਵੋਤਮ ਰਣਨੀਤੀ ਦੀ ਪਾਲਣਾ ਕਰਨ (**ਸ਼ੋਸ਼ਣ**) ਅਤੇ ਨਵੇਂ ਸੰਭਾਵਿਤ ਸਥਿਤੀਆਂ ਦੀ ਖੋਜ ਕਰਨ (**ਖੋਜ**) ਵਿੱਚ ਸੰਤੁਲਨ ਬਣਾਉਣਾ ਪੈਂਦਾ ਹੈ।

## OpenAI Gym

RL ਲਈ ਇੱਕ ਸ਼ਾਨਦਾਰ ਟੂਲ [OpenAI Gym](https://gym.openai.com/) ਹੈ - ਇੱਕ **ਸਿਮੂਲੇਸ਼ਨ ਮਾਹੌਲ**, ਜੋ ਕਈ ਵੱਖ-ਵੱਖ ਮਾਹੌਲਾਂ ਨੂੰ ਸਿਮੂਲੇਟ ਕਰ ਸਕਦਾ ਹੈ, ਜਿਵੇਂ ਕਿ Atari ਗੇਮਾਂ ਤੋਂ ਲੈ ਕੇ ਪੋਲ ਬੈਲੈਂਸਿੰਗ ਦੇ ਭੌਤਿਕ ਵਿਗਿਆਨ ਤੱਕ। ਇਹ ਰੀਇਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ ਐਲਗੋਰਿਦਮਾਂ ਨੂੰ ਟ੍ਰੇਨ ਕਰਨ ਲਈ ਸਭ ਤੋਂ ਪ੍ਰਸਿੱਧ ਸਿਮੂਲੇਸ਼ਨ ਮਾਹੌਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਹੈ ਅਤੇ ਇਸਨੂੰ [OpenAI](https://openai.com/) ਦੁਆਰਾ ਸੰਭਾਲਿਆ ਜਾਂਦਾ ਹੈ।

> **Note**: ਤੁਸੀਂ OpenAI Gym ਤੋਂ ਉਪਲਬਧ ਸਾਰੇ ਮਾਹੌਲ [ਇੱਥੇ](https://gym.openai.com/envs/#classic_control) ਦੇਖ ਸਕਦੇ ਹੋ।

## CartPole Balancing

ਤੁਹਾਨੂੰ ਸ਼ਾਇਦ ਸਾਰੇ ਆਧੁਨਿਕ ਬੈਲੈਂਸਿੰਗ ਡਿਵਾਈਸਾਂ ਜਿਵੇਂ ਕਿ *Segway* ਜਾਂ *Gyroscooters* ਦੇਖੇ ਹੋਣਗੇ। ਇਹ ਆਪਣੇ ਪਹੀਏ ਨੂੰ ਐਕਸਲੇਰੋਮੀਟਰ ਜਾਂ ਗਾਇਰੋਸਕੋਪ ਤੋਂ ਸਿਗਨਲ ਦੇ ਜਵਾਬ ਵਿੱਚ ਢਾਲ ਕੇ ਆਪਣੇ ਆਪ ਨੂੰ ਬੈਲੈਂਸ ਕਰਨ ਦੇ ਯੋਗ ਹੁੰਦੇ ਹਨ। ਇਸ ਭਾਗ ਵਿੱਚ, ਅਸੀਂ ਇੱਕ ਸਮਾਨ ਸਮੱਸਿਆ ਨੂੰ ਹੱਲ ਕਰਨ ਦੀ ਸਿੱਖਿਆ ਲਵਾਂਗੇ - ਪੋਲ ਨੂੰ ਬੈਲੈਂਸ ਕਰਨਾ। ਇਹ ਉਸ ਸਥਿਤੀ ਵਰਗਾ ਹੈ ਜਦੋਂ ਇੱਕ ਸਰਕਸ ਪਰਫਾਰਮਰ ਨੂੰ ਆਪਣੇ ਹੱਥ 'ਤੇ ਪੋਲ ਬੈਲੈਂਸ ਕਰਨਾ ਪੈਂਦਾ ਹੈ - ਪਰ ਇਹ ਪੋਲ ਬੈਲੈਂਸਿੰਗ ਸਿਰਫ 1D ਵਿੱਚ ਹੁੰਦੀ ਹੈ।

ਬੈਲੈਂਸਿੰਗ ਦਾ ਇੱਕ ਸਰਲ ਵਰਜਨ **CartPole** ਸਮੱਸਿਆ ਦੇ ਨਾਮ ਨਾਲ ਜਾਣਿਆ ਜਾਂਦਾ ਹੈ। CartPole ਦੁਨੀਆ ਵਿੱਚ, ਸਾਡੇ ਕੋਲ ਇੱਕ ਹੋਰਿਜ਼ਾਂਟਲ ਸਲਾਈਡਰ ਹੁੰਦਾ ਹੈ ਜੋ ਖੱਬੇ ਜਾਂ ਸੱਜੇ ਵੱਲ ਹਿਲ ਸਕਦਾ ਹੈ, ਅਤੇ ਉਦੇਸ਼ ਸਲਾਈਡਰ ਦੇ ਉੱਪਰ ਇੱਕ ਵਰਟਿਕਲ ਪੋਲ ਨੂੰ ਬੈਲੈਂਸ ਕਰਨਾ ਹੁੰਦਾ ਹੈ।

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

ਇਸ ਮਾਹੌਲ ਨੂੰ ਬਣਾਉਣ ਅਤੇ ਵਰਤਣ ਲਈ, ਸਾਨੂੰ ਕੁਝ ਪਾਈਥਨ ਕੋਡ ਦੀ ਲਾਈਨਾਂ ਦੀ ਲੋੜ ਹੈ:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

ਹਰ ਮਾਹੌਲ ਨੂੰ ਬਿਲਕੁਲ ਇੱਕੋ ਤਰੀਕੇ ਨਾਲ ਐਕਸੈਸ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ:
* `env.reset` ਇੱਕ ਨਵਾਂ ਪ੍ਰਯੋਗ ਸ਼ੁਰੂ ਕਰਦਾ ਹੈ
* `env.step` ਇੱਕ ਸਿਮੂਲੇਸ਼ਨ ਸਟੈਪ ਕਰਦਾ ਹੈ। ਇਹ **ਐਕਸ਼ਨ** ਨੂੰ **ਐਕਸ਼ਨ ਸਪੇਸ** ਤੋਂ ਪ੍ਰਾਪਤ ਕਰਦਾ ਹੈ, ਅਤੇ **ਅਬਜ਼ਰਵੇਸ਼ਨ** (ਅਬਜ਼ਰਵੇਸ਼ਨ ਸਪੇਸ ਤੋਂ), ਰਿਵਾਰਡ ਅਤੇ ਟਰਮੀਨੇਸ਼ਨ ਫਲੈਗ ਵਾਪਸ ਕਰਦਾ ਹੈ।

ਉਪਰੋਕਤ ਉਦਾਹਰਣ ਵਿੱਚ ਅਸੀਂ ਹਰ ਸਟੈਪ 'ਤੇ ਇੱਕ ਰੈਂਡਮ ਐਕਸ਼ਨ ਕਰਦੇ ਹਾਂ, ਜਿਸ ਕਰਕੇ ਪ੍ਰਯੋਗ ਦੀ ਉਮਰ ਬਹੁਤ ਛੋਟੀ ਹੁੰਦੀ ਹੈ:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL ਐਲਗੋਰਿਦਮ ਦਾ ਉਦੇਸ਼ ਇੱਕ ਮਾਡਲ - ਜਿਸਨੂੰ **ਪਾਲਿਸੀ** &pi; ਕਿਹਾ ਜਾਂਦਾ ਹੈ - ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ ਹੈ, ਜੋ ਦਿੱਤੀ ਸਥਿਤੀ ਦੇ ਜਵਾਬ ਵਿੱਚ ਐਕਸ਼ਨ ਵਾਪਸ ਕਰੇਗਾ। ਅਸੀਂ ਪਾਲਿਸੀ ਨੂੰ ਸੰਭਾਵਨਾਤਮਕ ਵੀ ਮੰਨ ਸਕਦੇ ਹਾਂ, ਉਦਾਹਰਣ ਲਈ, ਕਿਸੇ ਵੀ ਸਥਿਤੀ *s* ਅਤੇ ਐਕਸ਼ਨ *a* ਲਈ ਇਹ ਸੰਭਾਵਨਾ &pi;(*a*|*s*) ਵਾਪਸ ਕਰੇਗਾ ਕਿ ਸਥਿਤੀ *s* ਵਿੱਚ ਸਾਨੂੰ *a* ਲੈਣਾ ਚਾਹੀਦਾ ਹੈ।

## ਪਾਲਿਸੀ ਗ੍ਰੇਡੀਅੰਟਸ ਐਲਗੋਰਿਦਮ

ਪਾਲਿਸੀ ਨੂੰ ਮਾਡਲ ਕਰਨ ਦਾ ਸਭ ਤੋਂ ਸਪਸ਼ਟ ਤਰੀਕਾ ਇੱਕ ਨਿਊਰਲ ਨੈਟਵਰਕ ਬਣਾਉਣਾ ਹੈ ਜੋ ਸਥਿਤੀਆਂ ਨੂੰ ਇਨਪੁਟ ਵਜੋਂ ਲਵੇਗਾ ਅਤੇ ਸੰਬੰਧਿਤ ਐਕਸ਼ਨ (ਜਾਂ ਸਾਰੇ ਐਕਸ਼ਨਾਂ ਦੀਆਂ ਸੰਭਾਵਨਾਵਾਂ) ਵਾਪਸ ਕਰੇਗਾ। ਇੱਕ ਅਰਥ ਵਿੱਚ, ਇਹ ਇੱਕ ਆਮ ਕਲਾਸੀਫਿਕੇਸ਼ਨ ਟਾਸਕ ਵਰਗਾ ਹੋਵੇਗਾ, ਪਰ ਇੱਕ ਵੱਡਾ ਅੰਤਰ ਇਹ ਹੈ ਕਿ ਅਸੀਂ ਪਹਿਲਾਂ ਤੋਂ ਨਹੀਂ ਜਾਣਦੇ ਕਿ ਹਰ ਸਟੈਪ 'ਤੇ ਸਾਨੂੰ ਕਿਹੜੇ ਐਕਸ਼ਨ ਲੈਣੇ ਚਾਹੀਦੇ ਹਨ।

ਇੱਥੇ ਵਿਚਾਰ ਇਹ ਹੈ ਕਿ ਉਹ ਸੰਭਾਵਨਾਵਾਂ ਦਾ ਅੰਦਾਜ਼ਾ ਲਗਾਇਆ ਜਾਵੇ। ਅਸੀਂ **ਕੁਮੂਲਟਿਵ ਰਿਵਾਰਡਸ** ਦਾ ਇੱਕ ਵੇਕਟਰ ਬਣਾਉਂਦੇ ਹਾਂ ਜੋ ਪ੍ਰਯੋਗ ਦੇ ਹਰ ਸਟੈਪ 'ਤੇ ਸਾਡਾ ਕੁੱਲ ਰਿਵਾਰਡ ਦਿਖਾਉਂਦਾ ਹੈ। ਅਸੀਂ **ਰਿਵਾਰਡ ਡਿਸਕਾਊਂਟਿੰਗ** ਵੀ ਲਾਗੂ ਕਰਦੇ ਹਾਂ, ਜਦੋਂ ਪਹਿਲੇ ਰਿਵਾਰਡਸ ਨੂੰ ਕੁਝ ਗੁਣਕ &gamma;=0.99 ਨਾਲ ਗੁਣਾ ਕਰਦੇ ਹਾਂ, ਤਾਂ ਜੋ ਪਹਿਲੇ ਰਿਵਾਰਡਸ ਦੀ ਭੂਮਿਕਾ ਘਟਾਈ ਜਾ ਸਕੇ। ਫਿਰ, ਅਸੀਂ ਉਹ ਸਟੈਪਸ ਨੂੰ ਮਜ਼ਬੂਤ ਕਰਦੇ ਹਾਂ ਜੋ ਵਧੇਰੇ ਰਿਵਾਰਡਸ ਪ੍ਰਾਪਤ ਕਰਦੇ ਹਨ।

> ਪਾਲਿਸੀ ਗ੍ਰੇਡੀਅੰਟ ਐਲਗੋਰਿਦਮ ਬਾਰੇ ਹੋਰ ਜਾਣੋ ਅਤੇ ਇਸਨੂੰ ਕਾਰਵਾਈ ਵਿੱਚ ਦੇਖੋ [ਉਦਾਹਰਣ ਨੋਟਬੁੱਕ](CartPole-RL-TF.ipynb) ਵਿੱਚ।

## ਐਕਟਰ-ਕ੍ਰਿਟਿਕ ਐਲਗੋਰਿਦਮ

ਪਾਲਿਸੀ ਗ੍ਰੇਡੀਅੰਟਸ ਪਹੁੰਚ ਦਾ ਇੱਕ ਸੁਧਾਰਿਤ ਵਰਜਨ **ਐਕਟਰ-ਕ੍ਰਿਟਿਕ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ। ਇਸ ਦੇ ਪਿੱਛੇ ਮੁੱਖ ਵਿਚਾਰ ਇਹ ਹੈ ਕਿ ਨਿਊਰਲ ਨੈਟਵਰਕ ਨੂੰ ਦੋ ਚੀਜ਼ਾਂ ਵਾਪਸ ਕਰਨ ਲਈ ਟ੍ਰੇਨ ਕੀਤਾ ਜਾਵੇ:

* ਪਾਲਿਸੀ, ਜੋ ਇਹ ਨਿਰਧਾਰਤ ਕਰਦੀ ਹੈ ਕਿ ਕਿਹੜਾ ਐਕਸ਼ਨ ਲੈਣਾ ਹੈ। ਇਸ ਭਾਗ ਨੂੰ **ਐਕਟਰ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ।
* ਕੁੱਲ ਰਿਵਾਰਡ ਦਾ ਅੰਦਾਜ਼ਾ ਜੋ ਅਸੀਂ ਇਸ ਸਥਿਤੀ 'ਤੇ ਪ੍ਰਾਪਤ ਕਰਨ ਦੀ ਉਮੀਦ ਕਰ ਸਕਦੇ ਹਾਂ - ਇਸ ਭਾਗ ਨੂੰ **ਕ੍ਰਿਟਿਕ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

ਇੱਕ ਅਰਥ ਵਿੱਚ, ਇਹ ਆਰਕੀਟੈਕਚਰ [GAN](../../4-ComputerVision/10-GANs/README.md) ਵਰਗਾ ਹੈ, ਜਿੱਥੇ ਸਾਡੇ ਕੋਲ ਦੋ ਨੈਟਵਰਕ ਹਨ ਜੋ ਇੱਕ ਦੂਜੇ ਦੇ ਵਿਰੁੱਧ ਟ੍ਰੇਨ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਐਕਟਰ-ਕ੍ਰਿਟਿਕ ਮਾਡਲ ਵਿੱਚ, ਐਕਟਰ ਉਹ ਐਕਸ਼ਨ ਪ੍ਰਸਤਾਵਿਤ ਕਰਦਾ ਹੈ ਜੋ ਸਾਨੂੰ ਲੈਣਾ ਚਾਹੀਦਾ ਹੈ, ਅਤੇ ਕ੍ਰਿਟਿਕ ਨਤੀਜੇ ਦਾ ਅੰਦਾਜ਼ਾ ਲਗਾਉਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦਾ ਹੈ। ਹਾਲਾਂਕਿ, ਸਾਡਾ ਉਦੇਸ਼ ਉਹਨਾਂ ਨੈਟਵਰਕਸ ਨੂੰ ਇਕੱਠੇ ਟ੍ਰੇਨ ਕਰਨਾ ਹੈ।

ਕਿਉਂਕਿ ਸਾਨੂੰ ਪ੍ਰਯੋਗ ਦੌਰਾਨ ਅਸਲ ਕੁਮੂਲਟਿਵ ਰਿਵਾਰਡਸ ਅਤੇ ਕ੍ਰਿਟਿਕ ਦੁਆਰਾ ਵਾਪਸ ਕੀਤੇ ਨਤੀਜੇ ਦੋਵੇਂ ਪਤਾ ਹਨ, ਇਹ ਲੋਸ ਫੰਕਸ਼ਨ ਬਣਾਉਣਾ ਸੌਖਾ ਹੈ ਜੋ ਉਹਨਾਂ ਦੇ ਵਿਚਕਾਰ ਅੰਤਰ ਨੂੰ ਘਟਾਏਗਾ। ਇਹ ਸਾਨੂੰ **ਕ੍ਰਿਟਿਕ ਲੋਸ** ਦੇਵੇਗਾ। ਅਸੀਂ **ਐਕਟਰ ਲੋਸ** ਨੂੰ ਪਾਲਿਸੀ ਗ੍ਰੇਡੀਅੰਟ ਐਲਗੋਰਿਦਮ ਦੇ ਸਮਾਨ ਪਹੁੰਚ ਵਰਤ ਕੇ ਗਣਨਾ ਕਰ ਸਕਦੇ ਹਾਂ।

ਇਹਨਾਂ ਐਲਗੋਰਿਦਮਾਂ ਵਿੱਚੋਂ ਇੱਕ ਨੂੰ ਚਲਾਉਣ ਦੇ ਬਾਅਦ, ਅਸੀਂ ਆਪਣੇ CartPole ਤੋਂ ਇਸ ਤਰ੍ਹਾਂ ਦੇ ਵਿਹਾਰ ਦੀ ਉਮੀਦ ਕਰ ਸਕਦੇ ਹਾਂ:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ ਅਭਿਆਸ: ਪਾਲਿਸੀ ਗ੍ਰੇਡੀਅੰਟਸ ਅਤੇ ਐਕਟਰ-ਕ੍ਰਿਟਿਕ RL

ਹੇਠਾਂ ਦਿੱਤੇ ਨੋਟਬੁੱਕਸ ਵਿੱਚ ਆਪਣੀ ਸਿੱਖਿਆ ਜਾਰੀ ਰੱਖੋ:

* [RL in TensorFlow](CartPole-RL-TF.ipynb)
* [RL in PyTorch](CartPole-RL-PyTorch.ipynb)

## ਹੋਰ RL ਟਾਸਕਸ

ਅੱਜਕਲ ਰੀਇਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ ਇੱਕ ਤੇਜ਼ੀ ਨਾਲ ਵਧ ਰਹੇ ਖੋਜ ਖੇਤਰ ਹੈ। ਰੀਇਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ ਦੇ ਕੁਝ ਦਿਲਚਸਪ ਉਦਾਹਰਣ ਹਨ:

* ਕੰਪਿਊਟਰ ਨੂੰ **Atari Games** ਖੇਡਣ ਦੀ ਸਿੱਖਿਆ ਦੇਣਾ। ਇਸ ਸਮੱਸਿਆ ਵਿੱਚ ਚੁਣੌਤੀਪੂਰਨ ਹਿੱਸਾ ਇਹ ਹੈ ਕਿ ਸਾਡੇ ਕੋਲ ਸਧਾਰਨ ਸਥਿਤੀ ਨਹੀਂ ਹੁੰਦੀ ਜੋ ਇੱਕ ਵੇਕਟਰ ਵਜੋਂ ਦਰਸਾਈ ਜਾ ਸਕੇ, ਪਰ ਇੱਕ ਸਕ੍ਰੀਨਸ਼ਾਟ ਹੁੰਦਾ ਹੈ - ਅਤੇ ਸਾਨੂੰ ਇਸ ਸਕ੍ਰੀਨ ਚਿੱਤਰ ਨੂੰ ਇੱਕ ਫੀਚਰ ਵੇਕਟਰ ਵਿੱਚ ਬਦਲਣ ਜਾਂ ਰਿਵਾਰਡ ਜਾਣਕਾਰੀ ਕੱਢਣ ਲਈ CNN ਦੀ ਵਰਤੋਂ ਕਰਨ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ। Atari ਗੇਮਾਂ Gym ਵਿੱਚ ਉਪਲਬਧ ਹਨ।
* ਕੰਪਿਊਟਰ ਨੂੰ ਬੋਰਡ ਗੇਮਾਂ, ਜਿਵੇਂ ਕਿ Chess ਅਤੇ Go ਖੇਡਣ ਦੀ ਸਿੱਖਿਆ ਦੇਣਾ। ਹਾਲ ਹੀ ਵਿੱਚ **Alpha Zero** ਵਰਗੇ ਸਟੇਟ-ਆਫ-ਦ-ਆਰਟ ਪ੍ਰੋਗਰਾਮ ਦੋ ਏਜੰਟਾਂ ਦੁਆਰਾ ਇੱਕ ਦੂਜੇ ਦੇ ਵਿਰੁੱਧ ਖੇਡ ਕੇ ਅਤੇ ਹਰ ਸਟੈਪ 'ਤੇ ਸੁਧਾਰ ਕਰਕੇ ਸ਼ੁਰੂ ਤੋਂ ਟ੍ਰੇਨ ਕੀਤੇ ਗਏ।
* ਉਦਯੋਗ ਵਿੱਚ, ਸਿਮੂਲੇਸ਼ਨ ਤੋਂ ਕੰਟਰੋਲ ਸਿਸਟਮ ਬਣਾਉਣ ਲਈ RL ਦੀ ਵਰਤੋਂ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇੱਕ ਸੇਵਾ [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) ਇਸ ਲਈ ਖਾਸ ਤੌਰ 'ਤੇ ਡਿਜ਼ਾਈਨ ਕੀਤੀ ਗਈ ਹੈ।

## ਨਿਸਕਰਸ਼

ਅਸੀਂ ਹੁਣ ਸਿੱਖਿਆ ਲੈ ਚੁੱਕੇ ਹਾਂ ਕਿ ਕੇਵਲ ਇੱਕ ਰਿਵਾਰਡ ਫੰਕਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਕੇ, ਜੋ ਗੇਮ ਦੀ ਇੱਛਿਤ ਸਥਿਤੀ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦਾ ਹੈ, ਅਤੇ ਉਨ੍ਹਾਂ ਨੂੰ ਸਮਰਥਨਪੂਰਵਕ ਖੋਜ ਕਰਨ ਦਾ ਮੌਕਾ ਦੇ ਕੇ, ਏਜੰਟਾਂ ਨੂੰ ਚੰਗੇ ਨਤੀਜੇ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ ਟ੍ਰੇਨ ਕਿਵੇਂ ਕਰਨਾ ਹੈ। ਅਸੀਂ ਸਫਲਤਾਪੂਰਵਕ ਦੋ ਐਲਗੋਰਿਦਮਾਂ ਦੀ ਕੋਸ਼ਿਸ਼ ਕੀਤੀ ਹੈ ਅਤੇ ਇੱਕ ਸੰਬੰਧਿਤ ਛੋਟੇ ਸਮੇਂ ਵਿੱਚ ਚੰਗਾ ਨਤੀਜਾ ਪ੍ਰਾਪਤ ਕੀਤਾ ਹੈ। ਹਾਲਾਂਕਿ, ਇਹ ਤੁਹਾਡੇ RL ਵਿੱਚ ਯਾਤਰਾ ਦੀ ਸਿਰਫ ਸ਼ੁਰੂਆਤ ਹੈ, ਅਤੇ ਜੇਕਰ ਤੁਸੀਂ ਹੋਰ ਡੂੰਘਾਈ ਵਿੱਚ ਜਾਣਾ ਚਾਹੁੰਦੇ ਹੋ ਤਾਂ ਤੁਹਾਨੂੰ ਨਿਸਚਿਤ ਤੌਰ 'ਤੇ ਇੱਕ ਵੱਖਰਾ ਕੋਰਸ ਕਰਨ ਬਾਰੇ ਸੋਚਣਾ ਚਾਹੀਦਾ ਹੈ।

## 🚀 ਚੁਣੌਤੀ

'ਹੋਰ RL ਟਾਸਕਸ' ਭਾਗ ਵਿੱਚ ਦਿੱਤੇ ਐਪਲੀਕੇਸ਼ਨਾਂ ਦੀ ਖੋਜ ਕਰੋ ਅਤੇ ਇੱਕ ਨੂੰ ਲਾਗੂ ਕਰਨ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰੋ!

## [ਪੋਸਟ-ਲੈਕਚਰ ਕਵਿਜ਼](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## ਸਮੀਖਿਆ ਅਤੇ ਸਵੈ-ਅਧਿਐਨ

ਸਾਡੇ [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) ਵਿੱਚ ਕਲਾਸੀਕਲ ਰੀਇਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ ਬਾਰੇ ਹੋਰ ਜਾਣੋ।

ਇਹ ਸ਼ਾਨਦਾਰ ਵੀਡੀਓ [ਇੱਥੇ](https://www.youtube.com/watch?v=qv6UVOQ0F44) ਦੇਖੋ ਜੋ ਦਿਖਾਉਂਦੀ ਹੈ ਕਿ ਕੰਪਿਊਟਰ Super Mario ਖੇਡਣ ਦੀ ਸਿੱਖਿਆ ਕਿਵੇਂ ਲੈ ਸਕਦਾ ਹੈ।

## ਅਸਾਈਨਮੈਂਟ: [Train a Mountain Car](lab/README.md)

ਤੁਹਾਡਾ ਉਦੇਸ਼ ਇਸ ਅਸਾਈਨਮੈਂਟ ਦੌਰਾਨ ਇੱਕ ਵੱਖਰੇ Gym ਮਾਹੌਲ - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) ਨੂੰ ਟ੍ਰੇਨ ਕਰਨਾ ਹੋਵੇਗਾ।

---

