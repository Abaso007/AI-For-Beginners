{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utaratibu wa Uangalizi na Transformer\n",
    "\n",
    "Changamoto kubwa ya mitandao ya kurudiarudia (recurrent networks) ni kwamba maneno yote katika mlolongo yana athari sawa kwenye matokeo. Hii husababisha utendaji duni katika mifano ya kawaida ya LSTM encoder-decoder kwa kazi za mlolongo hadi mlolongo, kama vile Utambuzi wa Vitu Vilivyotajwa (Named Entity Recognition) na Tafsiri ya Mashine. Kwa kweli, maneno maalum katika mlolongo wa pembejeo mara nyingi huwa na athari kubwa zaidi kwenye matokeo ya mlolongo kuliko mengine.\n",
    "\n",
    "Fikiria mfano wa mlolongo hadi mlolongo, kama vile tafsiri ya mashine. Hii inatekelezwa na mitandao miwili ya kurudiarudia, ambapo mtandao mmoja (**encoder**) unakusanya mlolongo wa pembejeo katika hali iliyofichwa, na mwingine, **decoder**, unafungua hali hii iliyofichwa kuwa matokeo yaliyotafsiriwa. Tatizo na mbinu hii ni kwamba hali ya mwisho ya mtandao itakuwa na ugumu wa kukumbuka mwanzo wa sentensi, hivyo kusababisha ubora duni wa mfano kwenye sentensi ndefu.\n",
    "\n",
    "**Utaratibu wa Uangalizi** hutoa njia ya kupima athari ya muktadha wa kila vector ya pembejeo kwenye kila utabiri wa RNN. Hii inatekelezwa kwa kuunda njia za mkato kati ya hali za kati za RNN ya pembejeo na RNN ya matokeo. Kwa njia hii, tunapozalisha alama ya matokeo $y_t$, tutazingatia hali zote za siri za pembejeo $h_i$, kwa kutumia viwango tofauti vya uzito $\\alpha_{t,i}$.\n",
    "\n",
    "![Picha inayoonyesha mfano wa encoder/decoder na safu ya uangalizi wa kuongeza](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.sw.png)  \n",
    "*Mfano wa encoder-decoder na utaratibu wa uangalizi wa kuongeza katika [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), iliyonukuliwa kutoka [blogu hii](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Matriki ya uangalizi $\\{\\alpha_{i,j}\\}$ inawakilisha kiwango ambacho maneno fulani ya pembejeo yanachangia katika uzalishaji wa neno fulani katika mlolongo wa matokeo. Hapo chini kuna mfano wa matriki kama hiyo:\n",
    "\n",
    "![Picha inayoonyesha mpangilio wa mfano uliopatikana na RNNsearch-50, kutoka Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.sw.png)  \n",
    "\n",
    "*Picha imetolewa kutoka [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Mchoro wa 3)*\n",
    "\n",
    "Utaratibu wa uangalizi unachangia sana hali ya juu ya sasa au karibu na hali ya juu katika Usindikaji wa Lugha Asilia. Hata hivyo, kuongeza uangalizi huongeza sana idadi ya vigezo vya mfano, jambo ambalo lilisababisha changamoto za kupanua RNNs. Kizuizi kikuu cha kupanua RNNs ni kwamba asili ya kurudiarudia ya mifano hufanya iwe changamoto kuendesha mafunzo kwa vikundi na kwa sambamba. Katika RNN, kila kipengele cha mlolongo kinahitaji kushughulikiwa kwa mpangilio wa mlolongo, jambo ambalo linamaanisha kuwa haiwezi kufanywa kwa urahisi kwa sambamba.\n",
    "\n",
    "Kupitishwa kwa utaratibu wa uangalizi pamoja na kizuizi hiki kulisababisha kuundwa kwa Transformer Models, ambazo sasa ni hali ya juu ya sanaa, tunazozijua na kuzitumia leo, kama vile BERT na OpenGPT3.\n",
    "\n",
    "## Mifano ya Transformer\n",
    "\n",
    "Badala ya kupeleka muktadha wa kila utabiri wa awali katika hatua inayofuata ya tathmini, **mifano ya transformer** hutumia **usimbaji wa nafasi** na uangalizi ili kunasa muktadha wa pembejeo fulani ndani ya dirisha lililotolewa la maandishi. Picha hapa chini inaonyesha jinsi usimbaji wa nafasi pamoja na uangalizi unavyoweza kunasa muktadha ndani ya dirisha lililotolewa.\n",
    "\n",
    "![GIF inayoonyesha jinsi tathmini inavyofanywa katika mifano ya transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)  \n",
    "\n",
    "Kwa kuwa kila nafasi ya pembejeo inachanganuliwa kwa uhuru kwa kila nafasi ya matokeo, transformer zinaweza kufanya kazi kwa sambamba zaidi kuliko RNNs, jambo ambalo linawezesha mifano mikubwa zaidi na yenye kueleza zaidi ya lugha. Kila kichwa cha uangalizi kinaweza kutumika kujifunza mahusiano tofauti kati ya maneno, jambo ambalo huboresha kazi za Usindikaji wa Lugha Asilia.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) ni mtandao mkubwa sana wa transformer wenye tabaka 12 kwa *BERT-base*, na 24 kwa *BERT-large*. Mfano huu kwanza hufunzwa awali kwenye hifadhidata kubwa ya maandishi (WikiPedia + vitabu) kwa kutumia mafunzo yasiyo ya kusimamiwa (kutabiri maneno yaliyofichwa katika sentensi). Wakati wa mafunzo ya awali, mfano huu hujifunza kiwango kikubwa cha uelewa wa lugha ambacho kinaweza kutumika na hifadhidata nyingine kwa kutumia marekebisho madogo. Mchakato huu unaitwa **ujifunzaji wa uhamisho**.\n",
    "\n",
    "![Picha kutoka http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.sw.png)  \n",
    "\n",
    "Kuna tofauti nyingi za usanifu wa Transformer ikiwa ni pamoja na BERT, DistilBERT, BigBird, OpenGPT3 na nyinginezo ambazo zinaweza kufanyiwa marekebisho madogo. [Paket ya HuggingFace](https://github.com/huggingface/) inatoa hifadhidata ya mafunzo kwa usanifu mwingi wa aina hii kwa kutumia PyTorch.\n",
    "\n",
    "## Kutumia BERT kwa uainishaji wa maandishi\n",
    "\n",
    "Hebu tuone jinsi tunavyoweza kutumia mfano wa BERT uliokwisha funzwa kwa kutatua kazi yetu ya jadi: uainishaji wa mlolongo. Tutatambua hifadhidata yetu ya awali ya AG News.  \n",
    "\n",
    "Kwanza, hebu tupakie maktaba ya HuggingFace na hifadhidata yetu:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kwa sababu tutatumia modeli ya BERT iliyofunzwa awali, tutahitaji kutumia tokenizer maalum. Kwanza, tutapakia tokenizer inayohusiana na modeli ya BERT iliyofunzwa awali.\n",
    "\n",
    "Maktaba ya HuggingFace ina hazina ya modeli zilizofunzwa awali, ambazo unaweza kutumia kwa kutaja tu majina yao kama hoja kwa kazi za `from_pretrained`. Faili zote muhimu za binary za modeli zitapakuliwa moja kwa moja.\n",
    "\n",
    "Hata hivyo, wakati fulani utahitaji kupakia modeli zako mwenyewe, ambapo unaweza kutaja saraka inayojumuisha faili zote husika, ikiwa ni pamoja na vigezo vya tokenizer, faili ya `config.json` yenye vigezo vya modeli, uzito wa binary, n.k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kipengele cha `tokenizer` kina kazi ya `encode` ambayo inaweza kutumika moja kwa moja kusimba maandishi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kisha, wacha tuunde iterators ambazo tutatumia wakati wa mafunzo kufikia data. Kwa sababu BERT hutumia kazi yake ya usimbaji, tungehitaji kufafanua kazi ya padding sawa na `padify` tuliyofafanua hapo awali:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katika kesi yetu, tutatumia modeli ya BERT iliyofunzwa awali inayoitwa `bert-base-uncased`. Wacha tupakie modeli kwa kutumia kifurushi cha `BertForSequenceClassfication`. Hii inahakikisha kwamba modeli yetu tayari ina usanifu unaohitajika kwa uainishaji, ikijumuisha uainishaji wa mwisho. Utaona ujumbe wa onyo ukisema kwamba uzito wa uainishaji wa mwisho haujaanzishwa, na modeli ingehitaji mafunzo ya awali - hilo ni sawa kabisa, kwa sababu ni hasa kile tunachotarajia kufanya!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasa tuko tayari kuanza mafunzo! Kwa sababu BERT tayari imefundishwa awali, tunataka kuanza na kiwango kidogo cha kujifunza ili tusiharibu uzito wa awali.\n",
    "\n",
    "Kazi ngumu yote inafanywa na modeli ya `BertForSequenceClassification`. Tunapoiita modeli kwenye data ya mafunzo, inarudisha hasara na matokeo ya mtandao kwa kila kundi dogo la pembejeo. Tunatumia hasara kwa ajili ya uboreshaji wa vigezo (`loss.backward()` hufanya mchakato wa kurudi nyuma), na `out` kwa kuhesabu usahihi wa mafunzo kwa kulinganisha lebo zilizopatikana `labs` (zinazohesabiwa kwa kutumia `argmax`) na lebo zinazotarajiwa `labels`.\n",
    "\n",
    "Ili kudhibiti mchakato, tunakusanya hasara na usahihi kwa mizunguko kadhaa, na kuzichapisha kila mizunguko ya mafunzo ya `report_freq`.\n",
    "\n",
    "Mafunzo haya yanaweza kuchukua muda mrefu sana, kwa hivyo tunapunguza idadi ya mizunguko.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unaweza kuona (hasa ikiwa utaongeza idadi ya marudio na kusubiri kwa muda mrefu) kwamba uainishaji wa BERT unatoa usahihi mzuri sana! Hii ni kwa sababu BERT tayari inaelewa vizuri muundo wa lugha, na tunahitaji tu kurekebisha kidogo uainishaji wa mwisho. Hata hivyo, kwa sababu BERT ni mfano mkubwa, mchakato mzima wa mafunzo unachukua muda mrefu, na unahitaji nguvu kubwa ya kompyuta! (GPU, na ikiwezekana zaidi ya moja).\n",
    "\n",
    "> **Note:** Katika mfano wetu, tumekuwa tukitumia mojawapo ya mifano midogo zaidi ya BERT iliyokwisha funzwa. Kuna mifano mikubwa zaidi ambayo ina uwezekano wa kutoa matokeo bora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kutathmini Utendaji wa Modeli\n",
    "\n",
    "Sasa tunaweza kutathmini utendaji wa modeli yetu kwenye seti ya data ya majaribio. Mchakato wa tathmini unafanana sana na mchakato wa mafunzo, lakini hatupaswi kusahau kubadilisha modeli kuwa hali ya tathmini kwa kupiga `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muhimu\n",
    "\n",
    "Katika sehemu hii, tumeona jinsi ilivyo rahisi kuchukua modeli ya lugha iliyofunzwa awali kutoka maktaba ya **transformers** na kuibadilisha kwa kazi yetu ya uainishaji wa maandishi. Vivyo hivyo, modeli za BERT zinaweza kutumika kwa uchimbaji wa huluki, kujibu maswali, na kazi nyingine za NLP.\n",
    "\n",
    "Modeli za transformer zinawakilisha hali ya juu zaidi ya teknolojia ya NLP kwa sasa, na katika hali nyingi zinapaswa kuwa suluhisho la kwanza unaloanza kulijaribu unapotekeleza suluhisho maalum za NLP. Hata hivyo, kuelewa kanuni za msingi za mitandao ya neva ya kurudia zilizojadiliwa katika moduli hii ni muhimu sana ikiwa unataka kujenga modeli za neva za hali ya juu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Kanusho**:  \nHati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-29T15:57:10+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "sw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}