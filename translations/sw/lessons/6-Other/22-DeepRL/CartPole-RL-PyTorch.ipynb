{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kufundisha RL Kusawazisha Cartpole\n",
    "\n",
    "Notibuku hii ni sehemu ya [Mtaala wa AI kwa Kompyuta](http://aka.ms/ai-beginners). Imechangiwa na [mafunzo rasmi ya PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) na [utekelezaji huu wa Cartpole kwa kutumia PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Katika mfano huu, tutatumia RL kufundisha modeli kusawazisha fimbo juu ya gari ambalo linaweza kusogea kushoto na kulia kwenye mstari wa usawa. Tutatumia mazingira ya [OpenAI Gym](https://www.gymlibrary.ml/) kuiga hali ya fimbo.\n",
    "\n",
    "> **Note**: Unaweza kuendesha msimbo wa somo hili kwenye kompyuta yako (mfano, ukitumia Visual Studio Code), ambapo uigaji utafunguka kwenye dirisha jipya. Unapoendesha msimbo mtandaoni, huenda ukahitaji kufanya marekebisho madogo kwenye msimbo, kama ilivyoelezwa [hapa](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Tutaanza kwa kuhakikisha Gym imewekwa:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasa hebu tuunde mazingira ya CartPole na tuone jinsi ya kuyatumia. Mazingira yana sifa zifuatazo:\n",
    "\n",
    "* **Action space** ni seti ya hatua zinazowezekana ambazo tunaweza kuchukua katika kila hatua ya simulizi\n",
    "* **Observation space** ni nafasi ya uchunguzi tunaoweza kufanya\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hebu tuone jinsi simulizi inavyofanya kazi. Kitanzi kifuatacho kinaendesha simulizi, hadi `env.step` hairejeshi bendera ya kukamilisha `done`. Tutachagua hatua kwa nasibu kwa kutumia `env.action_space.sample()`, ambayo inamaanisha jaribio litashindwa haraka sana (mazingira ya CartPole hukamilika pale kasi ya CartPole, nafasi yake au pembe yake zinapokuwa nje ya mipaka fulani).\n",
    "\n",
    "> Simulizi itafunguka katika dirisha jipya. Unaweza kuendesha msimbo mara kadhaa na kuona jinsi inavyotenda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unaweza kugundua kuwa uchunguzi una namba 4. Hizi ni:\n",
    "- Nafasi ya gari\n",
    "- Kasi ya gari\n",
    "- Pembe ya nguzo\n",
    "- Kiwango cha mzunguko wa nguzo\n",
    "\n",
    "`rew` ni zawadi tunayoipokea katika kila hatua. Unaweza kuona kwamba katika mazingira ya CartPole unapewa alama 1 kwa kila hatua ya simulizi, na lengo ni kuongeza jumla ya zawadi, yaani muda ambao CartPole inaweza kusawazika bila kuanguka.\n",
    "\n",
    "Wakati wa kujifunza kwa kuimarisha, lengo letu ni kufundisha **sera** $\\pi$, ambayo kwa kila hali $s$ itatuambia ni hatua gani $a$ tuchukue, kwa hivyo kimsingi $a = \\pi(s)$.\n",
    "\n",
    "Ikiwa unataka suluhisho la uwezekano, unaweza kufikiria sera kama inavyorudisha seti ya uwezekano kwa kila hatua, yaani $\\pi(a|s)$ ingemaanisha uwezekano kwamba tunapaswa kuchukua hatua $a$ katika hali $s$.\n",
    "\n",
    "## Njia ya Gradient ya Sera\n",
    "\n",
    "Katika algorithimu rahisi zaidi ya RL, inayoitwa **Policy Gradient**, tutafundisha mtandao wa neva kutabiri hatua inayofuata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutafundisha mtandao kwa kufanya majaribio mengi, na kusasisha mtandao wetu baada ya kila jaribio. Hebu tueleze kazi ambayo itaendesha jaribio na kurudisha matokeo (inayoitwa **trace**) - hali zote, vitendo (na uwezekano wao uliopendekezwa), na zawadi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unaweza kuendesha kipindi kimoja na mtandao usiofunzwa na kuona kuwa jumla ya zawadi (yaani urefu wa kipindi) ni ndogo sana:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moja ya mambo magumu ya algoriti ya sera ya gradienti ni kutumia **zawadi zilizopunguzwa**. Wazo ni kwamba tunahesabu vekta ya jumla ya zawadi katika kila hatua ya mchezo, na wakati wa mchakato huu tunapunguza zawadi za awali kwa kutumia kipengele fulani $gamma$. Pia tunanormalisha vekta inayopatikana, kwa sababu tuta itumia kama uzito wa kuathiri mafunzo yetu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasa tuanze mafunzo halisi! Tutakimbia vipindi 300, na katika kila kipindi tutafanya yafuatayo:\n",
    "\n",
    "1. Endesha jaribio na kukusanya mfuatano wa matukio.\n",
    "2. Hesabu tofauti (`gradients`) kati ya hatua zilizochukuliwa na uwezekano uliotabiriwa. Kadri tofauti inavyokuwa ndogo, ndivyo tunavyokuwa na uhakika zaidi kwamba tumefanya hatua sahihi.\n",
    "3. Hesabu zawadi zilizopunguzwa na zidisha gradients kwa zawadi hizo zilizopunguzwa - hii itahakikisha kwamba hatua zilizo na zawadi kubwa zaidi zitakuwa na athari kubwa kwenye matokeo ya mwisho kuliko zile zilizo na zawadi ndogo.\n",
    "4. Hatua lengwa zinazotarajiwa kwa mtandao wetu wa neva zitachukuliwa kwa sehemu kutoka kwa uwezekano uliotabiriwa wakati wa mchakato, na kwa sehemu kutoka kwa gradients zilizohesabiwa. Tutatumia kipengele `alpha` kuamua kwa kiwango gani gradients na zawadi zinazingatiwa - hii inaitwa *kiwango cha kujifunza* cha algorithimu ya uimarishaji.\n",
    "5. Hatimaye, tunafundisha mtandao wetu kwa hali na hatua zinazotarajiwa, kisha tunarudia mchakato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasa wacha tuendeshe kipindi na uonyeshaji ili kuona matokeo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunaweza kuona kwamba sasa pole inaweza kusawazika vizuri!\n",
    "\n",
    "## Mfano wa Actor-Critic\n",
    "\n",
    "Mfano wa Actor-Critic ni maendeleo zaidi ya gradients za sera, ambapo tunajenga mtandao wa neva ili kujifunza sera na thawabu zinazokadiriwa. Mtandao huu utakuwa na matokeo mawili (au unaweza kuiona kama mitandao miwili tofauti):\n",
    "* **Actor** itapendekeza hatua ya kuchukua kwa kutupa usambazaji wa uwezekano wa hali, kama ilivyo kwenye mfano wa gradient ya sera.\n",
    "* **Critic** itakadiria thawabu zitakazotokana na hatua hizo. Inarudisha jumla ya thawabu zinazokadiriwa za baadaye katika hali iliyopo.\n",
    "\n",
    "Hebu tuelezee mfano kama huo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunahitaji kurekebisha kidogo kazi zetu za `discounted_rewards` na `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasa tutaendesha mzunguko mkuu wa mafunzo. Tutatumia mchakato wa mafunzo wa mtandao wa mwongozo kwa kuhesabu kazi sahihi za hasara na kusasisha vigezo vya mtandao:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muhimu\n",
    "\n",
    "Tumeona mbinu mbili za RL katika onyesho hili: gradient rahisi ya sera, na mbinu ya hali ya juu ya actor-critic. Unaweza kuona kwamba mbinu hizo zinatumia dhana za hali, hatua, na zawadi kwa njia ya kificho - hivyo zinaweza kutumika katika mazingira tofauti kabisa.\n",
    "\n",
    "Kujifunza kwa kuimarisha kunatuwezesha kujifunza mkakati bora wa kutatua tatizo kwa kuangalia tu zawadi ya mwisho. Ukweli kwamba hatuhitaji seti za data zilizo na lebo unaturuhusu kurudia masimulizi mara nyingi ili kuboresha mifano yetu. Hata hivyo, bado kuna changamoto nyingi katika RL, ambazo unaweza kujifunza ikiwa utaamua kuzingatia zaidi eneo hili la kuvutia la AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Kanusho**:  \nHati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, inashauriwa kutumia huduma ya tafsiri ya kibinadamu ya kitaalamu. Hatutawajibika kwa maelewano mabaya au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T13:04:52+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "sw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}