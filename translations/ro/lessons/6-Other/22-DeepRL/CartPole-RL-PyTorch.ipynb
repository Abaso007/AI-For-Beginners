{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antrenarea RL pentru echilibrarea Cartpole\n",
    "\n",
    "Acest notebook face parte din [Curriculumul AI pentru Începători](http://aka.ms/ai-beginners). A fost inspirat de [tutorialul oficial PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) și de [această implementare Cartpole în PyTorch](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "În acest exemplu, vom folosi RL pentru a antrena un model să echilibreze un stâlp pe un cărucior care se poate deplasa la stânga și la dreapta pe o scară orizontală. Vom folosi mediul [OpenAI Gym](https://www.gymlibrary.ml/) pentru a simula stâlpul.\n",
    "\n",
    "> **Note**: Puteți rula codul lecției local (de exemplu, din Visual Studio Code), caz în care simularea se va deschide într-o fereastră nouă. Când rulați codul online, este posibil să fie nevoie să faceți unele ajustări ale codului, așa cum este descris [aici](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Vom începe prin a ne asigura că Gym este instalat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum să creăm mediul CartPole și să vedem cum să lucrăm cu el. Un mediu are următoarele proprietăți:\n",
    "\n",
    "* **Spațiul de acțiuni** este setul de acțiuni posibile pe care le putem efectua la fiecare pas al simulării\n",
    "* **Spațiul de observații** este spațiul observațiilor pe care le putem face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Să vedem cum funcționează simularea. Următorul buclă rulează simularea până când `env.step` nu mai returnează indicatorul de terminare `done`. Vom alege acțiuni în mod aleatoriu folosind `env.action_space.sample()`, ceea ce înseamnă că experimentul va eșua probabil foarte repede (mediul CartPole se termină atunci când viteza CartPole, poziția sau unghiul său depășesc anumite limite).\n",
    "\n",
    "> Simularea se va deschide într-o fereastră nouă. Poți rula codul de mai multe ori și observa cum se comportă.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poți observa că observațiile conțin 4 numere. Acestea sunt:\n",
    "- Poziția căruciorului\n",
    "- Viteza căruciorului\n",
    "- Unghiul stâlpului\n",
    "- Rata de rotație a stâlpului\n",
    "\n",
    "`rew` este recompensa pe care o primim la fiecare pas. Poți observa că, în mediul CartPole, primești 1 punct pentru fiecare pas de simulare, iar scopul este să maximizezi recompensa totală, adică timpul în care CartPole reușește să se echilibreze fără să cadă.\n",
    "\n",
    "În timpul învățării prin întărire, scopul nostru este să antrenăm o **politică** $\\pi$, care pentru fiecare stare $s$ ne va spune ce acțiune $a$ să luăm, practic $a = \\pi(s)$.\n",
    "\n",
    "Dacă dorești o soluție probabilistică, poți considera politica ca returnând un set de probabilități pentru fiecare acțiune, adică $\\pi(a|s)$ ar însemna probabilitatea că ar trebui să luăm acțiunea $a$ în starea $s$.\n",
    "\n",
    "## Metoda Gradientului Politicii\n",
    "\n",
    "În cel mai simplu algoritm RL, numit **Gradientul Politicii**, vom antrena o rețea neuronală pentru a prezice următoarea acțiune.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vom antrena rețeaua rulând multe experimente și actualizând rețeaua noastră după fiecare rulare. Să definim o funcție care va rula experimentul și va returna rezultatele (așa-numitul **trace**) - toate stările, acțiunile (și probabilitățile recomandate ale acestora) și recompensele:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poți rula un episod cu rețeaua neantrenată și observa că recompensa totală (cunoscută și ca lungimea episodului) este foarte mică:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unul dintre aspectele dificile ale algoritmului de gradient de politică este utilizarea **recompenselor reduse**. Ideea este că calculăm vectorul recompenselor totale la fiecare pas al jocului, iar în acest proces reducem recompensele timpurii folosind un coeficient $gamma$. De asemenea, normalizăm vectorul rezultat, deoarece îl vom folosi ca greutate pentru a influența antrenamentul nostru:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum să trecem la antrenamentul propriu-zis! Vom rula 300 de episoade, iar la fiecare episod vom face următoarele:\n",
    "\n",
    "1. Rulăm experimentul și colectăm traseul\n",
    "1. Calculăm diferența (`gradients`) între acțiunile întreprinse și probabilitățile prezise. Cu cât diferența este mai mică, cu atât suntem mai siguri că am luat acțiunea corectă.\n",
    "1. Calculăm recompensele actualizate și înmulțim gradientele cu recompensele actualizate - acest lucru va asigura că pașii cu recompense mai mari vor avea un impact mai mare asupra rezultatului final decât cei cu recompense mai mici.\n",
    "1. Acțiunile țintă așteptate pentru rețeaua noastră neuronală vor fi parțial preluate din probabilitățile prezise în timpul rulării și parțial din gradienții calculați. Vom folosi parametrul `alpha` pentru a determina în ce măsură sunt luate în considerare gradientele și recompensele - acest lucru se numește *rata de învățare* a algoritmului de întărire.\n",
    "1. În cele din urmă, antrenăm rețeaua noastră pe stări și acțiuni așteptate și repetăm procesul.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum să rulăm episodul cu redare pentru a vedea rezultatul:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sperăm că acum poți observa că bara se poate echilibra destul de bine!\n",
    "\n",
    "## Modelul Actor-Critic\n",
    "\n",
    "Modelul Actor-Critic reprezintă o dezvoltare ulterioară a gradientelor de politică, în care construim o rețea neuronală pentru a învăța atât politica, cât și recompensele estimate. Rețeaua va avea două ieșiri (sau poate fi privită ca două rețele separate):\n",
    "* **Actorul** va recomanda acțiunea de luat, oferindu-ne distribuția probabilistică a stării, la fel ca în modelul cu gradient de politică.\n",
    "* **Criticul** va estima ce recompensă ar rezulta din acele acțiuni. Acesta returnează recompensele totale estimate în viitor pentru starea dată.\n",
    "\n",
    "Să definim un astfel de model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ar trebui să modificăm ușor funcțiile noastre `discounted_rewards` și `run_episode`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum vom rula bucla principală de antrenament. Vom folosi procesul manual de antrenare a rețelei prin calcularea funcțiilor de pierdere corespunzătoare și actualizarea parametrilor rețelei:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluzii\n",
    "\n",
    "Am văzut două algoritmi RL în această demonstrație: gradient de politică simplu și actor-critic mai sofisticat. Puteți observa că acești algoritmi operează cu noțiuni abstracte de stare, acțiune și recompensă - astfel, pot fi aplicați în medii foarte diferite.\n",
    "\n",
    "Învățarea prin întărire ne permite să descoperim cea mai bună strategie pentru a rezolva problema doar analizând recompensa finală. Faptul că nu avem nevoie de seturi de date etichetate ne permite să repetăm simulările de mai multe ori pentru a optimiza modelele noastre. Totuși, există încă multe provocări în RL, pe care le puteți descoperi dacă decideți să vă concentrați mai mult pe acest domeniu fascinant al inteligenței artificiale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Declinare de responsabilitate**:  \nAcest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa natală ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist uman. Nu ne asumăm responsabilitatea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-29T22:58:50+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "ro"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}