<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "789d6c3fb6fc7948a470b33078a5983a",
  "translation_date": "2025-09-23T07:50:58+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "bn"
}
-->
# নিউরাল নেটওয়ার্কের পরিচিতি: মাল্টি-লেয়ার পারসেপট্রন

পূর্ববর্তী অংশে, আপনি সবচেয়ে সহজ নিউরাল নেটওয়ার্ক মডেল - এক-স্তর বিশিষ্ট পারসেপট্রন, একটি লিনিয়ার দুই-শ্রেণীর শ্রেণীবিন্যাস মডেল সম্পর্কে শিখেছেন।

এই অংশে আমরা এই মডেলটিকে আরও নমনীয় কাঠামোতে প্রসারিত করব, যা আমাদেরকে সক্ষম করবে:

* **বহু-শ্রেণীর শ্রেণীবিন্যাস** সম্পাদন করা, দুই-শ্রেণীর শ্রেণীবিন্যাসের পাশাপাশি
* **রিগ্রেশন সমস্যা** সমাধান করা, শ্রেণীবিন্যাসের পাশাপাশি
* এমন শ্রেণীগুলি আলাদা করা যা লিনিয়ারভাবে পৃথকযোগ্য নয়

আমরা Python-এ আমাদের নিজস্ব মডুলার কাঠামোও তৈরি করব, যা আমাদের বিভিন্ন নিউরাল নেটওয়ার্ক আর্কিটেকচার তৈরি করতে সাহায্য করবে।

## [পূর্ব-লেকচার কুইজ](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## মেশিন লার্নিং-এর আনুষ্ঠানিকীকরণ

চলুন মেশিন লার্নিং সমস্যাটিকে আনুষ্ঠানিকভাবে শুরু করি। ধরুন আমাদের কাছে একটি প্রশিক্ষণ ডেটাসেট **X** এবং লেবেল **Y** রয়েছে, এবং আমাদের এমন একটি মডেল *f* তৈরি করতে হবে যা সবচেয়ে সঠিক পূর্বাভাস দেবে। পূর্বাভাসের গুণমান **লস ফাংশন** &lagran; দ্বারা পরিমাপ করা হয়। নিম্নলিখিত লস ফাংশনগুলি প্রায়ই ব্যবহৃত হয়:

* রিগ্রেশন সমস্যার জন্য, যখন আমাদের একটি সংখ্যা পূর্বাভাস দিতে হবে, আমরা **অ্যাবসোলিউট এরর** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, অথবা **স্কোয়ারড এরর** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> ব্যবহার করতে পারি।
* শ্রেণীবিন্যাসের জন্য, আমরা **0-1 লস** (যা মূলত মডেলের **সঠিকতা**), অথবা **লজিস্টিক লস** ব্যবহার করি।

এক-স্তর পারসেপট্রনের ক্ষেত্রে, ফাংশন *f* একটি লিনিয়ার ফাংশন হিসাবে সংজ্ঞায়িত করা হয়েছিল *f(x)=wx+b* (এখানে *w* হলো ওজন ম্যাট্রিক্স, *x* হলো ইনপুট বৈশিষ্ট্যের ভেক্টর, এবং *b* হলো বায়াস ভেক্টর)। বিভিন্ন নিউরাল নেটওয়ার্ক আর্কিটেকচারের জন্য, এই ফাংশনটি আরও জটিল আকার নিতে পারে।

> শ্রেণীবিন্যাসের ক্ষেত্রে, প্রায়ই সংশ্লিষ্ট শ্রেণীর সম্ভাবনা নেটওয়ার্ক আউটপুট হিসাবে পাওয়া পছন্দনীয়। যেকোনো সংখ্যাকে সম্ভাবনায় রূপান্তর করতে (যেমন আউটপুটকে স্বাভাবিক করতে), আমরা প্রায়ই **সফটম্যাক্স** ফাংশন &sigma; ব্যবহার করি, এবং ফাংশন *f* হয়ে যায় *f(x)=&sigma;(wx+b)*

উপরের *f* এর সংজ্ঞায়, *w* এবং *b* কে **প্যারামিটার** &theta;=⟨*w,b*⟩ বলা হয়। ডেটাসেট ⟨**X**,**Y**⟩ দেওয়া হলে, আমরা পুরো ডেটাসেটের উপর সামগ্রিক ত্রুটি প্যারামিটার &theta; এর একটি ফাংশন হিসাবে গণনা করতে পারি।

> ✅ **নিউরাল নেটওয়ার্ক প্রশিক্ষণের লক্ষ্য হলো প্যারামিটার &theta; পরিবর্তন করে ত্রুটি কমানো**

## গ্রেডিয়েন্ট ডিসেন্ট অপ্টিমাইজেশন

ফাংশন অপ্টিমাইজেশনের একটি সুপরিচিত পদ্ধতি হলো **গ্রেডিয়েন্ট ডিসেন্ট**। ধারণাটি হলো আমরা লস ফাংশনের প্যারামিটারের সাথে সম্পর্কিত ডেরিভেটিভ (মাল্টি-ডাইমেনশনাল ক্ষেত্রে **গ্রেডিয়েন্ট** বলা হয়) গণনা করতে পারি এবং প্যারামিটার পরিবর্তন করতে পারি যাতে ত্রুটি কমে যায়। এটি নিম্নরূপ আনুষ্ঠানিকভাবে প্রকাশ করা যেতে পারে:

* প্যারামিটারগুলি কিছু র্যান্ডম মান w<sup>(0)</sup>, b<sup>(0)</sup> দিয়ে আরম্ভ করুন
* নিম্নলিখিত ধাপটি বারবার করুন:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

প্রশিক্ষণের সময়, অপ্টিমাইজেশন ধাপগুলি পুরো ডেটাসেট বিবেচনা করে গণনা করা উচিত (মনে রাখবেন যে লস সমস্ত প্রশিক্ষণ নমুনার মাধ্যমে যোগফল হিসাবে গণনা করা হয়)। তবে, বাস্তবে আমরা ডেটাসেটের ছোট অংশগুলি, যাকে **মিনিব্যাচ** বলা হয়, গ্রহণ করি এবং ডেটার একটি উপসেটের উপর ভিত্তি করে গ্রেডিয়েন্ট গণনা করি। যেহেতু প্রতিবার র্যান্ডমভাবে উপসেট নেওয়া হয়, এই পদ্ধতিকে **স্টোকাস্টিক গ্রেডিয়েন্ট ডিসেন্ট** (SGD) বলা হয়।

## মাল্টি-লেয়ার পারসেপট্রন এবং ব্যাকপ্রোপাগেশন

এক-স্তর নেটওয়ার্ক, যেমন আমরা উপরে দেখেছি, লিনিয়ারভাবে পৃথকযোগ্য শ্রেণীগুলি শ্রেণীবিন্যাস করতে সক্ষম। একটি সমৃদ্ধ মডেল তৈরি করতে, আমরা নেটওয়ার্কের কয়েকটি স্তর একত্রিত করতে পারি। গাণিতিকভাবে এটি বোঝায় যে ফাংশন *f* আরও জটিল আকার ধারণ করবে এবং এটি কয়েকটি ধাপে গণনা করা হবে:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

এখানে, &alpha; হলো একটি **নন-লিনিয়ার অ্যাক্টিভেশন ফাংশন**, &sigma; হলো সফটম্যাক্স ফাংশন, এবং প্যারামিটার &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>।

গ্রেডিয়েন্ট ডিসেন্ট অ্যালগরিদম একই থাকবে, তবে গ্রেডিয়েন্ট গণনা করা আরও কঠিন হবে। চেইন ডিফারেনশিয়েশন নিয়ম অনুসারে, আমরা ডেরিভেটিভগুলি নিম্নরূপ গণনা করতে পারি:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ✅ চেইন ডিফারেনশিয়েশন নিয়মটি লস ফাংশনের প্যারামিটারগুলির সাথে সম্পর্কিত ডেরিভেটিভ গণনা করতে ব্যবহৃত হয়।

মনে রাখবেন যে এই সমস্ত অভিব্যক্তির বাম দিকের অংশটি একই, এবং তাই আমরা কার্যকরভাবে লস ফাংশন থেকে শুরু করে "পেছনের দিকে" কম্পিউটেশনাল গ্রাফের মাধ্যমে ডেরিভেটিভগুলি গণনা করতে পারি। তাই মাল্টি-লেয়ার পারসেপট্রন প্রশিক্ষণের পদ্ধতিকে **ব্যাকপ্রোপাগেশন**, বা 'ব্যাকপ্রপ' বলা হয়।

<img alt="কম্পিউট গ্রাফ" src="images/ComputeGraphGrad.png"/>

> TODO: চিত্রের সূত্র

> ✅ আমরা আমাদের নোটবুক উদাহরণে ব্যাকপ্রপ আরও বিস্তারিতভাবে আলোচনা করব।  

## উপসংহার

এই পাঠে, আমরা আমাদের নিজস্ব নিউরাল নেটওয়ার্ক লাইব্রেরি তৈরি করেছি এবং এটি একটি সাধারণ দুই-মাত্রিক শ্রেণীবিন্যাস কাজের জন্য ব্যবহার করেছি।

## 🚀 চ্যালেঞ্জ

সংযুক্ত নোটবুকে, আপনি মাল্টি-লেয়ার পারসেপট্রন তৈরি এবং প্রশিক্ষণের জন্য আপনার নিজস্ব কাঠামো বাস্তবায়ন করবেন। আপনি বিস্তারিতভাবে দেখতে পারবেন কীভাবে আধুনিক নিউরাল নেটওয়ার্ক কাজ করে।

[OwnFramework](OwnFramework.ipynb) নোটবুকে যান এবং এটি সম্পন্ন করুন।

## [পোস্ট-লেকচার কুইজ](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## পর্যালোচনা ও স্ব-অধ্যয়ন

ব্যাকপ্রোপাগেশন AI এবং ML-এ ব্যবহৃত একটি সাধারণ অ্যালগরিদম, যা [বিস্তারিতভাবে অধ্যয়ন](https://wikipedia.org/wiki/Backpropagation) করার মতো।

## [অ্যাসাইনমেন্ট](lab/README.md)

এই ল্যাবে, আপনাকে এই পাঠে তৈরি করা কাঠামো ব্যবহার করে MNIST হাতে লেখা সংখ্যার শ্রেণীবিন্যাস সমাধান করতে বলা হয়েছে।

* [নির্দেশাবলী](lab/README.md)
* [নোটবুক](lab/MyFW_MNIST.ipynb)

---

