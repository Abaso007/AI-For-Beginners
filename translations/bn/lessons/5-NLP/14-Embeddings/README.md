<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-26T08:15:01+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "bn"
}
-->
# এম্বেডিংস

## [পূর্ব-লেকচার কুইজ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

BoW বা TF/IDF ভিত্তিক ক্লাসিফায়ার প্রশিক্ষণ করার সময়, আমরা উচ্চ-মাত্রার ব্যাগ-অফ-ওয়ার্ডস ভেক্টর নিয়ে কাজ করতাম যার দৈর্ঘ্য `vocab_size`, এবং আমরা স্পষ্টভাবে নিম্ন-মাত্রার পজিশনাল রিপ্রেজেন্টেশন ভেক্টর থেকে স্পার্স ওয়ান-হট রিপ্রেজেন্টেশনে রূপান্তর করতাম। তবে, এই ওয়ান-হট রিপ্রেজেন্টেশন মেমোরি-ইফিশিয়েন্ট নয়। এছাড়াও, প্রতিটি শব্দকে একে অপরের থেকে স্বাধীনভাবে বিবেচনা করা হয়, অর্থাৎ ওয়ান-হট এনকোডেড ভেক্টরগুলো শব্দগুলোর মধ্যে কোনো সেমান্টিক সাদৃশ্য প্রকাশ করে না।

**এম্বেডিং** ধারণাটি হলো শব্দগুলোকে নিম্ন-মাত্রার ডেন্স ভেক্টর দিয়ে উপস্থাপন করা, যা কোনোভাবে শব্দের সেমান্টিক অর্থ প্রতিফলিত করে। আমরা পরে আলোচনা করব কীভাবে অর্থপূর্ণ শব্দ এম্বেডিং তৈরি করা যায়, তবে আপাতত এম্বেডিংকে শব্দ ভেক্টরের মাত্রা কমানোর একটি উপায় হিসেবে ভাবুন।

এম্বেডিং লেয়ার একটি শব্দকে ইনপুট হিসেবে গ্রহণ করবে এবং নির্দিষ্ট `embedding_size` এর একটি আউটপুট ভেক্টর তৈরি করবে। এক অর্থে, এটি একটি `Linear` লেয়ারের মতোই, তবে এটি ওয়ান-হট এনকোডেড ভেক্টর গ্রহণ করার পরিবর্তে একটি শব্দ নম্বর ইনপুট হিসেবে নিতে সক্ষম হবে, যা আমাদের বড় ওয়ান-হট-এনকোডেড ভেক্টর তৈরি করা এড়াতে সাহায্য করবে।

আমাদের ক্লাসিফায়ার নেটওয়ার্কে প্রথম লেয়ার হিসেবে একটি এম্বেডিং লেয়ার ব্যবহার করে, আমরা ব্যাগ-অফ-ওয়ার্ডস থেকে **এম্বেডিং ব্যাগ** মডেলে পরিবর্তন করতে পারি, যেখানে আমরা প্রথমে আমাদের টেক্সটের প্রতিটি শব্দকে সংশ্লিষ্ট এম্বেডিংয়ে রূপান্তর করি এবং তারপর সমস্ত এম্বেডিংয়ের উপর একটি অ্যাগ্রিগেট ফাংশন গণনা করি, যেমন `sum`, `average` বা `max`।  

![পাঁচটি সিকোয়েন্স শব্দের জন্য একটি এম্বেডিং ক্লাসিফায়ার দেখানো হয়েছে।](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.bn.png)

> লেখকের তৈরি চিত্র

## ✍️ অনুশীলন: এম্বেডিংস

নিম্নলিখিত নোটবুকগুলোতে আপনার শেখা চালিয়ে যান:
* [PyTorch দিয়ে এম্বেডিংস](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlow দিয়ে এম্বেডিংস](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## সেমান্টিক এম্বেডিংস: Word2Vec

যদিও এম্বেডিং লেয়ার শব্দগুলোকে ভেক্টর রিপ্রেজেন্টেশনে ম্যাপ করতে শিখেছে, তবে এই রিপ্রেজেন্টেশনে খুব বেশি সেমান্টিক অর্থ থাকতে পারে না। এমন একটি ভেক্টর রিপ্রেজেন্টেশন শেখা ভালো হবে যেখানে একই ধরনের শব্দ বা সমার্থক শব্দগুলো এমন ভেক্টর দ্বারা উপস্থাপিত হয় যা কিছু ভেক্টর দূরত্ব (যেমন ইউক্লিডিয়ান দূরত্ব) অনুযায়ী একে অপরের কাছাকাছি থাকে।

এটি করতে, আমাদের একটি বড় টেক্সট সংগ্রহে একটি নির্দিষ্ট উপায়ে আমাদের এম্বেডিং মডেল প্রি-ট্রেন করতে হবে। সেমান্টিক এম্বেডিং প্রশিক্ষণের একটি উপায় হলো [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)। এটি দুটি প্রধান আর্কিটেকচারের উপর ভিত্তি করে তৈরি, যা শব্দগুলোর একটি বিতরণকৃত রিপ্রেজেন্টেশন তৈরি করতে ব্যবহৃত হয়:

 - **কন্টিনিউয়াস ব্যাগ-অফ-ওয়ার্ডস** (CBoW) — এই আর্কিটেকচারে, আমরা মডেলকে আশেপাশের প্রসঙ্গ থেকে একটি শব্দ পূর্বানুমান করতে প্রশিক্ষণ দিই। ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ দেওয়া হলে, মডেলের লক্ষ্য হলো $(W_{-2},W_{-1},W_1,W_2)$ থেকে $W_0$ পূর্বানুমান করা।
 - **কন্টিনিউয়াস স্কিপ-গ্রাম** CBoW এর বিপরীত। মডেল প্রসঙ্গ শব্দের আশেপাশের উইন্ডো ব্যবহার করে বর্তমান শব্দ পূর্বানুমান করে।

CBoW দ্রুততর, তবে স্কিপ-গ্রাম ধীরতর, কিন্তু কম ঘন ঘন ব্যবহৃত শব্দগুলোকে ভালোভাবে উপস্থাপন করে।

![CBoW এবং স্কিপ-গ্রাম অ্যালগরিদমগুলো শব্দগুলোকে ভেক্টরে রূপান্তর করার উদাহরণ দেখানো হয়েছে।](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.bn.png)

> [এই পেপার](https://arxiv.org/pdf/1301.3781.pdf) থেকে নেওয়া চিত্র

Word2Vec প্রি-ট্রেনড এম্বেডিংস (এবং GloVe এর মতো অন্যান্য অনুরূপ মডেল) নিউরাল নেটওয়ার্কে এম্বেডিং লেয়ারের পরিবর্তে ব্যবহার করা যেতে পারে। তবে, আমাদের ভোকাবুলারি নিয়ে কাজ করতে হবে, কারণ Word2Vec/GloVe প্রি-ট্রেন করার সময় ব্যবহৃত ভোকাবুলারি আমাদের টেক্সট কর্পাসের ভোকাবুলারি থেকে আলাদা হতে পারে। এই সমস্যাটি কীভাবে সমাধান করা যায় তা দেখতে উপরের নোটবুকগুলো দেখুন।

## প্রসঙ্গভিত্তিক এম্বেডিংস

প্রথাগত প্রি-ট্রেনড এম্বেডিং রিপ্রেজেন্টেশনের একটি প্রধান সীমাবদ্ধতা হলো শব্দের অর্থ বিভ্রান্তি। যদিও প্রি-ট্রেনড এম্বেডিংস প্রসঙ্গে শব্দগুলোর কিছু অর্থ ধারণ করতে পারে, একটি শব্দের প্রতিটি সম্ভাব্য অর্থ একই এম্বেডিংয়ে এনকোড করা হয়। এটি ডাউনস্ট্রিম মডেলগুলোতে সমস্যা সৃষ্টি করতে পারে, কারণ অনেক শব্দ যেমন 'play' শব্দটি প্রসঙ্গ অনুযায়ী বিভিন্ন অর্থ ধারণ করে।

উদাহরণস্বরূপ, 'play' শব্দটি এই দুটি বাক্যে ভিন্ন অর্থ প্রকাশ করে:

- আমি থিয়েটারে একটি **play** দেখতে গিয়েছিলাম।
- জন তার বন্ধুদের সাথে **play** করতে চায়।

উপরের প্রি-ট্রেনড এম্বেডিংগুলো 'play' শব্দটির এই দুটি অর্থ একই এম্বেডিংয়ে উপস্থাপন করে। এই সীমাবদ্ধতা কাটিয়ে উঠতে, আমাদের **ল্যাঙ্গুয়েজ মডেল** ভিত্তিক এম্বেডিং তৈরি করতে হবে, যা একটি বড় টেক্সট কর্পাসে প্রশিক্ষিত হয় এবং *জানে* কীভাবে শব্দগুলো বিভিন্ন প্রসঙ্গে একসাথে ব্যবহার করা যায়। প্রসঙ্গভিত্তিক এম্বেডিং নিয়ে আলোচনা এই টিউটোরিয়ালের আওতার বাইরে, তবে আমরা কোর্সের পরে ল্যাঙ্গুয়েজ মডেল নিয়ে আলোচনা করার সময় এটির কাছে ফিরে আসব।

## উপসংহার

এই পাঠে, আপনি TensorFlow এবং PyTorch-এ এম্বেডিং লেয়ার তৈরি এবং ব্যবহার করে কীভাবে শব্দগুলোর সেমান্টিক অর্থ আরও ভালোভাবে প্রতিফলিত করা যায় তা শিখেছেন।

## 🚀 চ্যালেঞ্জ

Word2Vec কিছু আকর্ষণীয় অ্যাপ্লিকেশনে ব্যবহৃত হয়েছে, যার মধ্যে রয়েছে গান এবং কবিতা তৈরি করা। [এই আর্টিকেলটি](https://www.politetype.com/blog/word2vec-color-poems) দেখুন, যেখানে লেখক Word2Vec ব্যবহার করে কবিতা তৈরি করার পদ্ধতি ব্যাখ্যা করেছেন। [ড্যান শিফম্যানের এই ভিডিওটি](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) দেখুন, যেখানে এই কৌশলটির একটি ভিন্ন ব্যাখ্যা দেওয়া হয়েছে। তারপর এই কৌশলগুলো আপনার নিজের টেক্সট কর্পাসে প্রয়োগ করার চেষ্টা করুন, যা হয়তো Kaggle থেকে সংগ্রহ করা যেতে পারে।

## [পোস্ট-লেকচার কুইজ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## পর্যালোচনা ও স্ব-অধ্যয়ন

Word2Vec নিয়ে এই পেপারটি পড়ুন: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [অ্যাসাইনমেন্ট: নোটবুক](assignment.md)

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসাধ্য সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। এর মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা দায়বদ্ধ থাকব না।