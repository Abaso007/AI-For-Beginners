{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# টেক্সট শ্রেণীবিন্যাস কাজ\n",
    "\n",
    "যেমনটি আমরা উল্লেখ করেছি, আমরা **AG_NEWS** ডেটাসেটের উপর ভিত্তি করে একটি সহজ টেক্সট শ্রেণীবিন্যাস কাজের উপর মনোযোগ দেব। এই কাজটি হলো সংবাদ শিরোনামগুলোকে চারটি বিভাগে শ্রেণীবদ্ধ করা: বিশ্ব, ক্রীড়া, ব্যবসা এবং বিজ্ঞান/প্রযুক্তি।\n",
    "\n",
    "## ডেটাসেট\n",
    "\n",
    "এই ডেটাসেটটি [`torchtext`](https://github.com/pytorch/text) মডিউলে অন্তর্ভুক্ত, তাই আমরা এটি সহজেই অ্যাক্সেস করতে পারি।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখানে, `train_dataset` এবং `test_dataset` এমন সংগ্রহসমূহ ধারণ করে যা যথাক্রমে শ্রেণীর সংখ্যা (লেবেল) এবং টেক্সটের জোড়া প্রদান করে, উদাহরণস্বরূপ:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "তাহলে, চলুন আমাদের ডেটাসেট থেকে প্রথম ১০টি নতুন শিরোনাম প্রিন্ট করি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "কারণ ডেটাসেটগুলি ইটারেটর, যদি আমরা ডেটা একাধিকবার ব্যবহার করতে চাই তবে আমাদের এটি তালিকায় রূপান্তর করতে হবে:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## টোকেনাইজেশন\n",
    "\n",
    "এখন আমাদের টেক্সটকে **সংখ্যায়** রূপান্তর করতে হবে যা টেনসর হিসেবে উপস্থাপন করা যায়। যদি আমরা শব্দ-স্তরের উপস্থাপনা চাই, তাহলে আমাদের দুটি কাজ করতে হবে:\n",
    "* **টোকেনাইজার** ব্যবহার করে টেক্সটকে **টোকেন** এ বিভক্ত করা\n",
    "* সেই টোকেনগুলোর একটি **ভোকাবুলারি** তৈরি করা।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "শব্দভাণ্ডার ব্যবহার করে, আমরা সহজেই আমাদের টোকেনাইজড স্ট্রিংকে সংখ্যার একটি সেটে এনকোড করতে পারি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## শব্দের ব্যাগ (Bag of Words) টেক্সট উপস্থাপনা\n",
    "\n",
    "কারণ শব্দগুলো অর্থ প্রকাশ করে, কখনও কখনও আমরা শুধুমাত্র পৃথক শব্দগুলো দেখে একটি টেক্সটের অর্থ বুঝতে পারি, বাক্যে তাদের ক্রম না দেখেও। উদাহরণস্বরূপ, সংবাদ শ্রেণীবিভাগ করার সময়, *আবহাওয়া*, *তুষারপাত* এর মতো শব্দগুলো সম্ভবত *আবহাওয়ার পূর্বাভাস* নির্দেশ করবে, যেখানে *শেয়ারবাজার*, *ডলার* এর মতো শব্দগুলো *আর্থিক সংবাদ* এর দিকে ইঙ্গিত করবে।\n",
    "\n",
    "**শব্দের ব্যাগ** (BoW) ভেক্টর উপস্থাপনাটি সবচেয়ে বেশি ব্যবহৃত ঐতিহ্যবাহী ভেক্টর উপস্থাপনাগুলোর একটি। প্রতিটি শব্দ একটি ভেক্টর সূচকের সাথে যুক্ত থাকে, এবং ভেক্টরের উপাদানটি একটি নির্দিষ্ট ডকুমেন্টে সেই শব্দটির উপস্থিতির সংখ্যা ধারণ করে।\n",
    "\n",
    "![একটি শব্দের ব্যাগ ভেক্টর উপস্থাপনাটি মেমোরিতে কীভাবে উপস্থাপিত হয় তা দেখানো একটি চিত্র।](../../../../../translated_images/bag-of-words-example.606fc1738f1d7ba98a9d693e3bcd706c6e83fa7bf8221e6e90d1a206d82f2ea4.bn.png) \n",
    "\n",
    "> **Note**: BoW-কে আপনি টেক্সটের পৃথক শব্দগুলোর জন্য এক-হট-এনকোডেড ভেক্টরগুলোর যোগফল হিসেবেও ভাবতে পারেন।\n",
    "\n",
    "নিচে Scikit Learn পাইথন লাইব্রেরি ব্যবহার করে কীভাবে একটি শব্দের ব্যাগ উপস্থাপনা তৈরি করা যায় তার একটি উদাহরণ দেওয়া হলো:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AG_NEWS ডেটাসেটের ভেক্টর উপস্থাপন থেকে ব্যাগ-অফ-ওয়ার্ডস ভেক্টর গণনা করতে, আমরা নিম্নলিখিত ফাংশনটি ব্যবহার করতে পারি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **নোট:** এখানে আমরা গ্লোবাল `vocab_size` ভেরিয়েবল ব্যবহার করছি শব্দভাণ্ডারের ডিফল্ট আকার নির্ধারণ করতে। যেহেতু প্রায়শই শব্দভাণ্ডারের আকার বেশ বড় হয়, আমরা সবচেয়ে বেশি ব্যবহৃত শব্দগুলোর উপর ভিত্তি করে শব্দভাণ্ডারের আকার সীমিত করতে পারি। `vocab_size` এর মান কমিয়ে কোডটি চালিয়ে দেখুন এটি কীভাবে সঠিকতাকে প্রভাবিত করে। আপনি কিছুটা সঠিকতার হ্রাস আশা করতে পারেন, তবে তা নাটকীয় নয়, বরং উচ্চতর কার্যক্ষমতার বিনিময়ে।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW শ্রেণীবিন্যাসকারী প্রশিক্ষণ\n",
    "\n",
    "এখন যেহেতু আমরা শিখেছি কীভাবে আমাদের টেক্সটের Bag-of-Words উপস্থাপনা তৈরি করতে হয়, চলুন এর উপর একটি শ্রেণীবিন্যাসকারী প্রশিক্ষণ দিই। প্রথমে, আমাদের ডেটাসেটকে এমনভাবে রূপান্তর করতে হবে যাতে সমস্ত পজিশনাল ভেক্টর উপস্থাপনাগুলি Bag-of-Words উপস্থাপনায় রূপান্তরিত হয়। এটি অর্জন করা সম্ভব `bowify` ফাংশনকে স্ট্যান্ডার্ড টর্চ `DataLoader`-এর `collate_fn` প্যারামিটার হিসেবে পাস করার মাধ্যমে:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন চলুন একটি সাধারণ শ্রেণীবিন্যাসকারী নিউরাল নেটওয়ার্ক সংজ্ঞায়িত করি যা একটি লিনিয়ার স্তর ধারণ করে। ইনপুট ভেক্টরের আকার `vocab_size` এর সমান, এবং আউটপুট আকার শ্রেণীর সংখ্যা (৪) এর সাথে সঙ্গতিপূর্ণ। যেহেতু আমরা শ্রেণীবিন্যাস কাজ সমাধান করছি, চূড়ান্ত সক্রিয়করণ ফাংশন হলো `LogSoftmax()`।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা স্ট্যান্ডার্ড PyTorch প্রশিক্ষণ লুপ সংজ্ঞায়িত করব। যেহেতু আমাদের ডেটাসেটটি বেশ বড়, আমাদের শিক্ষার উদ্দেশ্যে আমরা শুধুমাত্র এক এপোকের জন্য প্রশিক্ষণ দেব, এবং কখনও কখনও এক এপোকের চেয়েও কম (প্রশিক্ষণ সীমিত করতে `epoch_size` প্যারামিটার নির্ধারণ করা হয়)। আমরা প্রশিক্ষণের সময় সংগৃহীত প্রশিক্ষণ সঠিকতা রিপোর্ট করব; রিপোর্ট করার ফ্রিকোয়েন্সি `report_freq` প্যারামিটার ব্যবহার করে নির্ধারণ করা হয়।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## বাইগ্রাম, ট্রাইগ্রাম এবং এন-গ্রাম\n",
    "\n",
    "ব্যাগ অফ ওয়ার্ডস পদ্ধতির একটি সীমাবদ্ধতা হলো কিছু শব্দ বহু শব্দের অভিব্যক্তির অংশ হতে পারে। উদাহরণস্বরূপ, 'হট ডগ' শব্দটির অর্থ 'হট' এবং 'ডগ' শব্দগুলোর অন্য প্রসঙ্গে ব্যবহৃত অর্থ থেকে সম্পূর্ণ ভিন্ন। যদি আমরা 'হট' এবং 'ডগ' শব্দগুলোকে সবসময় একই ভেক্টর দিয়ে উপস্থাপন করি, তাহলে এটি আমাদের মডেলকে বিভ্রান্ত করতে পারে।\n",
    "\n",
    "এই সমস্যার সমাধানে, **এন-গ্রাম উপস্থাপনাগুলো** প্রায়ই ডকুমেন্ট শ্রেণীবিন্যাস পদ্ধতিতে ব্যবহৃত হয়, যেখানে প্রতিটি শব্দ, দ্বি-শব্দ বা ত্রি-শব্দের ফ্রিকোয়েন্সি শ্রেণীবিন্যাসকারী প্রশিক্ষণের জন্য একটি কার্যকর বৈশিষ্ট্য। উদাহরণস্বরূপ, বাইগ্রাম উপস্থাপনায়, আমরা মূল শব্দগুলোর পাশাপাশি সমস্ত শব্দ জোড়াকে ভোকাবুলারিতে যোগ করব।\n",
    "\n",
    "নিচে Scikit Learn ব্যবহার করে কীভাবে একটি বাইগ্রাম ব্যাগ অফ ওয়ার্ডস উপস্থাপনা তৈরি করা যায় তার একটি উদাহরণ দেওয়া হলো:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram পদ্ধতির প্রধান অসুবিধা হলো, শব্দভাণ্ডারের আকার অত্যন্ত দ্রুত বৃদ্ধি পেতে শুরু করে। বাস্তবে, আমাদের N-gram উপস্থাপনাকে কিছু মাত্রা হ্রাস করার কৌশলের সাথে সংযুক্ত করতে হয়, যেমন *embeddings*, যা আমরা পরবর্তী ইউনিটে আলোচনা করব।\n",
    "\n",
    "আমাদের **AG News** ডেটাসেটে N-gram উপস্থাপন ব্যবহার করতে হলে, আমাদের একটি বিশেষ ngram শব্দভাণ্ডার তৈরি করতে হবে:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমরা উপরের কোডটি ব্যবহার করে ক্লাসিফায়ার প্রশিক্ষণ দিতে পারতাম, তবে এটি খুবই মেমোরি-অকার্যকর হতো। পরবর্তী ইউনিটে, আমরা এম্বেডিং ব্যবহার করে বিগ্রাম ক্লাসিফায়ার প্রশিক্ষণ দেব।\n",
    "\n",
    "> **Note:** আপনি শুধুমাত্র সেই ngrams রাখতে পারেন যা নির্দিষ্ট সংখ্যার চেয়ে বেশি বার টেক্সটে উপস্থিত হয়। এটি নিশ্চিত করবে যে কম ঘনঘন বিগ্রামগুলো বাদ যাবে এবং ডাইমেনশনালিটি উল্লেখযোগ্যভাবে কমে যাবে। এটি করতে, `min_freq` প্যারামিটারকে একটি উচ্চতর মানে সেট করুন এবং ভোকাবুলারির দৈর্ঘ্যের পরিবর্তন পর্যবেক্ষণ করুন।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## টার্ম ফ্রিকোয়েন্সি ইনভার্স ডকুমেন্ট ফ্রিকোয়েন্সি (TF-IDF)\n",
    "\n",
    "BoW উপস্থাপনায়, শব্দের উপস্থিতি সমানভাবে ওজন দেওয়া হয়, শব্দটি কী তা বিবেচনা না করেই। তবে, এটা স্পষ্ট যে সাধারণ শব্দ যেমন *a*, *in* ইত্যাদি শ্রেণীবিন্যাসের জন্য বিশেষায়িত শব্দগুলোর তুলনায় অনেক কম গুরুত্বপূর্ণ। প্রকৃতপক্ষে, বেশিরভাগ NLP কাজের ক্ষেত্রে কিছু শব্দ অন্যগুলোর তুলনায় বেশি প্রাসঙ্গিক।\n",
    "\n",
    "**TF-IDF** এর পূর্ণরূপ হলো **টার্ম ফ্রিকোয়েন্সি–ইনভার্স ডকুমেন্ট ফ্রিকোয়েন্সি**। এটি ব্যাগ অফ ওয়ার্ডসের একটি পরিবর্তিত রূপ, যেখানে একটি শব্দের উপস্থিতি নির্দেশ করতে 0/1 বাইনারি মানের পরিবর্তে একটি ভাসমান-বিন্দু মান ব্যবহার করা হয়, যা কর্পাসে শব্দের উপস্থিতির ফ্রিকোয়েন্সির সাথে সম্পর্কিত।\n",
    "\n",
    "আরও আনুষ্ঠানিকভাবে, একটি শব্দ $i$ এর ওজন $w_{ij}$ একটি ডকুমেন্ট $j$ এ সংজ্ঞায়িত করা হয়:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "যেখানে\n",
    "* $tf_{ij}$ হলো $i$ শব্দটি $j$ ডকুমেন্টে যতবার উপস্থিত হয়েছে, অর্থাৎ আমরা আগে যে BoW মান দেখেছি\n",
    "* $N$ হলো সংগ্রহে থাকা ডকুমেন্টের সংখ্যা\n",
    "* $df_i$ হলো পুরো সংগ্রহে $i$ শব্দটি উপস্থিত থাকা ডকুমেন্টের সংখ্যা\n",
    "\n",
    "TF-IDF মান $w_{ij}$ একটি ডকুমেন্টে শব্দটি যতবার উপস্থিত হয় তার সাথে আনুপাতিকভাবে বৃদ্ধি পায় এবং কর্পাসে শব্দটি উপস্থিত থাকা ডকুমেন্টের সংখ্যার দ্বারা সামঞ্জস্য করা হয়, যা এই বিষয়টি সমাধান করতে সাহায্য করে যে কিছু শব্দ অন্যগুলোর তুলনায় বেশি ঘন ঘন উপস্থিত হয়। উদাহরণস্বরূপ, যদি একটি শব্দ *প্রতিটি* ডকুমেন্টে উপস্থিত থাকে, $df_i=N$, এবং $w_{ij}=0$, এবং সেই শব্দগুলো সম্পূর্ণভাবে উপেক্ষা করা হবে।\n",
    "\n",
    "আপনি সহজেই Scikit Learn ব্যবহার করে টেক্সটের TF-IDF ভেক্টরাইজেশন তৈরি করতে পারেন:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## উপসংহার\n",
    "\n",
    "যদিও TF-IDF উপস্থাপনাগুলি বিভিন্ন শব্দের জন্য ফ্রিকোয়েন্সি ওজন প্রদান করে, সেগুলি অর্থ বা ক্রম উপস্থাপন করতে অক্ষম। ১৯৩৫ সালে বিখ্যাত ভাষাবিদ জে. আর. ফার্থ বলেছিলেন, \"একটি শব্দের সম্পূর্ণ অর্থ সর্বদা প্রাসঙ্গিক, এবং প্রসঙ্গ ছাড়া অর্থের কোনো অধ্যয়নকে গুরুত্ব সহকারে নেওয়া যায় না।\" আমরা এই কোর্সে পরে শিখব কীভাবে ভাষা মডেলিং ব্যবহার করে পাঠ্য থেকে প্রাসঙ্গিক তথ্য ধারণ করা যায়।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**অস্বীকৃতি**:  \nএই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা তার জন্য দায়ী থাকব না।\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-28T12:30:59+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "bn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}