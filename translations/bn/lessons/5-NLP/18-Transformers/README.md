<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-26T08:35:19+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "bn"
}
-->
# অ্যাটেনশন মেকানিজম এবং ট্রান্সফর্মার

## [পূর্ব-লেকচার কুইজ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

NLP ডোমেইনের অন্যতম গুরুত্বপূর্ণ সমস্যা হলো **মেশিন অনুবাদ**, যা Google Translate-এর মতো টুলগুলোর ভিত্তি। এই সেকশনে, আমরা মেশিন অনুবাদ বা আরও সাধারণভাবে, *সিকোয়েন্স-টু-সিকোয়েন্স* টাস্ক (যাকে **সেন্টেন্স ট্রান্সডাকশন**ও বলা হয়) নিয়ে আলোচনা করব।

RNN ব্যবহার করে, সিকোয়েন্স-টু-সিকোয়েন্স দুটি রিকরেন্ট নেটওয়ার্কের মাধ্যমে বাস্তবায়িত হয়, যেখানে একটি নেটওয়ার্ক, **এনকোডার**, ইনপুট সিকোয়েন্সকে একটি হিডেন স্টেটে সংকুচিত করে, এবং অন্য নেটওয়ার্ক, **ডিকোডার**, এই হিডেন স্টেটকে একটি অনুবাদিত ফলাফলে রূপান্তরিত করে। এই পদ্ধতির কিছু সমস্যা রয়েছে:

* এনকোডার নেটওয়ার্কের চূড়ান্ত স্টেট একটি বাক্যের শুরু মনে রাখতে অসুবিধা করে, যার ফলে দীর্ঘ বাক্যের জন্য মডেলের গুণমান খারাপ হয়।
* একটি সিকোয়েন্সের সমস্ত শব্দের ফলাফলের উপর একই প্রভাব থাকে। তবে বাস্তবে, ইনপুট সিকোয়েন্সের নির্দিষ্ট শব্দগুলো সিকোয়েন্সিয়াল আউটপুটে অন্যদের তুলনায় বেশি প্রভাব ফেলে।

**অ্যাটেনশন মেকানিজম** প্রতিটি ইনপুট ভেক্টরের প্রাসঙ্গিক প্রভাবকে প্রতিটি আউটপুট প্রেডিকশনে ওজন দেওয়ার একটি উপায় প্রদান করে। এটি বাস্তবায়িত হয় ইনপুট RNN এবং আউটপুট RNN-এর মধ্যবর্তী স্টেটগুলোর মধ্যে শর্টকাট তৈরি করে। এইভাবে, যখন আউটপুট প্রতীক y<sub>t</sub> তৈরি করা হয়, তখন আমরা সমস্ত ইনপুট হিডেন স্টেট h<sub>i</sub> বিবেচনা করব, বিভিন্ন ওজন সহগ α<sub>t,i</sub> সহ।

![এনকোডার/ডিকোডার মডেল একটি অ্যাডিটিভ অ্যাটেনশন লেয়ার সহ](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.bn.png)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf)-এ অ্যাডিটিভ অ্যাটেনশন মেকানিজম সহ এনকোডার-ডিকোডার মডেল, [এই ব্লগ পোস্ট](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) থেকে উদ্ধৃত।

অ্যাটেনশন ম্যাট্রিক্স {α<sub>i,j</sub>} একটি আউটপুট সিকোয়েন্সে একটি নির্দিষ্ট শব্দ তৈরিতে ইনপুট শব্দগুলোর ভূমিকা কতটা তা উপস্থাপন করবে। নিচে এমন একটি ম্যাট্রিক্সের উদাহরণ দেওয়া হয়েছে:

![Bahdanau - arviz.org থেকে নেওয়া RNNsearch-50 দ্বারা পাওয়া একটি নমুনা অ্যালাইনমেন্ট](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.bn.png)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) থেকে চিত্র।

অ্যাটেনশন মেকানিজম NLP-তে বর্তমান বা প্রায় বর্তমান স্টেট-অফ-দ্য-আর্টের জন্য দায়ী। তবে অ্যাটেনশন যোগ করার ফলে মডেলের প্যারামিটারের সংখ্যা উল্লেখযোগ্যভাবে বৃদ্ধি পায়, যা RNN-এর স্কেলিং সমস্যার দিকে নিয়ে যায়। RNN স্কেল করার একটি প্রধান সীমাবদ্ধতা হলো মডেলের পুনরাবৃত্তিমূলক প্রকৃতি, যা ব্যাচ এবং ট্রেনিং প্যারালালাইজ করা কঠিন করে তোলে। RNN-এ একটি সিকোয়েন্সের প্রতিটি উপাদানকে ক্রমানুসারে প্রক্রিয়া করতে হয়, যার অর্থ এটি সহজে প্যারালালাইজ করা যায় না।

![অ্যাটেনশন সহ এনকোডার ডিকোডার](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> [গুগলের ব্লগ](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) থেকে চিত্র।

অ্যাটেনশন মেকানিজমের গ্রহণযোগ্যতা এবং এই সীমাবদ্ধতার সংমিশ্রণ বর্তমান স্টেট-অফ-দ্য-আর্ট ট্রান্সফর্মার মডেলগুলোর সৃষ্টি করেছে, যেমন BERT থেকে Open-GPT3।

## ট্রান্সফর্মার মডেল

ট্রান্সফর্মারের পিছনের প্রধান ধারণাগুলোর একটি হলো RNN-এর ক্রমানুসার প্রকৃতি এড়ানো এবং ট্রেনিংয়ের সময় প্যারালালাইজযোগ্য একটি মডেল তৈরি করা। এটি দুটি ধারণা বাস্তবায়নের মাধ্যমে অর্জিত হয়:

* পজিশনাল এনকোডিং
* RNN (বা CNN)-এর পরিবর্তে প্যাটার্ন ক্যাপচার করতে **সেলফ-অ্যাটেনশন মেকানিজম** ব্যবহার করা (এই কারণেই ট্রান্সফর্মার পরিচিতি পেপারটির নাম *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### পজিশনাল এনকোডিং/এম্বেডিং

পজিশনাল এনকোডিংয়ের ধারণাটি নিম্নরূপ:
1. RNN ব্যবহার করার সময়, টোকেনগুলোর আপেক্ষিক অবস্থান স্টেপের সংখ্যা দ্বারা উপস্থাপিত হয়, এবং তাই এটি স্পষ্টভাবে উপস্থাপন করার প্রয়োজন হয় না।
2. তবে, একবার আমরা অ্যাটেনশন ব্যবহার শুরু করলে, আমাদের সিকোয়েন্সের মধ্যে টোকেনগুলোর আপেক্ষিক অবস্থান জানতে হবে।
3. পজিশনাল এনকোডিং পেতে, আমরা আমাদের টোকেন সিকোয়েন্সকে সিকোয়েন্সে টোকেন পজিশনের একটি সিকোয়েন্স (যেমন, সংখ্যা 0,1, ...) দিয়ে বৃদ্ধি করি।
4. আমরা টোকেন পজিশনকে একটি টোকেন এম্বেডিং ভেক্টরের সাথে মিশ্রিত করি। পজিশন (ইন্টিজার)কে একটি ভেক্টরে রূপান্তর করতে, আমরা বিভিন্ন পদ্ধতি ব্যবহার করতে পারি:

* টোকেন এম্বেডিংয়ের মতো ট্রেনযোগ্য এম্বেডিং। এটি এখানে বিবেচিত পদ্ধতি। আমরা টোকেন এবং তাদের পজিশনের উপর এম্বেডিং লেয়ার প্রয়োগ করি, যার ফলে একই মাত্রার এম্বেডিং ভেক্টর তৈরি হয়, যা আমরা একসাথে যোগ করি।
* মূল পেপারে প্রস্তাবিত স্থির পজিশন এনকোডিং ফাংশন।

<img src="images/pos-embedding.png" width="50%"/>

> লেখকের তৈরি চিত্র।

পজিশনাল এম্বেডিংয়ের মাধ্যমে আমরা মূল টোকেন এবং তার সিকোয়েন্সের মধ্যে অবস্থান উভয়কেই এম্বেড করি।

### মাল্টি-হেড সেলফ-অ্যাটেনশন

এরপর, আমাদের সিকোয়েন্সের মধ্যে কিছু প্যাটার্ন ক্যাপচার করতে হবে। এটি করতে, ট্রান্সফর্মার **সেলফ-অ্যাটেনশন** মেকানিজম ব্যবহার করে, যা মূলত ইনপুট এবং আউটপুট হিসেবে একই সিকোয়েন্সে অ্যাটেনশন প্রয়োগ। সেলফ-অ্যাটেনশন প্রয়োগ করে আমরা বাক্যের মধ্যে **প্রসঙ্গ** বিবেচনা করতে পারি এবং দেখতে পারি কোন শব্দগুলো আন্তঃসম্পর্কিত। উদাহরণস্বরূপ, এটি আমাদের দেখতে দেয় কোন শব্দগুলো *it* দ্বারা উল্লেখ করা হয়েছে এবং প্রসঙ্গ বিবেচনা করতে দেয়:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.bn.png)

> [গুগলের ব্লগ](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html) থেকে চিত্র।

ট্রান্সফর্মারে, আমরা **মাল্টি-হেড অ্যাটেনশন** ব্যবহার করি যাতে নেটওয়ার্ক বিভিন্ন ধরনের নির্ভরতা ক্যাপচার করতে পারে, যেমন দীর্ঘমেয়াদী বনাম স্বল্পমেয়াদী শব্দ সম্পর্ক, কো-রেফারেন্স বনাম অন্য কিছু ইত্যাদি।

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)-এ ট্রান্সফর্মার লেয়ারের বাস্তবায়ন সম্পর্কে আরও বিস্তারিত রয়েছে।

### এনকোডার-ডিকোডার অ্যাটেনশন

ট্রান্সফর্মারে, অ্যাটেনশন দুটি স্থানে ব্যবহৃত হয়:

* ইনপুট টেক্সটের মধ্যে প্যাটার্ন ক্যাপচার করতে সেলফ-অ্যাটেনশন ব্যবহার করে।
* সিকোয়েন্স অনুবাদ করতে - এটি এনকোডার এবং ডিকোডারের মধ্যে অ্যাটেনশন লেয়ার।

এনকোডার-ডিকোডার অ্যাটেনশন RNN-এ ব্যবহৃত অ্যাটেনশন মেকানিজমের মতোই, যা এই সেকশনের শুরুতে বর্ণনা করা হয়েছে। এই অ্যানিমেটেড ডায়াগ্রামটি এনকোডার-ডিকোডার অ্যাটেনশনের ভূমিকা ব্যাখ্যা করে।

![অ্যানিমেটেড GIF ট্রান্সফর্মার মডেলে মূল্যায়ন কীভাবে সম্পন্ন হয় তা দেখাচ্ছে।](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

যেহেতু প্রতিটি ইনপুট অবস্থান স্বাধীনভাবে প্রতিটি আউটপুট অবস্থানে ম্যাপ করা হয়, ট্রান্সফর্মার RNN-এর তুলনায় ভালোভাবে প্যারালালাইজ করতে পারে, যা অনেক বড় এবং আরও এক্সপ্রেসিভ ভাষার মডেল সক্ষম করে। প্রতিটি অ্যাটেনশন হেড শব্দগুলোর মধ্যে বিভিন্ন সম্পর্ক শেখার জন্য ব্যবহার করা যেতে পারে, যা ডাউনস্ট্রিম NLP টাস্ক উন্নত করে।

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) একটি খুব বড় মাল্টি লেয়ার ট্রান্সফর্মার নেটওয়ার্ক, যেখানে *BERT-base*-এর জন্য 12 লেয়ার এবং *BERT-large*-এর জন্য 24 লেয়ার রয়েছে। মডেলটি প্রথমে একটি বড় টেক্সট ডেটা কর্পাস (উইকিপিডিয়া + বই) ব্যবহার করে আনসুপারভাইজড ট্রেনিং (একটি বাক্যে মাস্ক করা শব্দগুলো প্রেডিক্ট করা) দিয়ে প্রি-ট্রেন করা হয়। প্রি-ট্রেনিংয়ের সময় মডেল উল্লেখযোগ্য স্তরের ভাষা বোঝার ক্ষমতা অর্জন করে, যা পরে অন্যান্য ডেটাসেটের সাথে ফাইন টিউনিংয়ের মাধ্যমে ব্যবহার করা যায়। এই প্রক্রিয়াকে **ট্রান্সফার লার্নিং** বলা হয়।

![http://jalammar.github.io/illustrated-bert/ থেকে ছবি](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.bn.png)

> [উৎস](http://jalammar.github.io/illustrated-bert/)

## ✍️ অনুশীলন: ট্রান্সফর্মার

নিম্নলিখিত নোটবুকে আপনার শেখা চালিয়ে যান:

* [PyTorch-এ ট্রান্সফর্মার](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [TensorFlow-এ ট্রান্সফর্মার](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## উপসংহার

এই পাঠে আপনি ট্রান্সফর্মার এবং অ্যাটেনশন মেকানিজম সম্পর্কে শিখেছেন, যা NLP টুলবক্সের অপরিহার্য উপকরণ। ট্রান্সফর্মার আর্কিটেকচারের অনেক বৈচিত্র্য রয়েছে, যার মধ্যে BERT, DistilBERT, BigBird, OpenGPT3 এবং আরও অনেক কিছু রয়েছে, যা ফাইন টিউন করা যায়। [HuggingFace প্যাকেজ](https://github.com/huggingface/) PyTorch এবং TensorFlow উভয়ের সাথে এই আর্কিটেকচারগুলো ট্রেনিংয়ের জন্য একটি রিপোজিটরি প্রদান করে।

## 🚀 চ্যালেঞ্জ

## [পোস্ট-লেকচার কুইজ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## পর্যালোচনা এবং স্ব-অধ্যয়ন

* [ব্লগ পোস্ট](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), যা ট্রান্সফর্মার সম্পর্কিত ক্লাসিক্যাল [Attention is all you need](https://arxiv.org/abs/1706.03762) পেপারটি ব্যাখ্যা করে।
* [ব্লগ পোস্টের একটি সিরিজ](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452), যা ট্রান্সফর্মার আর্কিটেকচার বিস্তারিতভাবে ব্যাখ্যা করে।

## [অ্যাসাইনমেন্ট](assignment.md)

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। এর মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা তার জন্য দায়ী থাকব না।