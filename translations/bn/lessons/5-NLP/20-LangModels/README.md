<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T07:53:25+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "bn"
}
-->
# প্রি-ট্রেইনড লার্জ ল্যাঙ্গুয়েজ মডেলস

আমাদের পূর্ববর্তী সমস্ত কাজগুলোতে, আমরা একটি নির্দিষ্ট কাজ সম্পাদনের জন্য লেবেলড ডেটাসেট ব্যবহার করে একটি নিউরাল নেটওয়ার্ক প্রশিক্ষণ করতাম। বড় ট্রান্সফরমার মডেলগুলোর ক্ষেত্রে, যেমন BERT, আমরা সেল্ফ-সুপারভাইজড পদ্ধতিতে ল্যাঙ্গুয়েজ মডেলিং ব্যবহার করে একটি ভাষার মডেল তৈরি করি, যা পরে নির্দিষ্ট ডাউনস্ট্রিম কাজের জন্য আরও ডোমেইন-নির্দিষ্ট প্রশিক্ষণের মাধ্যমে বিশেষায়িত হয়। তবে, এটি প্রমাণিত হয়েছে যে বড় ভাষার মডেলগুলো কোনো ডোমেইন-নির্দিষ্ট প্রশিক্ষণ ছাড়াই অনেক কাজ সমাধান করতে পারে। এই ধরনের মডেলের একটি পরিবারকে **GPT** (Generative Pre-Trained Transformer) বলা হয়।

## [পূর্ব-লেকচার কুইজ](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## টেক্সট জেনারেশন এবং পারপ্লেক্সিটি

ডাউনস্ট্রিম প্রশিক্ষণ ছাড়াই একটি নিউরাল নেটওয়ার্ক সাধারণ কাজ করতে পারে এই ধারণাটি [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) পেপারে উপস্থাপন করা হয়েছে। মূল ধারণাটি হলো অনেক কাজকে **টেক্সট জেনারেশন** ব্যবহার করে মডেল করা যায়, কারণ টেক্সট বোঝা মানে সেটি তৈরি করতে পারা। যেহেতু মডেলটি বিশাল পরিমাণ টেক্সটের উপর প্রশিক্ষিত যা মানব জ্ঞানকে অন্তর্ভুক্ত করে, এটি বিভিন্ন বিষয়ে জ্ঞানী হয়ে ওঠে।

> টেক্সট বোঝা এবং তৈরি করতে পারা মানে হলো আমাদের চারপাশের পৃথিবী সম্পর্কে কিছু জানা। মানুষও অনেকাংশে পড়ার মাধ্যমে শেখে, এবং GPT নেটওয়ার্ক এই দিক থেকে অনুরূপ।

টেক্সট জেনারেশন নেটওয়ার্কগুলো পরবর্তী শব্দের সম্ভাবনা $$P(w_N)$$ পূর্বাভাস করে কাজ করে। তবে, পরবর্তী শব্দের শর্তহীন সম্ভাবনা টেক্সট কর্পাসে এই শব্দটির ফ্রিকোয়েন্সির সমান। GPT আমাদেরকে পূর্ববর্তী শব্দগুলোর ভিত্তিতে পরবর্তী শব্দের **শর্তাধীন সম্ভাবনা** দিতে সক্ষম: $$P(w_N | w_{n-1}, ..., w_0)$$

> সম্ভাবনা সম্পর্কে আরও জানতে, আমাদের [ডেটা সায়েন্স ফর বিগিনার্স কারিকুলাম](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) পড়ুন।

ভাষা তৈরি মডেলের গুণমান **পারপ্লেক্সিটি** ব্যবহার করে সংজ্ঞায়িত করা যায়। এটি একটি অন্তর্নিহিত মেট্রিক যা আমাদেরকে কোনো কাজ-নির্দিষ্ট ডেটাসেট ছাড়াই মডেলের গুণমান পরিমাপ করতে দেয়। এটি *একটি বাক্যের সম্ভাবনা* ধারণার উপর ভিত্তি করে - মডেলটি এমন বাক্যকে উচ্চ সম্ভাবনা দেয় যা বাস্তব হওয়ার সম্ভাবনা বেশি (অর্থাৎ মডেলটি এতে **বিভ্রান্ত** হয় না), এবং কম সম্ভাবনা দেয় এমন বাক্যগুলোকে যা কম অর্থবহ (যেমন *Can it does what?*)। যখন আমরা আমাদের মডেলকে বাস্তব টেক্সট কর্পাস থেকে বাক্য দিই, আমরা আশা করি সেগুলোর সম্ভাবনা বেশি হবে এবং **পারপ্লেক্সিটি** কম হবে। গাণিতিকভাবে, এটি পরীক্ষার সেটের স্বাভাবিককৃত বিপরীত সম্ভাবনা হিসাবে সংজ্ঞায়িত:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**আপনি [Hugging Face-এর GPT-চালিত টেক্সট এডিটর](https://transformer.huggingface.co/doc/gpt2-large)** ব্যবহার করে টেক্সট জেনারেশনের সাথে পরীক্ষা করতে পারেন। এই এডিটরে, আপনি আপনার টেক্সট লেখা শুরু করবেন, এবং **[TAB]** চাপলে আপনাকে কয়েকটি সম্পূর্ণকরণের বিকল্প দেওয়া হবে। যদি সেগুলো খুব ছোট হয়, বা আপনি সেগুলোতে সন্তুষ্ট না হন - [TAB] আবার চাপুন, এবং আপনি আরও বিকল্প পাবেন, যার মধ্যে দীর্ঘ টেক্সটও থাকতে পারে।

## GPT একটি পরিবার

GPT একটি একক মডেল নয়, বরং এটি [OpenAI](https://openai.com) দ্বারা উন্নত এবং প্রশিক্ষিত মডেলগুলোর একটি সংগ্রহ।

GPT মডেলগুলোর মধ্যে রয়েছে:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| ভাষার মডেল যার ১.৫ বিলিয়ন প্যারামিটার পর্যন্ত রয়েছে। | ভাষার মডেল যার ১৭৫ বিলিয়ন প্যারামিটার পর্যন্ত রয়েছে। | ১০০ ট্রিলিয়ন প্যারামিটার এবং এটি টেক্সট ও ইমেজ ইনপুট গ্রহণ করে এবং টেক্সট আউটপুট দেয়। |

GPT-3 এবং GPT-4 মডেলগুলো [Microsoft Azure-এর কগনিটিভ সার্ভিস](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) এবং [OpenAI API](https://openai.com/api/) হিসেবে উপলব্ধ।

## প্রম্পট ইঞ্জিনিয়ারিং

GPT বিশাল পরিমাণ ডেটার উপর প্রশিক্ষিত হয়েছে ভাষা এবং কোড বোঝার জন্য, তাই এটি ইনপুট (প্রম্পট) এর প্রতিক্রিয়ায় আউটপুট প্রদান করে। প্রম্পট হলো GPT-তে ইনপুট বা প্রশ্ন, যেখানে মডেলকে একটি কাজ সম্পন্ন করার জন্য নির্দেশনা দেওয়া হয়। কাঙ্ক্ষিত ফলাফল পেতে, সবচেয়ে কার্যকর প্রম্পট প্রয়োজন, যা সঠিক শব্দ, ফরম্যাট, বাক্যাংশ বা এমনকি প্রতীক নির্বাচন করার মাধ্যমে তৈরি হয়। এই পদ্ধতিকে [প্রম্পট ইঞ্জিনিয়ারিং](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) বলা হয়।

[এই ডকুমেন্টেশন](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) আপনাকে প্রম্পট ইঞ্জিনিয়ারিং সম্পর্কে আরও তথ্য প্রদান করে।

## ✍️ উদাহরণ নোটবুক: [OpenAI-GPT নিয়ে খেলা](GPT-PyTorch.ipynb)

নিম্নলিখিত নোটবুকগুলোতে আপনার শেখা চালিয়ে যান:

* [OpenAI-GPT এবং Hugging Face Transformers দিয়ে টেক্সট তৈরি](GPT-PyTorch.ipynb)

## উপসংহার

নতুন সাধারণ প্রি-ট্রেইনড ভাষার মডেলগুলো কেবল ভাষার গঠন মডেল করে না, বরং এতে বিশাল পরিমাণ প্রাকৃতিক ভাষার জ্ঞানও অন্তর্ভুক্ত থাকে। সুতরাং, এগুলো জিরো-শট বা ফিউ-শট সেটিংসে কিছু NLP কাজ সমাধানে কার্যকরভাবে ব্যবহার করা যেতে পারে।

## [পোস্ট-লেকচার কুইজ](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

