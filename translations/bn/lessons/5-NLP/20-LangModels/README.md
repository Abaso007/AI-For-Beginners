<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-26T08:42:59+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "bn"
}
-->
# প্রি-ট্রেইনড বড় ভাষার মডেল

আমাদের পূর্ববর্তী সমস্ত কাজগুলোতে, আমরা একটি নির্দিষ্ট কাজ সম্পাদনের জন্য লেবেলযুক্ত ডেটাসেট ব্যবহার করে একটি নিউরাল নেটওয়ার্ক প্রশিক্ষণ করছিলাম। বড় ট্রান্সফর্মার মডেলগুলোর ক্ষেত্রে, যেমন BERT, আমরা স্ব-পরিচালিত পদ্ধতিতে ভাষা মডেলিং ব্যবহার করে একটি ভাষার মডেল তৈরি করি, যা পরে নির্দিষ্ট ডাউনস্ট্রিম কাজের জন্য আরও ডোমেইন-নির্দিষ্ট প্রশিক্ষণের মাধ্যমে বিশেষায়িত হয়। তবে, এটি প্রমাণিত হয়েছে যে বড় ভাষার মডেলগুলো অনেক কাজ সম্পাদন করতে পারে কোনো ডোমেইন-নির্দিষ্ট প্রশিক্ষণ ছাড়াই। এমন মডেলের একটি পরিবারকে বলা হয় **GPT**: জেনারেটিভ প্রি-ট্রেইনড ট্রান্সফর্মার।

## [পূর্ব-লেকচার কুইজ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/120)

## টেক্সট জেনারেশন এবং পারপ্লেক্সিটি

একটি নিউরাল নেটওয়ার্ক ডাউনস্ট্রিম প্রশিক্ষণ ছাড়াই সাধারণ কাজ করতে পারে এই ধারণাটি [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) পেপারে উপস্থাপন করা হয়েছে। মূল ধারণাটি হলো অনেক অন্যান্য কাজকে **টেক্সট জেনারেশন** ব্যবহার করে মডেল করা যেতে পারে, কারণ টেক্সট বোঝা মূলত এটি তৈরি করতে সক্ষম হওয়া। যেহেতু মডেলটি বিশাল পরিমাণ টেক্সটের উপর প্রশিক্ষিত যা মানব জ্ঞানকে অন্তর্ভুক্ত করে, এটি বিভিন্ন বিষয়ে জ্ঞানী হয়ে ওঠে।

> টেক্সট বোঝা এবং তৈরি করতে সক্ষম হওয়া মানে আমাদের চারপাশের পৃথিবী সম্পর্কে কিছু জানাও। মানুষও অনেকাংশে পড়ার মাধ্যমে শেখে, এবং GPT নেটওয়ার্ক এই দিক থেকে অনুরূপ।

টেক্সট জেনারেশন নেটওয়ার্কগুলো কাজ করে পরবর্তী শব্দের সম্ভাবনা $$P(w_N)$$ পূর্বাভাস দিয়ে। তবে, পরবর্তী শব্দের শর্তহীন সম্ভাবনা টেক্সট কর্পাসে এই শব্দের ফ্রিকোয়েন্সির সমান। GPT আমাদেরকে পূর্ববর্তী শব্দগুলো দেওয়া অবস্থায় পরবর্তী শব্দের **শর্তযুক্ত সম্ভাবনা** প্রদান করতে সক্ষম: $$P(w_N | w_{n-1}, ..., w_0)$$

> আপনি আমাদের [ডেটা সায়েন্স ফর বিগিনারস কারিকুলাম](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) এ সম্ভাবনা সম্পর্কে আরও পড়তে পারেন।

ভাষা তৈরি মডেলের গুণমান **পারপ্লেক্সিটি** ব্যবহার করে সংজ্ঞায়িত করা যেতে পারে। এটি একটি অন্তর্নিহিত মেট্রিক যা আমাদেরকে কোনো কাজ-নির্দিষ্ট ডেটাসেট ছাড়াই মডেলের গুণমান পরিমাপ করতে দেয়। এটি *একটি বাক্যের সম্ভাবনা* ধারণার উপর ভিত্তি করে - মডেলটি একটি বাক্যকে উচ্চ সম্ভাবনা প্রদান করে যা বাস্তব হওয়ার সম্ভাবনা বেশি (অর্থাৎ মডেলটি এটি দ্বারা **বিভ্রান্ত** হয় না), এবং কম সম্ভাবনা প্রদান করে এমন বাক্যগুলোকে যা কম অর্থপূর্ণ (যেমন *Can it does what?*)। যখন আমরা আমাদের মডেলকে বাস্তব টেক্সট কর্পাস থেকে বাক্য প্রদান করি, আমরা আশা করি সেগুলো উচ্চ সম্ভাবনা এবং কম **পারপ্লেক্সিটি** থাকবে। গাণিতিকভাবে, এটি পরীক্ষার সেটের স্বাভাবিককৃত বিপরীত সম্ভাবনা হিসাবে সংজ্ঞায়িত করা হয়:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**আপনি [Hugging Face-এর GPT-চালিত টেক্সট এডিটর](https://transformer.huggingface.co/doc/gpt2-large)** ব্যবহার করে টেক্সট জেনারেশনের সাথে পরীক্ষা করতে পারেন। এই এডিটরে, আপনি আপনার টেক্সট লিখতে শুরু করেন এবং **[TAB]** চাপলে আপনাকে কয়েকটি সম্পূর্ণকরণ বিকল্প দেওয়া হবে। যদি সেগুলো খুব ছোট হয়, বা আপনি সেগুলোতে সন্তুষ্ট না হন - [TAB] আবার চাপুন, এবং আপনি আরও বিকল্প পাবেন, যার মধ্যে দীর্ঘ টেক্সটও অন্তর্ভুক্ত।

## GPT একটি পরিবার

GPT একটি একক মডেল নয়, বরং [OpenAI](https://openai.com) দ্বারা উন্নত এবং প্রশিক্ষিত মডেলের একটি সংগ্রহ।

GPT মডেলগুলোর মধ্যে রয়েছে:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| ভাষা মডেল যার ১.৫ বিলিয়ন পর্যন্ত প্যারামিটার রয়েছে। | ভাষা মডেল যার ১৭৫ বিলিয়ন পর্যন্ত প্যারামিটার রয়েছে। | ১০০ ট্রিলিয়ন প্যারামিটার এবং এটি উভয় ইমেজ এবং টেক্সট ইনপুট গ্রহণ করে এবং টেক্সট আউটপুট প্রদান করে। |

GPT-3 এবং GPT-4 মডেলগুলো [Microsoft Azure থেকে একটি কগনিটিভ সার্ভিস](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) এবং [OpenAI API](https://openai.com/api/) হিসেবে উপলব্ধ।

## প্রম্পট ইঞ্জিনিয়ারিং

GPT বিশাল পরিমাণ ডেটার উপর প্রশিক্ষিত হয়েছে ভাষা এবং কোড বুঝতে, তাই তারা ইনপুট (প্রম্পট) এর প্রতিক্রিয়ায় আউটপুট প্রদান করে। প্রম্পট হলো GPT ইনপুট বা প্রশ্ন যেখানে একজন মডেলগুলোকে নির্দিষ্ট কাজ সম্পাদনের জন্য নির্দেশনা প্রদান করে। কাঙ্ক্ষিত ফলাফল পেতে, সবচেয়ে কার্যকর প্রম্পট প্রয়োজন যা সঠিক শব্দ, ফরম্যাট, বাক্যাংশ বা এমনকি প্রতীক নির্বাচন জড়িত। এই পদ্ধতিকে বলা হয় [প্রম্পট ইঞ্জিনিয়ারিং](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum)।

[এই ডকুমেন্টেশন](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) আপনাকে প্রম্পট ইঞ্জিনিয়ারিং সম্পর্কে আরও তথ্য প্রদান করে।

## ✍️ উদাহরণ নোটবুক: [OpenAI-GPT নিয়ে খেলা](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

নিম্নলিখিত নোটবুকগুলোতে আপনার শেখা চালিয়ে যান:

* [OpenAI-GPT এবং Hugging Face Transformers ব্যবহার করে টেক্সট তৈরি করা](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## উপসংহার

নতুন সাধারণ প্রি-ট্রেইনড ভাষার মডেলগুলো শুধুমাত্র ভাষার গঠন মডেল করে না, বরং বিশাল পরিমাণ প্রাকৃতিক ভাষাও ধারণ করে। সুতরাং, সেগুলো কিছু NLP কাজকে জিরো-শট বা ফিউ-শট সেটিংসে কার্যকরভাবে সমাধান করতে ব্যবহার করা যেতে পারে।

## [পোস্ট-লেকচার কুইজ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/220)

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসাধ্য সঠিক অনুবাদের চেষ্টা করি, তবে দয়া করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। নথিটির মূল ভাষায় থাকা সংস্করণটিকেই প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।