<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ae074cd940fc2f4dc24fc07b66ccbd99",
  "translation_date": "2025-08-26T09:47:53+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md",
  "language_code": "bn"
}
-->
# ডিপ লার্নিং ট্রেনিং কৌশল

যেহেতু নিউরাল নেটওয়ার্কগুলো আরও গভীর হচ্ছে, সেগুলোর প্রশিক্ষণ প্রক্রিয়া ক্রমশ চ্যালেঞ্জিং হয়ে উঠছে। একটি বড় সমস্যা হলো তথাকথিত [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) বা [exploding gradients](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.)। [এই পোস্টটি](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) এই সমস্যাগুলোর একটি ভালো পরিচিতি প্রদান করে।

গভীর নেটওয়ার্কের প্রশিক্ষণ আরও কার্যকর করতে, কিছু কৌশল ব্যবহার করা যেতে পারে।

## মানগুলোকে যুক্তিসঙ্গত সীমার মধ্যে রাখা

সংখ্যাগত গণনাগুলো আরও স্থিতিশীল করতে, আমরা নিশ্চিত করতে চাই যে আমাদের নিউরাল নেটওয়ার্কের সব মান যুক্তিসঙ্গত স্কেলে রয়েছে, সাধারণত [-1..1] বা [0..1]। এটি খুব কঠোর প্রয়োজনীয়তা নয়, তবে ফ্লোটিং পয়েন্ট গণনার প্রকৃতি এমন যে বিভিন্ন মাত্রার মানগুলোকে একসাথে সঠিকভাবে পরিচালনা করা যায় না। উদাহরণস্বরূপ, যদি আমরা 10<sup>-10</sup> এবং 10<sup>10</sup> যোগ করি, আমরা সম্ভবত 10<sup>10</sup> পাব, কারণ ছোট মানটি বড় মানের মতো একই ক্রমে "রূপান্তরিত" হবে এবং ফলস্বরূপ মানটিসা হারিয়ে যাবে।

বেশিরভাগ অ্যাক্টিভেশন ফাংশনের [-1..1] এর চারপাশে নন-লিনিয়ারিটি থাকে, এবং তাই সমস্ত ইনপুট ডেটাকে [-1..1] বা [0..1] সীমার মধ্যে স্কেল করা যৌক্তিক।

## প্রাথমিক ওজন আরম্ভ

আদর্শভাবে, আমরা চাই যে নেটওয়ার্ক স্তরগুলো পার হওয়ার পর মানগুলো একই সীমার মধ্যে থাকুক। তাই ওজনগুলো এমনভাবে আরম্ভ করা গুরুত্বপূর্ণ যাতে মানগুলোর বণ্টন সংরক্ষিত থাকে।

**N(0,1)** এর মতো নরমাল বণ্টন ভালো ধারণা নয়, কারণ যদি আমাদের *n* ইনপুট থাকে, আউটপুটের স্ট্যান্ডার্ড ডেভিয়েশন হবে *n*, এবং মানগুলো [0..1] সীমার বাইরে চলে যেতে পারে।

নিম্নলিখিত আরম্ভগুলো প্রায়শই ব্যবহৃত হয়:

 * ইউনিফর্ম বণ্টন -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/√n_in)** নিশ্চিত করে যে শূন্য গড় এবং ১ স্ট্যান্ডার্ড ডেভিয়েশন সহ ইনপুটগুলোর জন্য একই গড়/স্ট্যান্ডার্ড ডেভিয়েশন থাকবে
 * **N(0,√2/(n_in+n_out))** -- তথাকথিত **Xavier initialization** (`glorot`), এটি সিগন্যালগুলোকে ফরোয়ার্ড এবং ব্যাকওয়ার্ড প্রোপাগেশনের সময় সীমার মধ্যে রাখতে সাহায্য করে

## ব্যাচ নরমালাইজেশন

যথাযথ ওজন আরম্ভের পরেও, প্রশিক্ষণের সময় ওজনগুলো বড় বা ছোট হতে পারে, এবং তারা সিগন্যালগুলোকে সঠিক সীমার বাইরে নিয়ে যেতে পারে। আমরা **নরমালাইজেশন** কৌশল ব্যবহার করে সিগন্যালগুলোকে সঠিক সীমায় ফিরিয়ে আনতে পারি। যদিও এর বিভিন্ন পদ্ধতি রয়েছে (Weight Normalization, Layer Normalization), সবচেয়ে বেশি ব্যবহৃত হয় ব্যাচ নরমালাইজেশন।

**ব্যাচ নরমালাইজেশন** এর ধারণা হলো মিনিব্যাচ জুড়ে সমস্ত মান বিবেচনা করা এবং সেই মানগুলোর উপর ভিত্তি করে নরমালাইজেশন করা (যেমন গড় বিয়োগ করা এবং স্ট্যান্ডার্ড ডেভিয়েশন দ্বারা ভাগ করা)। এটি একটি নেটওয়ার্ক স্তর হিসেবে প্রয়োগ করা হয় যা ওজন প্রয়োগের পরে, কিন্তু অ্যাক্টিভেশন ফাংশনের আগে এই নরমালাইজেশন করে। এর ফলে, আমরা সাধারণত উচ্চতর চূড়ান্ত নির্ভুলতা এবং দ্রুত প্রশিক্ষণ দেখতে পাই।

এখানে ব্যাচ নরমালাইজেশনের [মূল পেপার](https://arxiv.org/pdf/1502.03167.pdf), [উইকিপিডিয়ার ব্যাখ্যা](https://en.wikipedia.org/wiki/Batch_normalization), এবং [একটি ভালো পরিচিতিমূলক ব্লগ পোস্ট](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (এবং একটি [রাশিয়ান ভাষায়](https://habrahabr.ru/post/309302/))।

## ড্রপআউট

**ড্রপআউট** একটি আকর্ষণীয় কৌশল যা প্রশিক্ষণের সময় এলোমেলোভাবে নির্দিষ্ট শতাংশ নিউরন সরিয়ে দেয়। এটি একটি স্তর হিসেবে প্রয়োগ করা হয় যার একটি প্যারামিটার থাকে (সরানোর জন্য নিউরনের শতাংশ, সাধারণত ১০%-৫০%), এবং প্রশিক্ষণের সময় এটি ইনপুট ভেক্টরের এলোমেলো উপাদানগুলো শূন্য করে দেয়, পরবর্তী স্তরে পাঠানোর আগে।

যদিও এটি একটি অদ্ভুত ধারণা মনে হতে পারে, আপনি [`Dropout.ipynb`](../../../../../lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb) নোটবুকে MNIST ডিজিট ক্লাসিফায়ার প্রশিক্ষণের উপর ড্রপআউটের প্রভাব দেখতে পারেন। এটি প্রশিক্ষণকে দ্রুত করে এবং কম প্রশিক্ষণ ইপোকের মধ্যে উচ্চতর নির্ভুলতা অর্জন করতে সাহায্য করে।

এই প্রভাবটি বিভিন্নভাবে ব্যাখ্যা করা যেতে পারে:

 * এটি মডেলের জন্য একটি এলোমেলো শকিং ফ্যাক্টর হিসেবে বিবেচিত হতে পারে, যা অপ্টিমাইজেশনকে স্থানীয় ন্যূনতম থেকে বের করে আনে
 * এটি *অন্তর্নিহিত মডেল গড়* হিসেবে বিবেচিত হতে পারে, কারণ ড্রপআউটের সময় আমরা সামান্য ভিন্ন মডেল প্রশিক্ষণ দিচ্ছি

> *কিছু লোক বলে যে যখন একজন মাতাল ব্যক্তি কিছু শেখার চেষ্টা করে, সে এটি পরের সকালে আরও ভালোভাবে মনে রাখে, একজন সচেতন ব্যক্তির তুলনায়, কারণ কিছু ত্রুটিপূর্ণ নিউরন সহ একটি মস্তিষ্ক অর্থটি আরও ভালোভাবে ধরার জন্য মানিয়ে নিতে চেষ্টা করে। আমরা নিজেরা এটি সত্য কিনা তা পরীক্ষা করিনি।*

## ওভারফিটিং প্রতিরোধ

ডিপ লার্নিংয়ের একটি খুব গুরুত্বপূর্ণ দিক হলো [ওভারফিটিং](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) প্রতিরোধ করতে সক্ষম হওয়া। যদিও খুব শক্তিশালী নিউরাল নেটওয়ার্ক মডেল ব্যবহার করা লোভনীয় হতে পারে, আমাদের সর্বদা মডেল প্যারামিটারের সংখ্যা এবং প্রশিক্ষণ নমুনার সংখ্যার মধ্যে ভারসাম্য বজায় রাখা উচিত।

> নিশ্চিত করুন যে আপনি [ওভারফিটিং](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) ধারণাটি বুঝেছেন যা আমরা আগে পরিচয় করিয়েছি!

ওভারফিটিং প্রতিরোধের কয়েকটি উপায় রয়েছে:

 * প্রাথমিক থামানো -- যাচাইকরণ সেটে ত্রুটি ক্রমাগত পর্যবেক্ষণ করা এবং যাচাইকরণ ত্রুটি বাড়তে শুরু করলে প্রশিক্ষণ বন্ধ করা।
 * স্পষ্ট ওজন ক্ষয় / নিয়মিতকরণ -- ক্ষতির ফাংশনে উচ্চ ওজনের জন্য একটি অতিরিক্ত শাস্তি যোগ করা, যা মডেলকে খুব অস্থিতিশীল ফলাফল থেকে রক্ষা করে
 * মডেল গড় -- একাধিক মডেল প্রশিক্ষণ এবং তারপরে ফলাফল গড় করা। এটি ভ্যারিয়েন্স কমাতে সাহায্য করে।
 * ড্রপআউট (অন্তর্নিহিত মডেল গড়)

## অপ্টিমাইজার / প্রশিক্ষণ অ্যালগরিদম

প্রশিক্ষণের আরেকটি গুরুত্বপূর্ণ দিক হলো একটি ভালো প্রশিক্ষণ অ্যালগরিদম নির্বাচন করা। যদিও ক্লাসিক্যাল **gradient descent** একটি যুক্তিসঙ্গত পছন্দ, এটি কখনও কখনও খুব ধীর হতে পারে, বা অন্যান্য সমস্যার কারণ হতে পারে।

ডিপ লার্নিংয়ে, আমরা **Stochastic Gradient Descent** (SGD) ব্যবহার করি, যা প্রশিক্ষণ সেট থেকে এলোমেলোভাবে নির্বাচিত মিনিব্যাচে প্রয়োগ করা একটি গ্রেডিয়েন্ট ডিসেন্ট। ওজনগুলো এই সূত্র ব্যবহার করে সামঞ্জস্য করা হয়:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### মোমেন্টাম

**মোমেন্টাম SGD**-তে, আমরা পূর্ববর্তী ধাপগুলোর গ্রেডিয়েন্টের একটি অংশ ধরে রাখি। এটি এমন যে আমরা কোথাও জড়তার সাথে চলছি, এবং আমরা একটি ভিন্ন দিকে ধাক্কা পাই, আমাদের গতিপথ তাৎক্ষণিকভাবে পরিবর্তিত হয় না, বরং মূল গতির কিছু অংশ ধরে রাখে। এখানে আমরা *গতি* উপস্থাপন করতে একটি ভেক্টর v প্রবর্তন করি:

* v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
* w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

এখানে প্যারামিটার γ নির্দেশ করে যে আমরা জড়তাকে কতটা বিবেচনায় নিচ্ছি: γ=0 ক্লাসিক্যাল SGD-এর সাথে মিলে যায়; γ=1 একটি বিশুদ্ধ গতির সমীকরণ।

### অ্যাডাম, অ্যাডাগ্রাড, ইত্যাদি

যেহেতু প্রতিটি স্তরে আমরা কিছু ম্যাট্রিক্স W<sub>i</sub> দ্বারা সিগন্যালগুলো গুণ করি, ||W<sub>i</sub>|| এর উপর নির্ভর করে, গ্রেডিয়েন্ট হয় ক্ষুদ্র হয়ে 0 এর কাছাকাছি চলে যেতে পারে, অথবা অনির্দিষ্টভাবে বৃদ্ধি পেতে পারে। এটি Exploding/Vanishing Gradients সমস্যার মূল বিষয়।

এই সমস্যার একটি সমাধান হলো সমীকরণে গ্রেডিয়েন্টের কেবল দিকটি ব্যবহার করা, এবং পরম মানটি উপেক্ষা করা, যেমন:

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), যেখানে ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

এই অ্যালগরিদমটি **Adagrad** নামে পরিচিত। একই ধারণা ব্যবহার করে অন্যান্য অ্যালগরিদম: **RMSProp**, **Adam**

> **Adam** অনেক অ্যাপ্লিকেশনের জন্য একটি খুব কার্যকর অ্যালগরিদম হিসেবে বিবেচিত হয়, তাই আপনি যদি নিশ্চিত না হন কোনটি ব্যবহার করবেন - Adam ব্যবহার করুন।

### গ্রেডিয়েন্ট ক্লিপিং

গ্রেডিয়েন্ট ক্লিপিং উপরের ধারণার একটি সম্প্রসারণ। যখন ||∇ℒ|| ≤ θ, আমরা ওজন অপ্টিমাইজেশনে মূল গ্রেডিয়েন্ট বিবেচনা করি, এবং যখন ||∇ℒ|| > θ - আমরা গ্রেডিয়েন্টকে তার নর্ম দ্বারা ভাগ করি। এখানে θ একটি প্যারামিটার, বেশিরভাগ ক্ষেত্রে আমরা θ=1 বা θ=10 নিতে পারি।

### লার্নিং রেট ক্ষয়

প্রশিক্ষণের সাফল্য প্রায়শই লার্নিং রেট প্যারামিটার η এর উপর নির্ভর করে। এটি অনুমান করা যৌক্তিক যে η এর বড় মান দ্রুত প্রশিক্ষণের ফলাফল দেয়, যা আমরা সাধারণত প্রশিক্ষণের শুরুতে চাই, এবং তারপরে η এর ছোট মান নেটওয়ার্ককে সূক্ষ্মভাবে টিউন করতে দেয়। তাই, বেশিরভাগ ক্ষেত্রে আমরা প্রশিক্ষণের প্রক্রিয়ায় η হ্রাস করতে চাই।

এটি প্রতিটি প্রশিক্ষণ ইপোকের পরে η কে একটি সংখ্যা (যেমন 0.98) দ্বারা গুণ করে করা যেতে পারে, অথবা আরও জটিল **লার্নিং রেট শিডিউল** ব্যবহার করে।

## বিভিন্ন নেটওয়ার্ক আর্কিটেকচার

আপনার সমস্যার জন্য সঠিক নেটওয়ার্ক আর্কিটেকচার নির্বাচন করা কঠিন হতে পারে। সাধারণত, আমরা এমন একটি আর্কিটেকচার নেব যা আমাদের নির্দিষ্ট কাজের (বা অনুরূপ) জন্য কাজ করার প্রমাণিত হয়েছে। এখানে কম্পিউটার ভিশনের জন্য নিউরাল নেটওয়ার্ক আর্কিটেকচারের একটি [ভালো ওভারভিউ](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) রয়েছে।

> এটি গুরুত্বপূর্ণ যে আমাদের কাছে থাকা প্রশিক্ষণ নমুনার সংখ্যার জন্য যথেষ্ট শক্তিশালী একটি আর্কিটেকচার নির্বাচন করা। খুব শক্তিশালী মডেল নির্বাচন করলে [ওভারফিটিং](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) হতে পারে।

আরেকটি ভালো উপায় হলো এমন একটি আর্কিটেকচার ব্যবহার করা যা প্রয়োজনীয় জটিলতার সাথে স্বয়ংক্রিয়ভাবে সামঞ্জস্য করবে। কিছুটা হলেও, **ResNet** আর্কিটেকচার এবং **Inception** স্ব-সামঞ্জস্যপূর্ণ। [কম্পিউটার ভিশন আর্কিটেকচারের উপর আরও](../07-ConvNets/CNN_Architectures.md)।

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিক অনুবাদের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। নথিটির মূল ভাষায় থাকা সংস্করণটিকেই প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ ব্যবহার করার পরামর্শ দেওয়া হয়। এই অনুবাদ ব্যবহারের ফলে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।