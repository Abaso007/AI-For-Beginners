<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-26T09:14:51+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "bn"
}
-->
# জেনারেটিভ অ্যাডভার্সারিয়াল নেটওয়ার্কস

পূর্ববর্তী অংশে আমরা **জেনারেটিভ মডেল** সম্পর্কে শিখেছি: এমন মডেল যা প্রশিক্ষণ ডেটাসেটের মতো নতুন ছবি তৈরি করতে পারে। VAE ছিল জেনারেটিভ মডেলের একটি ভালো উদাহরণ।

## [পূর্ব-লেকচার কুইজ](https://ff-quizzes.netlify.app/en/ai/quiz/19)

তবে, যদি আমরা কিছু সত্যিই অর্থবহ তৈরি করতে চাই, যেমন একটি যুক্তিসঙ্গত রেজোলিউশনের পেইন্টিং, VAE ব্যবহার করে, আমরা দেখতে পাবো যে প্রশিক্ষণ ভালোভাবে কনভার্জ করে না। এই ব্যবহারের ক্ষেত্রে, আমাদের একটি নির্দিষ্ট জেনারেটিভ মডেলের জন্য লক্ষ্যযুক্ত আরেকটি আর্কিটেকচার সম্পর্কে শিখতে হবে - **জেনারেটিভ অ্যাডভার্সারিয়াল নেটওয়ার্কস**, বা GANs।

GAN-এর মূল ধারণা হলো দুটি নিউরাল নেটওয়ার্ককে একে অপরের বিরুদ্ধে প্রশিক্ষণ দেওয়া:

<img src="images/gan_architecture.png" width="70%"/>

> ছবি: [Dmitry Soshnikov](http://soshnikov.com)

> ✅ কিছু শব্দভাণ্ডার:
> * **জেনারেটর** হলো একটি নেটওয়ার্ক যা কিছু র‍্যান্ডম ভেক্টর গ্রহণ করে এবং ফলাফল হিসেবে একটি ছবি তৈরি করে।
> * **ডিসক্রিমিনেটর** হলো একটি নেটওয়ার্ক যা একটি ছবি গ্রহণ করে এবং এটি বলার চেষ্টা করে যে এটি একটি বাস্তব ছবি (প্রশিক্ষণ ডেটাসেট থেকে) নাকি এটি জেনারেটর দ্বারা তৈরি। এটি মূলত একটি ইমেজ ক্লাসিফায়ার।

### ডিসক্রিমিনেটর

ডিসক্রিমিনেটরের আর্কিটেকচার সাধারণ ইমেজ ক্লাসিফিকেশন নেটওয়ার্ক থেকে আলাদা নয়। সহজতম ক্ষেত্রে এটি একটি সম্পূর্ণ-সংযুক্ত ক্লাসিফায়ার হতে পারে, তবে সম্ভবত এটি একটি [কনভোলিউশনাল নেটওয়ার্ক](../07-ConvNets/README.md) হবে।

> ✅ কনভোলিউশনাল নেটওয়ার্কের উপর ভিত্তি করে GAN-কে [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) বলা হয়।

একটি CNN ডিসক্রিমিনেটর নিম্নলিখিত স্তরগুলি নিয়ে গঠিত: কয়েকটি কনভোলিউশন+পুলিং (যার স্পেশিয়াল সাইজ ক্রমশ কমছে) এবং এক বা একাধিক সম্পূর্ণ-সংযুক্ত স্তর যা "ফিচার ভেক্টর" তৈরি করে, এবং চূড়ান্ত বাইনারি ক্লাসিফায়ার।

> ✅ 'পুলিং' হলো একটি কৌশল যা ছবির আকার কমিয়ে দেয়। "পুলিং স্তরগুলি ডেটার মাত্রা কমিয়ে দেয়, একটি স্তরের নিউরন ক্লাস্টারের আউটপুটকে পরবর্তী স্তরের একটি নিউরনে একত্রিত করে।" - [উৎস](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### জেনারেটর

জেনারেটর একটু বেশি জটিল। আপনি এটিকে একটি উল্টানো ডিসক্রিমিনেটর হিসেবে বিবেচনা করতে পারেন। একটি ল্যাটেন্ট ভেক্টর থেকে শুরু করে (ফিচার ভেক্টরের জায়গায়), এটি একটি সম্পূর্ণ-সংযুক্ত স্তর ব্যবহার করে প্রয়োজনীয় আকার/আকৃতিতে রূপান্তরিত করে, তারপর ডিকনভোলিউশন+আপস্কেলিং। এটি [অটোএনকোডার](../09-Autoencoders/README.md)-এর *ডিকোডার* অংশের মতো।

> ✅ যেহেতু কনভোলিউশন স্তরটি একটি লিনিয়ার ফিল্টার হিসেবে ছবির উপর দিয়ে চলে, ডিকনভোলিউশন মূলত কনভোলিউশনের মতোই এবং একই স্তর লজিক ব্যবহার করে বাস্তবায়িত হতে পারে।

<img src="images/gan_arch_detail.png" width="70%"/>

> ছবি: [Dmitry Soshnikov](http://soshnikov.com)

### GAN প্রশিক্ষণ

GAN-কে **অ্যাডভার্সারিয়াল** বলা হয় কারণ জেনারেটর এবং ডিসক্রিমিনেটরের মধ্যে একটি ক্রমাগত প্রতিযোগিতা থাকে। এই প্রতিযোগিতার সময়, জেনারেটর এবং ডিসক্রিমিনেটর উভয়ই উন্নত হয়, ফলে নেটওয়ার্ক আরও ভালো ছবি তৈরি করতে শেখে।

প্রশিক্ষণ দুটি ধাপে ঘটে:

* **ডিসক্রিমিনেটর প্রশিক্ষণ**। এই কাজটি বেশ সহজ: আমরা জেনারেটর দ্বারা তৈরি একটি ব্যাচ ছবি তৈরি করি, তাদের লেবেল করি 0, যা নকল ছবির জন্য দাঁড়ায়, এবং ইনপুট ডেটাসেট থেকে একটি ব্যাচ ছবি গ্রহণ করি (লেবেল 1, বাস্তব ছবি)। আমরা কিছু *ডিসক্রিমিনেটর লস* পাই এবং ব্যাকপ্রপ করি।
* **জেনারেটর প্রশিক্ষণ**। এটি একটু বেশি জটিল, কারণ আমরা সরাসরি জেনারেটরের জন্য প্রত্যাশিত আউটপুট জানি না। আমরা পুরো GAN নেটওয়ার্ক (জেনারেটর এবং ডিসক্রিমিনেটর) গ্রহণ করি, এটিকে কিছু র‍্যান্ডম ভেক্টর দিয়ে ফিড করি এবং আশা করি ফলাফল হবে 1 (বাস্তব ছবির সাথে সম্পর্কিত)। আমরা ডিসক্রিমিনেটরের প্যারামিটারগুলি ফ্রিজ করি (এই ধাপে আমরা এটি প্রশিক্ষণ দিতে চাই না) এবং ব্যাকপ্রপ করি।

এই প্রক্রিয়ার সময়, জেনারেটর এবং ডিসক্রিমিনেটর উভয়ের লস উল্লেখযোগ্যভাবে কমে না। আদর্শ পরিস্থিতিতে, তারা দোলায়মান হওয়া উচিত, যা উভয় নেটওয়ার্কের কর্মক্ষমতা উন্নতির সাথে সম্পর্কিত।

## ✍️ অনুশীলন: GANs

* [GAN নোটবুক TensorFlow/Keras-এ](../../../../../lessons/4-ComputerVision/10-GANs/GANTF.ipynb)
* [GAN নোটবুক PyTorch-এ](../../../../../lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb)

### GAN প্রশিক্ষণের সমস্যা

GAN প্রশিক্ষণ বিশেষভাবে কঠিন বলে পরিচিত। এখানে কয়েকটি সমস্যা:

* **মোড কলাপস**। এই শব্দটি দ্বারা বোঝানো হয় যে জেনারেটর একটি সফল ছবি তৈরি করতে শেখে যা ডিসক্রিমিনেটরকে ধোঁকা দেয়, কিন্তু বিভিন্ন ধরনের ছবি তৈরি করে না।
* **হাইপারপ্যারামিটারের প্রতি সংবেদনশীলতা**। প্রায়ই দেখা যায় যে GAN একেবারেই কনভার্জ করে না, এবং তারপর হঠাৎ করে লার্নিং রেট কমিয়ে কনভার্জ করে।
* জেনারেটর এবং ডিসক্রিমিনেটরের মধ্যে **সামঞ্জস্য** বজায় রাখা। অনেক ক্ষেত্রে ডিসক্রিমিনেটরের লস দ্রুত শূন্যে নেমে যেতে পারে, যার ফলে জেনারেটর আর প্রশিক্ষণ দিতে পারে না। এটি কাটিয়ে উঠতে, আমরা জেনারেটর এবং ডিসক্রিমিনেটরের জন্য ভিন্ন লার্নিং রেট সেট করার চেষ্টা করতে পারি, অথবা যদি লস ইতিমধ্যে খুব কম হয় তবে ডিসক্রিমিনেটর প্রশিক্ষণ এড়িয়ে যেতে পারি।
* **উচ্চ রেজোলিউশনের জন্য প্রশিক্ষণ**। অটোএনকোডারের মতো একই সমস্যার প্রতিফলন, এই সমস্যাটি ট্রিগার হয় কারণ কনভোলিউশনাল নেটওয়ার্কের খুব বেশি স্তর পুনর্গঠন করার ফলে আর্টিফ্যাক্ট তৈরি হয়। এই সমস্যাটি সাধারণত **প্রগ্রেসিভ গ্রোইং** ব্যবহার করে সমাধান করা হয়, যেখানে প্রথমে কয়েকটি স্তর কম-রেজোলিউশন ছবিতে প্রশিক্ষণ দেওয়া হয়, তারপর স্তরগুলি "আনব্লক" বা যোগ করা হয়। আরেকটি সমাধান হবে স্তরগুলির মধ্যে অতিরিক্ত সংযোগ যোগ করা এবং একাধিক রেজোলিউশন একসাথে প্রশিক্ষণ দেওয়া - বিস্তারিত জানার জন্য এই [মাল্টি-স্কেল গ্রেডিয়েন্ট GANs পেপার](https://arxiv.org/abs/1903.06048) দেখুন।

## স্টাইল ট্রান্সফার

GANs হলো শিল্পসম্মত ছবি তৈরি করার একটি চমৎকার উপায়। আরেকটি আকর্ষণীয় কৌশল হলো **স্টাইল ট্রান্সফার**, যা একটি **কন্টেন্ট ইমেজ** গ্রহণ করে এবং এটি একটি ভিন্ন স্টাইলে পুনরায় আঁকে, **স্টাইল ইমেজ** থেকে ফিল্টার প্রয়োগ করে।

এর কাজ করার পদ্ধতি হলো নিম্নরূপ:
* আমরা একটি র‍্যান্ডম নয়েজ ইমেজ দিয়ে শুরু করি (বা একটি কন্টেন্ট ইমেজ দিয়ে শুরু করি, তবে বোঝার সুবিধার জন্য র‍্যান্ডম নয়েজ দিয়ে শুরু করা সহজ)
* আমাদের লক্ষ্য হবে এমন একটি ছবি তৈরি করা, যা কন্টেন্ট ইমেজ এবং স্টাইল ইমেজ উভয়ের কাছাকাছি হবে। এটি দুটি লস ফাংশন দ্বারা নির্ধারিত হবে:
   - **কন্টেন্ট লস** বর্তমান ছবি এবং কন্টেন্ট ইমেজ থেকে CNN দ্বারা নির্দিষ্ট স্তরে বের করা বৈশিষ্ট্যের উপর ভিত্তি করে গণনা করা হয়।
   - **স্টাইল লস** বর্তমান ছবি এবং স্টাইল ইমেজের মধ্যে একটি চতুর উপায়ে গ্রাম ম্যাট্রিক্স ব্যবহার করে গণনা করা হয় (বিস্তারিত [উদাহরণ নোটবুকে](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) দেখুন)।
* ছবিকে মসৃণ এবং নয়েজ সরাতে, আমরা **ভেরিয়েশন লস** পরিচয় করাই, যা প্রতিবেশী পিক্সেলের গড় দূরত্ব গণনা করে।
* প্রধান অপ্টিমাইজেশন লুপ বর্তমান ছবিকে গ্রেডিয়েন্ট ডিসেন্ট (বা অন্য কোনো অপ্টিমাইজেশন অ্যালগরিদম) ব্যবহার করে সামগ্রিক লস কমানোর জন্য সামঞ্জস্য করে।

## ✍️ উদাহরণ: [স্টাইল ট্রান্সফার](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)

## [পোস্ট-লেকচার কুইজ](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## উপসংহার

এই পাঠে, আপনি GANs এবং সেগুলিকে কীভাবে প্রশিক্ষণ দিতে হয় তা শিখেছেন। আপনি এই ধরনের নিউরাল নেটওয়ার্কের বিশেষ চ্যালেঞ্জগুলি এবং সেগুলি কাটিয়ে ওঠার কিছু কৌশলও শিখেছেন।

## 🚀 চ্যালেঞ্জ

[স্টাইল ট্রান্সফার নোটবুক](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) চালান এবং আপনার নিজের ছবিগুলি ব্যবহার করুন।

## পর্যালোচনা ও স্ব-অধ্যয়ন

GANs সম্পর্কে আরও পড়ার জন্য, এই রিসোর্সগুলি দেখুন:

* Marco Pasini, [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), একটি *ডি ফ্যাক্টো* GAN আর্কিটেকচার বিবেচনা করার জন্য
* [Creating Generative Art using GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## অ্যাসাইনমেন্ট

এই পাঠের সাথে সম্পর্কিত দুটি নোটবুকের মধ্যে একটি পুনরায় দেখুন এবং GAN-কে আপনার নিজের ছবিতে পুনরায় প্রশিক্ষণ দিন। আপনি কী তৈরি করতে পারেন?

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিক অনুবাদের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। নথিটির মূল ভাষায় লেখা সংস্করণটিকেই প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ ব্যবহার করার পরামর্শ দেওয়া হয়। এই অনুবাদ ব্যবহারের ফলে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।