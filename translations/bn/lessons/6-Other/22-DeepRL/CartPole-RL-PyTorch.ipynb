{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# কার্টপোল ব্যালেন্সিংয়ের জন্য RL প্রশিক্ষণ\n",
    "\n",
    "এই নোটবুকটি [AI for Beginners Curriculum](http://aka.ms/ai-beginners)-এর অংশ। এটি [PyTorch-এর অফিসিয়াল টিউটোরিয়াল](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) এবং [এই কার্টপোল PyTorch ইমপ্লিমেন্টেশন](https://github.com/yc930401/Actor-Critic-pytorch) দ্বারা অনুপ্রাণিত হয়েছে।\n",
    "\n",
    "এই উদাহরণে, আমরা RL ব্যবহার করে একটি মডেল প্রশিক্ষণ করব যাতে একটি কার্টের উপর একটি পোল ব্যালেন্স করা যায়। কার্টটি অনুভূমিক স্কেলে বাম এবং ডান দিকে সরতে পারে। আমরা [OpenAI Gym](https://www.gymlibrary.ml/) পরিবেশ ব্যবহার করব পোলের সিমুলেশন করার জন্য।\n",
    "\n",
    "> **Note**: আপনি এই পাঠের কোড লোকালভাবে (যেমন Visual Studio Code থেকে) চালাতে পারেন, সেক্ষেত্রে সিমুলেশনটি একটি নতুন উইন্ডোতে খুলবে। অনলাইনে কোড চালানোর সময়, কোডে কিছু পরিবর্তন করতে হতে পারে, যেমনটি [এখানে](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) বর্ণনা করা হয়েছে।\n",
    "\n",
    "আমরা Gym ইনস্টল করা নিশ্চিত করার মাধ্যমে শুরু করব:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন চলুন CartPole পরিবেশ তৈরি করি এবং এটি কীভাবে পরিচালনা করতে হয় তা দেখি। একটি পরিবেশের নিম্নলিখিত বৈশিষ্ট্যগুলি রয়েছে:\n",
    "\n",
    "* **অ্যাকশন স্পেস** হলো সম্ভাব্য ক্রিয়াগুলির সেট যা আমরা সিমুলেশনের প্রতিটি ধাপে সম্পাদন করতে পারি  \n",
    "* **অবজারভেশন স্পেস** হলো সেই পর্যবেক্ষণগুলির স্থান যা আমরা করতে পারি  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "চলুন দেখি সিমুলেশনটি কীভাবে কাজ করে। নিচের লুপটি সিমুলেশন চালায়, যতক্ষণ না `env.step` টার্মিনেশন ফ্ল্যাগ `done` ফেরত দেয়। আমরা এলোমেলোভাবে অ্যাকশন নির্বাচন করব `env.action_space.sample()` ব্যবহার করে, যার মানে পরীক্ষাটি সম্ভবত খুব দ্রুত ব্যর্থ হবে (CartPole পরিবেশটি তখনই শেষ হয় যখন CartPole-এর গতি, অবস্থান বা কোণ নির্দিষ্ট সীমার বাইরে চলে যায়)।\n",
    "\n",
    "> সিমুলেশনটি একটি নতুন উইন্ডোতে খুলবে। আপনি কোডটি একাধিকবার চালাতে পারেন এবং এটি কীভাবে আচরণ করে তা দেখতে পারেন।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আপনি লক্ষ্য করবেন যে পর্যবেক্ষণগুলোতে ৪টি সংখ্যা রয়েছে। সেগুলো হলো:\n",
    "- কার্টের অবস্থান\n",
    "- কার্টের বেগ\n",
    "- পোলের কোণ\n",
    "- পোলের ঘূর্ণনের হার\n",
    "\n",
    "`rew` হলো সেই পুরস্কার যা আমরা প্রতিটি ধাপে পাই। CartPole পরিবেশে আপনি প্রতিটি সিমুলেশন ধাপের জন্য ১ পয়েন্ট পুরস্কৃত হন, এবং লক্ষ্য হলো মোট পুরস্কার সর্বাধিক করা, অর্থাৎ CartPole যতক্ষণ পর্যন্ত না পড়ে যায় ততক্ষণ এটি ভারসাম্য বজায় রাখতে সক্ষম হওয়া।\n",
    "\n",
    "রিইনফোর্সমেন্ট লার্নিংয়ের সময়, আমাদের লক্ষ্য হলো একটি **পলিসি** $\\pi$ প্রশিক্ষণ দেওয়া, যা প্রতিটি অবস্থার $s$ জন্য আমাদের জানাবে কোন ক্রিয়া $a$ গ্রহণ করতে হবে, অর্থাৎ মূলত $a = \\pi(s)$।\n",
    "\n",
    "যদি আপনি একটি প্রোবাবিলিস্টিক সমাধান চান, তবে আপনি পলিসিকে প্রতিটি ক্রিয়ার জন্য সম্ভাবনার একটি সেট প্রদানকারী হিসেবে ভাবতে পারেন, অর্থাৎ $\\pi(a|s)$ বোঝাবে যে আমরা অবস্থান $s$-এ ক্রিয়া $a$ গ্রহণ করার সম্ভাবনা কত।\n",
    "\n",
    "## পলিসি গ্রেডিয়েন্ট পদ্ধতি\n",
    "\n",
    "সবচেয়ে সহজ RL অ্যালগরিদমে, যাকে **পলিসি গ্রেডিয়েন্ট** বলা হয়, আমরা একটি নিউরাল নেটওয়ার্ক প্রশিক্ষণ দেবো যাতে এটি পরবর্তী ক্রিয়া পূর্বানুমান করতে পারে।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমরা নেটওয়ার্কটি প্রশিক্ষণ দেব অনেক পরীক্ষা চালিয়ে, এবং প্রতিটি চালানোর পরে আমাদের নেটওয়ার্ক আপডেট করব। চলুন একটি ফাংশন সংজ্ঞায়িত করি যা পরীক্ষা চালাবে এবং ফলাফলগুলি (যাকে **ট্রেস** বলা হয়) ফিরিয়ে দেবে - সমস্ত অবস্থা, ক্রিয়া (এবং তাদের সুপারিশকৃত সম্ভাবনা), এবং পুরস্কার:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আপনি প্রশিক্ষণবিহীন নেটওয়ার্ক দিয়ে একটি পর্ব চালাতে পারেন এবং লক্ষ্য করতে পারেন যে মোট পুরস্কার (অর্থাৎ পর্বের দৈর্ঘ্য) খুব কম:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "পলিসি গ্রেডিয়েন্ট অ্যালগরিদমের একটি জটিল দিক হল **ডিসকাউন্টেড রিওয়ার্ডস** ব্যবহার করা। ধারণাটি হল যে আমরা গেমের প্রতিটি ধাপে মোট রিওয়ার্ডের ভেক্টর গণনা করি, এবং এই প্রক্রিয়ার সময় আমরা কিছু সহগ $gamma$ ব্যবহার করে প্রাথমিক রিওয়ার্ডগুলো ডিসকাউন্ট করি। আমরা ফলাফলস্বরূপ ভেক্টরটিকে স্বাভাবিকীকরণও করি, কারণ আমরা এটি আমাদের প্রশিক্ষণে প্রভাবিত করার ওজন হিসেবে ব্যবহার করব।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা প্রকৃত প্রশিক্ষণ শুরু করব! আমরা ৩০০টি এপিসোড চালাব, এবং প্রতিটি এপিসোডে আমরা নিম্নলিখিত কাজগুলো করব:\n",
    "\n",
    "1. পরীক্ষাটি চালান এবং ট্রেস সংগ্রহ করুন।\n",
    "1. নেওয়া পদক্ষেপ এবং পূর্বাভাসিত সম্ভাবনার মধ্যে পার্থক্য (`gradients`) গণনা করুন। পার্থক্য যত কম হবে, আমরা তত বেশি নিশ্চিত হতে পারব যে সঠিক পদক্ষেপ নেওয়া হয়েছে।\n",
    "1. ডিসকাউন্টেড রিওয়ার্ডস গণনা করুন এবং ডিসকাউন্টেড রিওয়ার্ডস দ্বারা গ্রেডিয়েন্টসকে গুণ করুন - এটি নিশ্চিত করবে যে উচ্চ রিওয়ার্ডযুক্ত পদক্ষেপগুলো চূড়ান্ত ফলাফলে কম রিওয়ার্ডযুক্ত পদক্ষেপগুলোর তুলনায় বেশি প্রভাব ফেলবে।\n",
    "1. আমাদের নিউরাল নেটওয়ার্কের জন্য প্রত্যাশিত লক্ষ্য পদক্ষেপ আংশিকভাবে রান চলাকালীন পূর্বাভাসিত সম্ভাবনা থেকে এবং আংশিকভাবে গণিত করা গ্রেডিয়েন্টস থেকে নেওয়া হবে। আমরা `alpha` প্যারামিটার ব্যবহার করব গ্রেডিয়েন্টস এবং রিওয়ার্ডসকে কতটা বিবেচনা করা হবে তা নির্ধারণ করতে - এটিকে *রিইনফোর্সমেন্ট অ্যালগরিদমের* শেখার হার বলা হয়।\n",
    "1. শেষ পর্যন্ত, আমরা আমাদের নেটওয়ার্ককে স্টেটস এবং প্রত্যাশিত পদক্ষেপগুলোর উপর প্রশিক্ষণ দেব এবং প্রক্রিয়াটি পুনরাবৃত্তি করব।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন চলুন পর্বটি রেন্ডারিং সহ চালাই এবং ফলাফল দেখি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আশা করি, আপনি দেখতে পাচ্ছেন যে এখন পোলটি বেশ ভালোভাবে ব্যালেন্স করতে পারছে!\n",
    "\n",
    "## অ্যাক্টর-ক্রিটিক মডেল\n",
    "\n",
    "অ্যাক্টর-ক্রিটিক মডেল হলো পলিসি গ্রেডিয়েন্টের আরও উন্নত সংস্করণ, যেখানে আমরা একটি নিউরাল নেটওয়ার্ক তৈরি করি যা পলিসি এবং অনুমানকৃত রিওয়ার্ড উভয়ই শিখতে পারে। এই নেটওয়ার্কের দুটি আউটপুট থাকবে (অথবা আপনি এটিকে দুটি পৃথক নেটওয়ার্ক হিসেবে দেখতে পারেন):\n",
    "* **অ্যাক্টর** আমাদের স্টেট প্রোবাবিলিটি ডিস্ট্রিবিউশন দিয়ে অ্যাকশন নেওয়ার সুপারিশ করবে, যেমনটি পলিসি গ্রেডিয়েন্ট মডেলে হয়।\n",
    "* **ক্রিটিক** অনুমান করবে যে ঐ অ্যাকশনগুলো থেকে রিওয়ার্ড কী হতে পারে। এটি প্রদত্ত স্টেটে ভবিষ্যতে মোট অনুমানকৃত রিওয়ার্ড প্রদান করবে।\n",
    "\n",
    "চলুন এমন একটি মডেল সংজ্ঞায়িত করি:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমাদের `discounted_rewards` এবং `run_episode` ফাংশনগুলোকে সামান্য পরিবর্তন করতে হবে:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা প্রধান প্রশিক্ষণ লুপ চালাব। আমরা সঠিক ক্ষতি ফাংশন গণনা করে এবং নেটওয়ার্ক প্যারামিটার আপডেট করে ম্যানুয়াল নেটওয়ার্ক প্রশিক্ষণ প্রক্রিয়া ব্যবহার করব:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## মূল বিষয়\n",
    "\n",
    "আমরা এই ডেমোতে দুটি RL অ্যালগরিদম দেখেছি: সাধারণ পলিসি গ্রেডিয়েন্ট এবং আরও উন্নত অ্যাক্টর-ক্রিটিক। আপনি দেখতে পাবেন যে এই অ্যালগরিদমগুলো অবস্থা, ক্রিয়া এবং পুরস্কারের বিমূর্ত ধারণার সাথে কাজ করে - তাই এগুলো খুব ভিন্ন পরিবেশেও প্রয়োগ করা যেতে পারে।\n",
    "\n",
    "রিইনফোর্সমেন্ট লার্নিং আমাদেরকে শুধুমাত্র চূড়ান্ত পুরস্কার দেখে সমস্যার সেরা কৌশল শিখতে সাহায্য করে। লেবেলযুক্ত ডেটাসেটের প্রয়োজন না হওয়ার কারণে আমরা আমাদের মডেলগুলোকে অপ্টিমাইজ করার জন্য সিমুলেশনগুলো বারবার চালাতে পারি। তবে, RL-এ এখনও অনেক চ্যালেঞ্জ রয়েছে, যা আপনি শিখতে পারবেন যদি আপনি AI-এর এই আকর্ষণীয় ক্ষেত্রে আরও গভীরভাবে মনোযোগ দেওয়ার সিদ্ধান্ত নেন।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**অস্বীকৃতি**:  \nএই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিক অনুবাদ প্রদানের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা দায়বদ্ধ থাকব না।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-28T10:40:47+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "bn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}