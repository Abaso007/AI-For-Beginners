<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-26T10:13:26+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "bn"
}
-->
# ডিপ রিইনফোর্সমেন্ট লার্নিং

রিইনফোর্সমেন্ট লার্নিং (RL) সুপারভাইজড লার্নিং এবং আনসুপারভাইজড লার্নিং-এর পাশাপাশি মেশিন লার্নিং-এর একটি মৌলিক প্যারাডাইম হিসেবে বিবেচিত হয়। যেখানে সুপারভাইজড লার্নিং পরিচিত ফলাফল সহ ডেটাসেটের উপর নির্ভর করে, RL **কাজ করে শেখার** উপর ভিত্তি করে। উদাহরণস্বরূপ, যখন আমরা প্রথমবার একটি কম্পিউটার গেম দেখি, আমরা খেলা শুরু করি, এমনকি নিয়ম না জেনেও, এবং শীঘ্রই শুধুমাত্র খেলার মাধ্যমে এবং আমাদের আচরণ সামঞ্জস্য করে আমাদের দক্ষতা উন্নত করতে সক্ষম হই।

## [পূর্ব-লেকচার কুইজ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RL সম্পাদন করতে আমাদের প্রয়োজন:

* একটি **পরিবেশ** বা **সিমুলেটর**, যা গেমের নিয়ম নির্ধারণ করে। আমাদের সিমুলেটরে পরীক্ষা চালানোর এবং ফলাফল পর্যবেক্ষণ করার সক্ষমতা থাকা উচিত।
* কিছু **রিওয়ার্ড ফাংশন**, যা আমাদের পরীক্ষার সফলতার ইঙ্গিত দেয়। কম্পিউটার গেম খেলা শেখার ক্ষেত্রে, রিওয়ার্ড হবে আমাদের চূড়ান্ত স্কোর।

রিওয়ার্ড ফাংশনের উপর ভিত্তি করে, আমাদের আচরণ সামঞ্জস্য করার এবং আমাদের দক্ষতা উন্নত করার সক্ষমতা থাকা উচিত, যাতে পরবর্তীবার আমরা আরও ভালো খেলতে পারি। অন্যান্য মেশিন লার্নিং-এর ধরণের সাথে RL-এর প্রধান পার্থক্য হলো RL-এ আমরা সাধারণত গেম শেষ না হওয়া পর্যন্ত জানি না যে আমরা জিতেছি বা হেরেছি। তাই, আমরা বলতে পারি না যে একটি নির্দিষ্ট পদক্ষেপ একাই ভালো বা খারাপ - আমরা শুধুমাত্র গেমের শেষে রিওয়ার্ড পাই।

RL-এর সময়, আমরা সাধারণত অনেক পরীক্ষা চালাই। প্রতিটি পরীক্ষার সময়, আমাদের শিখে নেওয়া সর্বোত্তম কৌশল অনুসরণ করা (**এক্সপ্লয়টেশন**) এবং নতুন সম্ভাব্য অবস্থাগুলি অন্বেষণ করা (**এক্সপ্লোরেশন**) এর মধ্যে ভারসাম্য বজায় রাখতে হয়।

## ওপেনএআই জিম

RL-এর জন্য একটি চমৎকার টুল হলো [OpenAI Gym](https://gym.openai.com/) - একটি **সিমুলেশন পরিবেশ**, যা বিভিন্ন পরিবেশ সিমুলেট করতে পারে, যেমন অ্যাটারি গেম থেকে শুরু করে পোল ব্যালেন্সিং-এর পদার্থবিদ্যা। এটি রিইনফোর্সমেন্ট লার্নিং অ্যালগরিদম প্রশিক্ষণের জন্য সবচেয়ে জনপ্রিয় সিমুলেশন পরিবেশগুলির মধ্যে একটি এবং এটি [OpenAI](https://openai.com/) দ্বারা রক্ষণাবেক্ষণ করা হয়।

> **Note**: OpenAI Gym থেকে উপলব্ধ সমস্ত পরিবেশ [এখানে](https://gym.openai.com/envs/#classic_control) দেখতে পারেন।

## কার্টপোল ব্যালেন্সিং

আপনারা সম্ভবত আধুনিক ব্যালেন্সিং ডিভাইস যেমন *Segway* বা *Gyroscooters* দেখেছেন। এগুলো স্বয়ংক্রিয়ভাবে ব্যালেন্স করতে সক্ষম হয়, যা অ্যাক্সিলেরোমিটার বা জাইরোস্কোপ থেকে সংকেতের প্রতিক্রিয়ায় তাদের চাকা সামঞ্জস্য করে। এই অংশে, আমরা একটি অনুরূপ সমস্যা সমাধান করতে শিখব - একটি পোল ব্যালেন্স করা। এটি একটি সার্কাস পারফর্মারের তার হাতে একটি পোল ব্যালেন্স করার মতো পরিস্থিতির অনুরূপ - তবে এই পোল ব্যালেন্সিং শুধুমাত্র 1D-তে ঘটে।

ব্যালেন্সিং-এর একটি সরলীকৃত সংস্করণকে **CartPole** সমস্যা বলা হয়। কার্টপোল জগতে, আমাদের একটি অনুভূমিক স্লাইডার রয়েছে যা বাম বা ডানে সরতে পারে, এবং লক্ষ্য হলো স্লাইডারের উপরে একটি উল্লম্ব পোল ব্যালেন্স করা।

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

এই পরিবেশ তৈরি এবং ব্যবহার করতে, আমাদের কয়েকটি লাইন পাইথন কোড প্রয়োজন:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

প্রতিটি পরিবেশ ঠিক একইভাবে অ্যাক্সেস করা যায়:
* `env.reset` একটি নতুন পরীক্ষা শুরু করে
* `env.step` একটি সিমুলেশন ধাপ সম্পাদন করে। এটি **অ্যাকশন স্পেস** থেকে একটি **অ্যাকশন** গ্রহণ করে এবং একটি **অবজারভেশন** (অবজারভেশন স্পেস থেকে), একটি রিওয়ার্ড এবং একটি টার্মিনেশন ফ্ল্যাগ প্রদান করে।

উপরের উদাহরণে, আমরা প্রতিটি ধাপে একটি র্যান্ডম অ্যাকশন সম্পাদন করি, যার কারণে পরীক্ষার জীবন খুবই সংক্ষিপ্ত:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL অ্যালগরিদমের লক্ষ্য হলো একটি মডেল প্রশিক্ষণ করা - তথাকথিত **পলিসি** π - যা একটি প্রদত্ত অবস্থার প্রতিক্রিয়ায় অ্যাকশন প্রদান করবে। আমরা পলিসিকে সম্ভাবনামূলকও বিবেচনা করতে পারি, যেমন কোনো অবস্থান *s* এবং অ্যাকশন *a* এর জন্য এটি সম্ভাবনা π(*a*|*s*) প্রদান করবে যে আমরা *s* অবস্থায় *a* গ্রহণ করব।

## পলিসি গ্রেডিয়েন্টস অ্যালগরিদম

পলিসি মডেল করার সবচেয়ে স্পষ্ট উপায় হলো একটি নিউরাল নেটওয়ার্ক তৈরি করা যা ইনপুট হিসেবে অবস্থান গ্রহণ করবে এবং সংশ্লিষ্ট অ্যাকশন (বা বরং সমস্ত অ্যাকশনের সম্ভাবনা) প্রদান করবে। এক অর্থে, এটি একটি সাধারণ ক্লাসিফিকেশন টাস্কের মতো হবে, একটি বড় পার্থক্য সহ - আমরা আগে থেকে জানি না যে প্রতিটি ধাপে কোন অ্যাকশন গ্রহণ করা উচিত।

এখানে ধারণাটি হলো এই সম্ভাবনাগুলি অনুমান করা। আমরা **কিউমুলেটিভ রিওয়ার্ডস** এর একটি ভেক্টর তৈরি করি যা পরীক্ষার প্রতিটি ধাপে আমাদের মোট রিওয়ার্ড দেখায়। আমরা **রিওয়ার্ড ডিসকাউন্টিং** প্রয়োগ করি, কিছু গুণক γ=0.99 দ্বারা পূর্ববর্তী রিওয়ার্ডগুলিকে গুণ করে, যাতে পূর্ববর্তী রিওয়ার্ডগুলির ভূমিকা কমানো যায়। তারপর, আমরা পরীক্ষার পথে বৃহত্তর রিওয়ার্ড প্রদানকারী ধাপগুলিকে শক্তিশালী করি।

> পলিসি গ্রেডিয়েন্ট অ্যালগরিদম সম্পর্কে আরও জানুন এবং এটি কার্যকরভাবে দেখতে [উদাহরণ নোটবুক](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) দেখুন।

## অ্যাক্টর-ক্রিটিক অ্যালগরিদম

পলিসি গ্রেডিয়েন্টস পদ্ধতির একটি উন্নত সংস্করণকে **Actor-Critic** বলা হয়। এর প্রধান ধারণা হলো নিউরাল নেটওয়ার্ককে দুটি জিনিস প্রদান করতে প্রশিক্ষণ দেওয়া:

* পলিসি, যা নির্ধারণ করে কোন অ্যাকশন গ্রহণ করতে হবে। এই অংশটিকে **অ্যাক্টর** বলা হয়।
* আমরা এই অবস্থায় মোট রিওয়ার্ড কতটা আশা করতে পারি তার একটি অনুমান - এই অংশটিকে **ক্রিটিক** বলা হয়।

এক অর্থে, এই আর্কিটেকচারটি একটি [GAN](../../4-ComputerVision/10-GANs/README.md)-এর মতো, যেখানে দুটি নেটওয়ার্ক একে অপরের বিরুদ্ধে প্রশিক্ষিত হয়। অ্যাক্টর-ক্রিটিক মডেলে, অ্যাক্টর আমাদের গ্রহণ করার জন্য অ্যাকশন প্রস্তাব করে, এবং ক্রিটিক সমালোচনামূলক হয়ে ফলাফল অনুমান করার চেষ্টা করে। তবে, আমাদের লক্ষ্য হলো এই নেটওয়ার্কগুলিকে একসঙ্গে প্রশিক্ষণ দেওয়া।

কারণ আমরা পরীক্ষার সময় উভয় বাস্তব কিউমুলেটিভ রিওয়ার্ড এবং ক্রিটিক দ্বারা প্রদত্ত ফলাফল জানি, এটি তাদের মধ্যে পার্থক্য কমানোর জন্য একটি লস ফাংশন তৈরি করা তুলনামূলকভাবে সহজ। এটি আমাদের **ক্রিটিক লস** প্রদান করবে। আমরা **অ্যাক্টর লস** গণনা করতে পারি পলিসি গ্রেডিয়েন্ট অ্যালগরিদমের মতো একই পদ্ধতি ব্যবহার করে।

এই অ্যালগরিদমগুলির একটি চালানোর পরে, আমরা আশা করতে পারি আমাদের কার্টপোল এইভাবে আচরণ করবে:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ অনুশীলন: পলিসি গ্রেডিয়েন্টস এবং অ্যাক্টর-ক্রিটিক RL

নিম্নলিখিত নোটবুকগুলিতে আপনার শেখা চালিয়ে যান:

* [TensorFlow-এ RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch-এ RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## অন্যান্য RL টাস্ক

বর্তমানে রিইনফোর্সমেন্ট লার্নিং একটি দ্রুত বর্ধনশীল গবেষণা ক্ষেত্র। রিইনফোর্সমেন্ট লার্নিং-এর কিছু আকর্ষণীয় উদাহরণ হলো:

* **অ্যাটারি গেম** খেলা শেখানো। এই সমস্যার চ্যালেঞ্জিং অংশ হলো আমাদের কাছে একটি সাধারণ অবস্থান ভেক্টর হিসেবে উপস্থাপিত হয় না, বরং একটি স্ক্রিনশট থাকে - এবং আমাদের CNN ব্যবহার করে এই স্ক্রিন ইমেজকে একটি ফিচার ভেক্টরে রূপান্তর করতে হবে, বা রিওয়ার্ড তথ্য বের করতে হবে। অ্যাটারি গেমগুলি Gym-এ উপলব্ধ।
* কম্পিউটারকে বোর্ড গেম খেলতে শেখানো, যেমন চেস এবং গো। সম্প্রতি **Alpha Zero** এর মতো অত্যাধুনিক প্রোগ্রাম দুটি এজেন্ট একে অপরের বিরুদ্ধে খেলে এবং প্রতিটি ধাপে উন্নতি করে শূন্য থেকে প্রশিক্ষিত হয়েছে।
* শিল্পে, RL সিমুলেশন থেকে কন্ট্রোল সিস্টেম তৈরি করতে ব্যবহৃত হয়। [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) নামে একটি পরিষেবা বিশেষভাবে এর জন্য ডিজাইন করা হয়েছে।

## উপসংহার

আমরা এখন শিখেছি কীভাবে এজেন্টদের শুধুমাত্র একটি রিওয়ার্ড ফাংশন প্রদান করে, যা গেমের কাঙ্ক্ষিত অবস্থাকে সংজ্ঞায়িত করে, এবং তাদের বুদ্ধিমত্তার সাথে অনুসন্ধান স্থানটি অন্বেষণ করার সুযোগ দিয়ে ভালো ফলাফল অর্জন করতে প্রশিক্ষণ দিতে হয়। আমরা সফলভাবে দুটি অ্যালগরিদম চেষ্টা করেছি এবং তুলনামূলকভাবে স্বল্প সময়ের মধ্যে একটি ভালো ফলাফল অর্জন করেছি। তবে, এটি RL-এ আপনার যাত্রার শুধুমাত্র শুরু, এবং আপনি যদি আরও গভীরে যেতে চান তবে একটি পৃথক কোর্স নেওয়ার কথা বিবেচনা করা উচিত।

## 🚀 চ্যালেঞ্জ

'অন্যান্য RL টাস্ক' বিভাগে তালিকাভুক্ত অ্যাপ্লিকেশনগুলি অন্বেষণ করুন এবং একটি বাস্তবায়নের চেষ্টা করুন!

## [পোস্ট-লেকচার কুইজ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## পর্যালোচনা এবং স্ব-অধ্যয়ন

আমাদের [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)-এ ক্লাসিকাল রিইনফোর্সমেন্ট লার্নিং সম্পর্কে আরও জানুন।

[এই চমৎকার ভিডিওটি](https://www.youtube.com/watch?v=qv6UVOQ0F44) দেখুন যেখানে একটি কম্পিউটার কীভাবে সুপার মারিও খেলতে শিখতে পারে তা আলোচনা করা হয়েছে।

## অ্যাসাইনমেন্ট: [মাউন্টেন কার প্রশিক্ষণ দিন](lab/README.md)

এই অ্যাসাইনমেন্টের সময় আপনার লক্ষ্য হবে একটি ভিন্ন Gym পরিবেশ - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) প্রশিক্ষণ দেওয়া।

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসাধ্য সঠিক অনুবাদের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। নথিটির মূল ভাষায় থাকা সংস্করণটিকেই প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।