<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T07:44:58+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "bn"
}
-->
# ডিপ রিইনফোর্সমেন্ট লার্নিং

রিইনফোর্সমেন্ট লার্নিং (RL) সুপারভাইজড লার্নিং এবং আনসুপারভাইজড লার্নিং-এর পাশাপাশি অন্যতম মৌলিক মেশিন লার্নিং পদ্ধতি হিসেবে বিবেচিত হয়। যেখানে সুপারভাইজড লার্নিং-এ আমরা পরিচিত ফলাফলসহ ডেটাসেটের উপর নির্ভর করি, RL **কাজ করে শেখার** উপর ভিত্তি করে। উদাহরণস্বরূপ, যখন আমরা প্রথমবার একটি কম্পিউটার গেম দেখি, আমরা খেলা শুরু করি, এমনকি নিয়ম না জেনেও, এবং শুধুমাত্র খেলার মাধ্যমে এবং আমাদের আচরণ সামঞ্জস্য করে আমরা দ্রুত দক্ষতা উন্নত করতে পারি।

## [পূর্ব-লেকচার কুইজ](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL সম্পাদন করতে আমাদের প্রয়োজন:

* একটি **পরিবেশ** বা **সিমুলেটর**, যা গেমের নিয়ম নির্ধারণ করে। আমাদের সিমুলেটরে পরীক্ষা চালানোর এবং ফলাফল পর্যবেক্ষণ করার সক্ষমতা থাকতে হবে।
* কিছু **রিওয়ার্ড ফাংশন**, যা নির্দেশ করে আমাদের পরীক্ষা কতটা সফল হয়েছে। কম্পিউটার গেম খেলা শেখার ক্ষেত্রে, রিওয়ার্ড হবে আমাদের চূড়ান্ত স্কোর।

রিওয়ার্ড ফাংশনের উপর ভিত্তি করে, আমাদের আচরণ সামঞ্জস্য করে দক্ষতা উন্নত করতে হবে, যাতে পরবর্তীবার আমরা আরও ভালো খেলতে পারি। অন্যান্য মেশিন লার্নিং পদ্ধতির সাথে RL-এর প্রধান পার্থক্য হলো RL-এ আমরা সাধারণত জানি না যে আমরা জিতব বা হারব যতক্ষণ না গেমটি শেষ হয়। তাই, আমরা বলতে পারি না যে একটি নির্দিষ্ট পদক্ষেপ একাই ভালো কিনা - আমরা শুধুমাত্র গেমের শেষে রিওয়ার্ড পাই।

RL-এর সময়, আমরা সাধারণত অনেক পরীক্ষা চালাই। প্রতিটি পরীক্ষার সময়, আমাদের শিখে নেওয়া সর্বোত্তম কৌশল অনুসরণ করা (**এক্সপ্লয়টেশন**) এবং নতুন সম্ভাব্য অবস্থাগুলি অন্বেষণ করা (**এক্সপ্লোরেশন**) এর মধ্যে ভারসাম্য বজায় রাখতে হয়।

## OpenAI Gym

RL-এর জন্য একটি চমৎকার টুল হলো [OpenAI Gym](https://gym.openai.com/) - একটি **সিমুলেশন পরিবেশ**, যা বিভিন্ন পরিবেশ সিমুলেট করতে পারে, যেমন অ্যাটারি গেম থেকে শুরু করে পোল ব্যালেন্সিং-এর পদার্থবিদ্যা। এটি রিইনফোর্সমেন্ট লার্নিং অ্যালগরিদম প্রশিক্ষণের জন্য সবচেয়ে জনপ্রিয় সিমুলেশন পরিবেশগুলির মধ্যে একটি এবং এটি [OpenAI](https://openai.com/) দ্বারা রক্ষণাবেক্ষণ করা হয়।

> **Note**: OpenAI Gym থেকে উপলব্ধ সমস্ত পরিবেশ আপনি [এখানে](https://gym.openai.com/envs/#classic_control) দেখতে পারেন।

## CartPole ব্যালেন্সিং

আপনারা সম্ভবত আধুনিক ব্যালেন্সিং ডিভাইস যেমন *Segway* বা *Gyroscooters* দেখেছেন। এগুলো স্বয়ংক্রিয়ভাবে ব্যালেন্স করতে পারে, একটি অ্যাক্সিলোমিটার বা জাইরোস্কোপ থেকে সংকেতের প্রতিক্রিয়ায় তাদের চাকা সামঞ্জস্য করে। এই অংশে, আমরা একটি অনুরূপ সমস্যা সমাধান করতে শিখব - একটি পোল ব্যালেন্স করা। এটি একটি সার্কাস পারফর্মার তার হাতে একটি পোল ব্যালেন্স করার মতো পরিস্থিতির অনুরূপ - তবে এই পোল ব্যালেন্সিং শুধুমাত্র 1D-তে ঘটে।

ব্যালেন্সিং-এর একটি সরলীকৃত সংস্করণকে **CartPole** সমস্যা বলা হয়। CartPole জগতে, আমাদের একটি অনুভূমিক স্লাইডার রয়েছে যা বাম বা ডানে সরতে পারে, এবং লক্ষ্য হলো স্লাইডারের উপরে একটি উল্লম্ব পোল ব্যালেন্স করা।

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

এই পরিবেশ তৈরি এবং ব্যবহার করতে আমাদের কয়েকটি লাইনের পাইথন কোড প্রয়োজন:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

প্রতিটি পরিবেশ একইভাবে অ্যাক্সেস করা যায়:
* `env.reset` একটি নতুন পরীক্ষা শুরু করে
* `env.step` একটি সিমুলেশন ধাপ সম্পাদন করে। এটি **অ্যাকশন স্পেস** থেকে একটি **অ্যাকশন** গ্রহণ করে এবং একটি **অবজারভেশন** (অবজারভেশন স্পেস থেকে), একটি রিওয়ার্ড এবং একটি টার্মিনেশন ফ্ল্যাগ প্রদান করে।

উপরের উদাহরণে আমরা প্রতিটি ধাপে একটি র্যান্ডম অ্যাকশন সম্পাদন করি, যার ফলে পরীক্ষার জীবন খুবই সংক্ষিপ্ত হয়:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL অ্যালগরিদমের লক্ষ্য হলো একটি মডেল প্রশিক্ষণ করা - তথাকথিত **পলিসি** &pi; - যা একটি প্রদত্ত অবস্থার প্রতিক্রিয়ায় অ্যাকশন প্রদান করবে। আমরা পলিসিকে সম্ভাবনামূলকও বিবেচনা করতে পারি, যেমন কোনো অবস্থান *s* এবং অ্যাকশন *a* এর জন্য এটি &pi;(*a*|*s*) সম্ভাবনা প্রদান করবে যে আমরা *s* অবস্থায় *a* গ্রহণ করব।

## পলিসি গ্রেডিয়েন্টস অ্যালগরিদম

পলিসি মডেল করার সবচেয়ে স্পষ্ট উপায় হলো একটি নিউরাল নেটওয়ার্ক তৈরি করা যা ইনপুট হিসেবে অবস্থান গ্রহণ করবে এবং সংশ্লিষ্ট অ্যাকশন (বা বরং সমস্ত অ্যাকশনের সম্ভাবনা) প্রদান করবে। এক অর্থে, এটি একটি সাধারণ ক্লাসিফিকেশন টাস্কের মতো হবে, একটি বড় পার্থক্য সহ - আমরা আগে থেকে জানি না যে প্রতিটি ধাপে কোন অ্যাকশন গ্রহণ করা উচিত।

এখানে ধারণা হলো এই সম্ভাবনাগুলি অনুমান করা। আমরা **কিউমুলেটিভ রিওয়ার্ডস** এর একটি ভেক্টর তৈরি করি যা পরীক্ষার প্রতিটি ধাপে আমাদের মোট রিওয়ার্ড দেখায়। আমরা **রিওয়ার্ড ডিসকাউন্টিং** প্রয়োগ করি, কিছু গুণক &gamma;=0.99 দ্বারা পূর্ববর্তী রিওয়ার্ডগুলিকে গুণ করে, যাতে পূর্ববর্তী রিওয়ার্ডগুলির ভূমিকা কমানো যায়। তারপর, আমরা পরীক্ষার পথে এমন ধাপগুলিকে শক্তিশালী করি যা বড় রিওয়ার্ড দেয়।

> পলিসি গ্রেডিয়েন্ট অ্যালগরিদম সম্পর্কে আরও জানুন এবং এটি কার্যকরভাবে দেখতে [উদাহরণ নোটবুক](CartPole-RL-TF.ipynb) দেখুন।

## অ্যাক্টর-ক্রিটিক অ্যালগরিদম

পলিসি গ্রেডিয়েন্টস পদ্ধতির একটি উন্নত সংস্করণকে **অ্যাক্টর-ক্রিটিক** বলা হয়। এর প্রধান ধারণা হলো নিউরাল নেটওয়ার্ককে দুটি জিনিস ফেরত দেওয়ার জন্য প্রশিক্ষণ দেওয়া:

* পলিসি, যা নির্ধারণ করে কোন অ্যাকশন নিতে হবে। এই অংশটিকে **অ্যাক্টর** বলা হয়।
* আমরা এই অবস্থায় মোট রিওয়ার্ড কতটা আশা করতে পারি তার একটি অনুমান - এই অংশটিকে **ক্রিটিক** বলা হয়।

এক অর্থে, এই আর্কিটেকচারটি একটি [GAN](../../4-ComputerVision/10-GANs/README.md)-এর মতো, যেখানে দুটি নেটওয়ার্ক একে অপরের বিরুদ্ধে প্রশিক্ষিত হয়। অ্যাক্টর-ক্রিটিক মডেলে, অ্যাক্টর আমাদের নিতে হবে এমন অ্যাকশন প্রস্তাব করে, এবং ক্রিটিক সমালোচনামূলক হয়ে ফলাফল অনুমান করার চেষ্টা করে। তবে, আমাদের লক্ষ্য হলো এই নেটওয়ার্কগুলিকে একসঙ্গে প্রশিক্ষণ দেওয়া।

কারণ আমরা পরীক্ষার সময় উভয়ই প্রকৃত কিউমুলেটিভ রিওয়ার্ড এবং ক্রিটিক দ্বারা ফেরত দেওয়া ফলাফল জানি, এটি তাদের মধ্যে পার্থক্য কমানোর জন্য একটি লস ফাংশন তৈরি করা তুলনামূলকভাবে সহজ। এটি আমাদের **ক্রিটিক লস** প্রদান করবে। আমরা পলিসি গ্রেডিয়েন্ট অ্যালগরিদমে ব্যবহৃত একই পদ্ধতি ব্যবহার করে **অ্যাক্টর লস** গণনা করতে পারি।

এই অ্যালগরিদমগুলির একটি চালানোর পরে, আমরা আশা করতে পারি আমাদের CartPole এইরকম আচরণ করবে:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ অনুশীলন: পলিসি গ্রেডিয়েন্টস এবং অ্যাক্টর-ক্রিটিক RL

নিম্নলিখিত নোটবুকগুলিতে আপনার শেখা চালিয়ে যান:

* [TensorFlow-এ RL](CartPole-RL-TF.ipynb)
* [PyTorch-এ RL](CartPole-RL-PyTorch.ipynb)

## অন্যান্য RL টাস্ক

বর্তমানে রিইনফোর্সমেন্ট লার্নিং একটি দ্রুত বর্ধনশীল গবেষণা ক্ষেত্র। রিইনফোর্সমেন্ট লার্নিং-এর কিছু আকর্ষণীয় উদাহরণ হলো:

* **অ্যাটারি গেম** খেলা শেখানো। এই সমস্যার চ্যালেঞ্জিং অংশ হলো আমাদের কাছে একটি সাধারণ অবস্থান ভেক্টর হিসেবে উপস্থাপিত নয়, বরং একটি স্ক্রিনশট রয়েছে - এবং আমাদের এই স্ক্রিন ইমেজকে একটি ফিচার ভেক্টরে রূপান্তর করতে বা রিওয়ার্ড তথ্য বের করতে CNN ব্যবহার করতে হবে। অ্যাটারি গেমগুলি Gym-এ উপলব্ধ।
* কম্পিউটারকে বোর্ড গেম খেলতে শেখানো, যেমন চেস এবং গো। সম্প্রতি **Alpha Zero** এর মতো অত্যাধুনিক প্রোগ্রাম দুটি এজেন্ট একে অপরের বিরুদ্ধে খেলে এবং প্রতিটি ধাপে উন্নতি করে শূন্য থেকে প্রশিক্ষিত হয়েছে।
* শিল্পে, সিমুলেশন থেকে কন্ট্রোল সিস্টেম তৈরি করতে RL ব্যবহার করা হয়। [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) নামে একটি পরিষেবা বিশেষভাবে এর জন্য ডিজাইন করা হয়েছে।

## উপসংহার

আমরা এখন শিখেছি কীভাবে এজেন্টদের প্রশিক্ষণ দিয়ে ভালো ফলাফল অর্জন করা যায় শুধুমাত্র একটি রিওয়ার্ড ফাংশন প্রদান করে যা গেমের কাঙ্ক্ষিত অবস্থাকে সংজ্ঞায়িত করে এবং তাদের অনুসন্ধান স্থানটি বুদ্ধিমত্তার সাথে অন্বেষণ করার সুযোগ দিয়ে। আমরা সফলভাবে দুটি অ্যালগরিদম চেষ্টা করেছি এবং তুলনামূলকভাবে স্বল্প সময়ের মধ্যে একটি ভালো ফলাফল অর্জন করেছি। তবে, এটি RL-এ আপনার যাত্রার শুরু মাত্র, এবং আপনি যদি আরও গভীরে যেতে চান তবে একটি পৃথক কোর্স নেওয়ার কথা বিবেচনা করা উচিত।

## 🚀 চ্যালেঞ্জ

'অন্যান্য RL টাস্ক' বিভাগে তালিকাভুক্ত অ্যাপ্লিকেশনগুলি অন্বেষণ করুন এবং একটি বাস্তবায়নের চেষ্টা করুন!

## [পোস্ট-লেকচার কুইজ](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## পর্যালোচনা ও স্ব-অধ্যয়ন

আমাদের [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)-এ ক্লাসিক্যাল রিইনফোর্সমেন্ট লার্নিং সম্পর্কে আরও জানুন।

[এই চমৎকার ভিডিওটি](https://www.youtube.com/watch?v=qv6UVOQ0F44) দেখুন যেখানে একটি কম্পিউটার কীভাবে সুপার মারিও খেলতে শিখতে পারে তা আলোচনা করা হয়েছে।

## অ্যাসাইনমেন্ট: [একটি মাউন্টেন কার প্রশিক্ষণ দিন](lab/README.md)

এই অ্যাসাইনমেন্টের সময় আপনার লক্ষ্য হবে একটি ভিন্ন Gym পরিবেশ - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) প্রশিক্ষণ দেওয়া।

---

