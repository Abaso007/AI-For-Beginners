{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL treniravimas balansavimui su Cartpole\n",
    "\n",
    "Šis užrašų knygelė yra dalis [AI pradedantiesiems mokymo programos](http://aka.ms/ai-beginners). Ji buvo įkvėpta [oficialaus PyTorch mokymo vadovo](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) ir [šios Cartpole PyTorch implementacijos](https://github.com/yc930401/Actor-Critic-pytorch).\n",
    "\n",
    "Šiame pavyzdyje naudosime RL, kad išmokytume modelį balansuoti stulpą ant vežimėlio, kuris gali judėti į kairę ir dešinę horizontaliai. Naudosime [OpenAI Gym](https://www.gymlibrary.ml/) aplinką, kad simuliuotume stulpą.\n",
    "\n",
    "> **Pastaba**: Šio pamokos kodo galite vykdyti lokaliai (pvz., naudojant Visual Studio Code), tokiu atveju simuliacija atsidarys naujame lange. Vykdant kodą internete, gali reikėti atlikti tam tikrus kodo pakeitimus, kaip aprašyta [čia](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "Pradėsime nuo to, kad įsitikinsime, jog Gym yra įdiegtas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar sukurkime CartPole aplinką ir pažiūrėkime, kaip su ja dirbti. Aplinka turi šias savybes:\n",
    "\n",
    "* **Veiksmų erdvė** – tai galimų veiksmų rinkinys, kuriuos galime atlikti kiekviename simuliacijos žingsnyje  \n",
    "* **Stebėjimų erdvė** – tai stebėjimų, kuriuos galime atlikti, erdvė  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pažiūrėkime, kaip veikia simuliacija. Toliau pateikta ciklo struktūra vykdo simuliaciją tol, kol `env.step` negrąžina užbaigimo ženklo `done`. Atsitiktinai pasirinksime veiksmus naudodami `env.action_space.sample()`, o tai reiškia, kad eksperimentas greičiausiai labai greitai nepavyks (CartPole aplinka baigiasi, kai CartPole greitis, pozicija ar kampas viršija tam tikras ribas).\n",
    "\n",
    "> Simuliacija atsidarys naujame lange. Galite paleisti kodą kelis kartus ir stebėti, kaip jis elgiasi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galite pastebėti, kad stebėjimai susideda iš 4 skaičių. Jie yra:\n",
    "- Vežimėlio pozicija\n",
    "- Vežimėlio greitis\n",
    "- Stulpo kampas\n",
    "- Stulpo sukimosi greitis\n",
    "\n",
    "`rew` yra atlygis, kurį gauname kiekviename žingsnyje. Galite pastebėti, kad CartPole aplinkoje už kiekvieną simuliacijos žingsnį gaunate 1 tašką, o tikslas yra maksimaliai padidinti bendrą atlygį, t. y. laiką, per kurį CartPole sugeba išlaikyti pusiausvyrą nenukritęs.\n",
    "\n",
    "Stiprinamojo mokymosi metu mūsų tikslas yra išmokyti **politiką** $\\pi$, kuri kiekvienai būsenai $s$ nurodys, kokį veiksmą $a$ atlikti, iš esmės $a = \\pi(s)$.\n",
    "\n",
    "Jei norite probabilistinio sprendimo, galite galvoti apie politiką kaip apie grąžinančią tikimybių rinkinį kiekvienam veiksmui, t. y. $\\pi(a|s)$ reikštų tikimybę, kad turėtume atlikti veiksmą $a$ būsenos $s$ metu.\n",
    "\n",
    "## Politikos gradientų metodas\n",
    "\n",
    "Paprasčiausiame RL algoritme, vadinamame **Politikos gradientu**, mes treniruosime neuroninį tinklą, kad jis numatytų kitą veiksmą.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mes treniruosime tinklą vykdydami daugybę eksperimentų ir atnaujindami tinklą po kiekvieno vykdymo. Apibrėžkime funkciją, kuri vykdys eksperimentą ir grąžins rezultatus (vadinamąjį **pėdsaką**) - visas būsenas, veiksmus (ir jų rekomenduojamas tikimybes) bei atlygius:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galite paleisti vieną epizodą su netreniruotu tinklu ir pastebėti, kad bendras atlygis (dar žinomas kaip epizodo trukmė) yra labai mažas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viena iš sudėtingų politikos gradientų algoritmo aspektų yra naudoti **diskontuotus apdovanojimus**. Idėja yra ta, kad mes apskaičiuojame bendrų apdovanojimų vektorių kiekviename žaidimo žingsnyje, o šio proceso metu ankstyvus apdovanojimus diskontuojame naudodami tam tikrą koeficientą $gamma$. Taip pat normalizuojame gautą vektorių, nes jį naudosime kaip svorį, kuris paveiks mūsų mokymąsi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar pradėkime mokymą! Mes vykdysime 300 epizodų, o kiekviename epizode atliksime šiuos veiksmus:\n",
    "\n",
    "1. Vykdysime eksperimentą ir surinksime seką.\n",
    "2. Apskaičiuosime skirtumą (`gradients`) tarp atliktų veiksmų ir numatytų tikimybių. Kuo mažesnis skirtumas, tuo labiau esame įsitikinę, kad pasirinkome teisingą veiksmą.\n",
    "3. Apskaičiuosime diskontuotus apdovanojimus ir padauginsime gradientus iš diskontuotų apdovanojimų – tai užtikrins, kad žingsniai su didesniais apdovanojimais turės didesnį poveikį galutiniam rezultatui nei tie, kurie gavo mažesnius apdovanojimus.\n",
    "4. Tikėtini tiksliniai veiksmai mūsų neuroniniam tinklui bus iš dalies paimti iš numatytų tikimybių eksperimentų metu, o iš dalies iš apskaičiuotų gradientų. Naudosime `alpha` parametrą, kad nustatytume, kiek gradientai ir apdovanojimai bus įtraukti – tai vadinama *mokymosi greičiu* stiprinimo algoritmo.\n",
    "5. Galiausiai, treniruosime tinklą pagal būsenas ir tikėtinus veiksmus, ir kartosime procesą.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar paleiskime epizodą su atvaizdavimu, kad pamatytume rezultatą:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tikimės, kad dabar galite matyti, jog stulpas gali gana gerai išlaikyti pusiausvyrą!\n",
    "\n",
    "## Aktoriaus-Kritiko modelis\n",
    "\n",
    "Aktoriaus-Kritiko modelis yra tolesnė politikos gradientų plėtra, kurioje mes kuriame neuroninį tinklą, kad išmoktume tiek politiką, tiek numatomus apdovanojimus. Tinklas turės du išėjimus (arba galite tai laikyti dviem atskirais tinklais):\n",
    "* **Aktorius** rekomenduos veiksmą, kurį reikia atlikti, pateikdamas mums būsenos tikimybių pasiskirstymą, kaip ir politikos gradientų modelyje.\n",
    "* **Kritikas** įvertins, kokie galėtų būti apdovanojimai už tuos veiksmus. Jis grąžina bendrą numatomą apdovanojimų sumą ateityje esant tam tikrai būsenai.\n",
    "\n",
    "Apibrėžkime tokį modelį:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mums reikėtų šiek tiek pakeisti mūsų `discounted_rewards` ir `run_episode` funkcijas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar vykdysime pagrindinį mokymo ciklą. Naudosime rankinį tinklo mokymo procesą, apskaičiuodami tinkamas nuostolių funkcijas ir atnaujindami tinklo parametrus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Svarbiausia\n",
    "\n",
    "Šioje demonstracijoje matėme du RL algoritmus: paprastą politikos gradientą ir sudėtingesnį aktoriaus-kritiko metodą. Galite pastebėti, kad šie algoritmai veikia su abstrakčiomis būsenos, veiksmo ir atlygio sąvokomis – todėl juos galima taikyti labai skirtingose aplinkose.\n",
    "\n",
    "Pastiprinimo mokymasis leidžia mums išmokti geriausią strategiją problemos sprendimui, tiesiog stebint galutinį atlygį. Tai, kad mums nereikia pažymėtų duomenų rinkinių, leidžia daug kartų kartoti simuliacijas, siekiant optimizuoti mūsų modelius. Tačiau RL srityje vis dar yra daug iššūkių, kuriuos galite išmokti, jei nuspręsite gilintis į šią įdomią dirbtinio intelekto sritį.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Atsakomybės apribojimas**:  \nŠis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama naudoti profesionalų žmogaus vertimą. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius dėl šio vertimo naudojimo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-31T12:31:01+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "lt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}