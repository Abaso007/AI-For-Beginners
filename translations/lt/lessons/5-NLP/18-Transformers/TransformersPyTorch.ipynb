{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dėmesio mechanizmai ir transformatoriai\n",
    "\n",
    "Viena iš pagrindinių pasikartojančių tinklų trūkumų yra ta, kad visi sekos žodžiai turi vienodą įtaką rezultatui. Tai lemia neoptimalų veikimą naudojant standartinius LSTM koduotojo-dekoduotojo modelius sekų užduotims, tokioms kaip pavadintų objektų atpažinimas ar mašininis vertimas. Iš tikrųjų tam tikri žodžiai įvesties sekoje dažnai turi didesnę įtaką išvesties sekai nei kiti.\n",
    "\n",
    "Apsvarstykime sekos į seką modelį, pavyzdžiui, mašininį vertimą. Jis įgyvendinamas naudojant du pasikartojančius tinklus, kur vienas tinklas (**koduotojas**) suspaudžia įvesties seką į paslėptą būseną, o kitas tinklas (**dekoduotojas**) išskleidžia šią paslėptą būseną į išverstą rezultatą. Problema su šiuo metodu yra ta, kad tinklo galutinė būsena sunkiai prisimena sakinio pradžią, todėl modelis prastai veikia su ilgais sakiniais.\n",
    "\n",
    "**Dėmesio mechanizmai** suteikia galimybę įvertinti kiekvieno įvesties vektoriaus kontekstinę įtaką kiekvienai RNN išvesties prognozei. Tai įgyvendinama sukuriant trumpesnius ryšius tarp įvesties RNN tarpinių būsenų ir išvesties RNN. Tokiu būdu, generuojant išvesties simbolį $y_t$, atsižvelgiama į visas įvesties paslėptas būsenas $h_i$, su skirtingais svorio koeficientais $\\alpha_{t,i}$.\n",
    "\n",
    "![Vaizdas, rodantis koduotojo/dekoduotojo modelį su adityviniu dėmesio sluoksniu](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)\n",
    "*Koduotojo-dekoduotojo modelis su adityviniu dėmesio mechanizmu iš [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cituota iš [šio tinklaraščio įrašo](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Dėmesio matrica $\\{\\alpha_{i,j}\\}$ atspindi, kokiu mastu tam tikri įvesties žodžiai prisideda prie tam tikro žodžio generavimo išvesties sekoje. Žemiau pateiktas tokios matricos pavyzdys:\n",
    "\n",
    "![Vaizdas, rodantis pavyzdinį suderinimą, rastą naudojant RNNsearch-50, paimta iš Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)\n",
    "\n",
    "*Paveikslas paimtas iš [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (3 pav.)*\n",
    "\n",
    "Dėmesio mechanizmai yra atsakingi už daugelį dabartinių ar beveik dabartinių pažangiausių rezultatų natūralios kalbos apdorojime. Tačiau dėmesio pridėjimas žymiai padidina modelio parametrų skaičių, o tai sukėlė mastelio problemas su RNN. Vienas iš pagrindinių RNN mastelio apribojimų yra tai, kad modelių pasikartojantis pobūdis apsunkina mokymo partijų kūrimą ir lygiagretinimą. RNN kiekvienas sekos elementas turi būti apdorojamas nuosekliai, todėl jo negalima lengvai lygiagrečiai apdoroti.\n",
    "\n",
    "Dėmesio mechanizmų pritaikymas kartu su šiuo apribojimu paskatino sukurti dabartinius pažangiausius transformatorių modelius, kuriuos šiandien naudojame, tokius kaip BERT ar OpenGPT3.\n",
    "\n",
    "## Transformatorių modeliai\n",
    "\n",
    "Užuot perdavę kiekvienos ankstesnės prognozės kontekstą į kitą vertinimo žingsnį, **transformatorių modeliai** naudoja **pozicinius kodavimus** ir dėmesį, kad užfiksuotų įvesties kontekstą tam tikrame teksto lange. Žemiau pateiktas vaizdas rodo, kaip poziciniai kodavimai su dėmesiu gali užfiksuoti kontekstą tam tikrame lange.\n",
    "\n",
    "![Animuotas GIF, rodantis, kaip atliekami vertinimai transformatorių modeliuose.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)\n",
    "\n",
    "Kadangi kiekviena įvesties pozicija yra nepriklausomai susieta su kiekviena išvesties pozicija, transformatoriai gali geriau lygiagrečiai apdoroti nei RNN, o tai leidžia kurti daug didesnius ir išraiškingesnius kalbos modelius. Kiekviena dėmesio galvutė gali būti naudojama mokytis skirtingų žodžių tarpusavio ryšių, kurie pagerina natūralios kalbos apdorojimo užduotis.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) yra labai didelis daugiasluoksnis transformatorių tinklas su 12 sluoksnių *BERT-base* versijoje ir 24 sluoksniais *BERT-large* versijoje. Modelis pirmiausia yra iš anksto apmokomas naudojant didelį tekstų korpusą (Vikipedija + knygos) taikant nesupervizuotą mokymąsi (prognozuojant užmaskuotus žodžius sakinyje). Išankstinio mokymo metu modelis įgyja reikšmingą kalbos supratimo lygį, kurį vėliau galima pritaikyti kitoms duomenų aibėms naudojant smulkųjį derinimą. Šis procesas vadinamas **perkėlimo mokymusi**.\n",
    "\n",
    "![Paveikslas iš http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)\n",
    "\n",
    "Yra daug transformatorių architektūrų variantų, įskaitant BERT, DistilBERT, BigBird, OpenGPT3 ir daugiau, kuriuos galima smulkiai derinti. [HuggingFace paketas](https://github.com/huggingface/) suteikia saugyklą daugeliui šių architektūrų mokyti naudojant PyTorch.\n",
    "\n",
    "## BERT naudojimas teksto klasifikavimui\n",
    "\n",
    "Pažiūrėkime, kaip galime naudoti iš anksto apmokytą BERT modelį, kad išspręstume tradicinę užduotį: sekos klasifikavimą. Mes klasifikuosime savo pradinį AG News duomenų rinkinį.\n",
    "\n",
    "Pirmiausia įkelkime HuggingFace biblioteką ir mūsų duomenų rinkinį:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kadangi naudosime iš anksto apmokytą BERT modelį, mums reikės naudoti specifinį tokenizatorių. Pirmiausia įkelsime tokenizatorių, susietą su iš anksto apmokytu BERT modeliu.\n",
    "\n",
    "HuggingFace biblioteka turi iš anksto apmokytų modelių saugyklą, kurią galite naudoti tiesiog nurodydami jų pavadinimus kaip argumentus `from_pretrained` funkcijoms. Visi reikalingi dvejetainiai modelio failai bus automatiškai atsisiųsti.\n",
    "\n",
    "Tačiau tam tikrais atvejais jums gali prireikti įkelti savo modelius. Tokiu atveju galite nurodyti katalogą, kuriame yra visi susiję failai, įskaitant tokenizatoriaus parametrus, `config.json` failą su modelio parametrais, dvejetainius svorius ir kt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` objektas turi `encode` funkciją, kuri gali būti tiesiogiai naudojama tekstui užkoduoti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada sukurkime iteratorius, kuriuos naudosime mokymo metu, kad pasiektume duomenis. Kadangi BERT naudoja savo kodavimo funkciją, mums reikės apibrėžti užpildymo funkciją, panašią į anksčiau apibrėžtą `padify`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mūsų atveju naudosime iš anksto apmokytą BERT modelį, vadinamą `bert-base-uncased`. Įkelkime modelį naudodami `BertForSequenceClassfication` paketą. Tai užtikrina, kad mūsų modelis jau turi reikiamą klasifikavimo architektūrą, įskaitant galutinį klasifikatorių. Pamatysite įspėjimo pranešimą, kad galutinio klasifikatoriaus svoriai nėra inicializuoti ir modelis reikalautų išankstinio apmokymo - tai visiškai normalu, nes būtent tai mes ir ketiname daryti!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar esame pasiruošę pradėti mokymus! Kadangi BERT jau yra iš anksto apmokytas, norime pradėti nuo gana mažo mokymosi greičio, kad nepažeistume pradinių svorių.\n",
    "\n",
    "Visą sunkų darbą atlieka `BertForSequenceClassification` modelis. Kai iškviečiame modelį su mokymo duomenimis, jis grąžina tiek nuostolį, tiek tinklo išvestį už pateiktą minibatch įvestį. Nuostolį naudojame parametrų optimizavimui (`loss.backward()` atlieka atgalinį skaičiavimą), o `out` naudojame mokymo tikslumui apskaičiuoti, lygindami gautas etiketes `labs` (apskaičiuotas naudojant `argmax`) su laukiamomis `labels`.\n",
    "\n",
    "Norėdami kontroliuoti procesą, kaupiame nuostolį ir tikslumą per kelias iteracijas ir juos atspausdiname kas `report_freq` mokymo ciklų.\n",
    "\n",
    "Šis mokymas greičiausiai užtruks gana ilgai, todėl ribojame iteracijų skaičių.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galite pastebėti (ypač jei padidinsite iteracijų skaičių ir palauksite pakankamai ilgai), kad BERT klasifikacija suteikia mums gana gerą tikslumą! Taip yra todėl, kad BERT jau gana gerai supranta kalbos struktūrą, o mums tereikia pritaikyti galutinį klasifikatorių. Tačiau, kadangi BERT yra didelis modelis, visas mokymo procesas užtrunka ilgai ir reikalauja didelių skaičiavimo resursų! (GPU, ir geriausia, jei jų būtų daugiau nei vienas).\n",
    "\n",
    "> **Pastaba:** Mūsų pavyzdyje naudojome vieną iš mažiausių iš anksto apmokytų BERT modelių. Yra didesnių modelių, kurie tikriausiai duotų geresnius rezultatus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelio veikimo vertinimas\n",
    "\n",
    "Dabar galime įvertinti mūsų modelio veikimą testavimo duomenų rinkinyje. Vertinimo ciklas yra gana panašus į mokymo ciklą, tačiau nepamirškime perjungti modelio į vertinimo režimą, iškviečiant `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagrindinės mintys\n",
    "\n",
    "Šiame skyriuje matėme, kaip lengva paimti iš anksto apmokytą kalbos modelį iš **transformers** bibliotekos ir pritaikyti jį mūsų teksto klasifikavimo užduočiai. Panašiai BERT modeliai gali būti naudojami objektų išskyrimui, klausimų atsakymui ir kitoms NLP užduotims.\n",
    "\n",
    "Transformeriai yra dabartinė pažangiausia technologija NLP srityje, ir daugeliu atvejų tai turėtų būti pirmasis sprendimas, kurį pradedate testuoti, kai įgyvendinate individualius NLP sprendimus. Tačiau suprasti pagrindinius pasikartojančių neuroninių tinklų principus, aptartus šiame modulyje, yra itin svarbu, jei norite kurti pažangius neuroninius modelius.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Atsakomybės apribojimas**:  \nŠis dokumentas buvo išverstas naudojant dirbtinio intelekto vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, atkreipiame dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama naudotis profesionalių vertėjų paslaugomis. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus aiškinimus, kylančius dėl šio vertimo naudojimo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-31T13:49:51+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "lt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}