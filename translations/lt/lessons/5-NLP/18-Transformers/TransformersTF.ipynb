{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dėmesio mechanizmai ir transformatoriai\n",
    "\n",
    "Viena iš pagrindinių pasikartojančių tinklų trūkumų yra ta, kad visi žodžiai sekoje turi vienodą įtaką rezultatui. Tai lemia neoptimalų standartinių LSTM koduotojo-dekoduotojo modelių veikimą atliekant sekos į seką užduotis, tokias kaip pavadintų objektų atpažinimas ar mašininis vertimas. Iš tiesų, tam tikri žodžiai įvesties sekoje dažnai turi didesnę įtaką išvesties sekai nei kiti.\n",
    "\n",
    "Apsvarstykime sekos į seką modelį, pavyzdžiui, mašininį vertimą. Jis įgyvendinamas naudojant du pasikartojančius tinklus, kur vienas tinklas (**koduotojas**) suspaudžia įvesties seką į paslėptą būseną, o kitas tinklas (**dekoduotojas**) išskleidžia šią paslėptą būseną į išverstą rezultatą. Problema su šiuo požiūriu yra ta, kad tinklo galutinė būsena sunkiai prisimena sakinio pradžią, todėl modelis prastai veikia su ilgais sakiniais.\n",
    "\n",
    "**Dėmesio mechanizmai** suteikia galimybę įvertinti kiekvieno įvesties vektoriaus kontekstinę įtaką kiekvienai RNN išvesties prognozei. Tai įgyvendinama sukuriant trumpesnius ryšius tarp tarpinių įvesties RNN būsenų ir išvesties RNN. Tokiu būdu, generuojant išvesties simbolį $y_t$, atsižvelgiama į visas įvesties paslėptas būsenas $h_i$, su skirtingais svorio koeficientais $\\alpha_{t,i}$. \n",
    "\n",
    "![Vaizdas, rodantis koduotojo/dekoduotojo modelį su adityviniu dėmesio sluoksniu](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)\n",
    "*Koduotojo-dekoduotojo modelis su adityviniu dėmesio mechanizmu iš [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cituota iš [šio tinklaraščio įrašo](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "Dėmesio matrica $\\{\\alpha_{i,j}\\}$ atspindi, kokiu mastu tam tikri įvesties žodžiai prisideda prie tam tikro žodžio generavimo išvesties sekoje. Žemiau pateiktas tokios matricos pavyzdys:\n",
    "\n",
    "![Vaizdas, rodantis pavyzdinį suderinimą, rastą naudojant RNNsearch-50, paimta iš Bahdanau - arviz.org](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)\n",
    "\n",
    "*Paveikslas paimtas iš [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (3 pav.)*\n",
    "\n",
    "Dėmesio mechanizmai yra atsakingi už dabartinę arba beveik dabartinę pažangiausią natūralios kalbos apdorojimo būklę. Tačiau dėmesio pridėjimas žymiai padidina modelio parametrų skaičių, o tai sukėlė mastelio problemas su RNN. Vienas iš pagrindinių RNN mastelio apribojimų yra tas, kad modelių pasikartojantis pobūdis apsunkina mokymo partijų kūrimą ir lygiagretinimą. RNN kiekvienas sekos elementas turi būti apdorojamas nuosekliai, todėl jų lygiagretinimas yra sudėtingas.\n",
    "\n",
    "Dėmesio mechanizmų pritaikymas kartu su šiuo apribojimu paskatino sukurti dabartinius pažangiausius transformatorių modelius, kuriuos šiandien naudojame, tokius kaip BERT ar OpenGPT3.\n",
    "\n",
    "## Transformatorių modeliai\n",
    "\n",
    "Užuot perdavę kiekvienos ankstesnės prognozės kontekstą į kitą vertinimo žingsnį, **transformatorių modeliai** naudoja **pozicinius kodavimus** ir **dėmesį**, kad užfiksuotų įvesties kontekstą tam tikrame teksto lange. Žemiau pateiktas paveikslas rodo, kaip poziciniai kodavimai kartu su dėmesiu gali užfiksuoti kontekstą tam tikrame lange.\n",
    "\n",
    "![Animuotas GIF, rodantis, kaip atliekami vertinimai transformatorių modeliuose.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Kadangi kiekviena įvesties pozicija yra nepriklausomai susieta su kiekviena išvesties pozicija, transformatoriai gali geriau lygiagretinti nei RNN, o tai leidžia kurti daug didesnius ir išraiškingesnius kalbos modelius. Kiekviena dėmesio galvutė gali būti naudojama mokytis skirtingų žodžių tarpusavio ryšių, kurie pagerina natūralios kalbos apdorojimo užduotis.\n",
    "\n",
    "## Paprasto transformatoriaus modelio kūrimas\n",
    "\n",
    "Keras neturi įmontuoto transformatoriaus sluoksnio, tačiau mes galime sukurti savo. Kaip ir anksčiau, sutelksime dėmesį į AG News duomenų rinkinio teksto klasifikavimą, tačiau verta paminėti, kad transformatorių modeliai geriausius rezultatus pasiekia sudėtingesnėse NLP užduotyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
    "\n",
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nauji sluoksniai Keras turėtų paveldėti `Layer` klasę ir įgyvendinti `call` metodą. Pradėkime nuo **Pozicinio Įterpimo** sluoksnio. Naudosime [šiek tiek kodo iš oficialios Keras dokumentacijos](https://keras.io/examples/nlp/text_classification_with_transformer/). Mes darysime prielaidą, kad visi įvesties sekos yra užpildytos iki ilgio `maxlen`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x+positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Šis sluoksnis susideda iš dviejų `Embedding` sluoksnių: vienas skirtas žodžių įterpimui (kaip aptarėme anksčiau), o kitas – pozicijų įterpimui. Pozicijos sukuriamos kaip natūralių skaičių seka nuo 0 iki `maxlen`, naudojant `tf.range`, ir tada perduodamos per įterpimo sluoksnį. Du gauti įterpimo vektoriai yra sudedami, sukuriant poziciškai įterptą įvesties reprezentaciją, kurios forma yra `maxlen`$\\times$`embed_dim`.\n",
    "\n",
    "Dabar įgyvendinkime transformatoriaus bloką. Jis priims anksčiau apibrėžto įterpimo sluoksnio išvestį:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='attn')\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar esame pasiruošę apibrėžti pilną transformatoriaus modelį:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 256, 32)           648192    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 256, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 659,592\n",
      "Trainable params: 659,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 256\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
    "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
    "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "938/938 [==============================] - 45s 39ms/step - loss: 0.4978 - acc: 0.8068 - val_loss: 0.2808 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c2427a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training tokenizer')\n",
    "model.layers[0].adapt(ds_train.map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT transformerių modeliai\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) yra labai didelis daugiasluoksnis transformerių tinklas, turintis 12 sluoksnių *BERT-base* versijoje ir 24 sluoksnius *BERT-large* versijoje. Modelis pirmiausia yra iš anksto apmokomas naudojant didelį tekstinių duomenų korpusą (Vikipedija + knygos) taikant nesupervizuotą mokymą (prognozuojant užmaskuotus žodžius sakinyje). Per šį išankstinį mokymą modelis įgyja reikšmingą kalbos supratimo lygį, kurį vėliau galima pritaikyti su kitais duomenų rinkiniais naudojant smulkųjį derinimą. Šis procesas vadinamas **perkėlimo mokymusi**.\n",
    "\n",
    "![paveikslėlis iš http://jalammar.github.io/illustrated-bert/](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)\n",
    "\n",
    "Yra daug transformerių architektūrų variantų, įskaitant BERT, DistilBERT, BigBird, OpenGPT3 ir kitus, kuriuos galima smulkiai derinti.\n",
    "\n",
    "Pažiūrėkime, kaip galime naudoti iš anksto apmokytą BERT modelį, kad išspręstume tradicinę sekų klasifikavimo problemą. Pasiskolinsime idėją ir šiek tiek kodo iš [oficialios dokumentacijos](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
    "\n",
    "Norėdami įkelti iš anksto apmokytus modelius, naudosime **Tensorflow hub**. Pirmiausia įkelkime BERT specifinį vektorizatorių:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41180/4216669875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text \n",
    "import tensorflow_hub as hub\n",
    "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(['I love transformers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Svarbu naudoti tą patį vektorizatorių, kuris buvo naudojamas originaliam tinklui treniruoti. Be to, BERT vektorizatorius grąžina tris komponentus:\n",
    "* `input_word_ids`, tai yra įvesties sakinio žodžių numerių seka\n",
    "* `input_mask`, rodanti, kuri seka yra tikroji įvestis, o kuri – užpildas. Tai panašu į kaukę, kurią sukuria `Masking` sluoksnis\n",
    "* `input_type_ids` naudojamas kalbos modeliavimo užduotims ir leidžia nurodyti du įvesties sakinius vienoje sekoje.\n",
    "\n",
    "Tuomet galime sukurti BERT funkcijų ištraukiklį:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_output -> (1, 128)\n",
      "encoder_outputs -> 4\n",
      "sequence_output -> (1, 128, 128)\n",
      "default -> (1, 128)\n"
     ]
    }
   ],
   "source": [
    "z = bert(vectorizer(['I love transformers']))\n",
    "for i,x in z.items():\n",
    "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taigi, BERT sluoksnis grąžina keletą naudingų rezultatų:\n",
    "* `pooled_output` yra sekos visų žetonų vidurkis. Galite tai laikyti kaip išmanų viso tinklo semantinį įterpimą. Tai atitinka `GlobalAveragePooling1D` sluoksnio išvestį mūsų ankstesniame modelyje.\n",
    "* `sequence_output` yra paskutinio transformatoriaus sluoksnio išvestis (atitinka `TransformerBlock` išvestį mūsų aukščiau esančiame modelyje).\n",
    "* `encoder_outputs` yra visų transformatorių sluoksnių išvestys. Kadangi įkėlėme 4 sluoksnių BERT modelį (kaip tikriausiai galite nuspėti iš pavadinimo, kuriame yra `4_H`), jis turi 4 tensorius. Paskutinis iš jų yra toks pat kaip `sequence_output`.\n",
    "\n",
    "Dabar apibrėšime viso proceso klasifikavimo modelį. Naudosime *funkcinį modelio apibrėžimą*, kai apibrėžiame modelio įvestį ir tada pateikiame seriją išraiškų, kad apskaičiuotume jo išvestį. Taip pat padarysime, kad BERT modelio svoriai nebūtų treniruojami, ir treniruosime tik galutinį klasifikatorių:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 516\n",
      "Non-trainable params: 4,782,465\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(shape=(),dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = bert(x)\n",
    "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
    "out = keras.layers.Dense(4,activation='softmax')(x)\n",
    "model = keras.models.Model(inp,out)\n",
    "bert.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 528s 559ms/step - loss: 0.8056 - acc: 0.6983 - val_loss: 0.5953 - val_acc: 0.7888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb1e36d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nepaisant to, kad treniruojamų parametrų yra nedaug, procesas vyksta gana lėtai, nes BERT požymių ištraukiklis yra skaičiavimo požiūriu sudėtingas. Panašu, kad mums nepavyko pasiekti tinkamo tikslumo, galbūt dėl nepakankamo treniravimo arba dėl modelio parametrų trūkumo.\n",
    "\n",
    "Pabandykime atšildyti BERT svorius ir taip pat jį treniruoti. Tam reikės labai mažo mokymosi greičio, taip pat atsargesnės treniravimo strategijos su **apšilimu**, naudojant **AdamW** optimizatorių. Naudosime `tf-models-official` paketą optimizatoriui sukurti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 4782465     keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,782,981\n",
      "Trainable params: 4,782,980\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "938/938 [==============================] - 629s 664ms/step - loss: 0.6344 - acc: 0.7658 - val_loss: 0.4876 - val_acc: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bb0bd0070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from official.nlp import optimization \n",
    "bert.trainable=True\n",
    "model.summary()\n",
    "epochs = 3\n",
    "opt = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=epochs*len(ds_train),\n",
    "    num_warmup_steps=0.1*epochs*len(ds_train),\n",
    "    optimizer_type='adamw')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
    "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaip matote, mokymas vyksta gana lėtai – tačiau galite eksperimentuoti ir treniruoti modelį kelis epochus (5–10), kad pamatytumėte, ar galite pasiekti geriausią rezultatą, palyginti su anksčiau naudotais metodais.\n",
    "\n",
    "## Huggingface Transformers biblioteka\n",
    "\n",
    "Kitas labai dažnas (ir šiek tiek paprastesnis) būdas naudoti Transformer modelius yra [HuggingFace paketas](https://github.com/huggingface/), kuris suteikia paprastus komponentus įvairioms NLP užduotims. Jis prieinamas tiek Tensorflow, tiek PyTorch – dar vienai labai populiariai neuroninių tinklų sistemai.\n",
    "\n",
    "> **Note**: Jei nesate suinteresuoti pamatyti, kaip veikia Transformers biblioteka – galite praleisti šio užrašų knygelės pabaigą, nes nieko iš esmės naujo, palyginti su tuo, ką darėme aukščiau, nepamatysite. Mes kartosime tuos pačius BERT modelio mokymo žingsnius, naudodami kitą biblioteką ir žymiai didesnį modelį. Taigi procesas apima gana ilgą mokymą, todėl galite tiesiog peržvelgti kodą.\n",
    "\n",
    "Pažiūrėkime, kaip mūsų problemą galima išspręsti naudojant [Huggingface Transformers](http://huggingface.co).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pirmiausia turime pasirinkti modelį, kurį naudosime. Be kelių įmontuotų modelių, Huggingface turi [internetinį modelių saugyklą](https://huggingface.co/models), kur bendruomenė dalijasi daugybe iš anksto apmokytų modelių. Visi šie modeliai gali būti įkelti ir naudojami tiesiog nurodant modelio pavadinimą. Visi reikalingi dvejetainiai failai modeliui bus automatiškai atsisiųsti.\n",
    "\n",
    "Tam tikrais atvejais jums gali prireikti įkelti savo modelius. Tokiu atveju galite nurodyti katalogą, kuriame yra visi susiję failai, įskaitant parametrus, skirtus tokenizeriui, `config.json` failą su modelio parametrais, dvejetainius svorius ir pan.\n",
    "\n",
    "Iš modelio pavadinimo galime sukurti tiek modelį, tiek tokenizerį. Pradėkime nuo tokenizerio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "#bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer` objektas turi `encode` funkciją, kuri gali būti tiesiogiai naudojama tekstui užkoduoti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Tensorflow is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mes taip pat galime naudoti tokenizatorių, kad užkoduotume seką taip, kad ji būtų tinkama perduoti modeliui, t. y. įtraukiant `token_ids`, `input_mask` laukus ir kt. Taip pat galime nurodyti, kad norime Tensorflow tensorių, pateikdami argumentą `return_tensors='tf'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hello, there'],return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Šiuo atveju naudosime iš anksto apmokytą BERT modelį, vadinamą `bert-base-uncased`. *Uncased* reiškia, kad modelis yra nejautrus raidžių dydžiui.\n",
    "\n",
    "Treniruojant modelį, turime pateikti tokenizuotą seką kaip įvestį, todėl sukursime duomenų apdorojimo procesą. Kadangi `tokenizer.encode` yra Python funkcija, naudosime tą patį metodą kaip ir paskutiniame skyriuje, iškviesdami ją naudojant `py_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
    "\n",
    "def process_fn(x):\n",
    "    s = x['title']+' '+x['description']\n",
    "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
    "    e.set_shape(MAX_SEQ_LEN)\n",
    "    return e,x['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar galime įkelti faktinį modelį naudodami `BertForSequenceClassification` paketą. Tai užtikrina, kad mūsų modelis jau turi reikiamą klasifikavimo architektūrą, įskaitant galutinį klasifikatorių. Pamatysite įspėjimo pranešimą, kad galutinio klasifikatoriaus svoriai nėra inicializuoti, ir modelis reikalautų išankstinio mokymo - tai visiškai normalu, nes būtent tai mes ketiname daryti!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaip matote iš `summary()`, modelis turi beveik 110 milijonų parametrų! Tikėtina, kad jei norime paprastos klasifikavimo užduoties su palyginti mažu duomenų rinkiniu, nenorime treniruoti BERT bazinio sluoksnio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "=================================================================\n",
      "Total params: 109,485,316\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar esame pasiruošę pradėti mokymus!\n",
    "\n",
    "> **Pastaba**: Pilno masto BERT modelio mokymas gali užtrukti labai daug laiko! Todėl mes jį treniruosime tik pirmosioms 32 partijoms. Tai tik tam, kad parodytume, kaip nustatomas modelio mokymas. Jei norite išbandyti pilno masto mokymą, tiesiog pašalinkite `steps_per_epoch` ir `validation_steps` parametrus ir pasiruoškite laukti!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 142s 4s/step - loss: 1.3896 - acc: 0.2500 - val_loss: 1.3863 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d40a4b6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jei padidinsite iteracijų skaičių, palauksite pakankamai ilgai ir treniruositės kelis epochus, galite tikėtis, kad BERT klasifikacija suteiks geriausią tikslumą! Taip yra todėl, kad BERT jau gana gerai supranta kalbos struktūrą, ir mums tereikia pritaikyti galutinį klasifikatorių. Tačiau, kadangi BERT yra didelis modelis, visas treniravimo procesas užtrunka ilgai ir reikalauja rimtų skaičiavimo išteklių! (GPU, ir pageidautina daugiau nei vieno).\n",
    "\n",
    "> **Pastaba:** Mūsų pavyzdyje naudojome vieną iš mažiausių iš anksto apmokytų BERT modelių. Yra didesnių modelių, kurie tikriausiai duotų geresnius rezultatus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagrindinės mintys\n",
    "\n",
    "Šiame skyriuje aptarėme naujausias modelių architektūras, pagrįstas **transformeriais**. Mes pritaikėme jas savo teksto klasifikavimo užduočiai, tačiau BERT modeliai taip pat gali būti naudojami entitetų išskyrimui, klausimų-atsakymų sistemoms ir kitoms NLP užduotims.\n",
    "\n",
    "Transformeriai šiuo metu yra pažangiausia technologija NLP srityje, ir daugeliu atvejų tai turėtų būti pirmasis sprendimas, kurį pradėsite išbandyti, kurdami individualius NLP sprendimus. Tačiau labai svarbu suprasti pagrindinius pasikartojančių neuroninių tinklų principus, aptartus šiame modulyje, jei norite kurti pažangius neuroninius modelius.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Atsakomybės apribojimas**:  \nŠis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant šį vertimą.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "ab59c532409774988ab875f2260e8e53",
   "translation_date": "2025-08-31T13:52:53+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersTF.ipynb",
   "language_code": "lt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}