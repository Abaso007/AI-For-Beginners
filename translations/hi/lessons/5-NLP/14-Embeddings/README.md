<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-24T09:48:25+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "hi"
}
-->
# एम्बेडिंग्स

## [प्री-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

जब हमने BoW या TF/IDF आधारित क्लासिफायर को ट्रेन किया, तो हम उच्च-आयामी बैग-ऑफ-वर्ड्स वेक्टर पर काम कर रहे थे, जिनकी लंबाई `vocab_size` होती है, और हम निम्न-आयामी पोज़िशनल रिप्रेज़ेंटेशन वेक्टर को स्पष्ट रूप से स्पार्स वन-हॉट रिप्रेज़ेंटेशन में बदल रहे थे। हालांकि, यह वन-हॉट रिप्रेज़ेंटेशन मेमोरी-एफिशिएंट नहीं है। इसके अलावा, प्रत्येक शब्द को एक-दूसरे से स्वतंत्र रूप से ट्रीट किया जाता है, यानी वन-हॉट एनकोडेड वेक्टर शब्दों के बीच किसी भी सेमांटिक समानता को व्यक्त नहीं करते।

**एम्बेडिंग** का विचार यह है कि शब्दों को निम्न-आयामी डेंस वेक्टर द्वारा रिप्रेज़ेंट किया जाए, जो किसी तरह से शब्द के सेमांटिक अर्थ को दर्शाते हैं। हम बाद में चर्चा करेंगे कि अर्थपूर्ण वर्ड एम्बेडिंग्स कैसे बनाई जाएं, लेकिन फिलहाल, एम्बेडिंग्स को शब्द वेक्टर की आयामीयता को कम करने के एक तरीके के रूप में सोचें।

तो, एम्बेडिंग लेयर एक शब्द को इनपुट के रूप में लेगी और निर्दिष्ट `embedding_size` का आउटपुट वेक्टर उत्पन्न करेगी। एक तरह से, यह `Linear` लेयर के समान है, लेकिन यह वन-हॉट एनकोडेड वेक्टर लेने के बजाय, एक शब्द संख्या को इनपुट के रूप में ले सकेगी, जिससे बड़े वन-हॉट-एनकोडेड वेक्टर बनाने से बचा जा सकेगा।

हमारे क्लासिफायर नेटवर्क में पहली लेयर के रूप में एम्बेडिंग लेयर का उपयोग करके, हम बैग-ऑफ-वर्ड्स से **एम्बेडिंग बैग** मॉडल में स्विच कर सकते हैं, जहां हम पहले अपने टेक्स्ट के प्रत्येक शब्द को संबंधित एम्बेडिंग में बदलते हैं, और फिर उन सभी एम्बेडिंग्स पर `sum`, `average` या `max` जैसे किसी समग्र फ़ंक्शन की गणना करते हैं।  

![पांच अनुक्रम शब्दों के लिए एक एम्बेडिंग क्लासिफायर दिखाने वाली छवि।](../../../../../lessons/5-NLP/14-Embeddings/images/embedding-classifier-example.png)

> लेखक द्वारा बनाई गई छवि

## ✍️ अभ्यास: एम्बेडिंग्स

अगले नोटबुक्स में अपनी सीख जारी रखें:
* [PyTorch के साथ एम्बेडिंग्स](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlow के साथ एम्बेडिंग्स](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## सेमांटिक एम्बेडिंग्स: Word2Vec

हालांकि एम्बेडिंग लेयर ने शब्दों को वेक्टर रिप्रेज़ेंटेशन में मैप करना सीखा, लेकिन यह रिप्रेज़ेंटेशन जरूरी नहीं कि बहुत सेमांटिक अर्थ रखता हो। यह अच्छा होगा कि एक ऐसा वेक्टर रिप्रेज़ेंटेशन सीखा जाए, जिसमें समान शब्द या पर्यायवाची शब्द ऐसे वेक्टर के अनुरूप हों, जो किसी वेक्टर दूरी (जैसे, यूक्लिडियन दूरी) के संदर्भ में एक-दूसरे के करीब हों।

इसके लिए, हमें अपने एम्बेडिंग मॉडल को एक बड़े टेक्स्ट संग्रह पर एक विशिष्ट तरीके से प्री-ट्रेन करना होगा। सेमांटिक एम्बेडिंग्स को ट्रेन करने के एक तरीके को [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) कहा जाता है। यह दो मुख्य आर्किटेक्चर पर आधारित है, जो शब्दों के वितरित रिप्रेज़ेंटेशन को उत्पन्न करने के लिए उपयोग किए जाते हैं:

 - **कंटीन्यूस बैग-ऑफ-वर्ड्स** (CBoW) — इस आर्किटेक्चर में, हम मॉडल को संदर्भ के आधार पर एक शब्द की भविष्यवाणी करने के लिए ट्रेन करते हैं। दिए गए ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ में, मॉडल का लक्ष्य $(W_{-2},W_{-1},W_1,W_2)$ से $W_0$ की भविष्यवाणी करना है।
 - **कंटीन्यूस स्किप-ग्राम** CBoW के विपरीत है। मॉडल संदर्भ शब्दों की विंडो का उपयोग करके वर्तमान शब्द की भविष्यवाणी करता है।

CBoW तेज़ है, जबकि स्किप-ग्राम धीमा है, लेकिन दुर्लभ शब्दों को बेहतर तरीके से रिप्रेज़ेंट करता है।

![शब्दों को वेक्टर में बदलने के लिए CBoW और स्किप-ग्राम एल्गोरिदम दिखाने वाली छवि।](../../../../../lessons/5-NLP/14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> छवि [इस पेपर](https://arxiv.org/pdf/1301.3781.pdf) से

Word2Vec प्री-ट्रेंड एम्बेडिंग्स (और अन्य समान मॉडल जैसे GloVe) को न्यूरल नेटवर्क्स में एम्बेडिंग लेयर के स्थान पर भी उपयोग किया जा सकता है। हालांकि, हमें शब्दावली से निपटना होगा, क्योंकि Word2Vec/GloVe को प्री-ट्रेन करने के लिए उपयोग की गई शब्दावली हमारे टेक्स्ट कॉर्पस की शब्दावली से भिन्न हो सकती है। इस समस्या को हल करने के लिए ऊपर दिए गए नोटबुक्स को देखें।

## कॉन्टेक्स्चुअल एम्बेडिंग्स

पारंपरिक प्री-ट्रेंड एम्बेडिंग रिप्रेज़ेंटेशन जैसे Word2Vec की एक प्रमुख सीमा शब्द अर्थ अस्पष्टता की समस्या है। जबकि प्री-ट्रेंड एम्बेडिंग्स संदर्भ में शब्दों के कुछ अर्थों को कैप्चर कर सकती हैं, एक शब्द के हर संभावित अर्थ को एक ही एम्बेडिंग में एनकोड किया जाता है। यह डाउनस्ट्रीम मॉडल्स में समस्याएं पैदा कर सकता है, क्योंकि कई शब्दों, जैसे 'play', के अलग-अलग अर्थ होते हैं, जो उनके उपयोग के संदर्भ पर निर्भर करते हैं।

उदाहरण के लिए, 'play' शब्द का इन दो वाक्यों में काफी अलग अर्थ है:

- मैं थिएटर में एक **play** देखने गया।
- जॉन अपने दोस्तों के साथ **play** करना चाहता है।

ऊपर दी गई प्री-ट्रेंड एम्बेडिंग्स 'play' शब्द के इन दोनों अर्थों को एक ही एम्बेडिंग में रिप्रेज़ेंट करती हैं। इस सीमा को दूर करने के लिए, हमें **भाषा मॉडल** पर आधारित एम्बेडिंग्स बनानी होंगी, जो एक बड़े टेक्स्ट कॉर्पस पर ट्रेन की जाती हैं, और *जानती हैं* कि विभिन्न संदर्भों में शब्दों को कैसे जोड़ा जा सकता है। कॉन्टेक्स्चुअल एम्बेडिंग्स पर चर्चा करना इस ट्यूटोरियल के दायरे से बाहर है, लेकिन हम भाषा मॉडलों पर चर्चा करते समय इस पर वापस आएंगे।

## निष्कर्ष

इस पाठ में, आपने TensorFlow और PyTorch में एम्बेडिंग लेयर्स को बनाना और उपयोग करना सीखा, ताकि शब्दों के सेमांटिक अर्थों को बेहतर तरीके से दर्शाया जा सके।

## 🚀 चुनौती

Word2Vec का उपयोग कुछ दिलचस्प अनुप्रयोगों के लिए किया गया है, जैसे गाने के बोल और कविता उत्पन्न करना। [इस लेख](https://www.politetype.com/blog/word2vec-color-poems) को देखें, जिसमें लेखक ने Word2Vec का उपयोग करके कविता उत्पन्न करने की प्रक्रिया को समझाया है। [डैन शिफमैन का यह वीडियो](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) भी देखें, जिसमें इस तकनीक की एक अलग व्याख्या दी गई है। फिर इन तकनीकों को अपने टेक्स्ट कॉर्पस पर लागू करने का प्रयास करें, जिसे आप शायद Kaggle से प्राप्त कर सकते हैं।

## [पोस्ट-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## समीक्षा और स्व-अध्ययन

Word2Vec पर इस पेपर को पढ़ें: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [असाइनमेंट: नोटबुक्स](assignment.md)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।