{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# टेक्स्ट वर्गीकरण कार्य\n",
    "\n",
    "जैसा कि हमने उल्लेख किया है, हम **AG_NEWS** डेटासेट पर आधारित एक सरल टेक्स्ट वर्गीकरण कार्य पर ध्यान केंद्रित करेंगे, जिसमें समाचार सुर्खियों को 4 श्रेणियों में वर्गीकृत करना है: विश्व, खेल, व्यवसाय और विज्ञान/तकनीक।\n",
    "\n",
    "## डेटासेट\n",
    "\n",
    "यह डेटासेट [`torchtext`](https://github.com/pytorch/text) मॉड्यूल में शामिल है, इसलिए हम इसे आसानी से एक्सेस कर सकते हैं।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "यहाँ, `train_dataset` और `test_dataset` में संग्रह होते हैं जो क्रमशः वर्ग (कक्षा की संख्या) और पाठ के जोड़े लौटाते हैं, उदाहरण के लिए:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "तो, चलिए हमारे डेटासेट से पहले 10 नई सुर्खियाँ प्रिंट करते हैं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "क्योंकि डेटासेट इटरेटर होते हैं, यदि हम डेटा का उपयोग कई बार करना चाहते हैं तो हमें इसे सूची में बदलना होगा:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## टोकनाइज़ेशन\n",
    "\n",
    "अब हमें टेक्स्ट को **संख्याओं** में बदलने की आवश्यकता है, जिन्हें टेन्सर के रूप में प्रस्तुत किया जा सके। यदि हम शब्द-स्तरीय प्रतिनिधित्व चाहते हैं, तो हमें दो चीजें करनी होंगी:\n",
    "* **टोकनाइज़र** का उपयोग करके टेक्स्ट को **टोकन** में विभाजित करना\n",
    "* उन टोकन का एक **शब्दकोश** बनाना।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "शब्दावली का उपयोग करके, हम आसानी से अपने टोकनयुक्त स्ट्रिंग को संख्याओं के सेट में एन्कोड कर सकते हैं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## शब्दों का थैला (Bag of Words) टेक्स्ट प्रतिनिधित्व\n",
    "\n",
    "क्योंकि शब्द अर्थ को दर्शाते हैं, कभी-कभी हम केवल व्यक्तिगत शब्दों को देखकर, उनके वाक्य में क्रम की परवाह किए बिना, टेक्स्ट का अर्थ समझ सकते हैं। उदाहरण के लिए, जब समाचार वर्गीकृत कर रहे हों, तो *मौसम*, *बर्फ* जैसे शब्द *मौसम पूर्वानुमान* का संकेत दे सकते हैं, जबकि *शेयर*, *डॉलर* जैसे शब्द *वित्तीय समाचार* की ओर इशारा करेंगे।\n",
    "\n",
    "**शब्दों का थैला** (BoW) वेक्टर प्रतिनिधित्व सबसे सामान्य रूप से उपयोग किया जाने वाला पारंपरिक वेक्टर प्रतिनिधित्व है। प्रत्येक शब्द को एक वेक्टर इंडेक्स से जोड़ा जाता है, और वेक्टर तत्व में किसी दिए गए दस्तावेज़ में शब्द की घटनाओं की संख्या होती है।\n",
    "\n",
    "![यह छवि दिखाती है कि शब्दों के थैले का वेक्टर प्रतिनिधित्व मेमोरी में कैसे दर्शाया जाता है।](../../../../../lessons/5-NLP/13-TextRep/images/bag-of-words-example.png)\n",
    "\n",
    "> **Note**: आप BoW को टेक्स्ट में व्यक्तिगत शब्दों के लिए सभी वन-हॉट-एन्कोडेड वेक्टर का योग भी मान सकते हैं।\n",
    "\n",
    "नीचे Scikit Learn पायथन लाइब्रेरी का उपयोग करके शब्दों के थैले का प्रतिनिधित्व बनाने का एक उदाहरण दिया गया है:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AG_NEWS डेटासेट के वेक्टर प्रतिनिधित्व से बैग-ऑफ-वर्ड्स वेक्टर की गणना करने के लिए, हम निम्नलिखित फ़ंक्शन का उपयोग कर सकते हैं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **नोट:** यहां हम वैश्विक `vocab_size` चर का उपयोग कर रहे हैं ताकि शब्दावली के डिफ़ॉल्ट आकार को निर्दिष्ट किया जा सके। चूंकि अक्सर शब्दावली का आकार काफी बड़ा होता है, हम सबसे अधिक बार उपयोग किए जाने वाले शब्दों तक शब्दावली के आकार को सीमित कर सकते हैं। `vocab_size` मान को कम करने और नीचे दिए गए कोड को चलाने का प्रयास करें, और देखें कि यह सटीकता को कैसे प्रभावित करता है। आपको कुछ सटीकता में गिरावट की उम्मीद करनी चाहिए, लेकिन प्रदर्शन में सुधार के बदले यह नाटकीय नहीं होगा।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW क्लासिफायर का प्रशिक्षण\n",
    "\n",
    "अब जब हमने अपने टेक्स्ट का बैग-ऑफ-वर्ड्स प्रतिनिधित्व बनाना सीख लिया है, तो आइए इसके ऊपर एक क्लासिफायर को प्रशिक्षित करें। सबसे पहले, हमें अपने डेटासेट को इस तरह से प्रशिक्षण के लिए बदलने की आवश्यकता है कि सभी पोजिशनल वेक्टर प्रतिनिधित्व को बैग-ऑफ-वर्ड्स प्रतिनिधित्व में परिवर्तित किया जा सके। इसे `bowify` फ़ंक्शन को मानक टॉर्च `DataLoader` में `collate_fn` पैरामीटर के रूप में पास करके प्राप्त किया जा सकता है:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब चलिए एक साधारण वर्गीकरण न्यूरल नेटवर्क को परिभाषित करते हैं जिसमें एक रैखिक परत होती है। इनपुट वेक्टर का आकार `vocab_size` के बराबर है, और आउटपुट आकार वर्गों की संख्या (4) के अनुरूप है। क्योंकि हम वर्गीकरण कार्य को हल कर रहे हैं, अंतिम सक्रियता फ़ंक्शन `LogSoftmax()` है।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब हम मानक PyTorch प्रशिक्षण लूप को परिभाषित करेंगे। क्योंकि हमारा डेटासेट काफी बड़ा है, शिक्षण उद्देश्य के लिए हम केवल एक युग के लिए प्रशिक्षण करेंगे, और कभी-कभी एक युग से भी कम (प्रशिक्षण को सीमित करने के लिए `epoch_size` पैरामीटर निर्दिष्ट किया जाता है)। हम प्रशिक्षण के दौरान संचित प्रशिक्षण सटीकता की भी रिपोर्ट करेंगे; रिपोर्टिंग की आवृत्ति `report_freq` पैरामीटर का उपयोग करके निर्दिष्ट की जाती है।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8028125\n",
      "6400: acc=0.8371875\n",
      "9600: acc=0.8534375\n",
      "12800: acc=0.85765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026090790722161722, 0.8620069296375267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## बाईग्राम्स, ट्राईग्राम्स और एन-ग्राम्स\n",
    "\n",
    "बैग ऑफ वर्ड्स दृष्टिकोण की एक सीमा यह है कि कुछ शब्द बहु-शब्द अभिव्यक्तियों का हिस्सा होते हैं। उदाहरण के लिए, 'हॉट डॉग' शब्द का अर्थ पूरी तरह से अलग होता है, जबकि 'हॉट' और 'डॉग' शब्द अन्य संदर्भों में अलग-अलग अर्थ रखते हैं। यदि हम हमेशा 'हॉट' और 'डॉग' शब्दों को एक ही वेक्टर द्वारा प्रदर्शित करें, तो यह हमारे मॉडल को भ्रमित कर सकता है।\n",
    "\n",
    "इस समस्या को हल करने के लिए, **एन-ग्राम प्रतिनिधित्व** का उपयोग अक्सर दस्तावेज़ वर्गीकरण की विधियों में किया जाता है, जहां प्रत्येक शब्द, द्वि-शब्द या त्रि-शब्द की आवृत्ति वर्गीकरणकर्ताओं को प्रशिक्षित करने के लिए एक उपयोगी विशेषता होती है। उदाहरण के लिए, बाईग्राम प्रतिनिधित्व में, हम मूल शब्दों के अलावा सभी शब्द युग्मों को शब्दावली में जोड़ेंगे।\n",
    "\n",
    "नीचे एक उदाहरण दिया गया है कि कैसे Scikit Learn का उपयोग करके बाईग्राम बैग ऑफ वर्ड्स प्रतिनिधित्व बनाया जा सकता है:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram दृष्टिकोण की मुख्य कमी यह है कि शब्दावली का आकार बहुत तेजी से बढ़ने लगता है। व्यवहार में, हमें N-gram प्रतिनिधित्व को कुछ आयाम-घटाने की तकनीकों, जैसे *embeddings*, के साथ संयोजित करने की आवश्यकता होती है, जिन पर हम अगले यूनिट में चर्चा करेंगे।\n",
    "\n",
    "हमारे **AG News** डेटासेट में N-gram प्रतिनिधित्व का उपयोग करने के लिए, हमें एक विशेष ngram शब्दावली बनानी होगी:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308842\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "हम ऊपर दिए गए कोड का उपयोग करके क्लासिफायर को ट्रेन कर सकते हैं, लेकिन यह मेमोरी के लिहाज से बहुत अक्षम होगा। अगले यूनिट में, हम एम्बेडिंग्स का उपयोग करके बिग्राम क्लासिफायर को ट्रेन करेंगे।\n",
    "\n",
    "> **नोट:** आप केवल उन्हीं ngrams को छोड़ सकते हैं जो टेक्स्ट में निर्दिष्ट संख्या से अधिक बार आते हैं। यह सुनिश्चित करेगा कि कम बार आने वाले बिग्राम्स को हटा दिया जाए, और डाइमेंशनलिटी को काफी हद तक कम किया जा सके। ऐसा करने के लिए, `min_freq` पैरामीटर को उच्च मान पर सेट करें, और शब्दावली की लंबाई में बदलाव को देखें।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## टर्म फ्रीक्वेंसी इनवर्स डॉक्यूमेंट फ्रीक्वेंसी (TF-IDF)\n",
    "\n",
    "BoW (बैग ऑफ वर्ड्स) प्रतिनिधित्व में, शब्दों की उपस्थिति को समान रूप से महत्व दिया जाता है, चाहे वह शब्द कोई भी हो। हालांकि, यह स्पष्ट है कि सामान्य शब्द जैसे *a*, *in* आदि वर्गीकरण के लिए उतने महत्वपूर्ण नहीं होते जितने कि विशेष शब्द। वास्तव में, अधिकांश NLP कार्यों में कुछ शब्द अन्य शब्दों की तुलना में अधिक प्रासंगिक होते हैं।\n",
    "\n",
    "**TF-IDF** का मतलब है **टर्म फ्रीक्वेंसी–इनवर्स डॉक्यूमेंट फ्रीक्वेंसी**। यह बैग ऑफ वर्ड्स का एक प्रकार है, जिसमें किसी दस्तावेज़ में शब्द की उपस्थिति को दर्शाने वाले 0/1 बाइनरी मान के बजाय एक फ्लोटिंग-पॉइंट मान का उपयोग किया जाता है, जो कॉर्पस में शब्द की उपस्थिति की आवृत्ति से संबंधित होता है।\n",
    "\n",
    "औपचारिक रूप से, किसी शब्द $i$ का वजन $w_{ij}$ किसी दस्तावेज़ $j$ में इस प्रकार परिभाषित किया जाता है:\n",
    "$$\n",
    "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "$$\n",
    "जहां:\n",
    "* $tf_{ij}$ किसी दस्तावेज़ $j$ में शब्द $i$ की उपस्थिति की संख्या है, यानी वह BoW मान जिसे हमने पहले देखा था\n",
    "* $N$ संग्रह में दस्तावेज़ों की कुल संख्या है\n",
    "* $df_i$ पूरे संग्रह में शब्द $i$ को शामिल करने वाले दस्तावेज़ों की संख्या है\n",
    "\n",
    "TF-IDF मान $w_{ij}$ किसी दस्तावेज़ में शब्द की उपस्थिति की संख्या के अनुपात में बढ़ता है और कॉर्पस में उस शब्द को शामिल करने वाले दस्तावेज़ों की संख्या से समायोजित होता है। यह इस तथ्य को संतुलित करने में मदद करता है कि कुछ शब्द अन्य शब्दों की तुलना में अधिक बार दिखाई देते हैं। उदाहरण के लिए, यदि कोई शब्द *हर* दस्तावेज़ में दिखाई देता है, तो $df_i=N$, और $w_{ij}=0$, और ऐसे शब्दों को पूरी तरह से नजरअंदाज कर दिया जाएगा।\n",
    "\n",
    "आप आसानी से Scikit Learn का उपयोग करके टेक्स्ट का TF-IDF वेक्टराइज़ेशन बना सकते हैं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## निष्कर्ष\n",
    "\n",
    "हालांकि TF-IDF प्रतिनिधित्व विभिन्न शब्दों को आवृत्ति भार प्रदान करते हैं, वे अर्थ या क्रम को व्यक्त करने में असमर्थ होते हैं। जैसा कि प्रसिद्ध भाषाविद् जे. आर. फर्थ ने 1935 में कहा था, \"शब्द का पूर्ण अर्थ हमेशा संदर्भात्मक होता है, और संदर्भ से अलग अर्थ का कोई भी अध्ययन गंभीरता से नहीं लिया जा सकता।\" हम इस पाठ्यक्रम में आगे सीखेंगे कि भाषा मॉडलिंग का उपयोग करके पाठ से संदर्भात्मक जानकारी कैसे प्राप्त करें।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "7b9040985e748e4e2d4c689892456ad7",
   "translation_date": "2025-08-31T15:29:56+00:00",
   "source_file": "lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb",
   "language_code": "hi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}