<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-24T09:49:32+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "hi"
}
-->
# टेक्स्ट को टेन्सर्स के रूप में प्रस्तुत करना

## [प्री-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## टेक्स्ट वर्गीकरण

इस सेक्शन के पहले भाग में, हम **टेक्स्ट वर्गीकरण** कार्य पर ध्यान केंद्रित करेंगे। हम [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) डेटासेट का उपयोग करेंगे, जिसमें निम्नलिखित जैसे समाचार लेख शामिल हैं:

* श्रेणी: विज्ञान/तकनीक  
* शीर्षक: Ky. कंपनी को पेप्टाइड्स का अध्ययन करने के लिए अनुदान मिला (AP)  
* बॉडी: AP - लुइसविले विश्वविद्यालय के एक रसायन शोधकर्ता द्वारा स्थापित एक कंपनी को विकास के लिए अनुदान मिला...  

हमारा लक्ष्य टेक्स्ट के आधार पर समाचार को एक श्रेणी में वर्गीकृत करना होगा।

## टेक्स्ट को प्रस्तुत करना

यदि हम न्यूरल नेटवर्क के साथ प्राकृतिक भाषा प्रसंस्करण (NLP) कार्यों को हल करना चाहते हैं, तो हमें टेक्स्ट को टेन्सर्स के रूप में प्रस्तुत करने का कोई तरीका चाहिए। कंप्यूटर पहले से ही टेक्स्ट के अक्षरों को संख्याओं के रूप में प्रस्तुत करते हैं, जो आपके स्क्रीन पर फोंट को मैप करते हैं, जैसे ASCII या UTF-8 एन्कोडिंग।

<img alt="एक अक्षर को ASCII और बाइनरी प्रतिनिधित्व में मैप करने वाले डायग्राम को दिखाने वाली छवि" src="images/ascii-character-map.png" width="50%"/>

> [छवि स्रोत](https://www.seobility.net/en/wiki/ASCII)

मनुष्य के रूप में, हम समझते हैं कि प्रत्येक अक्षर **क्या दर्शाता है**, और कैसे सभी अक्षर एक साथ मिलकर एक वाक्य के शब्द बनाते हैं। हालांकि, कंप्यूटर स्वयं ऐसा समझ नहीं पाते हैं, और न्यूरल नेटवर्क को प्रशिक्षण के दौरान अर्थ सीखना पड़ता है।

इसलिए, टेक्स्ट को प्रस्तुत करने के लिए हम विभिन्न दृष्टिकोणों का उपयोग कर सकते हैं:

* **कैरेक्टर-स्तरीय प्रतिनिधित्व**, जिसमें हम टेक्स्ट को प्रत्येक अक्षर को एक संख्या के रूप में मानकर प्रस्तुत करते हैं। मान लें कि हमारे टेक्स्ट कॉर्पस में *C* अलग-अलग अक्षर हैं, तो शब्द *Hello* को 5x*C* टेन्सर के रूप में प्रस्तुत किया जाएगा। प्रत्येक अक्षर एक-हॉट एन्कोडिंग में टेन्सर कॉलम के अनुरूप होगा।  
* **वर्ड-स्तरीय प्रतिनिधित्व**, जिसमें हम अपने टेक्स्ट के सभी शब्दों का एक **शब्दकोश** बनाते हैं और फिर शब्दों को एक-हॉट एन्कोडिंग का उपयोग करके प्रस्तुत करते हैं। यह दृष्टिकोण कुछ हद तक बेहतर है, क्योंकि प्रत्येक अक्षर अपने आप में बहुत अधिक अर्थ नहीं रखता है, और इसलिए उच्च-स्तरीय अर्थपूर्ण अवधारणाओं - शब्दों - का उपयोग करके हम न्यूरल नेटवर्क के लिए कार्य को सरल बनाते हैं। हालांकि, बड़े शब्दकोश आकार के कारण, हमें उच्च-आयामी विरल टेन्सर्स से निपटना पड़ता है।  

प्रतिनिधित्व चाहे जो भी हो, हमें पहले टेक्स्ट को **टोकन** के अनुक्रम में बदलना होगा, जिसमें एक टोकन एक अक्षर, एक शब्द, या कभी-कभी एक शब्द का हिस्सा होता है। फिर, हम टोकन को एक संख्या में बदलते हैं, आमतौर पर **शब्दकोश** का उपयोग करके, और इस संख्या को एक-हॉट एन्कोडिंग के माध्यम से न्यूरल नेटवर्क में फीड किया जा सकता है।

## एन-ग्राम्स

प्राकृतिक भाषा में, शब्दों का सटीक अर्थ केवल संदर्भ में ही निर्धारित किया जा सकता है। उदाहरण के लिए, *न्यूरल नेटवर्क* और *फिशिंग नेटवर्क* के अर्थ पूरी तरह से अलग हैं। इसे ध्यान में रखने के तरीकों में से एक है कि हम अपने मॉडल को शब्दों के जोड़ों पर बनाएं और शब्द जोड़ों को अलग-अलग शब्दकोश टोकन के रूप में मानें। इस तरह, वाक्य *I like to go fishing* को निम्नलिखित टोकन अनुक्रम द्वारा प्रस्तुत किया जाएगा: *I like*, *like to*, *to go*, *go fishing*। इस दृष्टिकोण की समस्या यह है कि शब्दकोश का आकार काफी बढ़ जाता है, और *go fishing* और *go shopping* जैसे संयोजन अलग-अलग टोकन द्वारा प्रस्तुत किए जाते हैं, जो समान क्रिया के बावजूद कोई अर्थपूर्ण समानता साझा नहीं करते हैं।  

कुछ मामलों में, हम त्रि-ग्राम्स -- तीन शब्दों के संयोजन -- का उपयोग करने पर विचार कर सकते हैं। इस प्रकार का दृष्टिकोण अक्सर **एन-ग्राम्स** कहलाता है। साथ ही, कैरेक्टर-स्तरीय प्रतिनिधित्व के साथ एन-ग्राम्स का उपयोग करना समझ में आता है, जिसमें एन-ग्राम्स मोटे तौर पर विभिन्न अक्षरों के समूहों के अनुरूप होंगे।

## बैग-ऑफ-वर्ड्स और TF/IDF

जब टेक्स्ट वर्गीकरण जैसे कार्यों को हल करते हैं, तो हमें टेक्स्ट को एक निश्चित आकार के वेक्टर द्वारा प्रस्तुत करने में सक्षम होना चाहिए, जिसे हम अंतिम घने वर्गीकरणकर्ता के इनपुट के रूप में उपयोग करेंगे। इसे करने के सबसे सरल तरीकों में से एक है सभी व्यक्तिगत शब्द प्रतिनिधित्वों को जोड़ना, जैसे कि उन्हें जोड़कर। यदि हम प्रत्येक शब्द की एक-हॉट एन्कोडिंग को जोड़ते हैं, तो हमें आवृत्तियों का एक वेक्टर मिलेगा, जो दिखाएगा कि टेक्स्ट के अंदर प्रत्येक शब्द कितनी बार दिखाई देता है। टेक्स्ट का ऐसा प्रतिनिधित्व **बैग ऑफ वर्ड्स** (BoW) कहलाता है।

<img src="images/bow.png" width="90%"/>

> लेखक द्वारा बनाई गई छवि

BoW मूल रूप से यह दर्शाता है कि टेक्स्ट में कौन से शब्द दिखाई देते हैं और किस मात्रा में, जो वास्तव में यह संकेत दे सकता है कि टेक्स्ट किस बारे में है। उदाहरण के लिए, राजनीति पर समाचार लेख में *president* और *country* जैसे शब्द होने की संभावना है, जबकि वैज्ञानिक प्रकाशन में *collider*, *discovered* जैसे शब्द हो सकते हैं। इस प्रकार, शब्द आवृत्तियां कई मामलों में टेक्स्ट सामग्री का एक अच्छा संकेतक हो सकती हैं।

BoW की समस्या यह है कि कुछ सामान्य शब्द, जैसे *and*, *is*, आदि अधिकांश टेक्स्ट में दिखाई देते हैं, और उनकी उच्चतम आवृत्तियां होती हैं, जो वास्तव में महत्वपूर्ण शब्दों को छिपा देती हैं। हम इन शब्दों के महत्व को कम कर सकते हैं, पूरे दस्तावेज़ संग्रह में शब्दों के होने की आवृत्ति को ध्यान में रखते हुए। यही मुख्य विचार है TF/IDF दृष्टिकोण के पीछे, जिसे इस पाठ से जुड़े नोटबुक्स में अधिक विस्तार से कवर किया गया है।

हालांकि, इनमें से कोई भी दृष्टिकोण टेक्स्ट के **अर्थ** को पूरी तरह से ध्यान में नहीं रख सकता। हमें इसे करने के लिए अधिक शक्तिशाली न्यूरल नेटवर्क मॉडल की आवश्यकता है, जिसे हम इस सेक्शन में बाद में चर्चा करेंगे।

## ✍️ अभ्यास: टेक्स्ट प्रतिनिधित्व

अगले नोटबुक्स में अपना अध्ययन जारी रखें:

* [PyTorch के साथ टेक्स्ट प्रतिनिधित्व](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [TensorFlow के साथ टेक्स्ट प्रतिनिधित्व](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## निष्कर्ष

अब तक, हमने ऐसी तकनीकों का अध्ययन किया है जो विभिन्न शब्दों को आवृत्ति भार प्रदान कर सकती हैं। हालांकि, वे अर्थ या क्रम को प्रस्तुत करने में असमर्थ हैं। प्रसिद्ध भाषाविद् जे. आर. फर्थ ने 1935 में कहा था, "शब्द का पूरा अर्थ हमेशा संदर्भात्मक होता है, और संदर्भ से अलग अर्थ का कोई अध्ययन गंभीरता से नहीं लिया जा सकता।" हम इस पाठ्यक्रम में बाद में सीखेंगे कि भाषा मॉडलिंग का उपयोग करके टेक्स्ट से संदर्भात्मक जानकारी कैसे प्राप्त करें।

## 🚀 चुनौती

बैग-ऑफ-वर्ड्स और विभिन्न डेटा मॉडल का उपयोग करके कुछ अन्य अभ्यास आज़माएं। आप इस [Kaggle प्रतियोगिता](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) से प्रेरित हो सकते हैं।

## [पोस्ट-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## समीक्षा और स्व-अध्ययन

[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) पर टेक्स्ट एम्बेडिंग और बैग-ऑफ-वर्ड्स तकनीकों के साथ अपने कौशल का अभ्यास करें।

## [असाइनमेंट: नोटबुक्स](assignment.md)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।