<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-24T09:48:58+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "hi"
}
-->
# जनरेटिव नेटवर्क्स

## [प्री-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

रिकरंट न्यूरल नेटवर्क्स (RNNs) और उनके गेटेड सेल वेरिएंट्स जैसे लॉन्ग शॉर्ट टर्म मेमोरी सेल्स (LSTMs) और गेटेड रिक्रंट यूनिट्स (GRUs) ने भाषा मॉडलिंग के लिए एक तंत्र प्रदान किया है, क्योंकि वे शब्दों के क्रम को सीख सकते हैं और अनुक्रम में अगले शब्द की भविष्यवाणी कर सकते हैं। यह हमें RNNs का उपयोग **जनरेटिव कार्यों** के लिए करने की अनुमति देता है, जैसे सामान्य टेक्स्ट जनरेशन, मशीन ट्रांसलेशन, और यहां तक कि इमेज कैप्शनिंग।

> ✅ उन सभी मौकों के बारे में सोचें जब आपने टेक्स्ट कंप्लीशन जैसे जनरेटिव कार्यों से लाभ उठाया है। अपने पसंदीदा एप्लिकेशन पर शोध करें और देखें कि क्या उन्होंने RNNs का उपयोग किया है।

पिछले यूनिट में चर्चा किए गए RNN आर्किटेक्चर में, प्रत्येक RNN यूनिट ने आउटपुट के रूप में अगला हिडन स्टेट उत्पन्न किया। हालांकि, हम प्रत्येक रिक्रंट यूनिट में एक और आउटपुट जोड़ सकते हैं, जो हमें एक **अनुक्रम** आउटपुट करने की अनुमति देगा (जो मूल अनुक्रम के बराबर लंबाई का होगा)। इसके अलावा, हम ऐसे RNN यूनिट्स का उपयोग कर सकते हैं जो प्रत्येक चरण में इनपुट स्वीकार नहीं करते, बल्कि केवल एक प्रारंभिक स्टेट वेक्टर लेते हैं और फिर आउटपुट का एक अनुक्रम उत्पन्न करते हैं।

यह विभिन्न न्यूरल आर्किटेक्चर की अनुमति देता है, जैसा कि नीचे दी गई तस्वीर में दिखाया गया है:

![रिकरंट न्यूरल नेटवर्क पैटर्न्स का सामान्य चित्र।](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/unreasonable-effectiveness-of-rnn.jpg)

> चित्र ब्लॉग पोस्ट [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) से लिया गया है, लेखक [Andrej Karpaty](http://karpathy.github.io/)

* **वन-टू-वन** एक पारंपरिक न्यूरल नेटवर्क है जिसमें एक इनपुट और एक आउटपुट होता है।
* **वन-टू-मेनी** एक जनरेटिव आर्किटेक्चर है जो एक इनपुट वैल्यू स्वीकार करता है और आउटपुट वैल्यू का एक अनुक्रम उत्पन्न करता है। उदाहरण के लिए, यदि हम एक **इमेज कैप्शनिंग** नेटवर्क को प्रशिक्षित करना चाहते हैं जो किसी चित्र का टेक्स्ट विवरण उत्पन्न करे, तो हम एक चित्र को इनपुट के रूप में ले सकते हैं, इसे CNN के माध्यम से पास कर सकते हैं ताकि इसका हिडन स्टेट प्राप्त हो, और फिर एक रिक्रंट चेन शब्द-दर-शब्द कैप्शन उत्पन्न कर सकता है।
* **मेनी-टू-वन** पिछले यूनिट में वर्णित RNN आर्किटेक्चर से मेल खाता है, जैसे टेक्स्ट क्लासिफिकेशन।
* **मेनी-टू-मेनी**, या **सीक्वेंस-टू-सीक्वेंस**, उन कार्यों से मेल खाता है जैसे **मशीन ट्रांसलेशन**, जहां पहला RNN इनपुट अनुक्रम से सभी जानकारी को हिडन स्टेट में संग्रहित करता है, और दूसरा RNN चेन इस स्टेट को आउटपुट अनुक्रम में अनरोल करता है।

इस यूनिट में, हम सरल जनरेटिव मॉडलों पर ध्यान केंद्रित करेंगे जो हमें टेक्स्ट उत्पन्न करने में मदद करते हैं। सरलता के लिए, हम कैरेक्टर-लेवल टोकनाइजेशन का उपयोग करेंगे।

हम इस RNN को चरण-दर-चरण टेक्स्ट उत्पन्न करने के लिए प्रशिक्षित करेंगे। प्रत्येक चरण में, हम `nchars` लंबाई के कैरेक्टर का एक अनुक्रम लेंगे और नेटवर्क से प्रत्येक इनपुट कैरेक्टर के लिए अगला आउटपुट कैरेक्टर उत्पन्न करने के लिए कहेंगे:

!['HELLO' शब्द के RNN जनरेशन का उदाहरण दिखाने वाला चित्र।](../../../../../lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate.png)

जब टेक्स्ट उत्पन्न किया जाता है (इंफरेंस के दौरान), तो हम किसी **प्रॉम्प्ट** से शुरू करते हैं, जिसे RNN सेल्स के माध्यम से पास किया जाता है ताकि इसका इंटरमीडिएट स्टेट उत्पन्न हो सके, और फिर इस स्टेट से जनरेशन शुरू होती है। हम एक बार में एक कैरेक्टर उत्पन्न करते हैं और स्टेट और उत्पन्न कैरेक्टर को अगले RNN सेल में पास करते हैं ताकि अगला कैरेक्टर उत्पन्न हो सके, जब तक कि पर्याप्त कैरेक्टर उत्पन्न न हो जाएं।

<img src="images/rnn-generate-inf.png" width="60%"/>

> चित्र लेखक द्वारा

## ✍️ अभ्यास: जनरेटिव नेटवर्क्स

निम्नलिखित नोटबुक्स में अपनी सीख जारी रखें:

* [PyTorch के साथ जनरेटिव नेटवर्क्स](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [TensorFlow के साथ जनरेटिव नेटवर्क्स](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## सॉफ्ट टेक्स्ट जनरेशन और टेम्परेचर

प्रत्येक RNN सेल का आउटपुट कैरेक्टर्स का एक प्रायिकता वितरण होता है। यदि हम हमेशा उस कैरेक्टर को लेते हैं जिसकी प्रायिकता सबसे अधिक है, तो उत्पन्न टेक्स्ट अक्सर "साइक्लिक" हो सकता है, जैसे कि यह बार-बार एक ही कैरेक्टर अनुक्रम में फंस जाता है, जैसा कि इस उदाहरण में दिखाया गया है:

```
today of the second the company and a second the company ...
```

हालांकि, यदि हम अगले कैरेक्टर के लिए प्रायिकता वितरण को देखें, तो यह हो सकता है कि कुछ उच्चतम प्रायिकताओं के बीच का अंतर बहुत बड़ा न हो, जैसे कि एक कैरेक्टर की प्रायिकता 0.2 हो और दूसरे की 0.19। उदाहरण के लिए, जब अनुक्रम '*play*' में अगले कैरेक्टर की तलाश की जाती है, तो अगला कैरेक्टर समान रूप से स्पेस या **e** (जैसे *player* शब्द में) हो सकता है।

यह हमें इस निष्कर्ष पर ले जाता है कि हमेशा उच्चतम प्रायिकता वाले कैरेक्टर को चुनना "न्यायसंगत" नहीं है, क्योंकि दूसरे उच्चतम को चुनना भी अर्थपूर्ण टेक्स्ट की ओर ले जा सकता है। यह अधिक समझदारी है कि नेटवर्क आउटपुट द्वारा दिए गए प्रायिकता वितरण से कैरेक्टर्स को **सैंपल** किया जाए। हम एक पैरामीटर, **टेम्परेचर**, का भी उपयोग कर सकते हैं, जो प्रायिकता वितरण को फ्लैट कर सकता है, यदि हम अधिक रैंडमनेस जोड़ना चाहते हैं, या इसे अधिक स्टेप बना सकता है, यदि हम उच्चतम-प्रायिकता कैरेक्टर्स पर अधिक टिके रहना चाहते हैं।

ऊपर दिए गए नोटबुक्स में देखें कि यह सॉफ्ट टेक्स्ट जनरेशन कैसे लागू की जाती है।

## निष्कर्ष

हालांकि टेक्स्ट जनरेशन अपने आप में उपयोगी हो सकता है, लेकिन मुख्य लाभ यह है कि RNNs का उपयोग करके किसी प्रारंभिक फीचर वेक्टर से टेक्स्ट उत्पन्न किया जा सकता है। उदाहरण के लिए, टेक्स्ट जनरेशन का उपयोग मशीन ट्रांसलेशन (सीक्वेंस-टू-सीक्वेंस, इस मामले में *एन्कोडर* से स्टेट वेक्टर का उपयोग करके अनुवादित संदेश को *डिकोड* करने के लिए) या किसी इमेज का टेक्स्ट विवरण उत्पन्न करने (इस मामले में फीचर वेक्टर CNN एक्सट्रैक्टर से आता है) के लिए किया जाता है।

## 🚀 चुनौती

Microsoft Learn पर इस विषय पर कुछ पाठ लें:

* [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste) के साथ टेक्स्ट जनरेशन

## [पोस्ट-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## समीक्षा और स्व-अध्ययन

अपना ज्ञान बढ़ाने के लिए यहां कुछ लेख दिए गए हैं:

* मार्कोव चेन, LSTM और GPT-2 के साथ टेक्स्ट जनरेशन के विभिन्न दृष्टिकोण: [ब्लॉग पोस्ट](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Keras डाक्यूमेंटेशन](https://keras.io/examples/generative/lstm_character_level_text_generation/) में टेक्स्ट जनरेशन का उदाहरण

## [असाइनमेंट](lab/README.md)

हमने देखा कि कैसे कैरेक्टर-दर-कैरेक्टर टेक्स्ट उत्पन्न किया जा सकता है। लैब में, आप वर्ड-लेवल टेक्स्ट जनरेशन का अन्वेषण करेंगे।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।