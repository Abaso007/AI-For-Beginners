<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ef02a9318257ea140ed3ed74442096d",
  "translation_date": "2025-08-24T09:47:25+00:00",
  "source_file": "lessons/5-NLP/README.md",
  "language_code": "hi"
}
-->
# प्राकृतिक भाषा प्रसंस्करण

![NLP कार्यों का एक स्केच](../../../../lessons/sketchnotes/ai-nlp.png)

इस खंड में, हम **प्राकृतिक भाषा प्रसंस्करण (NLP)** से संबंधित कार्यों को संभालने के लिए न्यूरल नेटवर्क का उपयोग करने पर ध्यान केंद्रित करेंगे। कई NLP समस्याएं हैं जिन्हें हम चाहते हैं कि कंप्यूटर हल कर सकें:

* **पाठ वर्गीकरण** एक सामान्य वर्गीकरण समस्या है जो पाठ अनुक्रमों से संबंधित है। उदाहरणों में ईमेल संदेशों को स्पैम और गैर-स्पैम के रूप में वर्गीकृत करना, या लेखों को खेल, व्यापार, राजनीति आदि के रूप में वर्गीकृत करना शामिल है। इसके अलावा, जब हम चैटबॉट विकसित करते हैं, तो हमें अक्सर यह समझने की आवश्यकता होती है कि उपयोगकर्ता क्या कहना चाहता था -- इस मामले में हम **इरादा वर्गीकरण** से निपट रहे हैं। इरादा वर्गीकरण में अक्सर हमें कई श्रेणियों से निपटना पड़ता है।  
* **भावना विश्लेषण** एक सामान्य प्रतिगमन समस्या है, जहां हमें एक वाक्य के अर्थ की सकारात्मकता/नकारात्मकता के अनुसार एक संख्या (भावना) को निर्दिष्ट करना होता है। भावना विश्लेषण का एक अधिक उन्नत संस्करण **पहलू-आधारित भावना विश्लेषण** (ABSA) है, जहां हम भावना को पूरे वाक्य के बजाय उसके विभिन्न भागों (पहलुओं) को निर्दिष्ट करते हैं, जैसे *इस रेस्तरां में, मुझे खाना पसंद आया, लेकिन माहौल भयानक था*।  
* **नामित इकाई पहचान** (NER) पाठ से कुछ विशिष्ट इकाइयों को निकालने की समस्या को संदर्भित करता है। उदाहरण के लिए, हमें यह समझने की आवश्यकता हो सकती है कि वाक्यांश *मुझे कल पेरिस जाना है* में *कल* शब्द DATE को संदर्भित करता है, और *पेरिस* एक LOCATION है।  
* **कीवर्ड निष्कर्षण** NER के समान है, लेकिन इसमें हमें वाक्य के अर्थ के लिए महत्वपूर्ण शब्दों को स्वचालित रूप से निकालना होता है, बिना विशिष्ट इकाई प्रकारों के लिए पूर्व-प्रशिक्षण के।  
* **पाठ क्लस्टरिंग** तब उपयोगी हो सकती है जब हम समान वाक्यों को एक साथ समूहित करना चाहते हैं, जैसे तकनीकी सहायता वार्तालापों में समान अनुरोध।  
* **प्रश्न उत्तर देना** एक मॉडल की क्षमता को संदर्भित करता है कि वह किसी विशिष्ट प्रश्न का उत्तर दे सके। मॉडल को एक पाठ अंश और एक प्रश्न इनपुट के रूप में प्राप्त होता है, और इसे पाठ में उस स्थान को प्रदान करना होता है जहां प्रश्न का उत्तर निहित है (या, कभी-कभी, उत्तर पाठ उत्पन्न करना होता है)।  
* **पाठ निर्माण** एक मॉडल की नई पाठ उत्पन्न करने की क्षमता है। इसे एक वर्गीकरण कार्य के रूप में माना जा सकता है जो कुछ *पाठ संकेत* के आधार पर अगला अक्षर/शब्द भविष्यवाणी करता है। उन्नत पाठ निर्माण मॉडल, जैसे GPT-3, [प्रॉम्प्ट प्रोग्रामिंग](https://towardsdatascience.com/software-3-0-how-prompting-will-change-the-rules-of-the-game-a982fbfe1e0) या [प्रॉम्प्ट इंजीनियरिंग](https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29) नामक तकनीक का उपयोग करके अन्य NLP कार्यों जैसे वर्गीकरण को हल करने में सक्षम हैं।  
* **पाठ सारांशण** एक तकनीक है जब हम चाहते हैं कि कंप्यूटर लंबे पाठ को "पढ़े" और उसे कुछ वाक्यों में संक्षेपित करे।  
* **मशीन अनुवाद** को एक भाषा में पाठ को समझने और दूसरी भाषा में पाठ उत्पन्न करने के संयोजन के रूप में देखा जा सकता है।  

शुरुआत में, अधिकांश NLP कार्य पारंपरिक तरीकों जैसे व्याकरण का उपयोग करके हल किए गए थे। उदाहरण के लिए, मशीन अनुवाद में पार्सर का उपयोग प्रारंभिक वाक्य को एक सिंटैक्स ट्री में बदलने के लिए किया जाता था, फिर उच्च-स्तरीय अर्थ संरचनाओं को वाक्य के अर्थ का प्रतिनिधित्व करने के लिए निकाला जाता था, और इस अर्थ और लक्ष्य भाषा के व्याकरण के आधार पर परिणाम उत्पन्न किया जाता था। आजकल, कई NLP कार्य न्यूरल नेटवर्क का उपयोग करके अधिक प्रभावी ढंग से हल किए जाते हैं।  

> कई पारंपरिक NLP विधियां [नेचुरल लैंग्वेज प्रोसेसिंग टूलकिट (NLTK)](https://www.nltk.org) पायथन लाइब्रेरी में लागू की गई हैं। एक शानदार [NLTK बुक](https://www.nltk.org/book/) ऑनलाइन उपलब्ध है जो बताती है कि विभिन्न NLP कार्यों को NLTK का उपयोग करके कैसे हल किया जा सकता है।  

हमारे पाठ्यक्रम में, हम मुख्य रूप से NLP के लिए न्यूरल नेटवर्क का उपयोग करने पर ध्यान केंद्रित करेंगे, और जहां आवश्यक होगा, वहां NLTK का उपयोग करेंगे।  

हमने पहले ही तालिका डेटा और छवियों से निपटने के लिए न्यूरल नेटवर्क का उपयोग करना सीखा है। उन प्रकार के डेटा और पाठ के बीच मुख्य अंतर यह है कि पाठ एक परिवर्तनीय लंबाई का अनुक्रम है, जबकि छवियों के मामले में इनपुट आकार पहले से ज्ञात होता है। जबकि कन्वोल्यूशनल नेटवर्क इनपुट डेटा से पैटर्न निकाल सकते हैं, पाठ में पैटर्न अधिक जटिल होते हैं। उदाहरण के लिए, हमारे पास विषय से अलग नकारात्मकता हो सकती है, जो कई शब्दों के लिए मनमाना हो सकता है (जैसे *मुझे संतरे पसंद नहीं हैं* बनाम *मुझे वे बड़े रंगीन स्वादिष्ट संतरे पसंद नहीं हैं*), और इसे अभी भी एक पैटर्न के रूप में व्याख्या किया जाना चाहिए। इस प्रकार, भाषा को संभालने के लिए हमें नए न्यूरल नेटवर्क प्रकारों को पेश करने की आवश्यकता है, जैसे *रिकरेंट नेटवर्क* और *ट्रांसफॉर्मर्स*।  

## लाइब्रेरी इंस्टॉल करें

यदि आप इस पाठ्यक्रम को चलाने के लिए स्थानीय पायथन इंस्टॉलेशन का उपयोग कर रहे हैं, तो आपको NLP के लिए आवश्यक सभी लाइब्रेरी निम्नलिखित कमांड का उपयोग करके इंस्टॉल करनी पड़ सकती हैं:

**PyTorch के लिए**  
```bash
pip install -r requirements-torch.txt
```  
**TensorFlow के लिए**  
```bash
pip install -r requirements-tf.txt
```  

> आप [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/?WT.mc_id=academic-77998-cacaste) पर TensorFlow के साथ NLP आज़मा सकते हैं।  

## GPU चेतावनी

इस खंड में, कुछ उदाहरणों में हम काफी बड़े मॉडल का प्रशिक्षण करेंगे।  
* **GPU-सक्षम कंप्यूटर का उपयोग करें**: बड़े मॉडलों के साथ काम करते समय प्रतीक्षा समय को कम करने के लिए अपने नोटबुक को GPU-सक्षम कंप्यूटर पर चलाना उचित है।  
* **GPU मेमोरी सीमाएं**: GPU पर चलने से ऐसी स्थितियां उत्पन्न हो सकती हैं जहां आप GPU मेमोरी से बाहर हो जाएं, विशेष रूप से बड़े मॉडलों को प्रशिक्षित करते समय।  
* **GPU मेमोरी खपत**: प्रशिक्षण के दौरान GPU मेमोरी की खपत विभिन्न कारकों पर निर्भर करती है, जिसमें मिनीबैच का आकार शामिल है।  
* **मिनीबैच आकार कम करें**: यदि आपको GPU मेमोरी समस्याओं का सामना करना पड़ता है, तो संभावित समाधान के रूप में अपने कोड में मिनीबैच आकार को कम करने पर विचार करें।  
* **TensorFlow GPU मेमोरी रिलीज़**: TensorFlow के पुराने संस्करण एक ही पायथन कर्नेल में कई मॉडलों को प्रशिक्षित करते समय GPU मेमोरी को सही ढंग से रिलीज़ नहीं कर सकते। GPU मेमोरी उपयोग को प्रभावी ढंग से प्रबंधित करने के लिए, आप TensorFlow को केवल आवश्यकतानुसार GPU मेमोरी आवंटित करने के लिए कॉन्फ़िगर कर सकते हैं।  
* **कोड शामिल करें**: TensorFlow को केवल आवश्यकतानुसार GPU मेमोरी आवंटन बढ़ाने के लिए, अपने नोटबुक में निम्नलिखित कोड शामिल करें:  

```python
physical_devices = tf.config.list_physical_devices('GPU') 
if len(physical_devices)>0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True) 
```  

यदि आप क्लासिक ML दृष्टिकोण से NLP सीखने में रुचि रखते हैं, तो [इस पाठों के संग्रह](https://github.com/microsoft/ML-For-Beginners/tree/main/6-NLP) पर जाएं।  

## इस खंड में
इस खंड में हम निम्नलिखित के बारे में जानेंगे:

* [पाठ को टेन्सर के रूप में प्रस्तुत करना](13-TextRep/README.md)  
* [शब्द एम्बेडिंग्स](14-Emdeddings/README.md)  
* [भाषा मॉडलिंग](15-LanguageModeling/README.md)  
* [रिकरेंट न्यूरल नेटवर्क](16-RNN/README.md)  
* [जनरेटिव नेटवर्क](17-GenerativeNetworks/README.md)  
* [ट्रांसफॉर्मर्स](18-Transformers/README.md)  

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।