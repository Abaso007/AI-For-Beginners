<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "97836d30a6bec736f8e3b4411c572bc2",
  "translation_date": "2025-09-23T13:36:21+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "hi"
}
-->
# प्री-ट्रेंड बड़े भाषा मॉडल

हमारे पिछले सभी कार्यों में, हमने एक न्यूरल नेटवर्क को एक निश्चित कार्य करने के लिए प्रशिक्षित किया था, जिसमें लेबल वाले डेटा सेट का उपयोग किया गया था। बड़े ट्रांसफॉर्मर मॉडल, जैसे BERT, में हम भाषा मॉडलिंग का उपयोग स्व-निगरानी तरीके से करते हैं ताकि एक भाषा मॉडल बनाया जा सके, जिसे बाद में विशेष डाउनस्ट्रीम कार्य के लिए डोमेन-विशिष्ट प्रशिक्षण के साथ अनुकूलित किया जाता है। हालांकि, यह साबित हो चुका है कि बड़े भाषा मॉडल कई कार्य बिना किसी डोमेन-विशिष्ट प्रशिक्षण के भी हल कर सकते हैं। ऐसे मॉडलों के समूह को **GPT**: जनरेटिव प्री-ट्रेंड ट्रांसफॉर्मर कहा जाता है।

## [प्री-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## टेक्स्ट जनरेशन और पर्प्लेक्सिटी

एक न्यूरल नेटवर्क द्वारा सामान्य कार्य बिना डाउनस्ट्रीम प्रशिक्षण के करने की क्षमता को [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) पेपर में प्रस्तुत किया गया है। मुख्य विचार यह है कि कई अन्य कार्यों को **टेक्स्ट जनरेशन** का उपयोग करके मॉडल किया जा सकता है, क्योंकि टेक्स्ट को समझने का मतलब है कि उसे उत्पन्न करने में सक्षम होना। चूंकि मॉडल को मानव ज्ञान को समेटने वाले विशाल मात्रा में टेक्स्ट पर प्रशिक्षित किया गया है, यह विभिन्न विषयों के बारे में भी जानकार बन जाता है।

> टेक्स्ट को समझना और उत्पन्न करने में सक्षम होना दुनिया के बारे में कुछ जानने का भी संकेत देता है। लोग भी बड़े पैमाने पर पढ़कर सीखते हैं, और GPT नेटवर्क इस दृष्टिकोण में समान है।

टेक्स्ट जनरेशन नेटवर्क अगले शब्द की संभावना $$P(w_N)$$ की भविष्यवाणी करके काम करते हैं। हालांकि, अगले शब्द की बिना शर्त संभावना टेक्स्ट कॉर्पस में इस शब्द की आवृत्ति के बराबर होती है। GPT हमें पिछले शब्दों को ध्यान में रखते हुए अगले शब्द की **सशर्त संभावना** प्रदान करता है: $$P(w_N | w_{n-1}, ..., w_0)$$

> आप हमारे [डेटा साइंस फॉर बिगिनर्स पाठ्यक्रम](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) में संभावनाओं के बारे में अधिक पढ़ सकते हैं।

भाषा उत्पन्न करने वाले मॉडल की गुणवत्ता को **पर्प्लेक्सिटी** का उपयोग करके परिभाषित किया जा सकता है। यह एक आंतरिक मीट्रिक है जो हमें किसी कार्य-विशिष्ट डेटा सेट के बिना मॉडल की गुणवत्ता को मापने की अनुमति देता है। यह *वाक्य की संभावना* की धारणा पर आधारित है - मॉडल उन वाक्यों को उच्च संभावना प्रदान करता है जो वास्तविक होने की संभावना रखते हैं (यानी मॉडल उनसे **परेशान** नहीं होता), और उन वाक्यों को कम संभावना प्रदान करता है जो कम समझ में आते हैं (जैसे *क्या यह कर सकता है क्या?*)। जब हम अपने मॉडल को वास्तविक टेक्स्ट कॉर्पस से वाक्य देते हैं, तो हम उम्मीद करते हैं कि वे उच्च संभावना और कम **पर्प्लेक्सिटी** दिखाएंगे। गणितीय रूप से, इसे परीक्षण सेट की सामान्यीकृत प्रतिलोम संभावना के रूप में परिभाषित किया गया है:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**आप [Hugging Face के GPT-संचालित टेक्स्ट एडिटर](https://transformer.huggingface.co/doc/gpt2-large)** का उपयोग करके टेक्स्ट जनरेशन के साथ प्रयोग कर सकते हैं। इस एडिटर में, आप अपना टेक्स्ट लिखना शुरू करते हैं, और **[TAB]** दबाने पर आपको कई पूर्णता विकल्प मिलते हैं। यदि वे बहुत छोटे हैं, या आप उनसे संतुष्ट नहीं हैं - [TAB] फिर से दबाएं, और आपको अधिक विकल्प मिलेंगे, जिनमें लंबे टेक्स्ट के टुकड़े भी शामिल हैं।

## GPT एक परिवार है

GPT एक एकल मॉडल नहीं है, बल्कि [OpenAI](https://openai.com) द्वारा विकसित और प्रशिक्षित मॉडलों का एक संग्रह है।

GPT मॉडलों के अंतर्गत:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| भाषा मॉडल जिसमें 1.5 बिलियन तक पैरामीटर हैं। | भाषा मॉडल जिसमें 175 बिलियन तक पैरामीटर हैं। | 100T पैरामीटर और यह टेक्स्ट और इमेज इनपुट स्वीकार करता है और टेक्स्ट आउटपुट देता है। |

GPT-3 और GPT-4 मॉडल [Microsoft Azure से एक कॉग्निटिव सर्विस](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) के रूप में और [OpenAI API](https://openai.com/api/) के रूप में उपलब्ध हैं।

## प्रॉम्प्ट इंजीनियरिंग

चूंकि GPT को भाषा और कोड को समझने के लिए विशाल मात्रा में डेटा पर प्रशिक्षित किया गया है, यह इनपुट (प्रॉम्प्ट) के जवाब में आउटपुट प्रदान करता है। प्रॉम्प्ट GPT इनपुट या क्वेरी होते हैं, जिनमें एक व्यक्ति मॉडल को कार्यों पर निर्देश देता है जिन्हें उसे पूरा करना है। वांछित परिणाम प्राप्त करने के लिए, आपको सबसे प्रभावी प्रॉम्प्ट की आवश्यकता होती है, जिसमें सही शब्द, प्रारूप, वाक्यांश या यहां तक कि प्रतीकों का चयन करना शामिल होता है। इस दृष्टिकोण को [प्रॉम्प्ट इंजीनियरिंग](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) कहा जाता है।

[यह दस्तावेज़](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) आपको प्रॉम्प्ट इंजीनियरिंग के बारे में अधिक जानकारी प्रदान करता है।

## ✍️ उदाहरण नोटबुक: [OpenAI-GPT के साथ खेलना](GPT-PyTorch.ipynb)

अगले नोटबुक्स में अपनी सीख जारी रखें:

* [OpenAI-GPT और Hugging Face Transformers के साथ टेक्स्ट जनरेशन](GPT-PyTorch.ipynb)

## निष्कर्ष

नए सामान्य प्री-ट्रेंड भाषा मॉडल केवल भाषा संरचना को मॉडल नहीं करते हैं, बल्कि विशाल मात्रा में प्राकृतिक भाषा भी समेटे हुए हैं। इसलिए, उन्हें कुछ NLP कार्यों को ज़ीरो-शॉट या फ्यू-शॉट सेटिंग्स में प्रभावी रूप से हल करने के लिए उपयोग किया जा सकता है।

## [पोस्ट-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/40)

---

