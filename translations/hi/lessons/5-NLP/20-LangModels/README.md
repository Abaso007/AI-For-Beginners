<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2efbb183384a50f0fc0cde02534d912f",
  "translation_date": "2025-08-24T10:21:39+00:00",
  "source_file": "lessons/5-NLP/20-LangModels/README.md",
  "language_code": "hi"
}
-->
# प्री-ट्रेंड बड़े भाषा मॉडल

हमने अब तक के सभी कार्यों में, एक न्यूरल नेटवर्क को एक निश्चित कार्य करने के लिए लेबल्ड डेटा सेट का उपयोग करके प्रशिक्षित किया। बड़े ट्रांसफॉर्मर मॉडल, जैसे कि BERT, में हम भाषा मॉडलिंग को सेल्फ-सुपरवाइज्ड तरीके से उपयोग करते हैं ताकि एक भाषा मॉडल बनाया जा सके, जिसे फिर विशेष डोमेन-विशिष्ट प्रशिक्षण के साथ किसी विशेष डाउनस्ट्रीम कार्य के लिए अनुकूलित किया जाता है। हालांकि, यह साबित हो चुका है कि बड़े भाषा मॉडल कई कार्य बिना किसी डोमेन-विशिष्ट प्रशिक्षण के भी हल कर सकते हैं। ऐसे मॉडलों के परिवार को **GPT**: जनरेटिव प्री-ट्रेंड ट्रांसफॉर्मर कहा जाता है।

## [प्री-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/39)

## टेक्स्ट जनरेशन और परप्लेक्सिटी

न्यूरल नेटवर्क द्वारा बिना डाउनस्ट्रीम प्रशिक्षण के सामान्य कार्य करने की क्षमता को [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) पेपर में प्रस्तुत किया गया है। मुख्य विचार यह है कि कई अन्य कार्यों को **टेक्स्ट जनरेशन** का उपयोग करके मॉडल किया जा सकता है, क्योंकि टेक्स्ट को समझने का मतलब है उसे उत्पन्न करने में सक्षम होना। चूंकि मॉडल को मानव ज्ञान को समाहित करने वाले विशाल मात्रा के टेक्स्ट पर प्रशिक्षित किया गया है, यह विभिन्न विषयों के बारे में भी ज्ञानवान बन जाता है।

> टेक्स्ट को समझना और उत्पन्न करने में सक्षम होना, हमारे आसपास की दुनिया के बारे में कुछ जानने का भी संकेत देता है। लोग भी बड़े पैमाने पर पढ़कर सीखते हैं, और GPT नेटवर्क इस दृष्टि से समान है।

टेक्स्ट जनरेशन नेटवर्क अगले शब्द की संभावना $$P(w_N)$$ की भविष्यवाणी करके काम करते हैं। हालांकि, अगले शब्द की बिना शर्त संभावना टेक्स्ट कॉर्पस में इस शब्द की आवृत्ति के बराबर होती है। GPT हमें पिछले शब्दों को ध्यान में रखते हुए अगले शब्द की **सशर्त संभावना** प्रदान करने में सक्षम है: $$P(w_N | w_{n-1}, ..., w_0)$$

> आप हमारे [डेटा साइंस फॉर बिगिनर्स करिकुलम](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) में संभावनाओं के बारे में अधिक पढ़ सकते हैं।

भाषा जनरेटिंग मॉडल की गुणवत्ता को **परप्लेक्सिटी** का उपयोग करके परिभाषित किया जा सकता है। यह एक आंतरिक मीट्रिक है जो हमें किसी कार्य-विशिष्ट डेटा सेट के बिना मॉडल की गुणवत्ता मापने की अनुमति देता है। यह *वाक्य की संभावना* की धारणा पर आधारित है - मॉडल उन वाक्यों को उच्च संभावना प्रदान करता है जो वास्तविक होने की संभावना रखते हैं (यानी मॉडल उनसे **भ्रमित** नहीं होता), और उन वाक्यों को कम संभावना देता है जो कम समझ में आते हैं (जैसे *क्या यह कर सकता है क्या?*)। जब हम अपने मॉडल को वास्तविक टेक्स्ट कॉर्पस से वाक्य देते हैं, तो हमें उनसे उच्च संभावना और कम **परप्लेक्सिटी** की उम्मीद होती है। गणितीय रूप से, इसे परीक्षण सेट की सामान्यीकृत प्रतिलोम संभावना के रूप में परिभाषित किया गया है:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**आप [Hugging Face के GPT-समर्थित टेक्स्ट एडिटर](https://transformer.huggingface.co/doc/gpt2-large)** का उपयोग करके टेक्स्ट जनरेशन के साथ प्रयोग कर सकते हैं। इस एडिटर में, आप अपना टेक्स्ट लिखना शुरू करते हैं, और **[TAB]** दबाने पर आपको कई पूर्णता विकल्प मिलते हैं। यदि वे बहुत छोटे हैं, या आप उनसे संतुष्ट नहीं हैं - [TAB] फिर से दबाएं, और आपको अधिक विकल्प मिलेंगे, जिनमें लंबे टेक्स्ट के टुकड़े भी शामिल हैं।

## GPT एक परिवार है

GPT एक एकल मॉडल नहीं है, बल्कि [OpenAI](https://openai.com) द्वारा विकसित और प्रशिक्षित मॉडलों का एक संग्रह है।

GPT मॉडलों के अंतर्गत, हमारे पास हैं:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT-3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| भाषा मॉडल जिसमें 1.5 बिलियन पैरामीटर तक हैं। | भाषा मॉडल जिसमें 175 बिलियन पैरामीटर तक हैं। | 100 ट्रिलियन पैरामीटर और यह टेक्स्ट और इमेज दोनों इनपुट स्वीकार करता है और टेक्स्ट आउटपुट देता है। |

GPT-3 और GPT-4 मॉडल [Microsoft Azure से एक कॉग्निटिव सर्विस के रूप में](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) और [OpenAI API](https://openai.com/api/) के रूप में उपलब्ध हैं।

## प्रॉम्प्ट इंजीनियरिंग

चूंकि GPT को भाषा और कोड को समझने के लिए विशाल मात्रा के डेटा पर प्रशिक्षित किया गया है, यह इनपुट (प्रॉम्प्ट) के जवाब में आउटपुट प्रदान करता है। प्रॉम्प्ट GPT के इनपुट या क्वेरी होते हैं, जिनमें आप मॉडल को कार्यों के लिए निर्देश देते हैं। वांछित परिणाम प्राप्त करने के लिए, आपको सबसे प्रभावी प्रॉम्प्ट की आवश्यकता होती है, जिसमें सही शब्द, प्रारूप, वाक्यांश या यहां तक कि प्रतीकों का चयन शामिल होता है। इस दृष्टिकोण को [प्रॉम्प्ट इंजीनियरिंग](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) कहा जाता है।

[यह दस्तावेज़](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) आपको प्रॉम्प्ट इंजीनियरिंग पर अधिक जानकारी प्रदान करता है।

## ✍️ उदाहरण नोटबुक: [OpenAI-GPT के साथ खेलना](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

अगले नोटबुक्स में अपनी सीख जारी रखें:

* [OpenAI-GPT और Hugging Face ट्रांसफॉर्मर्स के साथ टेक्स्ट जनरेट करना](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## निष्कर्ष

नए सामान्य प्री-ट्रेंड भाषा मॉडल केवल भाषा संरचना को मॉडल नहीं करते, बल्कि प्राकृतिक भाषा की विशाल मात्रा भी समाहित करते हैं। इस प्रकार, वे कुछ NLP कार्यों को ज़ीरो-शॉट या फ्यू-शॉट सेटिंग्स में प्रभावी ढंग से हल करने के लिए उपयोग किए जा सकते हैं।

## [पोस्ट-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/40)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।