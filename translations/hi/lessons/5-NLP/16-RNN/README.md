<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-24T09:47:53+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "hi"
}
-->
# पुनरावर्ती न्यूरल नेटवर्क्स

## [प्री-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

पिछले सेक्शन्स में, हमने टेक्स्ट के समृद्ध सेमांटिक रिप्रेजेंटेशन और एम्बेडिंग्स के ऊपर एक साधारण लीनियर क्लासिफायर का उपयोग किया है। यह आर्किटेक्चर वाक्य में शब्दों के समग्र अर्थ को कैप्चर करता है, लेकिन यह शब्दों के **क्रम** को ध्यान में नहीं रखता है, क्योंकि एम्बेडिंग्स के ऊपर की एग्रीगेशन प्रक्रिया ने मूल टेक्स्ट से यह जानकारी हटा दी है। चूंकि ये मॉडल शब्दों के क्रम को मॉडल करने में असमर्थ हैं, वे टेक्स्ट जनरेशन या प्रश्न उत्तर जैसे अधिक जटिल या अस्पष्ट कार्यों को हल नहीं कर सकते।

टेक्स्ट अनुक्रम के अर्थ को कैप्चर करने के लिए, हमें एक अन्य न्यूरल नेटवर्क आर्किटेक्चर का उपयोग करना होगा, जिसे **पुनरावर्ती न्यूरल नेटवर्क** या RNN कहा जाता है। RNN में, हम अपने वाक्य को नेटवर्क के माध्यम से एक समय में एक प्रतीक पास करते हैं, और नेटवर्क कुछ **स्टेट** उत्पन्न करता है, जिसे हम अगले प्रतीक के साथ नेटवर्क में फिर से पास करते हैं।

![RNN](../../../../../lessons/5-NLP/16-RNN/images/rnn.png)

> लेखक द्वारा बनाई गई छवि

टोकन के इनपुट अनुक्रम X<sub>0</sub>,...,X<sub>n</sub> को दिया गया, RNN एक न्यूरल नेटवर्क ब्लॉक्स का अनुक्रम बनाता है, और इस अनुक्रम को एंड-टू-एंड बैकप्रोपेगेशन का उपयोग करके प्रशिक्षित करता है। प्रत्येक नेटवर्क ब्लॉक (X<sub>i</sub>,S<sub>i</sub>) की एक जोड़ी को इनपुट के रूप में लेता है, और परिणामस्वरूप S<sub>i+1</sub> उत्पन्न करता है। अंतिम स्टेट S<sub>n</sub> या (आउटपुट Y<sub>n</sub>) को परिणाम उत्पन्न करने के लिए एक लीनियर क्लासिफायर में भेजा जाता है। सभी नेटवर्क ब्लॉक्स समान वेट्स साझा करते हैं, और एक बैकप्रोपेगेशन पास का उपयोग करके एंड-टू-एंड प्रशिक्षित होते हैं।

चूंकि स्टेट वेक्टर S<sub>0</sub>,...,S<sub>n</sub> नेटवर्क के माध्यम से पास होते हैं, यह शब्दों के बीच अनुक्रमिक निर्भरता को सीखने में सक्षम होता है। उदाहरण के लिए, जब शब्द *not* अनुक्रम में कहीं दिखाई देता है, तो यह स्टेट वेक्टर के भीतर कुछ तत्वों को नकारने के लिए सीख सकता है, जिससे नकारात्मकता उत्पन्न होती है।

> ✅ चूंकि ऊपर चित्र में सभी RNN ब्लॉक्स के वेट्स साझा किए गए हैं, वही चित्र एक ब्लॉक (दाईं ओर) के रूप में दर्शाया जा सकता है जिसमें एक पुनरावर्ती फीडबैक लूप होता है, जो नेटवर्क के आउटपुट स्टेट को इनपुट में वापस पास करता है।

## RNN सेल की संरचना

आइए देखें कि एक साधारण RNN सेल कैसे संगठित होता है। यह पिछले स्टेट S<sub>i-1</sub> और वर्तमान प्रतीक X<sub>i</sub> को इनपुट के रूप में स्वीकार करता है, और आउटपुट स्टेट S<sub>i</sub> उत्पन्न करना होता है (और, कभी-कभी, हम कुछ अन्य आउटपुट Y<sub>i</sub> में भी रुचि रखते हैं, जैसा कि जनरेटिव नेटवर्क्स के मामले में होता है)।

एक साधारण RNN सेल के अंदर दो वेट मैट्रिस होती हैं: एक इनपुट प्रतीक को ट्रांसफॉर्म करती है (इसे W कहते हैं), और दूसरी इनपुट स्टेट को ट्रांसफॉर्म करती है (H)। इस मामले में नेटवर्क का आउटपुट σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b) के रूप में गणना किया जाता है, जहां σ एक्टिवेशन फंक्शन है और b अतिरिक्त बायस है।

<img alt="RNN सेल की संरचना" src="images/rnn-anatomy.png" width="50%"/>

> लेखक द्वारा बनाई गई छवि

कई मामलों में, इनपुट टोकन RNN में प्रवेश करने से पहले एम्बेडिंग लेयर के माध्यम से पास किए जाते हैं ताकि आयाम को कम किया जा सके। इस मामले में, यदि इनपुट वेक्टर का आयाम *emb_size* है, और स्टेट वेक्टर का आयाम *hid_size* है - तो W का आकार *emb_size*×*hid_size* होगा, और H का आकार *hid_size*×*hid_size* होगा।

## लॉन्ग शॉर्ट टर्म मेमोरी (LSTM)

क्लासिकल RNNs की मुख्य समस्याओं में से एक **vanishing gradients** समस्या है। चूंकि RNNs को एक बैकप्रोपेगेशन पास में एंड-टू-एंड प्रशिक्षित किया जाता है, यह नेटवर्क की पहली लेयर तक त्रुटि को प्रोपेगेट करने में कठिनाई होती है, और इस प्रकार नेटवर्क दूरस्थ टोकन के बीच संबंधों को सीख नहीं सकता। इस समस्या से बचने के तरीकों में से एक **स्पष्ट स्टेट प्रबंधन** को **गेट्स** का उपयोग करके पेश करना है। इस प्रकार की दो प्रसिद्ध आर्किटेक्चर हैं: **लॉन्ग शॉर्ट टर्म मेमोरी** (LSTM) और **गेटेड रिले यूनिट** (GRU)।

![लॉन्ग शॉर्ट टर्म मेमोरी सेल का उदाहरण दिखाने वाली छवि](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> छवि स्रोत TBD

LSTM नेटवर्क RNN के समान तरीके से संगठित होता है, लेकिन दो स्टेट्स होते हैं जो लेयर से लेयर तक पास होते हैं: वास्तविक स्टेट C, और हिडन वेक्टर H। प्रत्येक यूनिट पर, हिडन वेक्टर H<sub>i</sub> इनपुट X<sub>i</sub> के साथ कंकेटनेट होता है, और वे **गेट्स** के माध्यम से स्टेट C के साथ क्या होता है उसे नियंत्रित करते हैं। प्रत्येक गेट एक न्यूरल नेटवर्क होता है जिसमें सिग्मॉइड एक्टिवेशन होता है (आउटपुट [0,1] की रेंज में), जिसे स्टेट वेक्टर के साथ गुणा करते समय बिटवाइज मास्क के रूप में सोचा जा सकता है। निम्नलिखित गेट्स होते हैं (ऊपर चित्र में बाएं से दाएं):

* **फॉरगेट गेट** हिडन वेक्टर लेता है और निर्धारित करता है कि वेक्टर C के कौन से घटकों को भूलना है और कौन से पास करना है।
* **इनपुट गेट** इनपुट और हिडन वेक्टर से कुछ जानकारी लेता है और इसे स्टेट में डालता है।
* **आउटपुट गेट** स्टेट को *tanh* एक्टिवेशन के साथ एक लीनियर लेयर के माध्यम से ट्रांसफॉर्म करता है, फिर हिडन वेक्टर H<sub>i</sub> का उपयोग करके इसके कुछ घटकों का चयन करता है ताकि नया स्टेट C<sub>i+1</sub> उत्पन्न हो सके।

स्टेट C के घटकों को कुछ फ्लैग्स के रूप में सोचा जा सकता है जिन्हें ऑन और ऑफ किया जा सकता है। उदाहरण के लिए, जब हम अनुक्रम में *Alice* नाम का सामना करते हैं, तो हम मान सकते हैं कि यह एक महिला चरित्र को संदर्भित करता है, और स्टेट में फ्लैग उठाते हैं कि हमारे पास वाक्य में एक महिला संज्ञा है। जब हम आगे *and Tom* वाक्यांशों का सामना करते हैं, तो हम फ्लैग उठाते हैं कि हमारे पास बहुवचन संज्ञा है। इस प्रकार स्टेट को हेरफेर करके हम वाक्य भागों के व्याकरणिक गुणों का ट्रैक रख सकते हैं।

> ✅ LSTM की आंतरिक संरचना को समझने के लिए एक उत्कृष्ट संसाधन है यह शानदार लेख [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) क्रिस्टोफर ओलाह द्वारा।

## बिडायरेक्शनल और मल्टीलेयर RNNs

हमने पुनरावर्ती नेटवर्क्स पर चर्चा की है जो एक दिशा में काम करते हैं, अनुक्रम की शुरुआत से अंत तक। यह स्वाभाविक लगता है, क्योंकि यह उस तरीके जैसा है जैसे हम पढ़ते हैं और भाषण सुनते हैं। हालांकि, चूंकि कई व्यावहारिक मामलों में हमारे पास इनपुट अनुक्रम तक रैंडम एक्सेस होता है, यह दोनों दिशाओं में पुनरावर्ती गणना चलाने के लिए समझ में आ सकता है। ऐसे नेटवर्क्स को **बिडायरेक्शनल** RNNs कहा जाता है। बिडायरेक्शनल नेटवर्क के साथ काम करते समय, हमें दो हिडन स्टेट वेक्टर की आवश्यकता होगी, प्रत्येक दिशा के लिए एक।

एक पुनरावर्ती नेटवर्क, चाहे वह एक-दिशात्मक हो या बिडायरेक्शनल, अनुक्रम के भीतर कुछ पैटर्न कैप्चर करता है, और उन्हें स्टेट वेक्टर में स्टोर कर सकता है या आउटपुट में पास कर सकता है। जैसे कि कन्वोल्यूशनल नेटवर्क्स के साथ, हम पहले लेयर द्वारा निकाले गए लो-लेवल पैटर्न से उच्च-स्तरीय पैटर्न कैप्चर करने के लिए पहले लेयर के ऊपर एक और पुनरावर्ती लेयर बना सकते हैं। यह हमें **मल्टी-लेयर RNN** की धारणा की ओर ले जाता है, जिसमें दो या अधिक पुनरावर्ती नेटवर्क्स होते हैं, जहां पिछले लेयर का आउटपुट अगले लेयर में इनपुट के रूप में पास होता है।

![मल्टीलेयर लॉन्ग-शॉर्ट-टर्म-मेमोरी RNN दिखाने वाली छवि](../../../../../lessons/5-NLP/16-RNN/images/multi-layer-lstm.jpg)

*चित्र [इस शानदार पोस्ट](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) फर्नांडो लोपेज़ द्वारा*

## ✍️ अभ्यास: एम्बेडिंग्स

अगले नोटबुक्स में अपना अध्ययन जारी रखें:

* [PyTorch के साथ RNNs](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow के साथ RNNs](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## निष्कर्ष

इस यूनिट में, हमने देखा कि RNNs का उपयोग अनुक्रम वर्गीकरण के लिए किया जा सकता है, लेकिन वास्तव में, वे कई अन्य कार्यों को संभाल सकते हैं, जैसे टेक्स्ट जनरेशन, मशीन ट्रांसलेशन, और अधिक। हम अगले यूनिट में उन कार्यों पर विचार करेंगे।

## 🚀 चुनौती

LSTMs के बारे में कुछ साहित्य पढ़ें और उनके अनुप्रयोगों पर विचार करें:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [पोस्ट-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## समीक्षा और स्व-अध्ययन

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) क्रिस्टोफर ओलाह द्वारा।

## [असाइनमेंट: नोटबुक्स](assignment.md)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।