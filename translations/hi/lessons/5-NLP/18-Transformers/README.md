<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-24T10:19:17+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "hi"
}
-->
# ध्यान तंत्र और ट्रांसफॉर्मर्स

## [प्री-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

NLP क्षेत्र में सबसे महत्वपूर्ण समस्याओं में से एक है **मशीन अनुवाद**, जो Google Translate जैसे टूल्स के लिए एक आवश्यक कार्य है। इस खंड में, हम मशीन अनुवाद पर ध्यान केंद्रित करेंगे, या सामान्य रूप से, किसी भी *सीक्वेंस-टू-सीक्वेंस* कार्य (जिसे **वाक्य ट्रांसडक्शन** भी कहा जाता है) पर चर्चा करेंगे।

RNNs के साथ, सीक्वेंस-टू-सीक्वेंस को दो पुनरावर्ती नेटवर्क्स द्वारा लागू किया जाता है, जहां एक नेटवर्क, **एन्कोडर**, इनपुट सीक्वेंस को एक हिडन स्टेट में संक्षेपित करता है, जबकि दूसरा नेटवर्क, **डिकोडर**, इस हिडन स्टेट को अनुवादित परिणाम में बदलता है। इस दृष्टिकोण में कुछ समस्याएं हैं:

* एन्कोडर नेटवर्क की अंतिम स्थिति को वाक्य की शुरुआत याद रखने में कठिनाई होती है, जिससे लंबे वाक्यों के लिए मॉडल की गुणवत्ता खराब हो जाती है।
* सीक्वेंस के सभी शब्दों का परिणाम पर समान प्रभाव होता है। हालांकि, वास्तविकता में, इनपुट सीक्वेंस के कुछ विशिष्ट शब्दों का अनुक्रमिक आउटपुट पर अधिक प्रभाव होता है।

**ध्यान तंत्र (Attention Mechanisms)** प्रत्येक इनपुट वेक्टर के संदर्भ प्रभाव को प्रत्येक आउटपुट भविष्यवाणी पर वेट करने का एक साधन प्रदान करते हैं। इसे लागू करने का तरीका यह है कि इनपुट RNN और आउटपुट RNN की मध्यवर्ती अवस्थाओं के बीच शॉर्टकट्स बनाए जाते हैं। इस प्रकार, जब आउटपुट प्रतीक y<sub>t</sub> उत्पन्न किया जाता है, तो हम सभी इनपुट हिडन स्टेट्स h<sub>i</sub> को विभिन्न वेट गुणांक α<sub>t,i</sub> के साथ ध्यान में रखते हैं।

![एन्कोडर/डिकोडर मॉडल जिसमें एडिटिव ध्यान लेयर है](../../../../../lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) में एडिटिव ध्यान तंत्र के साथ एन्कोडर-डिकोडर मॉडल, [इस ब्लॉग पोस्ट](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) से उद्धृत

ध्यान मैट्रिक्स {α<sub>i,j</sub>} यह दर्शाएगा कि आउटपुट सीक्वेंस में किसी दिए गए शब्द की पीढ़ी में कुछ इनपुट शब्दों की कितनी भूमिका है। नीचे एक उदाहरण दिया गया है:

![RNNsearch-50 द्वारा पाए गए एक नमूना संरेखण को दिखाने वाली छवि, Bahdanau - arviz.org से ली गई](../../../../../lessons/5-NLP/18-Transformers/images/bahdanau-fig3.png)

> [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3) से चित्र

ध्यान तंत्र NLP में वर्तमान या निकट वर्तमान की अत्याधुनिक स्थिति के लिए जिम्मेदार हैं। हालांकि, ध्यान जोड़ने से मॉडल के पैरामीटर की संख्या में काफी वृद्धि होती है, जिससे RNNs के साथ स्केलिंग समस्याएं उत्पन्न होती हैं। RNNs को स्केल करने की एक प्रमुख बाधा यह है कि मॉडल की पुनरावृत्त प्रकृति प्रशिक्षण को बैच और समानांतर बनाना चुनौतीपूर्ण बनाती है। RNN में, सीक्वेंस के प्रत्येक तत्व को क्रमिक क्रम में संसाधित करना पड़ता है, जिसका अर्थ है कि इसे आसानी से समानांतर नहीं किया जा सकता।

![ध्यान के साथ एन्कोडर डिकोडर](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> [Google के ब्लॉग](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) से चित्र

ध्यान तंत्रों को अपनाने और इस बाधा ने आज के अत्याधुनिक ट्रांसफॉर्मर मॉडल्स के निर्माण का मार्ग प्रशस्त किया, जैसे कि BERT और Open-GPT3।

## ट्रांसफॉर्मर मॉडल्स

ट्रांसफॉर्मर्स के पीछे मुख्य विचारों में से एक RNNs की क्रमिक प्रकृति से बचना और एक ऐसा मॉडल बनाना है जो प्रशिक्षण के दौरान समानांतर हो सके। इसे दो विचारों को लागू करके प्राप्त किया गया है:

* पोजिशनल एन्कोडिंग
* RNNs (या CNNs) के बजाय पैटर्न को कैप्चर करने के लिए सेल्फ-अटेंशन तंत्र का उपयोग करना (यही कारण है कि ट्रांसफॉर्मर्स को पेश करने वाले पेपर का नाम *[Attention is all you need](https://arxiv.org/abs/1706.03762)* है)।

### पोजिशनल एन्कोडिंग/एम्बेडिंग

पोजिशनल एन्कोडिंग का विचार निम्नलिखित है:
1. RNNs का उपयोग करते समय, टोकन की सापेक्ष स्थिति को चरणों की संख्या द्वारा दर्शाया जाता है, और इसलिए इसे स्पष्ट रूप से दर्शाने की आवश्यकता नहीं होती।
2. हालांकि, जब हम ध्यान का उपयोग करते हैं, तो हमें सीक्वेंस के भीतर टोकन की सापेक्ष स्थिति जानने की आवश्यकता होती है।
3. पोजिशनल एन्कोडिंग प्राप्त करने के लिए, हम अपने टोकन सीक्वेंस को सीक्वेंस में टोकन पोजिशन के एक अनुक्रम (जैसे, 0,1, ...) के साथ बढ़ाते हैं।
4. फिर हम टोकन पोजिशन को टोकन एम्बेडिंग वेक्टर के साथ मिलाते हैं। पोजिशन (पूर्णांक) को वेक्टर में बदलने के लिए, हम विभिन्न दृष्टिकोणों का उपयोग कर सकते हैं:

* टोकन एम्बेडिंग के समान ट्रेन करने योग्य एम्बेडिंग। यह वह दृष्टिकोण है जिसे हम यहां मानते हैं। हम टोकन और उनके पोजिशन दोनों पर एम्बेडिंग लेयर्स लागू करते हैं, जिससे समान आयामों के एम्बेडिंग वेक्टर प्राप्त होते हैं, जिन्हें हम फिर जोड़ते हैं।
* मूल पेपर में प्रस्तावित फिक्स्ड पोजिशन एन्कोडिंग फंक्शन।

<img src="images/pos-embedding.png" width="50%"/>

> लेखक द्वारा छवि

पोजिशनल एम्बेडिंग के साथ हमें जो परिणाम मिलता है, वह मूल टोकन और सीक्वेंस के भीतर उसकी स्थिति दोनों को एम्बेड करता है।

### मल्टी-हेड सेल्फ-अटेंशन

इसके बाद, हमें अपने सीक्वेंस के भीतर कुछ पैटर्न कैप्चर करने की आवश्यकता है। ऐसा करने के लिए, ट्रांसफॉर्मर्स **सेल्फ-अटेंशन** तंत्र का उपयोग करते हैं, जो मूल रूप से इनपुट और आउटपुट के रूप में एक ही सीक्वेंस पर लागू ध्यान है। सेल्फ-अटेंशन लागू करने से हमें वाक्य के भीतर **संदर्भ** को ध्यान में रखने और यह देखने की अनुमति मिलती है कि कौन से शब्द आपस में संबंधित हैं। उदाहरण के लिए, यह हमें यह देखने की अनुमति देता है कि *it* जैसे कोरफेरेंस द्वारा किन शब्दों का संदर्भ दिया गया है, और संदर्भ को ध्यान में रखता है:

![](../../../../../lessons/5-NLP/18-Transformers/images/CoreferenceResolution.png)

> [Google ब्लॉग](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html) से छवि

ट्रांसफॉर्मर्स में, हम **मल्टी-हेड अटेंशन** का उपयोग करते हैं ताकि नेटवर्क को विभिन्न प्रकार की निर्भरताओं को कैप्चर करने की शक्ति मिल सके, जैसे कि दीर्घकालिक बनाम अल्पकालिक शब्द संबंध, कोरफेरेंस बनाम कुछ और, आदि।

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) में ट्रांसफॉर्मर लेयर्स के कार्यान्वयन पर अधिक विवरण हैं।

### एन्कोडर-डिकोडर अटेंशन

ट्रांसफॉर्मर्स में, ध्यान का उपयोग दो स्थानों पर किया जाता है:

* इनपुट टेक्स्ट के भीतर पैटर्न को कैप्चर करने के लिए सेल्फ-अटेंशन का उपयोग करना।
* सीक्वेंस अनुवाद करने के लिए - यह एन्कोडर और डिकोडर के बीच ध्यान लेयर है।

एन्कोडर-डिकोडर अटेंशन RNNs में उपयोग किए गए ध्यान तंत्र के समान है, जैसा कि इस खंड की शुरुआत में वर्णित है। यह एनिमेटेड आरेख एन्कोडर-डिकोडर अटेंशन की भूमिका को समझाता है।

![एनिमेटेड GIF जो दिखाता है कि ट्रांसफॉर्मर मॉडल्स में मूल्यांकन कैसे किया जाता है।](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

चूंकि प्रत्येक इनपुट स्थिति को स्वतंत्र रूप से प्रत्येक आउटपुट स्थिति पर मैप किया जाता है, ट्रांसफॉर्मर्स RNNs की तुलना में बेहतर समानांतरता प्रदान कर सकते हैं, जिससे बड़े और अधिक अभिव्यक्तिपूर्ण भाषा मॉडल सक्षम होते हैं। प्रत्येक अटेंशन हेड का उपयोग शब्दों के बीच विभिन्न संबंधों को सीखने के लिए किया जा सकता है, जो डाउनस्ट्रीम नेचुरल लैंग्वेज प्रोसेसिंग कार्यों में सुधार करता है।

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) एक बहुत बड़ा मल्टी लेयर ट्रांसफॉर्मर नेटवर्क है, जिसमें *BERT-base* के लिए 12 लेयर्स और *BERT-large* के लिए 24 लेयर्स हैं। मॉडल को पहले एक बड़े टेक्स्ट डेटा कॉर्पस (विकिपीडिया + किताबें) पर अनसुपरवाइज्ड ट्रेनिंग (वाक्य में मास्क किए गए शब्दों की भविष्यवाणी) का उपयोग करके प्री-ट्रेन किया जाता है। प्री-ट्रेनिंग के दौरान, मॉडल भाषा समझ के महत्वपूर्ण स्तरों को अवशोषित करता है, जिसे फिर अन्य डेटासेट्स के साथ फाइन ट्यूनिंग के माध्यम से उपयोग किया जा सकता है। इस प्रक्रिया को **ट्रांसफर लर्निंग** कहा जाता है।

![http://jalammar.github.io/illustrated-bert/ से चित्र](../../../../../lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png)

> [स्रोत](http://jalammar.github.io/illustrated-bert/)

## ✍️ अभ्यास: ट्रांसफॉर्मर्स

निम्नलिखित नोटबुक्स में अपना अध्ययन जारी रखें:

* [PyTorch में ट्रांसफॉर्मर्स](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [TensorFlow में ट्रांसफॉर्मर्स](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## निष्कर्ष

इस पाठ में आपने ट्रांसफॉर्मर्स और ध्यान तंत्र के बारे में सीखा, जो NLP टूलबॉक्स के सभी आवश्यक उपकरण हैं। ट्रांसफॉर्मर आर्किटेक्चर के कई प्रकार हैं, जिनमें BERT, DistilBERT, BigBird, OpenGPT3 और अन्य शामिल हैं, जिन्हें फाइन ट्यून किया जा सकता है। [HuggingFace पैकेज](https://github.com/huggingface/) PyTorch और TensorFlow दोनों के साथ इन आर्किटेक्चर को ट्रेन करने के लिए रिपॉजिटरी प्रदान करता है।

## 🚀 चुनौती

## [पोस्ट-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## समीक्षा और स्व-अध्ययन

* [ब्लॉग पोस्ट](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), जो ट्रांसफॉर्मर्स पर क्लासिकल [Attention is all you need](https://arxiv.org/abs/1706.03762) पेपर को समझाती है।
* [ब्लॉग पोस्ट की एक श्रृंखला](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452), जो ट्रांसफॉर्मर्स की आर्किटेक्चर को विस्तार से समझाती है।

## [असाइनमेंट](assignment.md)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।