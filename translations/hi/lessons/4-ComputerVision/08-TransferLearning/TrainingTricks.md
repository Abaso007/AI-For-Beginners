<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ae074cd940fc2f4dc24fc07b66ccbd99",
  "translation_date": "2025-08-24T09:56:29+00:00",
  "source_file": "lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md",
  "language_code": "hi"
}
-->
# डीप लर्निंग ट्रेनिंग ट्रिक्स

जैसे-जैसे न्यूरल नेटवर्क गहरे होते जाते हैं, उनका प्रशिक्षण और अधिक चुनौतीपूर्ण हो जाता है। एक मुख्य समस्या है जिसे [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) या [exploding gradients](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.) कहा जाता है। [यह पोस्ट](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) इन समस्याओं का अच्छा परिचय देती है।

गहरे नेटवर्क के प्रशिक्षण को अधिक प्रभावी बनाने के लिए, कुछ तकनीकों का उपयोग किया जा सकता है।

## मानों को उचित सीमा में रखना

संख्यात्मक गणनाओं को अधिक स्थिर बनाने के लिए, हमें यह सुनिश्चित करना चाहिए कि हमारे न्यूरल नेटवर्क के सभी मान उचित स्केल में हों, आमतौर पर [-1..1] या [0..1]। यह कोई बहुत सख्त आवश्यकता नहीं है, लेकिन फ्लोटिंग पॉइंट गणनाओं की प्रकृति ऐसी है कि विभिन्न परिमाणों के मानों को एक साथ सटीक रूप से हेरफेर नहीं किया जा सकता। उदाहरण के लिए, यदि हम 10<sup>-10</sup> और 10<sup>10</sup> जोड़ते हैं, तो हमें संभवतः 10<sup>10</sup> मिलेगा, क्योंकि छोटा मान बड़े वाले के समान क्रम में "परिवर्तित" हो जाएगा, और इस प्रकार मैन्टिसा खो जाएगी।

अधिकांश एक्टिवेशन फंक्शन्स में [-1..1] के आसपास गैर-रेखीयता होती है, और इसलिए यह समझ में आता है कि सभी इनपुट डेटा को [-1..1] या [0..1] अंतराल में स्केल किया जाए।

## प्रारंभिक वेट इनिशियलाइज़ेशन

आदर्श रूप से, हम चाहते हैं कि नेटवर्क लेयर्स से गुजरने के बाद मान समान सीमा में रहें। इसलिए वेट्स को इस तरह से इनिशियलाइज़ करना महत्वपूर्ण है कि मानों का वितरण संरक्षित रहे।

सामान्य वितरण **N(0,1)** एक अच्छा विचार नहीं है, क्योंकि यदि हमारे पास *n* इनपुट्स हैं, तो आउटपुट का मानक विचलन *n* होगा, और मान [0..1] अंतराल से बाहर जा सकते हैं।

निम्नलिखित इनिशियलाइज़ेशन अक्सर उपयोग किए जाते हैं:

- यूनिफॉर्म वितरण -- `uniform`
- **N(0,1/n)** -- `gaussian`
- **N(0,1/√n_in)** यह सुनिश्चित करता है कि शून्य माध्य और मानक विचलन 1 वाले इनपुट्स के लिए वही माध्य/मानक विचलन बना रहेगा।
- **N(0,√2/(n_in+n_out))** -- जिसे **Xavier initialization** (`glorot`) कहा जाता है, यह फॉरवर्ड और बैकवर्ड प्रोपेगेशन दोनों के दौरान सिग्नल्स को सीमा में रखने में मदद करता है।

## बैच नॉर्मलाइज़ेशन

सही वेट इनिशियलाइज़ेशन के बावजूद, प्रशिक्षण के दौरान वेट्स बहुत बड़े या छोटे हो सकते हैं, और वे सिग्नल्स को उचित सीमा से बाहर ले जा सकते हैं। हम **नॉर्मलाइज़ेशन** तकनीकों का उपयोग करके सिग्नल्स को वापस ला सकते हैं। जबकि कई तकनीकें हैं (वेट नॉर्मलाइज़ेशन, लेयर नॉर्मलाइज़ेशन), सबसे अधिक उपयोग की जाने वाली तकनीक बैच नॉर्मलाइज़ेशन है।

**बैच नॉर्मलाइज़ेशन** का विचार यह है कि मिनीबैच के सभी मानों को ध्यान में रखा जाए, और उन मानों के आधार पर नॉर्मलाइज़ेशन (जैसे माध्य घटाना और मानक विचलन से विभाजित करना) किया जाए। इसे एक नेटवर्क लेयर के रूप में लागू किया जाता है, जो वेट्स लागू करने के बाद, लेकिन एक्टिवेशन फंक्शन से पहले यह नॉर्मलाइज़ेशन करती है। परिणामस्वरूप, हमें उच्च अंतिम सटीकता और तेज़ प्रशिक्षण देखने को मिलता है।

यहाँ बैच नॉर्मलाइज़ेशन पर [मूल पेपर](https://arxiv.org/pdf/1502.03167.pdf), [विकिपीडिया पर व्याख्या](https://en.wikipedia.org/wiki/Batch_normalization), और [एक अच्छा परिचयात्मक ब्लॉग पोस्ट](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (और [रूसी में](https://habrahabr.ru/post/309302/)) उपलब्ध है।

## ड्रॉपआउट

**ड्रॉपआउट** एक दिलचस्प तकनीक है जो प्रशिक्षण के दौरान यादृच्छिक न्यूरॉन्स का एक निश्चित प्रतिशत हटा देती है। इसे एक लेयर के रूप में लागू किया जाता है जिसमें एक पैरामीटर होता है (हटाए जाने वाले न्यूरॉन्स का प्रतिशत, आमतौर पर 10%-50%), और प्रशिक्षण के दौरान यह इनपुट वेक्टर के यादृच्छिक तत्वों को शून्य कर देता है, इससे पहले कि इसे अगली लेयर में भेजा जाए।

हालांकि यह विचार अजीब लग सकता है, आप [`Dropout.ipynb`](../../../../../lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb) नोटबुक में MNIST डिजिट क्लासिफायर के प्रशिक्षण पर ड्रॉपआउट के प्रभाव को देख सकते हैं। यह प्रशिक्षण को तेज़ करता है और कम प्रशिक्षण युगों में उच्च सटीकता प्राप्त करने की अनुमति देता है।

इस प्रभाव को कई तरीकों से समझाया जा सकता है:

- इसे मॉडल के लिए एक यादृच्छिक झटका कारक माना जा सकता है, जो अनुकूलन को स्थानीय न्यूनतम से बाहर ले जाता है।
- इसे *अप्रत्यक्ष मॉडल एवरेजिंग* के रूप में माना जा सकता है, क्योंकि हम कह सकते हैं कि ड्रॉपआउट के दौरान हम थोड़ा अलग मॉडल का प्रशिक्षण कर रहे हैं।

> *कुछ लोग कहते हैं कि जब कोई नशे में व्यक्ति कुछ सीखने की कोशिश करता है, तो वह इसे अगले दिन सुबह बेहतर याद करता है, एक सामान्य व्यक्ति की तुलना में, क्योंकि कुछ खराबी वाले न्यूरॉन्स वाला मस्तिष्क अर्थ को समझने के लिए बेहतर अनुकूलन करता है। हमने कभी यह परीक्षण नहीं किया कि यह सच है या नहीं।*

## ओवरफिटिंग को रोकना

डीप लर्निंग का एक बहुत महत्वपूर्ण पहलू [ओवरफिटिंग](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) को रोकने में सक्षम होना है। हालांकि एक बहुत शक्तिशाली न्यूरल नेटवर्क मॉडल का उपयोग करना आकर्षक हो सकता है, हमें हमेशा मॉडल पैरामीटर्स की संख्या और प्रशिक्षण नमूनों की संख्या के बीच संतुलन बनाए रखना चाहिए।

> सुनिश्चित करें कि आप [ओवरफिटिंग](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) की अवधारणा को समझते हैं जिसे हमने पहले पेश किया है!

ओवरफिटिंग को रोकने के कई तरीके हैं:

- अर्ली स्टॉपिंग -- वैलिडेशन सेट पर त्रुटि की लगातार निगरानी करना और जब वैलिडेशन त्रुटि बढ़ने लगे तो प्रशिक्षण रोक देना।
- स्पष्ट वेट डिके / रेग्युलराइज़ेशन -- लॉस फंक्शन में वेट्स के उच्च मानों के लिए एक अतिरिक्त दंड जोड़ना, जो मॉडल को बहुत अस्थिर परिणाम देने से रोकता है।
- मॉडल एवरेजिंग -- कई मॉडलों का प्रशिक्षण और फिर परिणामों का औसत निकालना। यह वेरिएंस को कम करने में मदद करता है।
- ड्रॉपआउट (अप्रत्यक्ष मॉडल एवरेजिंग)

## ऑप्टिमाइज़र्स / प्रशिक्षण एल्गोरिदम

प्रशिक्षण का एक और महत्वपूर्ण पहलू एक अच्छा प्रशिक्षण एल्गोरिदम चुनना है। जबकि पारंपरिक **ग्रेडिएंट डिसेंट** एक उचित विकल्प है, यह कभी-कभी बहुत धीमा हो सकता है, या अन्य समस्याओं का कारण बन सकता है।

डीप लर्निंग में, हम **स्टोकास्टिक ग्रेडिएंट डिसेंट** (SGD) का उपयोग करते हैं, जो प्रशिक्षण सेट से यादृच्छिक रूप से चुने गए मिनीबैच पर लागू ग्रेडिएंट डिसेंट है। वेट्स को इस सूत्र का उपयोग करके समायोजित किया जाता है:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### मोमेंटम

**मोमेंटम SGD** में, हम पिछले चरणों के ग्रेडिएंट का एक हिस्सा बनाए रखते हैं। यह उस स्थिति के समान है जब हम जड़त्व के साथ कहीं जा रहे होते हैं, और हमें एक अलग दिशा में धक्का मिलता है, तो हमारा प्रक्षेपवक्र तुरंत नहीं बदलता, बल्कि मूल गति का कुछ हिस्सा बनाए रखता है। यहाँ हम *गति* का प्रतिनिधित्व करने के लिए एक और वेक्टर v पेश करते हैं:

- v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
- w<sup>t+1</sup> = w<sup>t</sup> + v<sup>t+1</sup>

यहाँ पैरामीटर γ यह दर्शाता है कि हम जड़त्व को किस हद तक ध्यान में रखते हैं: γ=0 पारंपरिक SGD के अनुरूप है; γ=1 एक शुद्ध गति समीकरण है।

### एडम, एडाग्रेड, आदि

चूंकि प्रत्येक लेयर में हम सिग्नल्स को किसी मैट्रिक्स W<sub>i</sub> से गुणा करते हैं, ||W<sub>i</sub>|| के आधार पर, ग्रेडिएंट या तो घट सकता है और 0 के करीब हो सकता है, या अनिश्चित रूप से बढ़ सकता है। यह Exploding/Vanishing Gradients समस्या का सार है।

इस समस्या का एक समाधान यह है कि समीकरण में केवल ग्रेडिएंट की दिशा का उपयोग किया जाए, और इसके परिमाण को अनदेखा किया जाए, यानी:

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), जहाँ ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

इस एल्गोरिदम को **एडाग्रेड** कहा जाता है। इसी विचार का उपयोग करने वाले अन्य एल्गोरिदम: **RMSProp**, **Adam**

> **एडम** को कई अनुप्रयोगों के लिए एक बहुत ही कुशल एल्गोरिदम माना जाता है, इसलिए यदि आप सुनिश्चित नहीं हैं कि किसका उपयोग करना है - एडम का उपयोग करें।

### ग्रेडिएंट क्लिपिंग

ग्रेडिएंट क्लिपिंग उपरोक्त विचार का एक विस्तार है। जब ||∇ℒ|| ≤ θ, तो हम वेट ऑप्टिमाइज़ेशन में मूल ग्रेडिएंट पर विचार करते हैं, और जब ||∇ℒ|| > θ - तो हम ग्रेडिएंट को उसके नॉर्म से विभाजित करते हैं। यहाँ θ एक पैरामीटर है, अधिकांश मामलों में हम θ=1 या θ=10 ले सकते हैं।

### लर्निंग रेट डिके

प्रशिक्षण की सफलता अक्सर लर्निंग रेट पैरामीटर η पर निर्भर करती है। यह मान लेना तार्किक है कि η के बड़े मान तेज़ प्रशिक्षण का परिणाम देते हैं, जो कि हम आमतौर पर प्रशिक्षण की शुरुआत में चाहते हैं, और फिर η के छोटे मान हमें नेटवर्क को फाइन-ट्यून करने की अनुमति देते हैं। इसलिए, अधिकांश मामलों में हम प्रशिक्षण की प्रक्रिया में η को कम करना चाहते हैं।

यह प्रत्येक प्रशिक्षण युग के बाद η को किसी संख्या (जैसे 0.98) से गुणा करके किया जा सकता है, या अधिक जटिल **लर्निंग रेट शेड्यूल** का उपयोग करके।

## विभिन्न नेटवर्क आर्किटेक्चर

आपकी समस्या के लिए सही नेटवर्क आर्किटेक्चर का चयन करना मुश्किल हो सकता है। सामान्यतः, हम उस आर्किटेक्चर को चुनेंगे जो हमारे विशिष्ट कार्य (या समान कार्य) के लिए काम करने के लिए सिद्ध हुआ हो। यहाँ कंप्यूटर विज़न के लिए न्यूरल नेटवर्क आर्किटेक्चर का [एक अच्छा अवलोकन](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) है।

> यह महत्वपूर्ण है कि हम ऐसा आर्किटेक्चर चुनें जो हमारे पास उपलब्ध प्रशिक्षण नमूनों की संख्या के लिए पर्याप्त शक्तिशाली हो। बहुत शक्तिशाली मॉडल का चयन [ओवरफिटिंग](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) का कारण बन सकता है।

एक और अच्छा तरीका यह होगा कि ऐसा आर्किटेक्चर उपयोग करें जो आवश्यक जटिलता के अनुसार स्वचालित रूप से समायोजित हो। कुछ हद तक, **ResNet** आर्किटेक्चर और **Inception** स्व-समायोजित हैं। [कंप्यूटर विज़न आर्किटेक्चर पर अधिक जानकारी](../07-ConvNets/CNN_Architectures.md)।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।