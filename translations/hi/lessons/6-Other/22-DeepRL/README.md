<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-24T09:58:28+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "hi"
}
-->
# डीप रिइनफोर्समेंट लर्निंग

रिइनफोर्समेंट लर्निंग (RL) को मशीन लर्निंग के तीन मुख्य पैरेडाइम्स में से एक माना जाता है, जिसमें सुपरवाइज्ड लर्निंग और अनसुपरवाइज्ड लर्निंग भी शामिल हैं। जहां सुपरवाइज्ड लर्निंग में हमें ज्ञात परिणामों वाले डेटा सेट पर निर्भर रहना पड़ता है, वहीं RL **करके सीखने** पर आधारित है। उदाहरण के लिए, जब हम पहली बार कोई कंप्यूटर गेम देखते हैं, तो हम इसे खेलना शुरू कर देते हैं, भले ही हमें नियमों का पता न हो। जल्द ही, हम केवल खेलने और अपने व्यवहार को समायोजित करने की प्रक्रिया के माध्यम से अपने कौशल में सुधार करने में सक्षम हो जाते हैं।

## [प्री-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL करने के लिए हमें चाहिए:

* एक **एनवायरनमेंट** या **सिम्युलेटर**, जो गेम के नियम सेट करता है। हमें सिम्युलेटर में प्रयोग चलाने और परिणामों को देखने में सक्षम होना चाहिए।
* कुछ **रिवॉर्ड फंक्शन**, जो यह संकेत देता है कि हमारा प्रयोग कितना सफल रहा। कंप्यूटर गेम खेलना सीखने के मामले में, रिवॉर्ड हमारा अंतिम स्कोर होगा।

रिवॉर्ड फंक्शन के आधार पर, हमें अपने व्यवहार को समायोजित करने और अपने कौशल में सुधार करने में सक्षम होना चाहिए, ताकि अगली बार हम बेहतर खेल सकें। अन्य प्रकार के मशीन लर्निंग और RL के बीच मुख्य अंतर यह है कि RL में हमें आमतौर पर यह नहीं पता होता कि हम जीतेंगे या हारेंगे जब तक कि हम गेम खत्म नहीं कर लेते। इसलिए, हम यह नहीं कह सकते कि कोई विशेष चाल अकेले में अच्छी है या नहीं - हमें केवल गेम के अंत में रिवॉर्ड प्राप्त होता है।

RL के दौरान, हम आमतौर पर कई प्रयोग करते हैं। प्रत्येक प्रयोग के दौरान, हमें अब तक सीखी गई सबसे अच्छी रणनीति का पालन करने (**एक्सप्लॉइटेशन**) और नए संभावित स्टेट्स का पता लगाने (**एक्सप्लोरेशन**) के बीच संतुलन बनाना होता है।

## OpenAI Gym

RL के लिए एक बेहतरीन टूल [OpenAI Gym](https://gym.openai.com/) है - एक **सिम्युलेशन एनवायरनमेंट**, जो कई अलग-अलग एनवायरनमेंट्स को सिम्युलेट कर सकता है, जैसे कि Atari गेम्स से लेकर पोल बैलेंसिंग के पीछे की फिजिक्स तक। यह रिइनफोर्समेंट लर्निंग एल्गोरिदम को ट्रेन करने के लिए सबसे लोकप्रिय सिम्युलेशन एनवायरनमेंट्स में से एक है और इसे [OpenAI](https://openai.com/) द्वारा मेंटेन किया जाता है।

> **Note**: आप OpenAI Gym द्वारा उपलब्ध सभी एनवायरनमेंट्स [यहां](https://gym.openai.com/envs/#classic_control) देख सकते हैं।

## CartPole बैलेंसिंग

आपने शायद आधुनिक बैलेंसिंग डिवाइस जैसे *Segway* या *Gyroscooters* देखे होंगे। ये डिवाइस एक्सेलेरोमीटर या जाइरोस्कोप से सिग्नल प्राप्त करके अपने पहियों को समायोजित करके स्वचालित रूप से बैलेंस कर सकते हैं। इस सेक्शन में, हम एक समान समस्या को हल करना सीखेंगे - पोल को बैलेंस करना। यह एक सर्कस कलाकार के हाथ पर पोल को बैलेंस करने जैसी स्थिति के समान है - लेकिन यह बैलेंसिंग केवल 1D में होती है।

बैलेंसिंग का एक सरलीकृत संस्करण **CartPole** समस्या के रूप में जाना जाता है। CartPole की दुनिया में, हमारे पास एक क्षैतिज स्लाइडर होता है जो बाएं या दाएं चल सकता है, और लक्ष्य स्लाइडर के ऊपर एक वर्टिकल पोल को बैलेंस करना है।

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

इस एनवायरनमेंट को बनाने और उपयोग करने के लिए, हमें कुछ लाइनों का Python कोड चाहिए:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

प्रत्येक एनवायरनमेंट को बिल्कुल एक ही तरीके से एक्सेस किया जा सकता है:
* `env.reset` एक नया प्रयोग शुरू करता है
* `env.step` एक सिम्युलेशन स्टेप करता है। यह **एक्शन** को **एक्शन स्पेस** से प्राप्त करता है और **ऑब्जर्वेशन** (ऑब्जर्वेशन स्पेस से), साथ ही रिवॉर्ड और टर्मिनेशन फ्लैग को लौटाता है।

ऊपर दिए गए उदाहरण में, हम प्रत्येक स्टेप पर एक रैंडम एक्शन करते हैं, यही कारण है कि प्रयोग का जीवन बहुत छोटा है:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL एल्गोरिदम का लक्ष्य एक मॉडल - जिसे **पॉलिसी** π कहा जाता है - को ट्रेन करना है, जो दिए गए स्टेट के जवाब में एक्शन लौटाएगा। हम पॉलिसी को प्रॉबैबिलिस्टिक भी मान सकते हैं, जैसे कि किसी भी स्टेट *s* और एक्शन *a* के लिए यह प्रॉबैबिलिटी π(*a*|*s*) लौटाएगा कि हमें स्टेट *s* में *a* लेना चाहिए।

## पॉलिसी ग्रेडिएंट्स एल्गोरिदम

पॉलिसी को मॉडल करने का सबसे स्पष्ट तरीका एक न्यूरल नेटवर्क बनाना है, जो स्टेट्स को इनपुट के रूप में लेगा और संबंधित एक्शन (या सभी एक्शन की प्रॉबैबिलिटी) लौटाएगा। एक तरह से, यह एक सामान्य क्लासिफिकेशन टास्क के समान होगा, जिसमें एक बड़ा अंतर है - हमें पहले से यह नहीं पता होता कि प्रत्येक स्टेप पर कौन से एक्शन लेने चाहिए।

यहां विचार इन प्रॉबैबिलिटी का अनुमान लगाना है। हम **क्यूमुलेटिव रिवॉर्ड्स** का एक वेक्टर बनाते हैं, जो हमारे प्रयोग के प्रत्येक स्टेप पर हमारा कुल रिवॉर्ड दिखाता है। हम **रिवॉर्ड डिस्काउंटिंग** भी लागू करते हैं, पहले के रिवॉर्ड्स को कुछ गुणांक γ=0.99 से गुणा करके, ताकि पहले के रिवॉर्ड्स की भूमिका को कम किया जा सके। फिर, हम उन स्टेप्स को मजबूत करते हैं जो बड़े रिवॉर्ड्स देते हैं।

> पॉलिसी ग्रेडिएंट एल्गोरिदम के बारे में अधिक जानें और इसे [उदाहरण नोटबुक](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) में एक्शन में देखें।

## एक्टर-क्रिटिक एल्गोरिदम

पॉलिसी ग्रेडिएंट्स दृष्टिकोण का एक बेहतर संस्करण **Actor-Critic** कहलाता है। इसके पीछे मुख्य विचार यह है कि न्यूरल नेटवर्क को दो चीजें लौटाने के लिए ट्रेन किया जाएगा:

* पॉलिसी, जो निर्धारित करती है कि कौन सा एक्शन लेना है। इस हिस्से को **Actor** कहा जाता है।
* उस स्टेट पर हमें मिलने वाले कुल रिवॉर्ड का अनुमान - इस हिस्से को **Critic** कहा जाता है।

एक तरह से, यह आर्किटेक्चर [GAN](../../4-ComputerVision/10-GANs/README.md) जैसा दिखता है, जहां हमारे पास दो नेटवर्क होते हैं जो एक-दूसरे के खिलाफ ट्रेन किए जाते हैं। Actor-Critic मॉडल में, Actor वह एक्शन प्रस्तावित करता है जिसे हमें लेना चाहिए, और Critic आलोचनात्मक होकर परिणाम का अनुमान लगाने की कोशिश करता है। हालांकि, हमारा लक्ष्य इन नेटवर्क्स को एक साथ ट्रेन करना है।

क्योंकि हमें प्रयोग के दौरान वास्तविक क्यूमुलेटिव रिवॉर्ड्स और Critic द्वारा लौटाए गए परिणाम दोनों पता होते हैं, यह एक लॉस फंक्शन बनाना अपेक्षाकृत आसान है जो उनके बीच के अंतर को कम करेगा। यह हमें **Critic लॉस** देगा। हम **Actor लॉस** को पॉलिसी ग्रेडिएंट एल्गोरिदम के समान दृष्टिकोण का उपयोग करके गणना कर सकते हैं।

इन एल्गोरिदम में से एक को चलाने के बाद, हम उम्मीद कर सकते हैं कि हमारा CartPole इस तरह व्यवहार करेगा:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ अभ्यास: पॉलिसी ग्रेडिएंट्स और Actor-Critic RL

निम्नलिखित नोटबुक्स में अपना सीखना जारी रखें:

* [TensorFlow में RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch में RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## अन्य RL टास्क

आजकल रिइनफोर्समेंट लर्निंग अनुसंधान का एक तेजी से बढ़ता हुआ क्षेत्र है। रिइनफोर्समेंट लर्निंग के कुछ दिलचस्प उदाहरण हैं:

* कंप्यूटर को **Atari गेम्स** खेलना सिखाना। इस समस्या में चुनौती यह है कि हमारे पास एक साधारण स्टेट नहीं होता जो एक वेक्टर के रूप में प्रस्तुत किया गया हो, बल्कि एक स्क्रीनशॉट होता है - और हमें इस स्क्रीन इमेज को फीचर वेक्टर में बदलने या रिवॉर्ड जानकारी निकालने के लिए CNN का उपयोग करना होता है। Atari गेम्स Gym में उपलब्ध हैं।
* कंप्यूटर को बोर्ड गेम्स जैसे Chess और Go खेलना सिखाना। हाल ही में, **Alpha Zero** जैसे अत्याधुनिक प्रोग्राम्स को दो एजेंट्स द्वारा एक-दूसरे के खिलाफ खेलकर और प्रत्येक स्टेप पर सुधार करके स्क्रैच से ट्रेन किया गया।
* उद्योग में, RL का उपयोग सिम्युलेशन से कंट्रोल सिस्टम बनाने के लिए किया जाता है। [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) नामक एक सेवा विशेष रूप से इसके लिए डिज़ाइन की गई है।

## निष्कर्ष

अब हमने यह सीख लिया है कि केवल एक रिवॉर्ड फंक्शन प्रदान करके, जो गेम की वांछित स्थिति को परिभाषित करता है, और उन्हें बुद्धिमानी से सर्च स्पेस का पता लगाने का अवसर देकर एजेंट्स को अच्छे परिणाम प्राप्त करने के लिए कैसे ट्रेन किया जाए। हमने सफलतापूर्वक दो एल्गोरिदम आजमाए और अपेक्षाकृत कम समय में अच्छा परिणाम प्राप्त किया। हालांकि, यह RL में आपकी यात्रा की केवल शुरुआत है, और यदि आप गहराई से सीखना चाहते हैं तो आपको निश्चित रूप से एक अलग कोर्स लेने पर विचार करना चाहिए।

## 🚀 चुनौती

'अन्य RL टास्क' सेक्शन में सूचीबद्ध एप्लिकेशन का पता लगाएं और एक को लागू करने का प्रयास करें!

## [पोस्ट-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## समीक्षा और स्व-अध्ययन

हमारे [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) में क्लासिकल रिइनफोर्समेंट लर्निंग के बारे में अधिक जानें।

देखें [यह शानदार वीडियो](https://www.youtube.com/watch?v=qv6UVOQ0F44), जो बताता है कि कंप्यूटर Super Mario खेलना कैसे सीख सकता है।

## असाइनमेंट: [Train a Mountain Car](lab/README.md)

इस असाइनमेंट के दौरान आपका लक्ष्य एक अलग Gym एनवायरनमेंट - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) को ट्रेन करना होगा।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।