<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T13:27:39+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "hi"
}
-->
# डीप रिइनफोर्समेंट लर्निंग

रिइनफोर्समेंट लर्निंग (RL) को मशीन लर्निंग के तीन मुख्य पैरेडाइम्स में से एक माना जाता है, जिसमें सुपरवाइज्ड लर्निंग और अनसुपरवाइज्ड लर्निंग भी शामिल हैं। जहां सुपरवाइज्ड लर्निंग में हमें ज्ञात परिणामों वाले डेटा सेट पर निर्भर रहना पड़ता है, वहीं RL **करके सीखने** पर आधारित है। उदाहरण के लिए, जब हम पहली बार कोई कंप्यूटर गेम देखते हैं, तो हम इसे खेलना शुरू कर देते हैं, भले ही हमें नियमों का पता न हो। जल्द ही, हम केवल खेलते हुए और अपने व्यवहार को समायोजित करते हुए अपने कौशल में सुधार कर लेते हैं।

## [प्री-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/43)

RL करने के लिए हमें चाहिए:

* एक **एनवायरनमेंट** या **सिम्युलेटर**, जो खेल के नियम तय करता है। हमें सिम्युलेटर में प्रयोग चलाने और परिणामों को देखने में सक्षम होना चाहिए।
* कुछ **रिवॉर्ड फंक्शन**, जो यह संकेत देता है कि हमारा प्रयोग कितना सफल रहा। कंप्यूटर गेम खेलना सीखने के मामले में, रिवॉर्ड हमारा अंतिम स्कोर होगा।

रिवॉर्ड फंक्शन के आधार पर, हमें अपने व्यवहार को समायोजित करने और अपने कौशल में सुधार करने में सक्षम होना चाहिए, ताकि अगली बार हम बेहतर खेल सकें। अन्य प्रकार के मशीन लर्निंग और RL के बीच मुख्य अंतर यह है कि RL में हमें आमतौर पर यह नहीं पता होता कि हम जीत रहे हैं या हार रहे हैं जब तक कि खेल समाप्त न हो जाए। इसलिए, हम यह नहीं कह सकते कि कोई विशेष चाल अकेले में अच्छी है या नहीं - हमें केवल खेल के अंत में रिवॉर्ड मिलता है।

RL के दौरान, हम आमतौर पर कई प्रयोग करते हैं। प्रत्येक प्रयोग के दौरान, हमें अब तक सीखी गई सबसे अच्छी रणनीति का पालन करने (**एक्सप्लॉइटेशन**) और नए संभावित स्टेट्स का पता लगाने (**एक्सप्लोरेशन**) के बीच संतुलन बनाना होता है।

## OpenAI Gym

RL के लिए एक बेहतरीन टूल [OpenAI Gym](https://gym.openai.com/) है - एक **सिम्युलेशन एनवायरनमेंट**, जो कई अलग-अलग एनवायरनमेंट्स को सिम्युलेट कर सकता है, जैसे कि Atari गेम्स से लेकर पोल बैलेंसिंग के पीछे की फिजिक्स तक। यह रिइनफोर्समेंट लर्निंग एल्गोरिदम को ट्रेन करने के लिए सबसे लोकप्रिय सिम्युलेशन एनवायरनमेंट्स में से एक है और इसे [OpenAI](https://openai.com/) द्वारा मेंटेन किया जाता है।

> **Note**: आप OpenAI Gym द्वारा उपलब्ध सभी एनवायरनमेंट्स [यहां](https://gym.openai.com/envs/#classic_control) देख सकते हैं।

## CartPole बैलेंसिंग

आपने शायद आधुनिक बैलेंसिंग डिवाइस जैसे *Segway* या *Gyroscooters* देखे होंगे। ये डिवाइस एक्सेलेरोमीटर या जाइरोस्कोप से सिग्नल प्राप्त करके अपने पहियों को समायोजित करके स्वचालित रूप से बैलेंस कर सकते हैं। इस सेक्शन में, हम एक समान समस्या को हल करना सीखेंगे - पोल को बैलेंस करना। यह उस स्थिति के समान है जब एक सर्कस कलाकार अपने हाथ पर पोल को बैलेंस करता है - लेकिन यह पोल बैलेंसिंग केवल 1D में होती है।

बैलेंसिंग का एक सरलीकृत संस्करण **CartPole** समस्या के रूप में जाना जाता है। CartPole की दुनिया में, हमारे पास एक क्षैतिज स्लाइडर होता है जो बाएं या दाएं चल सकता है, और लक्ष्य स्लाइडर के ऊपर एक वर्टिकल पोल को बैलेंस करना है।

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

इस एनवायरनमेंट को बनाने और उपयोग करने के लिए, हमें कुछ पायथन कोड की आवश्यकता होगी:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

प्रत्येक एनवायरनमेंट को ठीक इसी तरह एक्सेस किया जा सकता है:
* `env.reset` एक नया प्रयोग शुरू करता है
* `env.step` एक सिम्युलेशन स्टेप करता है। यह **एक्शन** को **एक्शन स्पेस** से प्राप्त करता है और **ऑब्ज़र्वेशन** (ऑब्ज़र्वेशन स्पेस से), साथ ही रिवॉर्ड और टर्मिनेशन फ्लैग को लौटाता है।

ऊपर दिए गए उदाहरण में, हम प्रत्येक स्टेप पर एक रैंडम एक्शन करते हैं, यही कारण है कि प्रयोग का जीवन बहुत छोटा होता है:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL एल्गोरिदम का लक्ष्य एक मॉडल - जिसे **पॉलिसी** &pi; कहा जाता है - को ट्रेन करना है, जो दिए गए स्टेट के जवाब में एक्शन लौटाएगा। हम पॉलिसी को प्रॉबैबिलिस्टिक भी मान सकते हैं, जैसे कि किसी भी स्टेट *s* और एक्शन *a* के लिए यह &pi;(*a*|*s*) प्रॉबेबिलिटी लौटाएगा कि हमें स्टेट *s* में *a* लेना चाहिए।

## पॉलिसी ग्रेडिएंट्स एल्गोरिदम

पॉलिसी को मॉडल करने का सबसे स्पष्ट तरीका एक न्यूरल नेटवर्क बनाना है, जो स्टेट्स को इनपुट के रूप में लेगा और संबंधित एक्शन (या सभी एक्शन की प्रॉबेबिलिटी) लौटाएगा। एक अर्थ में, यह एक सामान्य क्लासिफिकेशन टास्क के समान होगा, लेकिन एक बड़ा अंतर है - हमें पहले से यह नहीं पता होता कि प्रत्येक स्टेप पर कौन सा एक्शन लेना चाहिए।

यहां विचार यह है कि इन प्रॉबेबिलिटी का अनुमान लगाया जाए। हम **क्यूमुलेटिव रिवॉर्ड्स** का एक वेक्टर बनाते हैं, जो हमारे प्रयोग के प्रत्येक स्टेप पर हमारा कुल रिवॉर्ड दिखाता है। हम **रिवॉर्ड डिस्काउंटिंग** भी लागू करते हैं, पहले के रिवॉर्ड्स को कुछ गुणांक &gamma;=0.99 से गुणा करके, ताकि पहले के रिवॉर्ड्स की भूमिका को कम किया जा सके। फिर, हम उन स्टेप्स को मजबूत करते हैं जो बड़े रिवॉर्ड्स देते हैं।

> पॉलिसी ग्रेडिएंट एल्गोरिदम के बारे में अधिक जानें और इसे [उदाहरण नोटबुक](CartPole-RL-TF.ipynb) में एक्शन में देखें।

## एक्टर-क्रिटिक एल्गोरिदम

पॉलिसी ग्रेडिएंट्स दृष्टिकोण का एक बेहतर संस्करण **Actor-Critic** कहलाता है। इसके पीछे मुख्य विचार यह है कि न्यूरल नेटवर्क को दो चीजें लौटाने के लिए प्रशिक्षित किया जाएगा:

* पॉलिसी, जो तय करती है कि कौन सा एक्शन लेना है। इस हिस्से को **Actor** कहा जाता है।
* उस स्टेट पर हमें मिलने वाले कुल रिवॉर्ड का अनुमान - इस हिस्से को **Critic** कहा जाता है।

एक अर्थ में, यह [GAN](../../4-ComputerVision/10-GANs/README.md) जैसा दिखता है, जहां हमारे पास दो नेटवर्क होते हैं जो एक-दूसरे के खिलाफ प्रशिक्षित होते हैं। Actor-Critic मॉडल में, Actor वह एक्शन प्रस्तावित करता है जिसे हमें लेना चाहिए, और Critic आलोचनात्मक होकर परिणाम का अनुमान लगाने की कोशिश करता है। हालांकि, हमारा लक्ष्य इन नेटवर्क्स को एक साथ प्रशिक्षित करना है।

क्योंकि हमें प्रयोग के दौरान वास्तविक क्यूमुलेटिव रिवॉर्ड्स और Critic द्वारा लौटाए गए परिणाम दोनों का पता होता है, यह एक लॉस फंक्शन बनाना अपेक्षाकृत आसान है जो उनके बीच के अंतर को कम करेगा। यह हमें **Critic Loss** देगा। हम **Actor Loss** को पॉलिसी ग्रेडिएंट एल्गोरिदम के समान दृष्टिकोण का उपयोग करके गणना कर सकते हैं।

इन एल्गोरिदम में से एक को चलाने के बाद, हम उम्मीद कर सकते हैं कि हमारा CartPole इस तरह व्यवहार करेगा:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ अभ्यास: पॉलिसी ग्रेडिएंट्स और Actor-Critic RL

अगले नोटबुक्स में अपनी सीख जारी रखें:

* [TensorFlow में RL](CartPole-RL-TF.ipynb)
* [PyTorch में RL](CartPole-RL-PyTorch.ipynb)

## अन्य RL कार्य

आजकल रिइनफोर्समेंट लर्निंग तेजी से बढ़ता हुआ शोध क्षेत्र है। रिइनफोर्समेंट लर्निंग के कुछ दिलचस्प उदाहरण हैं:

* **Atari गेम्स** खेलना कंप्यूटर को सिखाना। इस समस्या में चुनौती यह है कि हमारे पास एक सरल स्टेट नहीं है जो एक वेक्टर के रूप में प्रस्तुत हो, बल्कि एक स्क्रीनशॉट है - और हमें इस स्क्रीन इमेज को फीचर वेक्टर में बदलने या रिवॉर्ड जानकारी निकालने के लिए CNN का उपयोग करना होगा। Atari गेम्स Gym में उपलब्ध हैं।
* कंप्यूटर को बोर्ड गेम्स जैसे Chess और Go खेलना सिखाना। हाल ही में, **Alpha Zero** जैसे अत्याधुनिक प्रोग्राम्स को दो एजेंट्स के बीच खेलते हुए और प्रत्येक स्टेप पर सुधार करते हुए स्क्रैच से प्रशिक्षित किया गया।
* उद्योग में, सिम्युलेशन से कंट्रोल सिस्टम बनाने के लिए RL का उपयोग किया जाता है। [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) नामक एक सेवा विशेष रूप से इसके लिए डिज़ाइन की गई है।

## निष्कर्ष

अब हमने यह सीख लिया है कि एजेंट्स को केवल एक रिवॉर्ड फंक्शन प्रदान करके, जो खेल की वांछित स्थिति को परिभाषित करता है, और उन्हें खोज स्थान को बुद्धिमानी से एक्सप्लोर करने का अवसर देकर अच्छे परिणाम प्राप्त करने के लिए प्रशिक्षित कैसे किया जाए। हमने सफलतापूर्वक दो एल्गोरिदम आजमाए और अपेक्षाकृत कम समय में अच्छा परिणाम प्राप्त किया। हालांकि, यह RL में आपकी यात्रा की केवल शुरुआत है, और यदि आप गहराई से सीखना चाहते हैं तो आपको एक अलग कोर्स लेने पर विचार करना चाहिए।

## 🚀 चुनौती

'अन्य RL कार्य' सेक्शन में सूचीबद्ध एप्लिकेशन को एक्सप्लोर करें और उनमें से एक को लागू करने का प्रयास करें!

## [पोस्ट-लेक्चर क्विज़](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## समीक्षा और स्व-अध्ययन

हमारे [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) में क्लासिकल रिइनफोर्समेंट लर्निंग के बारे में अधिक जानें।

देखें [यह शानदार वीडियो](https://www.youtube.com/watch?v=qv6UVOQ0F44) जो बताता है कि कंप्यूटर Super Mario खेलना कैसे सीख सकता है।

## असाइनमेंट: [Train a Mountain Car](lab/README.md)

इस असाइनमेंट के दौरान आपका लक्ष्य एक अलग Gym एनवायरनमेंट - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) को प्रशिक्षित करना होगा।

---

