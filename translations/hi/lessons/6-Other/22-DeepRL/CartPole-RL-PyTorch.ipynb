{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# कार्टपोल बैलेंसिंग के लिए RL प्रशिक्षण\n",
    "\n",
    "यह नोटबुक [AI for Beginners Curriculum](http://aka.ms/ai-beginners) का हिस्सा है। इसे [आधिकारिक PyTorch ट्यूटोरियल](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) और [इस Cartpole PyTorch कार्यान्वयन](https://github.com/yc930401/Actor-Critic-pytorch) से प्रेरित किया गया है।\n",
    "\n",
    "इस उदाहरण में, हम RL का उपयोग करके एक मॉडल को प्रशिक्षित करेंगे ताकि वह एक पोल को संतुलित कर सके, जो एक गाड़ी पर रखा गया है और क्षैतिज पैमाने पर बाएं और दाएं चल सकता है। हम [OpenAI Gym](https://www.gymlibrary.ml/) वातावरण का उपयोग करेंगे ताकि इस पोल का अनुकरण किया जा सके।\n",
    "\n",
    "> **Note**: आप इस पाठ का कोड स्थानीय रूप से (जैसे Visual Studio Code से) चला सकते हैं, इस स्थिति में अनुकरण एक नई विंडो में खुलेगा। जब आप कोड ऑनलाइन चला रहे हों, तो आपको कोड में कुछ बदलाव करने की आवश्यकता हो सकती है, जैसा कि [यहां](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) वर्णित है।\n",
    "\n",
    "हम शुरुआत करेंगे यह सुनिश्चित करके कि Gym इंस्टॉल है:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब आइए CartPole एनवायरनमेंट बनाते हैं और देखते हैं कि इसे कैसे संचालित किया जाए। एक एनवायरनमेंट में निम्नलिखित गुण होते हैं:\n",
    "\n",
    "* **एक्शन स्पेस** वह सेट है जिसमें वे सभी संभावित क्रियाएं शामिल हैं जो हम सिमुलेशन के प्रत्येक चरण में कर सकते हैं।  \n",
    "* **ऑब्ज़र्वेशन स्पेस** वह स्पेस है जिसमें वे सभी ऑब्ज़र्वेशन शामिल हैं जो हम कर सकते हैं।  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आइए देखें कि सिमुलेशन कैसे काम करता है। निम्नलिखित लूप सिमुलेशन को तब तक चलाता है जब तक कि `env.step` समाप्ति फ्लैग `done` वापस नहीं करता। हम `env.action_space.sample()` का उपयोग करके क्रियाओं को यादृच्छिक रूप से चुनेंगे, जिसका मतलब है कि प्रयोग शायद बहुत जल्दी विफल हो जाएगा (CartPole वातावरण तब समाप्त होता है जब CartPole की गति, उसकी स्थिति या कोण कुछ सीमाओं के बाहर होते हैं)।\n",
    "\n",
    "> सिमुलेशन एक नई विंडो में खुलेगा। आप कोड को कई बार चला सकते हैं और देख सकते हैं कि यह कैसा व्यवहार करता है।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आप देख सकते हैं कि अवलोकन में 4 संख्याएँ होती हैं। ये हैं:\n",
    "- गाड़ी की स्थिति\n",
    "- गाड़ी की गति\n",
    "- पोल का कोण\n",
    "- पोल का घूर्णन दर\n",
    "\n",
    "`rew` वह इनाम है जो हमें प्रत्येक चरण पर मिलता है। आप देख सकते हैं कि CartPole वातावरण में आपको प्रत्येक सिमुलेशन चरण के लिए 1 अंक का इनाम मिलता है, और लक्ष्य कुल इनाम को अधिकतम करना है, यानी CartPole को गिरने के बिना संतुलन बनाए रखने का समय।\n",
    "\n",
    "पुनर्बलन शिक्षण (Reinforcement Learning) के दौरान, हमारा लक्ष्य एक **नीति** $\\pi$ को प्रशिक्षित करना है, जो प्रत्येक स्थिति $s$ के लिए हमें बताएगी कि कौन सा कार्य $a$ करना है, यानी मूल रूप से $a = \\pi(s)$।\n",
    "\n",
    "यदि आप संभाव्य समाधान चाहते हैं, तो आप नीति को प्रत्येक कार्य के लिए संभावनाओं का सेट लौटाने के रूप में सोच सकते हैं, यानी $\\pi(a|s)$ का मतलब होगा कि स्थिति $s$ पर हमें कार्य $a$ करने की कितनी संभावना है।\n",
    "\n",
    "## नीति ग्रेडिएंट विधि\n",
    "\n",
    "सबसे सरल RL एल्गोरिदम, जिसे **नीति ग्रेडिएंट** कहा जाता है, में हम एक न्यूरल नेटवर्क को प्रशिक्षित करेंगे ताकि वह अगला कार्य भविष्यवाणी कर सके।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_inputs, 128, bias=False, dtype=torch.float32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, num_actions, bias = False, dtype=torch.float32),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "हम नेटवर्क को कई प्रयोग चलाकर प्रशिक्षित करेंगे, और प्रत्येक रन के बाद अपने नेटवर्क को अपडेट करेंगे। आइए एक फ़ंक्शन परिभाषित करें जो प्रयोग चलाएगा और परिणाम लौटाएगा (जिसे **ट्रेस** कहा जाता है) - सभी अवस्थाएँ, क्रियाएँ (और उनकी अनुशंसित संभावनाएँ), और पुरस्कार:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(torch.from_numpy(np.expand_dims(state,0)))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs.detach().numpy()))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "आप बिना प्रशिक्षित नेटवर्क के एक एपिसोड चला सकते हैं और देख सकते हैं कि कुल इनाम (जिसे एपिसोड की लंबाई भी कहा जाता है) बहुत कम है:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "पॉलिसी ग्रेडिएंट एल्गोरिदम के जटिल पहलुओं में से एक **डिस्काउंटेड रिवार्ड्स** का उपयोग करना है। विचार यह है कि हम खेल के प्रत्येक चरण में कुल रिवार्ड्स का वेक्टर गणना करते हैं, और इस प्रक्रिया के दौरान हम प्रारंभिक रिवार्ड्स को कुछ गुणांक $gamma$ का उपयोग करके डिस्काउंट करते हैं। हम परिणामी वेक्टर को भी सामान्यीकृत करते हैं, क्योंकि हम इसे अपने प्रशिक्षण को प्रभावित करने के लिए वज़न के रूप में उपयोग करेंगे:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब हम असली प्रशिक्षण शुरू करेंगे! हम 300 एपिसोड चलाएंगे, और हर एपिसोड में निम्नलिखित कार्य करेंगे:\n",
    "\n",
    "1. प्रयोग चलाएं और ट्रेस एकत्र करें।\n",
    "1. लिए गए कार्यों और अनुमानित संभावनाओं के बीच का अंतर (`gradients`) गणना करें। अंतर जितना कम होगा, उतना ही अधिक हमें यकीन होगा कि हमने सही कार्य लिया है।\n",
    "1. छूटे हुए पुरस्कारों की गणना करें और उन्हें `gradients` के साथ गुणा करें - यह सुनिश्चित करेगा कि उच्च पुरस्कार वाले कदम अंतिम परिणाम पर कम पुरस्कार वाले कदमों की तुलना में अधिक प्रभाव डालें।\n",
    "1. हमारे न्यूरल नेटवर्क के लिए अपेक्षित लक्ष्य कार्य आंशिक रूप से रन के दौरान अनुमानित संभावनाओं से और आंशिक रूप से गणना किए गए `gradients` से लिए जाएंगे। हम `alpha` पैरामीटर का उपयोग करेंगे यह निर्धारित करने के लिए कि `gradients` और पुरस्कारों को किस हद तक ध्यान में रखा जाए - इसे *रीइन्फोर्समेंट एल्गोरिदम का लर्निंग रेट* कहा जाता है।\n",
    "1. अंत में, हम अपने नेटवर्क को स्टेट्स और अपेक्षित कार्यों पर प्रशिक्षित करेंगे, और इस प्रक्रिया को दोहराएंगे।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(x)\n",
    "    loss = -torch.mean(torch.log(predictions) * y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब चलिए एपिसोड को रेंडरिंग के साथ चलाते हैं ताकि परिणाम देख सकें:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "उम्मीद है कि अब आप देख सकते हैं कि पोल अब काफी अच्छी तरह से संतुलन बना सकता है!\n",
    "\n",
    "## एक्टर-क्रिटिक मॉडल\n",
    "\n",
    "एक्टर-क्रिटिक मॉडल पॉलिसी ग्रेडिएंट्स का एक और विकास है, जिसमें हम एक न्यूरल नेटवर्क बनाते हैं जो पॉलिसी और अनुमानित रिवॉर्ड्स दोनों को सीखता है। इस नेटवर्क के दो आउटपुट होंगे (या आप इसे दो अलग-अलग नेटवर्क के रूप में देख सकते हैं):\n",
    "* **एक्टर** हमें स्टेट प्रॉबेबिलिटी डिस्ट्रीब्यूशन देकर कार्रवाई की सिफारिश करेगा, जैसा कि पॉलिसी ग्रेडिएंट मॉडल में होता है।\n",
    "* **क्रिटिक** यह अनुमान लगाएगा कि उन कार्रवाइयों से रिवॉर्ड क्या होगा। यह दिए गए स्टेट पर भविष्य में कुल अनुमानित रिवॉर्ड्स लौटाता है।\n",
    "\n",
    "आइए ऐसा एक मॉडल परिभाषित करें:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "हमें अपनी `discounted_rewards` और `run_episode` फ़ंक्शनों को थोड़ा संशोधित करने की आवश्यकता होगी:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "अब हम मुख्य प्रशिक्षण लूप चलाएंगे। हम उचित हानि फ़ंक्शन की गणना करके और नेटवर्क पैरामीटर को अपडेट करके मैनुअल नेटवर्क प्रशिक्षण प्रक्रिया का उपयोग करेंगे:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## मुख्य बातें\n",
    "\n",
    "हमने इस डेमो में दो RL एल्गोरिदम देखे: सरल पॉलिसी ग्रेडिएंट और अधिक परिष्कृत एक्टर-क्रिटिक। आप देख सकते हैं कि ये एल्गोरिदम स्टेट, एक्शन और रिवॉर्ड जैसे अमूर्त विचारों के साथ काम करते हैं - इसलिए इन्हें बहुत अलग-अलग वातावरणों में लागू किया जा सकता है।\n",
    "\n",
    "रीइन्फोर्समेंट लर्निंग हमें केवल अंतिम रिवॉर्ड को देखकर समस्या को हल करने की सबसे अच्छी रणनीति सीखने की अनुमति देता है। यह तथ्य कि हमें लेबल किए गए डेटा सेट की आवश्यकता नहीं होती, हमें अपने मॉडल को अनुकूलित करने के लिए सिमुलेशन को कई बार दोहराने की सुविधा देता है। हालांकि, RL में अभी भी कई चुनौतियां हैं, जिन्हें आप सीख सकते हैं यदि आप AI के इस रोचक क्षेत्र पर अधिक ध्यान केंद्रित करने का निर्णय लेते हैं।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "coopTranslator": {
   "original_hash": "04f8d9978cd11281d81dd037cbf6ce20",
   "translation_date": "2025-08-31T14:26:12+00:00",
   "source_file": "lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb",
   "language_code": "hi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}