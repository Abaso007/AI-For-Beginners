<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9c592c26aca16ca085d268c732284187",
  "translation_date": "2025-08-24T09:59:02+00:00",
  "source_file": "lessons/X-Extras/X1-MultiModal/README.md",
  "language_code": "hi"
}
-->
# मल्टी-मोडल नेटवर्क्स

NLP कार्यों को हल करने के लिए ट्रांसफॉर्मर मॉडल की सफलता के बाद, समान या समान आर्किटेक्चर को कंप्यूटर विज़न कार्यों पर लागू किया गया। ऐसे मॉडल बनाने में बढ़ती रुचि है जो विज़न और प्राकृतिक भाषा क्षमताओं को *मिलाकर* काम कर सकें। ऐसा ही एक प्रयास OpenAI द्वारा किया गया, जिसे CLIP और DALL.E कहा जाता है।

## कंट्रास्टिव इमेज प्री-ट्रेनिंग (CLIP)

CLIP का मुख्य विचार यह है कि टेक्स्ट प्रॉम्प्ट्स और इमेज की तुलना की जा सके और यह निर्धारित किया जा सके कि इमेज प्रॉम्प्ट के साथ कितनी अच्छी तरह मेल खाती है।

![CLIP आर्किटेक्चर](../../../../../lessons/X-Extras/X1-MultiModal/images/clip-arch.png)

> *चित्र [इस ब्लॉग पोस्ट](https://openai.com/blog/clip/) से लिया गया है*

मॉडल को इंटरनेट से प्राप्त इमेज और उनके कैप्शन पर प्रशिक्षित किया गया है। प्रत्येक बैच के लिए, हम N जोड़े (इमेज, टेक्स्ट) लेते हैं और उन्हें कुछ वेक्टर रिप्रेजेंटेशन I और T में बदलते हैं। 

उन रिप्रेजेंटेशन को फिर एक साथ मिलाया जाता है। लॉस फंक्शन को इस तरह से परिभाषित किया गया है कि वह एक जोड़ी (जैसे I और T) के वेक्टर के बीच कोसाइन समानता को अधिकतम करे और अन्य सभी जोड़ों के बीच कोसाइन समानता को न्यूनतम करे। यही कारण है कि इस दृष्टिकोण को **कंट्रास्टिव** कहा जाता है।

CLIP मॉडल/लाइब्रेरी [OpenAI GitHub](https://github.com/openai/CLIP) से उपलब्ध है। इस दृष्टिकोण को [इस ब्लॉग पोस्ट](https://openai.com/blog/clip/) और अधिक विस्तार से [इस पेपर](https://arxiv.org/pdf/2103.00020.pdf) में वर्णित किया गया है।

एक बार जब यह मॉडल प्री-ट्रेन हो जाता है, तो हम इसे इमेज और टेक्स्ट प्रॉम्प्ट्स के बैच दे सकते हैं, और यह हमें संभावनाओं के साथ एक टेंसर लौटाएगा। CLIP का उपयोग कई कार्यों के लिए किया जा सकता है:

**इमेज क्लासिफिकेशन**

मान लीजिए हमें इमेज को बिल्ली, कुत्ते और इंसान के बीच वर्गीकृत करना है। इस मामले में, हम मॉडल को एक इमेज और टेक्स्ट प्रॉम्प्ट्स की एक श्रृंखला देते हैं: "*एक बिल्ली की तस्वीर*", "*एक कुत्ते की तस्वीर*", "*एक इंसान की तस्वीर*"। परिणामस्वरूप 3 संभावनाओं के वेक्टर में, हमें केवल सबसे अधिक मूल्य वाले इंडेक्स का चयन करना होगा।

![इमेज क्लासिफिकेशन के लिए CLIP](../../../../../lessons/X-Extras/X1-MultiModal/images/clip-class.png)

> *चित्र [इस ब्लॉग पोस्ट](https://openai.com/blog/clip/) से लिया गया है*

**टेक्स्ट-आधारित इमेज सर्च**

हम इसका उल्टा भी कर सकते हैं। यदि हमारे पास इमेज का एक संग्रह है, तो हम इस संग्रह को मॉडल में पास कर सकते हैं और एक टेक्स्ट प्रॉम्प्ट दे सकते हैं - यह हमें उस इमेज को देगा जो दिए गए प्रॉम्प्ट के सबसे समान है।

## ✍️ उदाहरण: [इमेज क्लासिफिकेशन और इमेज सर्च के लिए CLIP का उपयोग](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb)

CLIP को एक्शन में देखने के लिए [Clip.ipynb](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb) नोटबुक खोलें।

## VQGAN+CLIP के साथ इमेज जनरेशन

CLIP का उपयोग टेक्स्ट प्रॉम्प्ट से **इमेज जनरेशन** के लिए भी किया जा सकता है। ऐसा करने के लिए, हमें एक **जनरेटर मॉडल** की आवश्यकता होती है जो कुछ वेक्टर इनपुट के आधार पर इमेज जनरेट कर सके। ऐसा ही एक मॉडल [VQGAN](https://compvis.github.io/taming-transformers/) (वेक्टर-क्वांटाइज्ड GAN) कहलाता है।

VQGAN की मुख्य विशेषताएं जो इसे सामान्य [GAN](../../4-ComputerVision/10-GANs/README.md) से अलग करती हैं, निम्नलिखित हैं:
* ऑटोरेग्रेसिव ट्रांसफॉर्मर आर्किटेक्चर का उपयोग करके एक अनुक्रम उत्पन्न करना जो संदर्भ-समृद्ध विज़ुअल भागों को बनाता है जो इमेज को बनाते हैं। इन विज़ुअल भागों को [CNN](../../4-ComputerVision/07-ConvNets/README.md) द्वारा सीखा जाता है।
* उप-इमेज डिस्क्रिमिनेटर का उपयोग करना जो यह पता लगाता है कि इमेज के भाग "वास्तविक" हैं या "नकली" (पारंपरिक GAN में "ऑल-ऑर-नथिंग" दृष्टिकोण के विपरीत)।

VQGAN के बारे में अधिक जानें [Taming Transformers](https://compvis.github.io/taming-transformers/) वेबसाइट पर।

VQGAN और पारंपरिक GAN के बीच एक महत्वपूर्ण अंतर यह है कि पारंपरिक GAN किसी भी इनपुट वेक्टर से एक अच्छी इमेज बना सकता है, जबकि VQGAN संभवतः एक असंगत इमेज बना सकता है। इसलिए, हमें इमेज निर्माण प्रक्रिया को और अधिक मार्गदर्शन करने की आवश्यकता होती है, और यह CLIP का उपयोग करके किया जा सकता है।

![VQGAN+CLIP आर्किटेक्चर](../../../../../lessons/X-Extras/X1-MultiModal/images/vqgan.png)

टेक्स्ट प्रॉम्प्ट के अनुरूप इमेज जनरेट करने के लिए, हम कुछ रैंडम एनकोडिंग वेक्टर से शुरू करते हैं जिसे VQGAN के माध्यम से पास किया जाता है ताकि इमेज उत्पन्न हो सके। फिर CLIP का उपयोग लॉस फंक्शन उत्पन्न करने के लिए किया जाता है जो दिखाता है कि इमेज टेक्स्ट प्रॉम्प्ट के साथ कितनी अच्छी तरह मेल खाती है। लक्ष्य तब इस लॉस को न्यूनतम करना होता है, बैक प्रोपेगेशन का उपयोग करके इनपुट वेक्टर पैरामीटर को समायोजित करना।

VQGAN+CLIP को लागू करने वाली एक बेहतरीन लाइब्रेरी [Pixray](http://github.com/pixray/pixray) है।

![Pixray द्वारा बनाई गई तस्वीर](../../../../../lessons/X-Extras/X1-MultiModal/images/a_closeup_watercolor_portrait_of_young_male_teacher_of_literature_with_a_book.png) |  ![Pixray द्वारा बनाई गई तस्वीर](../../../../../lessons/X-Extras/X1-MultiModal/images/a_closeup_oil_portrait_of_young_female_teacher_of_computer_science_with_a_computer.png) | ![Pixray द्वारा बनाई गई तस्वीर](../../../../../lessons/X-Extras/X1-MultiModal/images/a_closeup_oil_portrait_of_old_male_teacher_of_math.png)
----|----|----
प्रॉम्प्ट से जनरेट की गई तस्वीर *एक युवा पुरुष साहित्य शिक्षक की किताब के साथ क्लोजअप वॉटरकलर पोर्ट्रेट* | प्रॉम्प्ट से जनरेट की गई तस्वीर *एक युवा महिला कंप्यूटर विज्ञान शिक्षक की कंप्यूटर के साथ क्लोजअप ऑयल पोर्ट्रेट* | प्रॉम्प्ट से जनरेट की गई तस्वीर *एक वृद्ध पुरुष गणित शिक्षक का ब्लैकबोर्ड के सामने क्लोजअप ऑयल पोर्ट्रेट*

> **आर्टिफिशियल टीचर्स** संग्रह से तस्वीरें [Dmitry Soshnikov](http://soshnikov.com) द्वारा

## DALL-E
### [DALL-E 1](https://openai.com/research/dall-e)
DALL-E GPT-3 का एक संस्करण है जिसे प्रॉम्प्ट्स से इमेज जनरेट करने के लिए प्रशिक्षित किया गया है। इसे 12-बिलियन पैरामीटर्स के साथ प्रशिक्षित किया गया है।

CLIP के विपरीत, DALL-E टेक्स्ट और इमेज दोनों को एक ही टोकन स्ट्रीम के रूप में प्राप्त करता है। इसलिए, कई प्रॉम्प्ट्स से, आप टेक्स्ट के आधार पर इमेज जनरेट कर सकते हैं।

### [DALL-E 2](https://openai.com/dall-e-2)
DALL-E 1 और 2 के बीच मुख्य अंतर यह है कि DALL-E 2 अधिक यथार्थवादी इमेज और कला जनरेट करता है।

DALL-E के साथ इमेज जनरेशन के उदाहरण:
![Pixray द्वारा बनाई गई तस्वीर](../../../../../lessons/X-Extras/X1-MultiModal/images/DALL·E%202023-06-20%2015.56.56%20-%20a%20closeup%20watercolor%20portrait%20of%20young%20male%20teacher%20of%20literature%20with%20a%20book.png) |  ![Pixray द्वारा बनाई गई तस्वीर](../../../../../lessons/X-Extras/X1-MultiModal/images/DALL·E%202023-06-20%2015.57.43%20-%20a%20closeup%20oil%20portrait%20of%20young%20female%20teacher%20of%20computer%20science%20with%20a%20computer.png) | ![Pixray द्वारा बनाई गई तस्वीर](../../../../../lessons/X-Extras/X1-MultiModal/images/DALL·E%202023-06-20%2015.58.42%20-%20%20a%20closeup%20oil%20portrait%20of%20old%20male%20teacher%20of%20mathematics%20in%20front%20of%20blackboard.png)
----|----|----
प्रॉम्प्ट से जनरेट की गई तस्वीर *एक युवा पुरुष साहित्य शिक्षक की किताब के साथ क्लोजअप वॉटरकलर पोर्ट्रेट* | प्रॉम्प्ट से जनरेट की गई तस्वीर *एक युवा महिला कंप्यूटर विज्ञान शिक्षक की कंप्यूटर के साथ क्लोजअप ऑयल पोर्ट्रेट* | प्रॉम्प्ट से जनरेट की गई तस्वीर *एक वृद्ध पुरुष गणित शिक्षक का ब्लैकबोर्ड के सामने क्लोजअप ऑयल पोर्ट्रेट*

## संदर्भ

* VQGAN पेपर: [Taming Transformers for High-Resolution Image Synthesis](https://compvis.github.io/taming-transformers/paper/paper.pdf)
* CLIP पेपर: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।