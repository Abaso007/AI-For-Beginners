<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-24T09:59:44+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "hi"
}
-->
# न्यूरल नेटवर्क का परिचय: मल्टी-लेयर्ड परसेप्ट्रॉन

पिछले भाग में, आपने सबसे सरल न्यूरल नेटवर्क मॉडल - एक-लेयर परसेप्ट्रॉन, एक रेखीय दो-वर्ग वर्गीकरण मॉडल के बारे में सीखा।

इस भाग में हम इस मॉडल को एक अधिक लचीले ढांचे में विस्तारित करेंगे, जिससे हम:

* **मल्टी-क्लास वर्गीकरण** कर सकें, दो-वर्ग के अलावा
* **रिग्रेशन समस्याओं** को हल कर सकें, वर्गीकरण के अलावा
* उन वर्गों को अलग कर सकें जो रेखीय रूप से अलग नहीं हैं

हम Python में अपना खुद का मॉड्यूलर ढांचा भी विकसित करेंगे, जो हमें विभिन्न न्यूरल नेटवर्क आर्किटेक्चर बनाने की अनुमति देगा।

## [प्री-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## मशीन लर्निंग का औपचारिककरण

चलो मशीन लर्निंग समस्या को औपचारिक रूप से समझते हैं। मान लीजिए हमारे पास एक प्रशिक्षण डेटासेट **X** है, जिसमें लेबल्स **Y** हैं, और हमें एक मॉडल *f* बनाना है जो सबसे सटीक भविष्यवाणियां करेगा। भविष्यवाणियों की गुणवत्ता को **लॉस फंक्शन** ℒ द्वारा मापा जाता है। निम्नलिखित लॉस फंक्शन्स अक्सर उपयोग किए जाते हैं:

* रिग्रेशन समस्या के लिए, जब हमें एक संख्या की भविष्यवाणी करनी होती है, हम **एब्सोल्यूट एरर** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, या **स्क्वेयर्ड एरर** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> का उपयोग कर सकते हैं।
* वर्गीकरण के लिए, हम **0-1 लॉस** (जो मूल रूप से मॉडल की **सटीकता** के समान है), या **लॉजिस्टिक लॉस** का उपयोग करते हैं।

एक-स्तरीय परसेप्ट्रॉन के लिए, फंक्शन *f* को एक रेखीय फंक्शन *f(x)=wx+b* के रूप में परिभाषित किया गया था (यहां *w* वेट मैट्रिक्स है, *x* इनपुट फीचर्स का वेक्टर है, और *b* बायस वेक्टर है)। विभिन्न न्यूरल नेटवर्क आर्किटेक्चर के लिए, यह फंक्शन अधिक जटिल रूप ले सकता है।

> वर्गीकरण के मामले में, अक्सर यह वांछनीय होता है कि नेटवर्क आउटपुट के रूप में संबंधित वर्गों की संभावनाएं प्राप्त हों। किसी भी संख्या को संभावनाओं में बदलने (जैसे आउटपुट को सामान्यीकृत करने के लिए), हम अक्सर **सॉफ्टमैक्स** फंक्शन σ का उपयोग करते हैं, और फंक्शन *f* बन जाता है *f(x)=σ(wx+b)*।

ऊपर *f* की परिभाषा में, *w* और *b* को **पैरामीटर्स** θ=⟨*w,b*⟩ कहा जाता है। दिए गए डेटासेट ⟨**X**,**Y**⟩ के आधार पर, हम पूरे डेटासेट पर समग्र त्रुटि को पैरामीटर्स θ के एक फंक्शन के रूप में गणना कर सकते हैं।

> ✅ **न्यूरल नेटवर्क प्रशिक्षण का लक्ष्य त्रुटि को पैरामीटर्स θ को बदलकर कम करना है।**

## ग्रेडिएंट डिसेंट ऑप्टिमाइजेशन

फंक्शन ऑप्टिमाइजेशन का एक प्रसिद्ध तरीका **ग्रेडिएंट डिसेंट** है। इसका विचार यह है कि हम लॉस फंक्शन का पैरामीटर्स के संदर्भ में डेरिवेटिव (बहु-आयामी मामले में **ग्रेडिएंट** कहा जाता है) की गणना कर सकते हैं, और पैरामीटर्स को इस तरह बदल सकते हैं कि त्रुटि कम हो जाए। इसे निम्नलिखित रूप में औपचारिक किया जा सकता है:

* पैरामीटर्स को कुछ रैंडम मानों w<sup>(0)</sup>, b<sup>(0)</sup> से प्रारंभ करें।
* निम्नलिखित चरण को कई बार दोहराएं:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

प्रशिक्षण के दौरान, ऑप्टिमाइजेशन चरण पूरे डेटासेट को ध्यान में रखते हुए गणना किए जाने चाहिए (याद रखें कि लॉस सभी प्रशिक्षण नमूनों के माध्यम से योग के रूप में गणना की जाती है)। हालांकि, वास्तविक जीवन में हम डेटासेट के छोटे हिस्सों को **मिनीबैचेस** कहा जाता है, और डेटा के एक उपसमुच्चय के आधार पर ग्रेडिएंट्स की गणना करते हैं। क्योंकि उपसमुच्चय हर बार रैंडम रूप से लिया जाता है, इस विधि को **स्टोकेस्टिक ग्रेडिएंट डिसेंट** (SGD) कहा जाता है।

## मल्टी-लेयर्ड परसेप्ट्रॉन और बैकप्रोपेगेशन

एक-लेयर नेटवर्क, जैसा कि हमने ऊपर देखा, रेखीय रूप से अलग वर्गों को वर्गीकृत करने में सक्षम है। एक अधिक समृद्ध मॉडल बनाने के लिए, हम नेटवर्क की कई लेयर्स को जोड़ सकते हैं। गणितीय रूप से इसका मतलब होगा कि फंक्शन *f* का एक अधिक जटिल रूप होगा, और इसे कई चरणों में गणना किया जाएगा:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

यहां, α एक **नॉन-लाइनियर एक्टिवेशन फंक्शन** है, σ एक सॉफ्टमैक्स फंक्शन है, और पैरामीटर्स θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> हैं।

ग्रेडिएंट डिसेंट एल्गोरिदम वही रहेगा, लेकिन ग्रेडिएंट्स की गणना करना अधिक कठिन होगा। चेन डिफरेंशिएशन नियम को ध्यान में रखते हुए, हम डेरिवेटिव्स की गणना कर सकते हैं:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ लॉस फंक्शन के पैरामीटर्स के संदर्भ में डेरिवेटिव्स की गणना करने के लिए चेन डिफरेंशिएशन नियम का उपयोग किया जाता है।

ध्यान दें कि इन सभी अभिव्यक्तियों का बायां भाग समान है, और इस प्रकार हम प्रभावी रूप से डेरिवेटिव्स की गणना लॉस फंक्शन से शुरू करके और "पीछे की ओर" कम्प्यूटेशनल ग्राफ के माध्यम से कर सकते हैं। इस प्रकार मल्टी-लेयर्ड परसेप्ट्रॉन को प्रशिक्षित करने की विधि को **बैकप्रोपेगेशन**, या 'बैकप्रॉप' कहा जाता है।

<img alt="कम्प्यूट ग्राफ" src="images/ComputeGraphGrad.png"/>

> TODO: इमेज स्रोत

> ✅ हम अपने नोटबुक उदाहरण में बैकप्रॉप को और अधिक विस्तार से कवर करेंगे।  

## निष्कर्ष

इस पाठ में, हमने अपना खुद का न्यूरल नेटवर्क लाइब्रेरी बनाया है, और इसे एक सरल दो-आयामी वर्गीकरण कार्य के लिए उपयोग किया है।

## 🚀 चुनौती

साथ वाले नोटबुक में, आप मल्टी-लेयर्ड परसेप्ट्रॉन बनाने और प्रशिक्षित करने के लिए अपना खुद का ढांचा लागू करेंगे। आप विस्तार से देख पाएंगे कि आधुनिक न्यूरल नेटवर्क कैसे काम करते हैं।

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) नोटबुक पर जाएं और इसे पूरा करें।

## [पोस्ट-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## समीक्षा और स्व-अध्ययन

बैकप्रोपेगेशन एक सामान्य एल्गोरिदम है जो AI और ML में उपयोग किया जाता है, इसे [अधिक विस्तार से](https://wikipedia.org/wiki/Backpropagation) अध्ययन करना उचित है।

## [असाइनमेंट](lab/README.md)

इस लैब में, आपसे अपेक्षा की जाती है कि आप इस पाठ में बनाए गए ढांचे का उपयोग करके MNIST हस्तलिखित अंकों के वर्गीकरण को हल करें।

* [निर्देश](lab/README.md)
* [नोटबुक](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।