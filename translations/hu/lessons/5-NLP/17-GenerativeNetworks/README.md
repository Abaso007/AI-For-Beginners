<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d9de7847385eeeda67cfdcce1640ab72",
  "translation_date": "2025-08-25T21:43:24+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "hu"
}
-->
# Generat√≠v h√°l√≥zatok

## [El≈ëad√°s el≈ëtti kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

A Rekurrens Neur√°lis H√°l√≥zatok (RNN-ek) √©s azok kapuzott cellav√°ltozatai, mint p√©ld√°ul a Long Short Term Memory Cells (LSTM-ek) √©s a Gated Recurrent Units (GRU-k), lehet≈ës√©get biztos√≠tanak a nyelvi modellez√©sre, mivel k√©pesek megtanulni a szavak sorrendj√©t, √©s el≈ëre jelezni a k√∂vetkez≈ë sz√≥t egy sorozatban. Ez lehet≈ëv√© teszi, hogy az RNN-eket **generat√≠v feladatokra** haszn√°ljuk, p√©ld√°ul egyszer≈± sz√∂veggener√°l√°sra, g√©pi ford√≠t√°sra, s≈ët ak√°r k√©pal√°√≠r√°sok k√©sz√≠t√©s√©re is.

> ‚úÖ Gondolj arra, h√°nyszor haszn√°lt√°l generat√≠v feladatokat, p√©ld√°ul sz√∂vegkieg√©sz√≠t√©st g√©pel√©s k√∂zben. N√©zz ut√°na kedvenc alkalmaz√°saidnak, hogy haszn√°ltak-e RNN-eket.

Az el≈ëz≈ë egys√©gben t√°rgyalt RNN architekt√∫r√°ban minden RNN egys√©g a k√∂vetkez≈ë rejtett √°llapotot adta ki eredm√©nyk√©nt. Azonban hozz√°adhatunk egy m√°sik kimenetet is minden rekurrens egys√©ghez, amely lehet≈ëv√© teszi, hogy egy **sorozatot** adjunk ki (ami az eredeti sorozattal azonos hossz√∫s√°g√∫). Tov√°bb√° haszn√°lhatunk olyan RNN egys√©geket is, amelyek nem fogadnak bemenetet minden l√©p√©sben, hanem csak egy kezdeti √°llapotvektort vesznek, √©s ebb≈ël √°ll√≠tanak el≈ë egy kimeneti sorozatot.

Ez k√ºl√∂nb√∂z≈ë neur√°lis architekt√∫r√°kat tesz lehet≈ëv√©, amelyeket az al√°bbi √°bra mutat:

![√Åbra, amely a rekurrens neur√°lis h√°l√≥zatok gyakori mint√°zatait mutatja.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.hu.jpg)

> K√©p Andrej Karpaty [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) c√≠m≈± blogbejegyz√©s√©b≈ël

* **Egy-az-egyhez**: hagyom√°nyos neur√°lis h√°l√≥zat egy bemenettel √©s egy kimenettel
* **Egy-a-sokhoz**: generat√≠v architekt√∫ra, amely egy bemeneti √©rt√©ket fogad, √©s egy kimeneti √©rt√©ksorozatot gener√°l. P√©ld√°ul, ha egy **k√©pal√°√≠r√°s-gener√°l√≥** h√°l√≥zatot szeretn√©nk tan√≠tani, amely egy k√©p sz√∂veges le√≠r√°s√°t √°ll√≠tja el≈ë, akkor a k√©pet bemenetk√©nt haszn√°lhatjuk, egy CNN-en kereszt√ºl rejtett √°llapotot nyerhet√ºnk ki, majd egy rekurrens l√°nc sz√≥ szerint gener√°lja az al√°√≠r√°st.
* **Sok-az-egyhez**: az el≈ëz≈ë egys√©gben t√°rgyalt RNN architekt√∫r√°knak felel meg, p√©ld√°ul sz√∂vegklasszifik√°ci√≥ eset√©n.
* **Sok-a-sokhoz**, vagy **sorozat-a-sorozathoz**: olyan feladatokhoz, mint p√©ld√°ul a **g√©pi ford√≠t√°s**, ahol az els≈ë RNN √∂sszegy≈±jti az √∂sszes inform√°ci√≥t a bemeneti sorozatb√≥l a rejtett √°llapotba, majd egy m√°sik RNN l√°nc ezt az √°llapotot bontja ki a kimeneti sorozatt√°.

Ebben az egys√©gben egyszer≈± generat√≠v modellekre fogunk √∂sszpontos√≠tani, amelyek seg√≠tenek sz√∂veget gener√°lni. Az egyszer≈±s√©g kedv√©√©rt karakteralap√∫ tokeniz√°l√°st fogunk haszn√°lni.

Ezt az RNN-t l√©p√©sr≈ël l√©p√©sre tan√≠tjuk sz√∂veg gener√°l√°s√°ra. Minden l√©p√©sben egy `nchars` hossz√∫s√°g√∫ karakterl√°ncot vesz√ºnk, √©s megk√©rj√ºk a h√°l√≥zatot, hogy minden bemeneti karakterhez gener√°lja a k√∂vetkez≈ë kimeneti karaktert:

![√Åbra, amely egy RNN √°ltal a 'HELLO' sz√≥ gener√°l√°s√°t mutatja.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.hu.png)

Sz√∂veg gener√°l√°sakor (inferencia sor√°n) egy **ind√≠t√≥** sz√∂veggel kezd√ºnk, amelyet az RNN cell√°kon kereszt√ºlvezet√ºnk, hogy el≈ë√°ll√≠tsuk a k√∂ztes √°llapotot, majd ebb≈ël az √°llapotb√≥l indul a gener√°l√°s. Egy karaktert gener√°lunk egyszerre, majd az √°llapotot √©s a gener√°lt karaktert tov√°bbadjuk egy m√°sik RNN cell√°nak, hogy gener√°lja a k√∂vetkez≈ët, am√≠g elegend≈ë karaktert nem gener√°lunk.

<img src="images/rnn-generate-inf.png" width="60%"/>

> K√©p a szerz≈ët≈ël

## ‚úçÔ∏è Gyakorlatok: Generat√≠v h√°l√≥zatok

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:

* [Generat√≠v h√°l√≥zatok PyTorch seg√≠ts√©g√©vel](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Generat√≠v h√°l√≥zatok TensorFlow seg√≠ts√©g√©vel](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## L√°gy sz√∂veggener√°l√°s √©s h≈ëm√©rs√©klet

Minden RNN cella kimenete egy karakterekre vonatkoz√≥ val√≥sz√≠n≈±s√©gi eloszl√°s. Ha mindig a legnagyobb val√≥sz√≠n≈±s√©g≈± karaktert v√°lasztjuk a gener√°lt sz√∂veg k√∂vetkez≈ë karakterek√©nt, a sz√∂veg gyakran "ciklusba" ker√ºlhet, √©s ugyanazokat a karakterl√°ncokat ism√©telheti √∫jra √©s √∫jra, mint ebben a p√©ld√°ban:

```
today of the second the company and a second the company ...
```

Azonban, ha megn√©zz√ºk a k√∂vetkez≈ë karakter val√≥sz√≠n≈±s√©gi eloszl√°s√°t, el≈ëfordulhat, hogy a n√©h√°ny legmagasabb val√≥sz√≠n≈±s√©g k√∂z√∂tti k√ºl√∂nbs√©g nem nagy, p√©ld√°ul az egyik karakter val√≥sz√≠n≈±s√©ge 0,2, m√≠g egy m√°sik√© 0,19 stb. P√©ld√°ul, amikor a '*play*' sorozat k√∂vetkez≈ë karakter√©t keress√ºk, a k√∂vetkez≈ë karakter lehet egyar√°nt sz√≥k√∂z vagy **e** (mint a *player* sz√≥ban).

Ez arra a k√∂vetkeztet√©sre vezet, hogy nem mindig "igazs√°gos" a legnagyobb val√≥sz√≠n≈±s√©g≈± karaktert v√°lasztani, mert a m√°sodik legnagyobb val√≥sz√≠n≈±s√©g≈± karakter v√°laszt√°sa is √©rtelmes sz√∂veghez vezethet. B√∂lcsebb, ha a karaktereket a h√°l√≥zat kimenete √°ltal adott val√≥sz√≠n≈±s√©gi eloszl√°sb√≥l **mintav√©telezz√ºk**. Haszn√°lhatunk egy **h≈ëm√©rs√©klet** nev≈± param√©tert is, amely kisim√≠tja a val√≥sz√≠n≈±s√©gi eloszl√°st, ha t√∂bb v√©letlenszer≈±s√©get szeretn√©nk hozz√°adni, vagy meredekebb√© teszi, ha ink√°bb a legnagyobb val√≥sz√≠n≈±s√©g≈± karakterekhez ragaszkodn√°nk.

Fedezd fel, hogyan val√≥sul meg ez a l√°gy sz√∂veggener√°l√°s a fent hivatkozott jegyzetf√ºzetekben.

## √ñsszegz√©s

B√°r a sz√∂veggener√°l√°s √∂nmag√°ban is hasznos lehet, a f≈ë el≈ëny√∂k abb√≥l sz√°rmaznak, hogy k√©pesek vagyunk sz√∂veget gener√°lni RNN-ek seg√≠ts√©g√©vel egy kezdeti jellemz≈ëvektorb√≥l. P√©ld√°ul a sz√∂veggener√°l√°st haszn√°lj√°k g√©pi ford√≠t√°s sor√°n (sorozat-a-sorozathoz, ebben az esetben az *enk√≥der* √°llapotvektor√°t haszn√°lj√°k a ford√≠tott √ºzenet gener√°l√°s√°ra vagy *dek√≥dol√°s√°ra*), vagy egy k√©p sz√∂veges le√≠r√°s√°nak gener√°l√°s√°ra (ebben az esetben a jellemz≈ëvektor egy CNN kinyer≈ëb≈ël sz√°rmazik).

## üöÄ Kih√≠v√°s

Vegy√©l r√©szt n√©h√°ny Microsoft Learn leck√©n ebben a t√©m√°ban:

* Sz√∂veggener√°l√°s [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste) seg√≠ts√©g√©vel

## [El≈ëad√°s ut√°ni kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

√çme n√©h√°ny cikk, amelyekkel b≈ëv√≠theted tud√°sodat:

* K√ºl√∂nb√∂z≈ë megk√∂zel√≠t√©sek sz√∂veggener√°l√°sra Markov-l√°nccal, LSTM-mel √©s GPT-2-vel: [blogbejegyz√©s](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Sz√∂veggener√°l√°si p√©lda a [Keras dokument√°ci√≥j√°ban](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Feladat](lab/README.md)

L√°ttuk, hogyan lehet karakterenk√©nt sz√∂veget gener√°lni. A laborban a sz√≥alap√∫ sz√∂veggener√°l√°st fogod felfedezni.

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az [Co-op Translator](https://github.com/Azure/co-op-translator) AI ford√≠t√°si szolg√°ltat√°s seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.