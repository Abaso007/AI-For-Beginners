<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "51be6057374d01d70e07dd5ec88ebc0d",
  "translation_date": "2025-09-23T11:17:06+00:00",
  "source_file": "lessons/5-NLP/17-GenerativeNetworks/README.md",
  "language_code": "hu"
}
-->
# Generat√≠v h√°l√≥zatok

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/33)

A Rekurrens Neur√°lis H√°l√≥zatok (RNN-ek) √©s azok kapuzott cellav√°ltozatai, mint p√©ld√°ul a Hossz√∫-R√∂vid T√°v√∫ Mem√≥riacell√°k (LSTM-ek) √©s Kapuzott Rekurrens Egys√©gek (GRU-k), lehet≈ës√©get biztos√≠tanak a nyelvi modellez√©sre, mivel k√©pesek megtanulni a szavak sorrendj√©t, √©s el≈ëre jelezni a k√∂vetkez≈ë sz√≥t egy sorozatban. Ez lehet≈ëv√© teszi, hogy az RNN-eket **generat√≠v feladatokra** haszn√°ljuk, p√©ld√°ul sz√∂veg gener√°l√°s√°ra, g√©pi ford√≠t√°sra, s≈ët ak√°r k√©pal√°√≠r√°s k√©sz√≠t√©s√©re is.

> ‚úÖ Gondolj arra, h√°nyszor haszn√°lt√°l generat√≠v feladatokat, p√©ld√°ul sz√∂vegkieg√©sz√≠t√©st g√©pel√©s k√∂zben. Kutass ut√°na kedvenc alkalmaz√°saidnak, hogy megtudd, haszn√°ltak-e RNN-eket.

Az el≈ëz≈ë egys√©gben t√°rgyalt RNN architekt√∫r√°ban minden RNN egys√©g a k√∂vetkez≈ë rejtett √°llapotot √°ll√≠totta el≈ë kimenetk√©nt. Azonban hozz√°adhatunk egy m√°sik kimenetet is minden rekurrens egys√©ghez, amely lehet≈ëv√© teszi, hogy egy **sorozatot** √°ll√≠tsunk el≈ë (amely megegyezik az eredeti sorozat hossz√°val). Tov√°bb√° haszn√°lhatunk olyan RNN egys√©geket, amelyek nem fogadnak bemenetet minden l√©p√©sn√©l, hanem csak egy kezdeti √°llapotvektort vesznek, √©s ebb≈ël √°ll√≠tanak el≈ë egy kimeneti sorozatot.

Ez k√ºl√∂nb√∂z≈ë neur√°lis architekt√∫r√°kat tesz lehet≈ëv√©, amelyeket az al√°bbi k√©pen l√°thatunk:

![K√©p, amely a rekurrens neur√°lis h√°l√≥zatok gyakori mint√°zatait mutatja.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.hu.jpg)

> K√©p Andrej Karpaty [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) c√≠m≈± blogbejegyz√©s√©b≈ël [Andrej Karpaty](http://karpathy.github.io/) √°ltal

* **Egy-az-egyhez**: hagyom√°nyos neur√°lis h√°l√≥zat egy bemenettel √©s egy kimenettel
* **Egy-az-sokhoz**: generat√≠v architekt√∫ra, amely egy bemeneti √©rt√©ket fogad, √©s egy kimeneti √©rt√©ksorozatot gener√°l. P√©ld√°ul, ha egy **k√©pal√°√≠r√°s k√©sz√≠t≈ë** h√°l√≥zatot szeretn√©nk tan√≠tani, amely egy k√©pr≈ël sz√∂veges le√≠r√°st k√©sz√≠t, akkor egy k√©pet adhatunk bemenetk√©nt, amelyet egy CNN-en kereszt√ºl rejtett √°llapott√° alak√≠tunk, majd egy rekurrens l√°nc sz√≥ szerint gener√°lja az al√°√≠r√°st.
* **Sok-az-egyhez**: az el≈ëz≈ë egys√©gben le√≠rt RNN architekt√∫r√°knak felel meg, p√©ld√°ul sz√∂vegklasszifik√°ci√≥
* **Sok-az-sokhoz**, vagy **sorozat-az-sorozathoz**: olyan feladatokat jelent, mint p√©ld√°ul a **g√©pi ford√≠t√°s**, ahol az els≈ë RNN √∂sszegy≈±jti az √∂sszes inform√°ci√≥t a bemeneti sorozatb√≥l a rejtett √°llapotba, majd egy m√°sik RNN l√°nc ezt az √°llapotot kimeneti sorozatt√° bontja ki.

Ebben az egys√©gben egyszer≈± generat√≠v modellekre fogunk √∂sszpontos√≠tani, amelyek seg√≠tenek sz√∂veget gener√°lni. Az egyszer≈±s√©g kedv√©√©rt karakter szint≈± tokeniz√°l√°st fogunk haszn√°lni.

Ezt az RNN-t arra fogjuk tan√≠tani, hogy l√©p√©sr≈ël l√©p√©sre gener√°ljon sz√∂veget. Minden l√©p√©sben egy `nchars` hossz√∫s√°g√∫ karakter sorozatot vesz√ºnk, √©s megk√©rj√ºk a h√°l√≥zatot, hogy gener√°lja a k√∂vetkez≈ë kimeneti karaktert minden bemeneti karakterhez:

![K√©p, amely az 'HELLO' sz√≥ RNN √°ltali gener√°l√°s√°t mutatja.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.hu.png)

Sz√∂veg gener√°l√°sakor (k√∂vetkeztet√©s sor√°n) egy **ind√≠t√≥sz√∂veggel** kezd√ºnk, amelyet RNN cell√°kon kereszt√ºl adunk √°t, hogy el≈ë√°ll√≠tsuk annak k√∂ztes √°llapot√°t, majd ebb≈ël az √°llapotb√≥l kezd≈ëdik a gener√°l√°s. Egy karaktert gener√°lunk egyszerre, √©s az √°llapotot √©s a gener√°lt karaktert √°tadjuk egy m√°sik RNN cell√°nak, hogy gener√°lja a k√∂vetkez≈ët, am√≠g elegend≈ë karaktert nem gener√°lunk.

<img src="images/rnn-generate-inf.png" width="60%"/>

> K√©p a szerz≈ët≈ël

## ‚úçÔ∏è Gyakorlatok: Generat√≠v h√°l√≥zatok

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:

* [Generat√≠v h√°l√≥zatok PyTorch seg√≠ts√©g√©vel](GenerativePyTorch.ipynb)
* [Generat√≠v h√°l√≥zatok TensorFlow seg√≠ts√©g√©vel](GenerativeTF.ipynb)

## L√°gy sz√∂veg gener√°l√°s √©s h≈ëm√©rs√©klet

Minden RNN cella kimenete egy karakterek val√≥sz√≠n≈±s√©gi eloszl√°sa. Ha mindig a legmagasabb val√≥sz√≠n≈±s√©g≈± karaktert v√°lasztjuk a gener√°lt sz√∂veg k√∂vetkez≈ë karakterek√©nt, a sz√∂veg gyakran "ciklusba" ker√ºlhet, √©s ugyanazokat a karakter sorozatokat ism√©telheti √∫jra √©s √∫jra, mint ebben a p√©ld√°ban:

```
today of the second the company and a second the company ...
```

Azonban, ha megn√©zz√ºk a k√∂vetkez≈ë karakter val√≥sz√≠n≈±s√©gi eloszl√°s√°t, el≈ëfordulhat, hogy a n√©h√°ny legmagasabb val√≥sz√≠n≈±s√©g k√∂z√∂tti k√ºl√∂nbs√©g nem jelent≈ës, p√©ld√°ul egy karakter val√≥sz√≠n≈±s√©ge lehet 0.2, egy m√°sik√© pedig 0.19, stb. P√©ld√°ul, amikor a '*play*' sorozat k√∂vetkez≈ë karakter√©t keress√ºk, a k√∂vetkez≈ë karakter lehet egyar√°nt sz√≥k√∂z vagy **e** (mint a *player* sz√≥ban).

Ez arra a k√∂vetkeztet√©sre vezet minket, hogy nem mindig "igazs√°gos" a magasabb val√≥sz√≠n≈±s√©g≈± karaktert v√°lasztani, mert a m√°sodik legmagasabb v√°laszt√°sa is √©rtelmes sz√∂veghez vezethet. B√∂lcsebb **mintav√©telezni** a karaktereket a h√°l√≥zat kimenete √°ltal adott val√≥sz√≠n≈±s√©gi eloszl√°sb√≥l. Haszn√°lhatunk egy param√©tert, **h≈ëm√©rs√©kletet**, amely kisim√≠tja a val√≥sz√≠n≈±s√©gi eloszl√°st, ha t√∂bb v√©letlenszer≈±s√©get szeretn√©nk hozz√°adni, vagy meredekebb√© teszi, ha ink√°bb a legmagasabb val√≥sz√≠n≈±s√©g≈± karakterekhez szeretn√©nk ragaszkodni.

Fedezd fel, hogyan val√≥sul meg ez a l√°gy sz√∂veg gener√°l√°s az el≈ëz≈ëekben linkelt jegyzetf√ºzetekben.

## √ñsszegz√©s

B√°r a sz√∂veg gener√°l√°sa √∂nmag√°ban is hasznos lehet, a f≈ë el≈ëny√∂k abb√≥l sz√°rmaznak, hogy k√©pesek vagyunk sz√∂veget gener√°lni RNN-ek seg√≠ts√©g√©vel egy kezdeti jellemz≈ëvektorb√≥l. P√©ld√°ul a sz√∂veg gener√°l√°sa r√©sze lehet a g√©pi ford√≠t√°snak (sorozat-az-sorozathoz, ebben az esetben az *enk√≥der* √°llapotvektor√°t haszn√°ljuk a leford√≠tott √ºzenet gener√°l√°s√°ra vagy *dek√≥dol√°s√°ra*), vagy egy k√©p sz√∂veges le√≠r√°s√°nak gener√°l√°s√°ra (ebben az esetben a jellemz≈ëvektor egy CNN kinyer≈ëb≈ël sz√°rmazik).

## üöÄ Kih√≠v√°s

Vegy√©l r√©szt n√©h√°ny Microsoft Learn leck√©n ebben a t√©m√°ban

* Sz√∂veg gener√°l√°sa [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste) seg√≠ts√©g√©vel

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/34)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

√çme n√©h√°ny cikk, amelyekkel b≈ëv√≠theted tud√°sodat

* K√ºl√∂nb√∂z≈ë megk√∂zel√≠t√©sek sz√∂veg gener√°l√°s√°ra Markov-l√°nccal, LSTM-mel √©s GPT-2-vel: [blogbejegyz√©s](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Sz√∂veg gener√°l√°s minta a [Keras dokument√°ci√≥ban](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Feladat](lab/README.md)

L√°ttuk, hogyan lehet karakterr≈ël karakterre sz√∂veget gener√°lni. A laborban a sz√≥ szint≈± sz√∂veg gener√°l√°st fogod felfedezni.

---

