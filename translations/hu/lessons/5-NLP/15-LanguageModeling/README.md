<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "31b46ba1f3aa78578134d4829f88be53",
  "translation_date": "2025-08-25T21:55:17+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "hu"
}
-->
# Nyelvi Modellez√©s

A szemantikus be√°gyaz√°sok, mint p√©ld√°ul a Word2Vec √©s a GloVe, val√≥j√°ban az els≈ë l√©p√©st jelentik a **nyelvi modellez√©s** fel√© ‚Äì olyan modellek l√©trehoz√°sa fel√©, amelyek valamilyen m√≥don *meg√©rtik* (vagy *reprezent√°lj√°k*) a nyelv term√©szet√©t.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/29)

A nyelvi modellez√©s f≈ë gondolata az, hogy c√≠mk√©zetlen adathalmazokon tan√≠tsuk ≈ëket fel√ºgyelet n√©lk√ºli m√≥don. Ez az√©rt fontos, mert hatalmas mennyis√©g≈± c√≠mk√©zetlen sz√∂veg √°ll rendelkez√©s√ºnkre, m√≠g a c√≠mk√©zett sz√∂vegek mennyis√©g√©t mindig korl√°tozza az a munka, amit a c√≠mk√©z√©sre ford√≠thatunk. Leggyakrabban olyan nyelvi modelleket √©p√≠thet√ºnk, amelyek k√©pesek **hi√°nyz√≥ szavakat megj√≥solni** a sz√∂vegben, mivel egyszer≈±en ki lehet takarni egy v√©letlenszer≈± sz√≥t a sz√∂vegben, √©s azt haszn√°lni tan√≠t√≥mintak√©nt.

## Be√°gyaz√°sok tan√≠t√°sa

Kor√°bbi p√©ld√°inkban el≈ëre betan√≠tott szemantikus be√°gyaz√°sokat haszn√°ltunk, de √©rdekes l√°tni, hogyan lehet ezeket a be√°gyaz√°sokat betan√≠tani. Sz√°mos lehets√©ges √∂tlet l√©tezik, amelyeket felhaszn√°lhatunk:

* **N-Gram** nyelvi modellez√©s, amikor egy tokent az el≈ëz≈ë N token alapj√°n j√≥solunk meg (N-gram).
* **Folytonos Sz√≥zs√°k** (CBoW), amikor a k√∂z√©ps≈ë tokent $W_0$ j√≥soljuk meg egy token sorozatban $W_{-N}$, ..., $W_N$.
* **Skip-gram**, ahol a k√∂z√©ps≈ë tokenb≈ël $W_0$ egy szomsz√©dos tokenhalmazt {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} j√≥solunk meg.

![k√©p egy tanulm√°nyb√≥l, amely a szavak vektorokk√° alak√≠t√°s√°t mutatja](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.hu.png)

> K√©p [ebb≈ël a tanulm√°nyb√≥l](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è P√©lda Jegyzetf√ºzetek: CBoW modell tan√≠t√°sa

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:

* [CBoW Word2Vec tan√≠t√°sa TensorFlow-val](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [CBoW Word2Vec tan√≠t√°sa PyTorch-csal](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## √ñsszegz√©s

Az el≈ëz≈ë leck√©ben l√°ttuk, hogy a sz√≥be√°gyaz√°sok szinte var√°zslatosan m≈±k√∂dnek! Most m√°r tudjuk, hogy a sz√≥be√°gyaz√°sok tan√≠t√°sa nem egy t√∫l bonyolult feladat, √©s k√©pesek lehet√ºnk saj√°t sz√≥be√°gyaz√°sokat tan√≠tani specifikus szakter√ºleti sz√∂vegekhez, ha sz√ºks√©ges.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## √Åttekint√©s √©s √ñn√°ll√≥ Tanul√°s

* [Hivatalos PyTorch oktat√≥anyag a nyelvi modellez√©sr≈ël](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Hivatalos TensorFlow oktat√≥anyag a Word2Vec modell tan√≠t√°s√°r√≥l](https://www.TensorFlow.org/tutorials/text/word2vec).
* A **gensim** keretrendszer haszn√°lata a leggyakrabban haszn√°lt be√°gyaz√°sok tan√≠t√°s√°ra n√©h√°ny k√≥dsorral [ebben a dokument√°ci√≥ban](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) van le√≠rva.

## üöÄ [Feladat: Skip-Gram Modell Tan√≠t√°sa](lab/README.md)

A laborban arra h√≠vunk ki, hogy m√≥dos√≠tsd az √≥rai k√≥dot, √©s tan√≠ts Skip-Gram modellt a CBoW helyett. [Olvasd el a r√©szleteket](lab/README.md).

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.