<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T11:17:34+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "hu"
}
-->
# Nyelvi Modellez√©s

A szemantikai be√°gyaz√°sok, mint p√©ld√°ul a Word2Vec √©s a GloVe, val√≥j√°ban az els≈ë l√©p√©st jelentik a **nyelvi modellez√©s** fel√© ‚Äì olyan modellek l√©trehoz√°sa fel√©, amelyek valamilyen m√≥don *meg√©rtik* (vagy *reprezent√°lj√°k*) a nyelv term√©szet√©t.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/29)

A nyelvi modellez√©s m√∂g√∂tt √°ll√≥ f≈ë √∂tlet az, hogy ezeket modelleket felc√≠mk√©zetlen adathalmazokon tan√≠tjuk, fel√ºgyelet n√©lk√ºli m√≥don. Ez az√©rt fontos, mert hatalmas mennyis√©g≈± felc√≠mk√©zetlen sz√∂veg √°ll rendelkez√©s√ºnkre, m√≠g a felc√≠mk√©zett sz√∂vegek mennyis√©ge mindig korl√°tozott lesz azzal az er≈ëfesz√≠t√©ssel, amit a c√≠mk√©z√©sre ford√≠tani tudunk. Leggyakrabban olyan nyelvi modelleket √©p√≠thet√ºnk, amelyek k√©pesek **hi√°nyz√≥ szavakat megj√≥solni** a sz√∂vegben, mivel k√∂nny≈± v√©letlenszer≈±en kitakarni egy sz√≥t a sz√∂vegben, √©s azt mint tan√≠t√°si mint√°t haszn√°lni.

## Be√°gyaz√°sok tan√≠t√°sa

Kor√°bbi p√©ld√°inkban el≈ëre betan√≠tott szemantikai be√°gyaz√°sokat haszn√°ltunk, de √©rdekes l√°tni, hogyan lehet ezeket a be√°gyaz√°sokat betan√≠tani. Sz√°mos lehets√©ges √∂tlet l√©tezik, amelyeket haszn√°lhatunk:

* **N-Gram** nyelvi modellez√©s, amikor egy token-t j√≥solunk meg az N el≈ëz≈ë token alapj√°n (N-gram).
* **Folytonos Sz√≥zs√°k** (CBoW), amikor a k√∂z√©ps≈ë token-t $W_0$ j√≥soljuk meg egy token sorozatban $W_{-N}$, ..., $W_N$.
* **Skip-gram**, ahol a k√∂z√©ps≈ë token $W_0$ alapj√°n egy szomsz√©dos tokenek halmaz√°t {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} j√≥soljuk meg.

![k√©p a szavak vektorokk√° alak√≠t√°s√°r√≥l sz√≥l√≥ tanulm√°nyb√≥l](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.hu.png)

> K√©p [ebb≈ël a tanulm√°nyb√≥l](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è P√©lda Jegyzetf√ºzetek: CBoW modell tan√≠t√°sa

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:

* [CBoW Word2Vec tan√≠t√°sa TensorFlow-val](CBoW-TF.ipynb)
* [CBoW Word2Vec tan√≠t√°sa PyTorch-al](CBoW-PyTorch.ipynb)

## K√∂vetkeztet√©s

Az el≈ëz≈ë leck√©ben l√°ttuk, hogy a szavak be√°gyaz√°sai szinte var√°zslatosan m≈±k√∂dnek! Most m√°r tudjuk, hogy a szavak be√°gyaz√°sainak tan√≠t√°sa nem egy nagyon bonyolult feladat, √©s k√©pesek vagyunk saj√°t be√°gyaz√°sokat tan√≠tani specifikus szakter√ºleti sz√∂vegekhez, ha sz√ºks√©ges. 

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## √Åttekint√©s √©s √ñn√°ll√≥ Tanul√°s

* [Hivatalos PyTorch oktat√≥anyag a nyelvi modellez√©sr≈ël](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Hivatalos TensorFlow oktat√≥anyag a Word2Vec modell tan√≠t√°s√°r√≥l](https://www.TensorFlow.org/tutorials/text/word2vec).
* A **gensim** keretrendszer haszn√°lata a leggyakrabban haszn√°lt be√°gyaz√°sok n√©h√°ny sor k√≥ddal t√∂rt√©n≈ë tan√≠t√°s√°hoz [ebben a dokument√°ci√≥ban](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) van le√≠rva.

## üöÄ [Feladat: Skip-Gram Modell Tan√≠t√°sa](lab/README.md)

A laborban arra h√≠vunk ki, hogy m√≥dos√≠tsd az ebben a leck√©ben szerepl≈ë k√≥dot, √©s tan√≠ts Skip-Gram modellt a CBoW helyett. [Olvasd el a r√©szleteket](lab/README.md)

---

