<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbd3f73e4139f030ecb2e20387d70fee",
  "translation_date": "2025-09-23T11:20:12+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "hu"
}
-->
# Sz√∂veg √°br√°zol√°sa tenzorokk√©nt

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/25)

## Sz√∂veg oszt√°lyoz√°sa

A szakasz els≈ë r√©sz√©ben a **sz√∂veg oszt√°lyoz√°s√°ra** fogunk √∂sszpontos√≠tani. Az [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) adathalmazt fogjuk haszn√°lni, amely olyan h√≠rcikkeket tartalmaz, mint p√©ld√°ul:

* Kateg√≥ria: Tudom√°ny/Technol√≥gia
* C√≠m: Ky. c√©g t√°mogat√°st nyert peptidek tanulm√°nyoz√°s√°ra (AP)
* Sz√∂veg: AP - Egy, a Louisville-i Egyetem k√©miai kutat√≥ja √°ltal alap√≠tott c√©g t√°mogat√°st nyert egy fejleszt√©shez...

C√©lunk az lesz, hogy a h√≠reket a sz√∂veg alapj√°n az egyik kateg√≥ri√°ba soroljuk.

## Sz√∂veg √°br√°zol√°sa

Ha neur√°lis h√°l√≥zatokkal szeretn√©nk megoldani term√©szetes nyelvfeldolgoz√°si (NLP) feladatokat, valamilyen m√≥don a sz√∂veget tenzorokk√©nt kell √°br√°zolnunk. A sz√°m√≠t√≥g√©pek m√°r most is sz√°mokk√©nt √°br√°zolj√°k a sz√∂veges karaktereket, amelyek a k√©perny≈ën megjelen≈ë bet≈±t√≠pusokhoz vannak hozz√°rendelve, p√©ld√°ul ASCII vagy UTF-8 k√≥dol√°s seg√≠ts√©g√©vel.

<img alt="K√©p, amely egy karakter ASCII √©s bin√°ris √°br√°zol√°s√°t mutatja" src="images/ascii-character-map.png" width="50%"/>

> [K√©p forr√°sa](https://www.seobility.net/en/wiki/ASCII)

Mi, emberek, meg√©rtj√ºk, hogy mit **jelent** egy-egy bet≈±, √©s hogyan √°llnak √∂ssze a karakterek egy mondat szavaiv√°. A sz√°m√≠t√≥g√©pek azonban √∂nmagukban nem rendelkeznek ilyen meg√©rt√©ssel, √©s a neur√°lis h√°l√≥zatnak a tanul√°s sor√°n kell elsaj√°t√≠tania a jelent√©st.

Ez√©rt k√ºl√∂nb√∂z≈ë megk√∂zel√≠t√©seket alkalmazhatunk a sz√∂veg √°br√°zol√°s√°ra:

* **Karakter szint≈± √°br√°zol√°s**, amikor a sz√∂veget √∫gy √°br√°zoljuk, hogy minden karaktert egy sz√°mk√©nt kezel√ºnk. Ha a sz√∂vegkorpuszunkban *C* k√ºl√∂nb√∂z≈ë karakter van, akkor a *Hello* sz√≥ egy 5x*C* m√©ret≈± tenzork√©nt lenne √°br√°zolva. Minden bet≈± egy-egy oszlopnak felelne meg a one-hot k√≥dol√°sban.
* **Sz√≥ szint≈± √°br√°zol√°s**, amelyben egy **sz√≥kincset** hozunk l√©tre a sz√∂veg √∂sszes szav√°b√≥l, majd a szavakat one-hot k√≥dol√°ssal √°br√°zoljuk. Ez a megk√∂zel√≠t√©s valamivel jobb, mert egy-egy bet≈± √∂nmag√°ban nem sok jelent√©ssel b√≠r, √≠gy magasabb szint≈± szemantikai fogalmak - szavak - haszn√°lat√°val egyszer≈±s√≠tj√ºk a neur√°lis h√°l√≥zat feladat√°t. Azonban a nagy sz√≥t√°rm√©ret miatt magas dimenzi√≥j√∫ ritka tenzorokkal kell dolgoznunk.

B√°rmelyik √°br√°zol√°st is v√°lasztjuk, el≈ësz√∂r a sz√∂veget **tokenek** sorozat√°v√° kell alak√≠tanunk, ahol egy token lehet egy karakter, egy sz√≥, vagy n√©ha ak√°r egy sz√≥ r√©sze is. Ezut√°n a tokent egy sz√°mm√° alak√≠tjuk, √°ltal√°ban **sz√≥kincs** seg√≠ts√©g√©vel, √©s ezt a sz√°mot one-hot k√≥dol√°ssal t√°pl√°lhatjuk be a neur√°lis h√°l√≥zatba.

## N-gramok

A term√©szetes nyelvben a szavak pontos jelent√©se csak a kontextusban hat√°rozhat√≥ meg. P√©ld√°ul a *neur√°lis h√°l√≥zat* √©s a *hal√°szh√°l√≥* jelent√©se teljesen elt√©r≈ë. Az egyik m√≥dja annak, hogy ezt figyelembe vegy√ºk, ha a modell√ºnket sz√≥p√°rokra √©p√≠tj√ºk, √©s a sz√≥p√°rokat k√ºl√∂n sz√≥kincsbeli tokenekk√©nt kezelj√ºk. √çgy a *Szeretek horg√°szni menni* mondat a k√∂vetkez≈ë tokenek sorozatak√©nt lenne √°br√°zolva: *Szeretek horg√°szni*, *horg√°szni menni*. Az ezzel a megk√∂zel√≠t√©ssel j√°r√≥ probl√©ma az, hogy a sz√≥t√°r m√©rete jelent≈ësen megn≈ë, √©s az olyan kombin√°ci√≥k, mint *horg√°szni menni* √©s *v√°s√°rolni menni* k√ºl√∂n tokenekk√©nt jelennek meg, amelyek nem osztoznak semmilyen szemantikai hasonl√≥s√°gon, annak ellen√©re, hogy ugyanaz az ige szerepel benn√ºk.

Bizonyos esetekben √©rdemes lehet tri-gramokat - h√°rom sz√≥b√≥l √°ll√≥ kombin√°ci√≥kat - is haszn√°lni. Ez√©rt ezt a megk√∂zel√≠t√©st gyakran **n-gramoknak** nevezik. √ârtelme lehet n-gramokat karakter szint≈± √°br√°zol√°ssal is haszn√°lni, ebben az esetben az n-gramok nagyj√°b√≥l k√ºl√∂nb√∂z≈ë sz√≥tagoknak feleln√©nek meg.

## Bag-of-Words √©s TF/IDF

Amikor olyan feladatokat oldunk meg, mint a sz√∂veg oszt√°lyoz√°sa, k√©pesnek kell lenn√ºnk a sz√∂veget egy fix m√©ret≈± vektorral √°br√°zolni, amelyet bemenetk√©nt haszn√°lunk a v√©gs≈ë s≈±r≈± oszt√°lyoz√≥hoz. Az egyik legegyszer≈±bb m√≥dja ennek az, ha az egyes szavak √°br√°zol√°sait kombin√°ljuk, p√©ld√°ul √∂sszeadjuk ≈ëket. Ha minden sz√≥ one-hot k√≥dol√°s√°t √∂sszeadjuk, egy gyakoris√°gi vektort kapunk, amely megmutatja, hogy egy-egy sz√≥ h√°nyszor fordul el≈ë a sz√∂vegben. Az ilyen sz√∂veg√°br√°zol√°st **bag-of-words**-nek (BoW) nevezz√ºk.

<img src="images/bow.png" width="90%"/>

> K√©p a szerz≈ët≈ël

A BoW l√©nyeg√©ben azt mutatja meg, hogy mely szavak jelennek meg a sz√∂vegben √©s milyen mennyis√©gben, ami val√≥ban j√≥ jelz≈ëje lehet annak, hogy mir≈ël sz√≥l a sz√∂veg. P√©ld√°ul egy politikai h√≠rcikk val√≥sz√≠n≈±leg olyan szavakat tartalmaz, mint *eln√∂k* √©s *orsz√°g*, m√≠g egy tudom√°nyos publik√°ci√≥ban olyan szavak szerepelhetnek, mint *√ºtk√∂ztet≈ë*, *felfedezett*, stb. √çgy a szavak gyakoris√°ga sok esetben j√≥ indik√°tora lehet a sz√∂veg tartalm√°nak.

A BoW probl√©m√°ja, hogy bizonyos gyakori szavak, mint p√©ld√°ul *√©s*, *van*, stb. a legt√∂bb sz√∂vegben el≈ëfordulnak, √©s a legmagasabb gyakoris√°ggal rendelkeznek, elnyomva az igaz√°n fontos szavakat. Cs√∂kkenthetj√ºk ezeknek a szavaknak a fontoss√°g√°t, ha figyelembe vessz√ºk, hogy milyen gyakran fordulnak el≈ë az eg√©sz dokumentumgy≈±jtem√©nyben. Ez a f≈ë √∂tlete a TF/IDF megk√∂zel√≠t√©snek, amelyet r√©szletesebben t√°rgyalunk az ehhez a leck√©hez csatolt jegyzetf√ºzetekben.

Azonban egyik megk√∂zel√≠t√©s sem k√©pes teljes m√©rt√©kben figyelembe venni a sz√∂veg **szemantik√°j√°t**. Ehhez er≈ësebb neur√°lis h√°l√≥zati modellekre van sz√ºks√©g√ºnk, amelyeket k√©s≈ëbb t√°rgyalunk ebben a szakaszban.

## ‚úçÔ∏è Gyakorlatok: Sz√∂veg √°br√°zol√°sa

Folytasd a tanul√°st a k√∂vetkez≈ë jegyzetf√ºzetekben:

* [Sz√∂veg √°br√°zol√°sa PyTorch seg√≠ts√©g√©vel](TextRepresentationPyTorch.ipynb)
* [Sz√∂veg √°br√°zol√°sa TensorFlow seg√≠ts√©g√©vel](TextRepresentationTF.ipynb)

## √ñsszegz√©s

Eddig olyan technik√°kat tanulm√°nyoztunk, amelyek s√∫lyt adnak a k√ºl√∂nb√∂z≈ë szavak gyakoris√°g√°nak. Ezek azonban nem k√©pesek a jelent√©st vagy a sorrendet √°br√°zolni. Ahogy a h√≠res nyelv√©sz, J. R. Firth 1935-ben mondta: "Egy sz√≥ teljes jelent√©se mindig kontextu√°lis, √©s a jelent√©s kontextus n√©lk√ºli tanulm√°nyoz√°sa nem vehet≈ë komolyan." A kurzus k√©s≈ëbbi r√©sz√©ben megtanuljuk, hogyan ragadhatjuk meg a sz√∂veg kontextu√°lis inform√°ci√≥it nyelvi modellez√©s seg√≠ts√©g√©vel.

## üöÄ Kih√≠v√°s

Pr√≥b√°lj ki m√°s gyakorlatokat a bag-of-words √©s k√ºl√∂nb√∂z≈ë adatmodellek haszn√°lat√°val. Inspir√°ci√≥t mer√≠thetsz ebb≈ël a [Kaggle versenyb≈ël](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/26)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Gyakorold a sz√∂vegbe√°gyaz√°sok √©s a bag-of-words technik√°k haszn√°lat√°t a [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) oldalon.

## [Feladat: Jegyzetf√ºzetek](assignment.md)

---

