<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4522e22e150be0845e03aa41209a39d5",
  "translation_date": "2025-08-25T21:49:11+00:00",
  "source_file": "lessons/5-NLP/13-TextRep/README.md",
  "language_code": "hu"
}
-->
# Sz√∂veg √°br√°zol√°sa tenzorokk√©nt

## [El≈ëad√°s el≈ëtti kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Sz√∂vegklasszifik√°ci√≥

A szekci√≥ els≈ë r√©sz√©ben a **sz√∂vegklasszifik√°ci√≥** feladatra fogunk √∂sszpontos√≠tani. Az [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) adathalmazt fogjuk haszn√°lni, amely olyan h√≠rcikkeket tartalmaz, mint p√©ld√°ul:

* Kateg√≥ria: Tudom√°ny/Technol√≥gia  
* C√≠m: Ky. c√©g t√°mogat√°st nyert peptidek tanulm√°nyoz√°s√°ra (AP)  
* Sz√∂veg: AP - Egy, a Louisville-i Egyetem k√©miai kutat√≥ja √°ltal alap√≠tott c√©g t√°mogat√°st nyert egy fejleszt√©shez...

C√©lunk az lesz, hogy a h√≠reket a sz√∂veg alapj√°n az egyik kateg√≥ri√°ba soroljuk.

## Sz√∂veg √°br√°zol√°sa

Ha neur√°lis h√°l√≥zatokkal szeretn√©nk term√©szetes nyelvfeldolgoz√°si (NLP) feladatokat megoldani, sz√ºks√©g√ºnk van egy m√≥dszerre, amellyel a sz√∂veget tenzorokk√©nt √°br√°zolhatjuk. A sz√°m√≠t√≥g√©pek m√°r most is sz√°mokk√©nt √°br√°zolj√°k a sz√∂veges karaktereket, amelyek a k√©perny≈ën megjelen≈ë bet≈±t√≠pusokhoz rendelhet≈ëk, p√©ld√°ul ASCII vagy UTF-8 k√≥dol√°s seg√≠ts√©g√©vel.

<img alt="K√©p, amely egy karakter ASCII √©s bin√°ris √°br√°zol√°s√°t mutatja" src="images/ascii-character-map.png" width="50%"/>

> [K√©p forr√°sa](https://www.seobility.net/en/wiki/ASCII)

Az emberek meg√©rtik, hogy egy-egy bet≈± **mit jelent**, √©s hogyan √°llnak √∂ssze a karakterek egy mondat szavaiv√°. A sz√°m√≠t√≥g√©pek azonban √∂nmagukban nem rendelkeznek ilyen meg√©rt√©ssel, √©s a neur√°lis h√°l√≥zatnak ezt a jelent√©st a tanul√°s sor√°n kell elsaj√°t√≠tania.

Ez√©rt k√ºl√∂nb√∂z≈ë megk√∂zel√≠t√©seket alkalmazhatunk a sz√∂veg √°br√°zol√°s√°ra:

* **Karakter szint≈± √°br√°zol√°s**, amikor a sz√∂veget √∫gy √°br√°zoljuk, hogy minden karaktert egy sz√°mk√©nt kezel√ºnk. Ha a sz√∂vegkorpusznak *C* k√ºl√∂nb√∂z≈ë karaktere van, akkor a *Hello* sz√≥ egy 5x*C* m√©ret≈± tenzork√©nt √°br√°zolhat√≥. Minden bet≈± egy-egy oszlopot jelentene a tenzorban, egy-egy one-hot k√≥dol√°ssal.
* **Sz√≥ szint≈± √°br√°zol√°s**, amelyben egy **sz√≥kincset** hozunk l√©tre a sz√∂veg √∂sszes szav√°b√≥l, majd a szavakat one-hot k√≥dol√°ssal √°br√°zoljuk. Ez a megk√∂zel√≠t√©s valamivel jobb, mert az egyes bet≈±k √∂nmagukban nem hordoznak sok jelent√©st, √≠gy a magasabb szint≈± szemantikai egys√©gek - a szavak - haszn√°lat√°val egyszer≈±s√≠tj√ºk a neur√°lis h√°l√≥zat feladat√°t. Azonban a nagy sz√≥t√°rm√©ret miatt magas dimenzi√≥j√∫, ritka tenzorokkal kell dolgoznunk.

B√°rmelyik √°br√°zol√°st is v√°lasztjuk, el≈ësz√∂r a sz√∂veget **tokenek** sorozat√°v√° kell alak√≠tanunk, ahol egy token lehet egy karakter, egy sz√≥, vagy ak√°r egy sz√≥ egy r√©sze is. Ezut√°n a tokent egy sz√°mm√° alak√≠tjuk, √°ltal√°ban egy **sz√≥kincs** seg√≠ts√©g√©vel, √©s ezt a sz√°mot one-hot k√≥dol√°ssal t√°pl√°ljuk be a neur√°lis h√°l√≥zatba.

## N-gramok

A term√©szetes nyelvben a szavak pontos jelent√©se csak a kontextusban √©rthet≈ë meg. P√©ld√°ul a *neur√°lis h√°l√≥zat* √©s a *hal√°szh√°l√≥* jelent√©se teljesen elt√©r≈ë. Az egyik m√≥dja annak, hogy ezt figyelembe vegy√ºk, ha a modell√ºnket sz√≥p√°rokra √©p√≠tj√ºk, √©s a sz√≥p√°rokat k√ºl√∂n sz√≥kincsbeli tokenekk√©nt kezelj√ºk. √çgy a *Szeretek horg√°szni menni* mondat a k√∂vetkez≈ë tokenek sorozatak√©nt √°br√°zolhat√≥: *Szeretek horg√°szni*, *horg√°szni menni*. Az ezzel a megk√∂zel√≠t√©ssel j√°r√≥ probl√©ma az, hogy a sz√≥t√°r m√©rete jelent≈ësen megn≈ë, √©s az olyan kombin√°ci√≥k, mint *horg√°szni menni* √©s *v√°s√°rolni menni* k√ºl√∂n tokenekk√©nt jelennek meg, amelyek nem osztoznak semmilyen szemantikai hasonl√≥s√°gon, annak ellen√©re, hogy ugyanaz az ige szerepel benn√ºk.

Bizonyos esetekben √©rdemes lehet tri-gramokat -- h√°rom sz√≥b√≥l √°ll√≥ kombin√°ci√≥kat -- is haszn√°lni. Ez√©rt ezt a megk√∂zel√≠t√©st gyakran **n-gramoknak** nevezik. √ârdemes lehet n-gramokat karakter szint≈± √°br√°zol√°sn√°l is haszn√°lni, ahol az n-gramok nagyj√°b√≥l a k√ºl√∂nb√∂z≈ë sz√≥tagoknak felelnek meg.

## Bag-of-Words √©s TF/IDF

Amikor olyan feladatokat oldunk meg, mint a sz√∂vegklasszifik√°ci√≥, sz√ºks√©g√ºnk van arra, hogy a sz√∂veget egy fix m√©ret≈± vektork√©nt √°br√°zoljuk, amelyet bemenetk√©nt haszn√°lhatunk a v√©gs≈ë s≈±r≈± oszt√°lyoz√≥hoz. Az egyik legegyszer≈±bb m√≥dja ennek az, ha az egyes szavak √°br√°zol√°sait kombin√°ljuk, p√©ld√°ul √∂sszead√°ssal. Ha minden sz√≥ one-hot k√≥dol√°s√°t √∂sszeadjuk, akkor egy gyakoris√°gi vektort kapunk, amely megmutatja, hogy az egyes szavak h√°nyszor fordulnak el≈ë a sz√∂vegben. Az ilyen sz√∂veg√°br√°zol√°st **bag-of-words**-nek (BoW) nevezz√ºk.

<img src="images/bow.png" width="90%"/>

> K√©p a szerz≈ët≈ël

A BoW l√©nyeg√©ben azt mutatja meg, hogy mely szavak fordulnak el≈ë a sz√∂vegben, √©s milyen mennyis√©gben, ami val√≥ban j√≥ indik√°tora lehet annak, hogy mir≈ël sz√≥l a sz√∂veg. P√©ld√°ul egy politikai h√≠rcikk val√≥sz√≠n≈±leg olyan szavakat tartalmaz, mint *eln√∂k* √©s *orsz√°g*, m√≠g egy tudom√°nyos publik√°ci√≥ban olyan szavak szerepelhetnek, mint *√ºtk√∂ztet≈ë*, *felfedez√©s*, stb. √çgy a szavak gyakoris√°ga sok esetben j√≥ indik√°tora lehet a sz√∂veg tartalm√°nak.

A BoW probl√©m√°ja, hogy bizonyos gyakori szavak, mint p√©ld√°ul *√©s*, *van*, stb., a legt√∂bb sz√∂vegben el≈ëfordulnak, √©s ezeknek a legmagasabb a gyakoris√°ga, elnyomva az igaz√°n fontos szavakat. Cs√∂kkenthetj√ºk ezeknek a szavaknak a fontoss√°g√°t, ha figyelembe vessz√ºk, hogy milyen gyakran fordulnak el≈ë az eg√©sz dokumentumgy≈±jtem√©nyben. Ez a f≈ë √∂tlete a TF/IDF megk√∂zel√≠t√©snek, amelyet r√©szletesebben t√°rgyalunk az ehhez a leck√©hez tartoz√≥ jegyzetf√ºzetekben.

Azonban egyik megk√∂zel√≠t√©s sem k√©pes teljes m√©rt√©kben figyelembe venni a sz√∂veg **szemantik√°j√°t**. Ehhez er≈ësebb neur√°lis h√°l√≥zati modellekre van sz√ºks√©g, amelyeket k√©s≈ëbb t√°rgyalunk ebben a szekci√≥ban.

## ‚úçÔ∏è Gyakorlatok: Sz√∂veg√°br√°zol√°s

Folytasd a tanul√°st a k√∂vetkez≈ë jegyzetf√ºzetekben:

* [Sz√∂veg√°br√°zol√°s PyTorch seg√≠ts√©g√©vel](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)  
* [Sz√∂veg√°br√°zol√°s TensorFlow seg√≠ts√©g√©vel](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)  

## √ñsszegz√©s

Eddig olyan technik√°kat tanulm√°nyoztunk, amelyek s√∫lyt adnak a k√ºl√∂nb√∂z≈ë szavak gyakoris√°g√°nak. Ezek azonban nem k√©pesek a jelent√©st vagy a sorrendet √°br√°zolni. Ahogy a h√≠res nyelv√©sz, J. R. Firth 1935-ben mondta: "Egy sz√≥ teljes jelent√©se mindig kontextu√°lis, √©s a kontextust√≥l f√ºggetlen jelent√©s tanulm√°nyoz√°sa nem vehet≈ë komolyan." A kurzus k√©s≈ëbbi r√©sz√©ben megtanuljuk, hogyan ragadhatjuk meg a sz√∂veg kontextu√°lis inform√°ci√≥it nyelvi modellez√©s seg√≠ts√©g√©vel.

## üöÄ Kih√≠v√°s

Pr√≥b√°lj ki m√°s gyakorlatokat a bag-of-words √©s k√ºl√∂nb√∂z≈ë adatmodellek haszn√°lat√°val. Inspir√°ci√≥t mer√≠thetsz ebb≈ël a [Kaggle versenyb≈ël](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words).

## [El≈ëad√°s ut√°ni kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Gyakorold a sz√∂vegbe√°gyaz√°sok √©s a bag-of-words technik√°k haszn√°lat√°t a [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) oldalon.

## [Feladat: Jegyzetf√ºzetek](assignment.md)

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.