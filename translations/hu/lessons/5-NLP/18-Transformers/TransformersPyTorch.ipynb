{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figyelemmechanizmusok és transzformerek\n",
    "\n",
    "A rekurens hálózatok egyik fő hátránya, hogy a szekvencia minden szavának azonos hatása van az eredményre. Ez azzal jár, hogy a standard LSTM kódoló-dekódoló modellek nem teljesítenek optimálisan szekvencia-szekvencia feladatoknál, például a névelem-felismerésnél vagy a gépi fordításnál. Valójában az input szekvencia bizonyos szavai gyakran nagyobb hatással vannak a kimeneti szekvenciára, mint mások.\n",
    "\n",
    "Vegyünk például egy szekvencia-szekvencia modellt, mint a gépi fordítás. Ez két rekurens hálózattal valósul meg, ahol az egyik hálózat (**kódoló**) az input szekvenciát egy rejtett állapotba sűríti, míg a másik, **dekódoló**, ezt a rejtett állapotot bontja ki a fordított eredményre. Ennek a megközelítésnek az a problémája, hogy a hálózat végső állapota nehezen tudja megjegyezni a mondat elejét, ami gyenge modellminőséget eredményez hosszú mondatok esetén.\n",
    "\n",
    "**Figyelemmechanizmusok** lehetőséget adnak arra, hogy súlyozzuk az egyes input vektorok kontextuális hatását az RNN minden egyes kimeneti előrejelzésére. Ez úgy valósul meg, hogy rövidítéseket hozunk létre az input RNN köztes állapotai és a kimeneti RNN között. Ily módon, amikor a $y_t$ kimeneti szimbólumot generáljuk, figyelembe vesszük az összes input rejtett állapotot $h_i$, különböző súlykoefficiensekkel $\\alpha_{t,i}$. \n",
    "\n",
    "![Kép egy kódoló/dekódoló modellről additív figyelemréteggel](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.hu.png)\n",
    "*A kódoló-dekódoló modell additív figyelemmechanizmussal [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), idézve [ebből a blogbejegyzésből](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
    "\n",
    "A figyelem mátrix $\\{\\alpha_{i,j}\\}$ azt mutatja, hogy az egyes input szavak milyen mértékben játszanak szerepet egy adott szó generálásában a kimeneti szekvenciában. Az alábbiakban egy ilyen mátrix példáját láthatjuk:\n",
    "\n",
    "![Kép egy mintaillesztésről, amelyet az RNNsearch-50 talált, Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.hu.png)\n",
    "\n",
    "*Ábra [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (3. ábra) alapján*\n",
    "\n",
    "A figyelemmechanizmusok felelősek a természetes nyelvfeldolgozás jelenlegi vagy közel jelenlegi csúcsteljesítményéért. A figyelem hozzáadása azonban jelentősen növeli a modell paramétereinek számát, ami méretezési problémákat okozott az RNN-eknél. Az RNN-ek méretezésének egyik kulcsfontosságú korlátja, hogy a modellek rekurzív jellege megnehezíti a tanítás batch-elését és párhuzamosítását. Egy RNN-ben a szekvencia minden elemét sorrendben kell feldolgozni, ami azt jelenti, hogy nem lehet könnyen párhuzamosítani.\n",
    "\n",
    "A figyelemmechanizmusok alkalmazása és ez a korlát vezettek a ma ismert és használt csúcsteljesítményű transzformer modellek létrehozásához, mint például a BERT és az OpenGPT3.\n",
    "\n",
    "## Transzformer modellek\n",
    "\n",
    "Ahelyett, hogy az előző előrejelzések kontextusát továbbítanák a következő értékelési lépésbe, a **transzformer modellek** **pozíciós kódolásokat** és figyelmet használnak, hogy megragadják az adott input kontextusát egy megadott szövegablakon belül. Az alábbi kép bemutatja, hogyan képesek a pozíciós kódolások és a figyelem megragadni a kontextust egy adott ablakon belül.\n",
    "\n",
    "![Animált GIF, amely bemutatja, hogyan történik az értékelés a transzformer modellekben.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif) \n",
    "\n",
    "Mivel minden input pozíciót függetlenül térképeznek a kimeneti pozíciókhoz, a transzformerek jobban párhuzamosíthatók, mint az RNN-ek, ami lehetővé teszi sokkal nagyobb és kifejezőbb nyelvi modellek létrehozását. Minden figyelemfej különböző kapcsolatok megtanulására használható a szavak között, ami javítja a természetes nyelvfeldolgozási feladatok eredményét.\n",
    "\n",
    "A **BERT** (Bidirectional Encoder Representations from Transformers) egy nagyon nagy, többrétegű transzformer hálózat, amelynek 12 rétege van a *BERT-base* esetében, és 24 a *BERT-large* esetében. A modellt először egy nagy szövegkorpuszra (WikiPedia + könyvek) tanítják be felügyelet nélküli tanítással (maszkolt szavak előrejelzése egy mondatban). Az előtanítás során a modell jelentős nyelvi megértést sajátít el, amelyet aztán más adathalmazokkal lehet finomhangolni. Ezt a folyamatot **transzfer tanulásnak** nevezzük. \n",
    "\n",
    "![Kép a http://jalammar.github.io/illustrated-bert/ oldalról](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.hu.png)\n",
    "\n",
    "Számos transzformer architektúra létezik, például BERT, DistilBERT, BigBird, OpenGPT3 és még sok más, amelyek finomhangolhatók. A [HuggingFace csomag](https://github.com/huggingface/) lehetőséget biztosít ezeknek az architektúráknak a tanítására PyTorch segítségével. \n",
    "\n",
    "## BERT használata szövegklasszifikációhoz\n",
    "\n",
    "Nézzük meg, hogyan használhatunk előre betanított BERT modellt hagyományos feladatunk megoldására: szekvencia klasszifikáció. Az eredeti AG News adathalmazunkat fogjuk osztályozni.\n",
    "\n",
    "Először töltsük be a HuggingFace könyvtárat és az adathalmazunkat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "import transformers\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mivel előre betanított BERT modellt fogunk használni, szükségünk lesz egy specifikus tokenizálóra. Először betöltünk egy tokenizálót, amely az előre betanított BERT modellhez tartozik.\n",
    "\n",
    "A HuggingFace könyvtár tartalmaz egy előre betanított modellek gyűjteményét, amelyeket egyszerűen használhatsz, ha megadod a nevüket a `from_pretrained` függvények argumentumaként. Az összes szükséges bináris fájl a modellhez automatikusan letöltésre kerül.\n",
    "\n",
    "Azonban bizonyos esetekben szükség lehet arra, hogy saját modelleket tölts be. Ilyenkor megadhatod annak a könyvtárnak az elérési útját, amely tartalmazza az összes releváns fájlt, beleértve a tokenizáló paramétereit, a `config.json` fájlt a modell paramétereivel, a bináris súlyokat stb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model from Internet repository using model name. \n",
    "# Use this if you are running from your own copy of the notebooks\n",
    "bert_model = 'bert-base-uncased' \n",
    "\n",
    "# To load the model from the directory on disk. Use this for Microsoft Learn module, because we have\n",
    "# prepared all required files for you.\n",
    "bert_model = './bert'\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `tokenizer` objektum tartalmazza az `encode` függvényt, amely közvetlenül használható szöveg kódolására:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1052, 22123, 2953, 2818, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('PyTorch is a great framework for NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezután hozzunk létre iterátorokat, amelyeket az edzés során az adatok eléréséhez fogunk használni. Mivel a BERT a saját kódolási függvényét használja, szükségünk lesz egy, a korábban definiált `padify`-hoz hasonló kitöltési függvény meghatározására:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bert(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [tokenizer.encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=pad_bert, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, collate_fn=pad_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esetünkben egy előre betanított BERT modellt fogunk használni, amelyet `bert-base-uncased`-nek hívnak. Töltsük be a modellt a `BertForSequenceClassfication` csomag segítségével. Ez biztosítja, hogy a modellünk már rendelkezik a szükséges osztályozási architektúrával, beleértve a végső osztályozót is. Figyelmeztető üzenetet fogsz látni, amely szerint a végső osztályozó súlyai nincsenek inicializálva, és a modell előzetes betanítást igényel - ez teljesen rendben van, mivel pontosan ezt fogjuk most elvégezni!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most már készen állunk az edzés megkezdésére! Mivel a BERT már előre betanított modell, érdemes viszonylag kicsi tanulási rátával kezdeni, hogy ne rontsuk el a kezdeti súlyokat.\n",
    "\n",
    "A nehéz munka nagy részét a `BertForSequenceClassification` modell végzi. Amikor meghívjuk a modellt a tanító adatokon, az visszaadja az input minibatch-hez tartozó veszteséget és a hálózat kimenetét. A veszteséget a paraméterek optimalizálására használjuk (`loss.backward()` végzi a visszaterjesztést), míg a `out` segítségével számítjuk ki a tanítási pontosságot azáltal, hogy az `argmax`-szal kapott címkéket (`labs`) összehasonlítjuk a várt `labels` értékekkel.\n",
    "\n",
    "A folyamat ellenőrzése érdekében a veszteséget és a pontosságot több iteráción keresztül halmozzuk, majd minden `report_freq` tanítási ciklus után kiírjuk azokat.\n",
    "\n",
    "Ez a tanítás valószínűleg elég hosszú időt vesz igénybe, ezért korlátozzuk az iterációk számát.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.1254194641113282, Accuracy = 0.585\n",
      "Loss = 0.6194715118408203, Accuracy = 0.83\n",
      "Loss = 0.46665248870849607, Accuracy = 0.8475\n",
      "Loss = 0.4309701919555664, Accuracy = 0.8575\n",
      "Loss = 0.35427074432373046, Accuracy = 0.8825\n",
      "Loss = 0.3306886291503906, Accuracy = 0.8975\n",
      "Loss = 0.30340143203735354, Accuracy = 0.8975\n",
      "Loss = 0.26139299392700194, Accuracy = 0.915\n",
      "Loss = 0.26708646774291994, Accuracy = 0.9225\n",
      "Loss = 0.3667240524291992, Accuracy = 0.8675\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "report_freq = 50\n",
    "iterations = 500 # make this larger to train for longer time!\n",
    "\n",
    "model.train()\n",
    "\n",
    "i,c = 0,0\n",
    "acc_loss = 0\n",
    "acc_acc = 0\n",
    "\n",
    "for labels,texts in train_loader:\n",
    "    labels = labels.to(device)-1 # get labels in the range 0-3         \n",
    "    texts = texts.to(device)\n",
    "    loss, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc = torch.mean((labs==labels).type(torch.float32))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc_loss += loss\n",
    "    acc_acc += acc\n",
    "    i+=1\n",
    "    c+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"Loss = {acc_loss.item()/c}, Accuracy = {acc_acc.item()/c}\")\n",
    "        c = 0\n",
    "        acc_loss = 0\n",
    "        acc_acc = 0\n",
    "    iterations-=1\n",
    "    if not iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Látható (különösen, ha növeled az iterációk számát és elég sokáig vársz), hogy a BERT osztályozás elég jó pontosságot ad! Ez azért van, mert a BERT már eleve elég jól érti a nyelv szerkezetét, és nekünk csak a végső osztályozót kell finomhangolnunk. Azonban, mivel a BERT egy nagy modell, az egész tanítási folyamat sok időt vesz igénybe, és komoly számítási kapacitást igényel! (GPU, és lehetőleg több is).\n",
    "\n",
    "> **Megjegyzés:** Példánkban az egyik legkisebb előre betanított BERT modellt használtuk. Vannak nagyobb modellek is, amelyek valószínűleg jobb eredményeket hoznak.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A modell teljesítményének értékelése\n",
    "\n",
    "Most kiértékelhetjük a modellünk teljesítményét a tesztadatokon. Az értékelési ciklus nagyon hasonló a tanítási ciklushoz, de ne felejtsük el, hogy a modellt értékelési módba kell kapcsolni a `model.eval()` hívásával.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9047029702970297\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iterations = 100\n",
    "acc = 0\n",
    "i = 0\n",
    "for labels,texts in test_loader:\n",
    "    labels = labels.to(device)-1      \n",
    "    texts = texts.to(device)\n",
    "    _, out = model(texts, labels=labels)[:2]\n",
    "    labs = out.argmax(dim=1)\n",
    "    acc += torch.mean((labs==labels).type(torch.float32))\n",
    "    i+=1\n",
    "    if i>iterations: break\n",
    "        \n",
    "print(f\"Final accuracy: {acc.item()/i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legfontosabb tanulság\n",
    "\n",
    "Ebben az egységben láthattuk, milyen egyszerű a **transformers** könyvtárból származó előre betanított nyelvi modellt adaptálni a szövegklasszifikációs feladatunkhoz. Hasonlóképpen, a BERT modellek használhatók entitáskinyerésre, kérdés-megválaszolásra és más NLP-feladatokra.\n",
    "\n",
    "A transzformer modellek jelenleg az NLP csúcstechnológiáját képviselik, és a legtöbb esetben ezekkel érdemes először kísérletezni, amikor egyedi NLP-megoldásokat valósítunk meg. Azonban rendkívül fontos megérteni az ebben a modulban tárgyalt rekurzív neurális hálózatok alapelveit, ha fejlettebb neurális modelleket szeretnénk építeni.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Felelősségkizárás**:  \nEzt a dokumentumot az [Co-op Translator](https://github.com/Azure/co-op-translator) AI fordítószolgáltatás segítségével fordítottuk le. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt a professzionális, emberi fordítás igénybevétele. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy téves értelmezésekért.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "coopTranslator": {
   "original_hash": "753865967678a92dbce7d7efbd36d980",
   "translation_date": "2025-08-29T15:58:13+00:00",
   "source_file": "lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}