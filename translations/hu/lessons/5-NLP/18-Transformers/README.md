<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f335dfcb4a993920504c387973a36957",
  "translation_date": "2025-09-23T11:17:48+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "hu"
}
-->
# Figyelem Mechanizmusok √©s Transzformerek

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Az NLP ter√ºlet egyik legfontosabb probl√©m√°ja a **g√©pi ford√≠t√°s**, amely alapvet≈ë feladat olyan eszk√∂z√∂k m√∂g√∂tt, mint a Google Ford√≠t√≥. Ebben a r√©szben a g√©pi ford√≠t√°sra, vagy √°ltal√°nosabban b√°rmilyen *sorozat-sorozat* feladatra (amit **mondattranszdukci√≥nak** is neveznek) fogunk √∂sszpontos√≠tani.

Az RNN-ekkel a sorozat-sorozat feladatot k√©t rekurz√≠v h√°l√≥zat val√≥s√≠tja meg, ahol az egyik h√°l√≥zat, az **enk√≥der**, egy bemeneti sorozatot egy rejtett √°llapotba s≈±r√≠t, m√≠g a m√°sik h√°l√≥zat, a **dek√≥der**, ezt a rejtett √°llapotot egy leford√≠tott eredm√©nny√© bontja ki. Ezzel a megk√∂zel√≠t√©ssel azonban van n√©h√°ny probl√©ma:

* Az enk√≥der h√°l√≥zat v√©gs≈ë √°llapota nehezen eml√©kszik a mondat elej√©re, ami gyenge modellmin≈ës√©get eredm√©nyez hossz√∫ mondatok eset√©n.
* A sorozat minden szava azonos hat√°ssal van az eredm√©nyre. A val√≥s√°gban azonban a bemeneti sorozat bizonyos szavai gyakran nagyobb hat√°ssal vannak a kimeneti sorozatra, mint m√°sok.

A **figyelem mechanizmusok** lehet≈ës√©get adnak arra, hogy s√∫lyozzuk az egyes bemeneti vektorok kontextu√°lis hat√°s√°t az RNN kimeneti el≈ërejelz√©seire. Ez √∫gy val√≥sul meg, hogy r√∂vid√≠t√©seket hozunk l√©tre a bemeneti RNN k√∂ztes √°llapotai √©s a kimeneti RNN k√∂z√∂tt. Ily m√≥don, amikor a y<sub>t</sub> kimeneti szimb√≥lumot gener√°ljuk, figyelembe vessz√ºk az √∂sszes bemeneti rejtett √°llapotot h<sub>i</sub>, k√ºl√∂nb√∂z≈ë s√∫lyoz√°si egy√ºtthat√≥kkal &alpha;<sub>t,i</sub>.

![K√©p egy enk√≥der/dek√≥der modellr≈ël addit√≠v figyelemr√©teggel](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.hu.png)

> Az enk√≥der-dek√≥der modell addit√≠v figyelem mechanizmussal [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), id√©zve [ebb≈ël a blogbejegyz√©sb≈ël](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

A figyelem m√°trix {&alpha;<sub>i,j</sub>} azt mutatja, hogy a bemeneti szavak milyen m√©rt√©kben j√°tszanak szerepet egy adott sz√≥ gener√°l√°s√°ban a kimeneti sorozatban. Az al√°bbiakban egy ilyen m√°trix p√©ld√°j√°t l√°thatjuk:

![K√©p egy mint√°zott igaz√≠t√°sr√≥l, amelyet az RNNsearch-50 tal√°lt, Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.hu.png)

> √Åbra [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (3. √°bra)

A figyelem mechanizmusok felel≈ësek az NLP jelenlegi vagy k√∂zel jelenlegi cs√∫csteljes√≠tm√©ny√©√©rt. A figyelem hozz√°ad√°sa azonban jelent≈ësen n√∂veli a modell param√©tereinek sz√°m√°t, ami m√©retez√©si probl√©m√°khoz vezetett az RNN-ekkel. Az RNN-ek m√©retez√©s√©nek egyik kulcsfontoss√°g√∫ korl√°tja, hogy a modellek rekurz√≠v jellege megnehez√≠ti az edz√©s csoportos√≠t√°s√°t √©s p√°rhuzamos√≠t√°s√°t. Egy RNN-ben a sorozat minden elem√©t sorrendben kell feldolgozni, ami azt jelenti, hogy nem lehet k√∂nnyen p√°rhuzamos√≠tani.

![Enk√≥der Dek√≥der Figyelemmel](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> √Åbra a [Google Blogb√≥l](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

A figyelem mechanizmusok elfogad√°sa √©s ez a korl√°t vezetett a ma ismert √©s haszn√°lt cs√∫cstechnol√≥gi√°s transzformer modellek, p√©ld√°ul a BERT √©s az Open-GPT3 l√©trehoz√°s√°hoz.

## Transzformer modellek

A transzformerek egyik f≈ë √∂tlete az, hogy elker√ºlj√©k az RNN-ek szekvenci√°lis jelleg√©t, √©s olyan modellt hozzanak l√©tre, amely az edz√©s sor√°n p√°rhuzamos√≠that√≥. Ezt k√©t √∂tlet megval√≥s√≠t√°s√°val √©rik el:

* poz√≠ci√≥s k√≥dol√°s
* √∂nfigyelem mechanizmus haszn√°lata mint√°k megragad√°s√°ra RNN-ek (vagy CNN-ek) helyett (ez√©rt h√≠vj√°k a transzformereket bemutat√≥ cikket *[Attention is all you need](https://arxiv.org/abs/1706.03762)* c√≠mmel)

### Poz√≠ci√≥s k√≥dol√°s/be√°gyaz√°s

A poz√≠ci√≥s k√≥dol√°s √∂tlete a k√∂vetkez≈ë:
1. RNN-ek haszn√°latakor a tokenek relat√≠v poz√≠ci√≥j√°t a l√©p√©sek sz√°ma k√©pviseli, √≠gy azt nem kell kifejezetten √°br√°zolni.
2. Azonban, ha √°tt√©r√ºnk a figyelem mechanizmusra, tudnunk kell a tokenek relat√≠v poz√≠ci√≥j√°t a sorozaton bel√ºl.
3. A poz√≠ci√≥s k√≥dol√°s el√©r√©s√©hez a tokenek sorozat√°t kieg√©sz√≠tj√ºk a sorozatbeli tokenpoz√≠ci√≥k sorozat√°val (azaz egy 0,1, ... sz√°mok sorozat√°val).
4. Ezut√°n a token poz√≠ci√≥j√°t √∂sszekeverj√ºk egy token be√°gyaz√°si vektorral. A poz√≠ci√≥ (eg√©sz sz√°m) vektorr√° alak√≠t√°s√°hoz k√ºl√∂nb√∂z≈ë megk√∂zel√≠t√©seket alkalmazhatunk:

* Tan√≠that√≥ be√°gyaz√°s, hasonl√≥an a token be√°gyaz√°shoz. Ezt a megk√∂zel√≠t√©st vessz√ºk figyelembe itt. Be√°gyaz√°si r√©tegeket alkalmazunk mind a tokenekre, mind azok poz√≠ci√≥ira, amelyek azonos dimenzi√≥j√∫ be√°gyaz√°si vektorokat eredm√©nyeznek, amelyeket ezut√°n √∂sszeadunk.
* Fix poz√≠ci√≥s k√≥dol√≥ f√ºggv√©ny, ahogy azt az eredeti cikk javasolta.

<img src="images/pos-embedding.png" width="50%"/>

> K√©p a szerz≈ët≈ël

Az eredm√©ny, amelyet a poz√≠ci√≥s be√°gyaz√°ssal kapunk, be√°gyazza mind az eredeti tokent, mind annak poz√≠ci√≥j√°t a sorozaton bel√ºl.

### T√∂bbfej≈± √∂nfigyelem

Ezut√°n meg kell ragadnunk n√©h√°ny mint√°t a sorozatunkon bel√ºl. Ehhez a transzformerek **√∂nfigyelem** mechanizmust haszn√°lnak, amely l√©nyeg√©ben figyelem, amelyet ugyanarra a sorozatra alkalmazunk bemenetk√©nt √©s kimenetk√©nt. Az √∂nfigyelem alkalmaz√°sa lehet≈ëv√© teszi sz√°munkra, hogy figyelembe vegy√ºk a mondaton bel√ºli **kontekztust**, √©s l√°ssuk, mely szavak kapcsol√≥dnak egym√°shoz. P√©ld√°ul lehet≈ëv√© teszi sz√°munkra, hogy l√°ssuk, mely szavakra utalnak visszautal√°sok, mint p√©ld√°ul *az*, √©s figyelembe vegy√ºk a kontextust is:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.hu.png)

> K√©p a [Google Blogb√≥l](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

A transzformerekben **t√∂bbfej≈± figyelmet** haszn√°lunk annak √©rdek√©ben, hogy a h√°l√≥zat k√©pes legyen t√∂bbf√©le f√ºgg≈ës√©get megragadni, p√©ld√°ul hossz√∫ t√°v√∫ √©s r√∂vid t√°v√∫ szavak k√∂z√∂tti kapcsolatokat, visszautal√°sokat √©s m√°sokat.

A [TensorFlow Notebook](TransformersTF.ipynb) tov√°bbi r√©szleteket tartalmaz a transzformer r√©tegek megval√≥s√≠t√°s√°r√≥l.

### Enk√≥der-Dek√≥der Figyelem

A transzformerekben a figyelmet k√©t helyen haszn√°ljuk:

* Mint√°k megragad√°s√°ra a bemeneti sz√∂vegen bel√ºl √∂nfigyelem seg√≠ts√©g√©vel.
* Sorozatford√≠t√°s v√©grehajt√°s√°ra - ez az enk√≥der √©s dek√≥der k√∂z√∂tti figyelemr√©teg.

Az enk√≥der-dek√≥der figyelem nagyon hasonl√≥ az RNN-ekben haszn√°lt figyelem mechanizmushoz, ahogy azt ennek a r√©sznek az elej√©n le√≠rtuk. Ez az anim√°lt diagram magyar√°zza az enk√≥der-dek√≥der figyelem szerep√©t.

![Anim√°lt GIF, amely bemutatja, hogyan t√∂rt√©nnek az √©rt√©kel√©sek a transzformer modellekben.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Mivel minden bemeneti poz√≠ci√≥t f√ºggetlen√ºl t√©rk√©pez√ºnk a kimeneti poz√≠ci√≥khoz, a transzformerek jobban p√°rhuzamos√≠that√≥k, mint az RNN-ek, ami lehet≈ëv√© teszi sokkal nagyobb √©s kifejez≈ëbb nyelvi modellek l√©trehoz√°s√°t. Minden figyelemfej k√ºl√∂nb√∂z≈ë kapcsolatok megtanul√°s√°ra haszn√°lhat√≥ a szavak k√∂z√∂tt, ami jav√≠tja a term√©szetes nyelvfeldolgoz√°si feladatokat.

## BERT

A **BERT** (Bidirectional Encoder Representations from Transformers) egy nagyon nagy, t√∂bbr√©teg≈± transzformer h√°l√≥zat, amelynek 12 r√©tege van a *BERT-base* eset√©ben, √©s 24 a *BERT-large* eset√©ben. A modellt el≈ësz√∂r egy nagy sz√∂vegkorpuszra (WikiPedia + k√∂nyvek) tan√≠tj√°k be fel√ºgyelet n√©lk√ºli tanul√°ssal (maszkolt szavak el≈ërejelz√©se egy mondatban). Az el≈ëk√©pz√©s sor√°n a modell jelent≈ës nyelvi meg√©rt√©st szerez, amelyet m√°s adathalmazokkal finomhangol√°ssal lehet kihaszn√°lni. Ezt a folyamatot **transzfer tanul√°snak** nevezz√ºk.

![k√©p a http://jalammar.github.io/illustrated-bert/ oldalr√≥l](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.hu.png)

> K√©p [forr√°sa](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Gyakorlatok: Transzformerek

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:

* [Transzformerek PyTorch-ban](TransformersPyTorch.ipynb)
* [Transzformerek TensorFlow-ban](TransformersTF.ipynb)

## √ñsszegz√©s

Ebben a leck√©ben megismerkedt√©l a Transzformerekkel √©s a Figyelem Mechanizmusokkal, amelyek az NLP eszk√∂zt√°r√°nak alapvet≈ë elemei. Sz√°mos transzformer architekt√∫ra l√©tezik, p√©ld√°ul BERT, DistilBERT, BigBird, OpenGPT3 √©s m√©g sok m√°s, amelyek finomhangolhat√≥k. A [HuggingFace csomag](https://github.com/huggingface/) lehet≈ës√©get ny√∫jt ezeknek az architekt√∫r√°knak a betan√≠t√°s√°ra mind PyTorch, mind TensorFlow seg√≠ts√©g√©vel.

## üöÄ Kih√≠v√°s

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

* [Blogbejegyz√©s](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), amely magyar√°zza a klasszikus [Attention is all you need](https://arxiv.org/abs/1706.03762) transzformer cikket.
* [Blogbejegyz√©s-sorozat](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) a transzformerekr≈ël, amely r√©szletesen magyar√°zza az architekt√∫r√°t.

## [Feladat](assignment.md)

---

