<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e617f0b8de85a43957a853aba09bfeb",
  "translation_date": "2025-08-25T21:59:45+00:00",
  "source_file": "lessons/5-NLP/18-Transformers/README.md",
  "language_code": "hu"
}
-->
# Figyelem Mechanizmusok √©s Transzformerek

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/35)

Az NLP ter√ºlet egyik legfontosabb probl√©m√°ja a **g√©pi ford√≠t√°s**, amely alapvet≈ë feladat olyan eszk√∂z√∂k m√∂g√∂tt, mint a Google Translate. Ebben a r√©szben a g√©pi ford√≠t√°sra, vagy √°ltal√°nosabban b√°rmilyen *sorr√≥l-sorra* feladatra (amit **mondattranszdukci√≥nak** is neveznek) fogunk √∂sszpontos√≠tani.

Az RNN-ekkel a sorr√≥l-sorra megval√≥s√≠t√°s k√©t rekurrens h√°l√≥zaton kereszt√ºl t√∂rt√©nik, ahol az egyik h√°l√≥zat, az **enk√≥der**, egy bemeneti sorozatot egy rejtett √°llapotba t√∂m√∂r√≠t, m√≠g a m√°sik h√°l√≥zat, a **dek√≥der**, ezt a rejtett √°llapotot egy leford√≠tott eredm√©nny√© bontja ki. Ezzel a megk√∂zel√≠t√©ssel azonban van n√©h√°ny probl√©ma:

* Az enk√≥der h√°l√≥zat v√©gs≈ë √°llapota nehezen eml√©kszik a mondat elej√©re, ami gyenge modellmin≈ës√©get eredm√©nyez hossz√∫ mondatok eset√©n.
* A sorozat minden szava ugyanolyan hat√°ssal van az eredm√©nyre. A val√≥s√°gban azonban a bemeneti sorozat bizonyos szavai gyakran nagyobb hat√°ssal vannak a kimeneti sorozatra, mint m√°sok.

A **Figyelem Mechanizmusok** lehet≈ës√©get ny√∫jtanak arra, hogy s√∫lyozzuk az egyes bemeneti vektorok kontextu√°lis hat√°s√°t az RNN kimeneti el≈ërejelz√©seire. Ez √∫gy val√≥sul meg, hogy r√∂vid√≠t√©seket hozunk l√©tre a bemeneti RNN k√∂ztes √°llapotai √©s a kimeneti RNN k√∂z√∂tt. Ily m√≥don, amikor a y<sub>t</sub> kimeneti szimb√≥lumot gener√°ljuk, figyelembe vessz√ºk az √∂sszes bemeneti rejtett √°llapotot h<sub>i</sub>, k√ºl√∂nb√∂z≈ë s√∫lyoz√°si egy√ºtthat√≥kkal Œ±<sub>t,i</sub>.

![K√©p egy enk√≥der/dek√≥der modellr≈ël addit√≠v figyelem r√©teggel](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.hu.png)

> Az enk√≥der-dek√≥der modell addit√≠v figyelem mechanizmussal [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), id√©zve [ebb≈ël a blogbejegyz√©sb≈ël](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

A figyelem m√°trix {Œ±<sub>i,j</sub>} azt reprezent√°lja, hogy bizonyos bemeneti szavak milyen m√©rt√©kben j√°tszanak szerepet egy adott sz√≥ gener√°l√°s√°ban a kimeneti sorozatban. Az al√°bbiakban egy ilyen m√°trix p√©ld√°j√°t l√°thatjuk:

![K√©p egy mint√°zott igaz√≠t√°sr√≥l, amelyet az RNNsearch-50 tal√°lt, Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.hu.png)

> √Åbra [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (3. √°bra)

A figyelem mechanizmusok felel≈ësek az NLP jelenlegi vagy k√∂zel jelenlegi cs√∫cstechnol√≥gi√°j√°√©rt. A figyelem hozz√°ad√°sa azonban jelent≈ësen n√∂veli a modell param√©tereinek sz√°m√°t, ami m√©retez√©si probl√©m√°kat okozott az RNN-ekkel. Az RNN-ek m√©retez√©s√©nek egyik kulcsfontoss√°g√∫ korl√°tja, hogy a modellek rekurrens term√©szete megnehez√≠ti a tan√≠t√°s p√°rhuzamos√≠t√°s√°t √©s csoportos√≠t√°s√°t. Az RNN-ben a sorozat minden elem√©t sorrendben kell feldolgozni, ami azt jelenti, hogy nem lehet k√∂nnyen p√°rhuzamos√≠tani.

![Enk√≥der Dek√≥der Figyelemmel](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> √Åbra a [Google Blogb√≥l](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

A figyelem mechanizmusok elfogad√°sa √©s ez a korl√°t vezetett a ma ismert √©s haszn√°lt cs√∫cstechnol√≥gi√°s Transzformer Modellek l√©trehoz√°s√°hoz, mint p√©ld√°ul a BERT √©s az Open-GPT3.

## Transzformer modellek

A transzformerek egyik f≈ë √∂tlete az RNN-ek szekvenci√°lis jelleg√©nek elker√ºl√©se, √©s egy olyan modell l√©trehoz√°sa, amely p√°rhuzamos√≠that√≥ a tan√≠t√°s sor√°n. Ezt k√©t √∂tlet megval√≥s√≠t√°s√°val √©rik el:

* poz√≠ci√≥s k√≥dol√°s
* √∂nfigyelem mechanizmus haszn√°lata mint√°k megragad√°s√°ra RNN-ek (vagy CNN-ek) helyett (ez√©rt h√≠vj√°k a transzformereket bemutat√≥ cikket *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Poz√≠ci√≥s K√≥dol√°s/Be√°gyaz√°s

A poz√≠ci√≥s k√≥dol√°s √∂tlete a k√∂vetkez≈ë:
1. Az RNN-ek haszn√°latakor a tokenek relat√≠v poz√≠ci√≥j√°t a l√©p√©sek sz√°ma k√©pviseli, √≠gy nem kell kifejezetten reprezent√°lni.
2. Azonban, amikor √°tt√©r√ºnk a figyelemre, tudnunk kell a tokenek relat√≠v poz√≠ci√≥j√°t a sorozaton bel√ºl.
3. A poz√≠ci√≥s k√≥dol√°s el√©r√©s√©hez a tokenek sorozat√°t kieg√©sz√≠tj√ºk a tokenek poz√≠ci√≥inak sorozat√°val a sorozatban (azaz egy 0,1,... sz√°mok sorozat√°val).
4. Ezut√°n a token poz√≠ci√≥j√°t √∂sszekeverj√ºk egy token be√°gyaz√°si vektorral. A poz√≠ci√≥ (eg√©sz sz√°m) vektorr√° alak√≠t√°s√°hoz k√ºl√∂nb√∂z≈ë megk√∂zel√≠t√©seket alkalmazhatunk:

* Tan√≠that√≥ be√°gyaz√°s, hasonl√≥an a token be√°gyaz√°shoz. Ez az itt t√°rgyalt megk√∂zel√≠t√©s. Be√°gyaz√°si r√©tegeket alkalmazunk mind a tokenekre, mind azok poz√≠ci√≥ira, √≠gy azonos dimenzi√≥j√∫ be√°gyaz√°si vektorokat kapunk, amelyeket √∂sszeadunk.
* Fix poz√≠ci√≥s k√≥dol√°si f√ºggv√©ny, ahogy az eredeti cikk javasolja.

<img src="images/pos-embedding.png" width="50%"/>

> K√©p a szerz≈ët≈ël

Az eredm√©ny, amit a poz√≠ci√≥s be√°gyaz√°ssal kapunk, be√°gyazza az eredeti tokent √©s annak poz√≠ci√≥j√°t a sorozaton bel√ºl.

### T√∂bbfej≈± √ñnsfigyelem

Ezut√°n mint√°kat kell megragadnunk a sorozaton bel√ºl. Ehhez a transzformerek **√∂nfigyelem** mechanizmust haszn√°lnak, amely l√©nyeg√©ben figyelem, amelyet ugyanarra a sorozatra alkalmaznak bemenetk√©nt √©s kimenetk√©nt. Az √∂nfigyelem alkalmaz√°sa lehet≈ëv√© teszi sz√°munkra, hogy figyelembe vegy√ºk a mondaton bel√ºli **kontekztust**, √©s l√°ssuk, mely szavak kapcsol√≥dnak egym√°shoz. P√©ld√°ul lehet≈ëv√© teszi sz√°munkra, hogy l√°ssuk, mely szavakra utalnak korreferenci√°k, mint p√©ld√°ul *az*, √©s figyelembe vegy√ºk a kontextust:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.hu.png)

> K√©p a [Google Blogb√≥l](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

A transzformerekben **T√∂bbfej≈± Figyelmet** haszn√°lunk annak √©rdek√©ben, hogy a h√°l√≥zat k√©pes legyen k√ºl√∂nb√∂z≈ë t√≠pus√∫ f√ºgg≈ës√©geket megragadni, p√©ld√°ul hossz√∫ t√°v√∫ vs. r√∂vid t√°v√∫ szavak kapcsolatait, korreferenci√°t vs. valami m√°st stb.

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) tov√°bbi r√©szleteket tartalmaz a transzformer r√©tegek megval√≥s√≠t√°s√°r√≥l.

### Enk√≥der-Dek√≥der Figyelem

A transzformerekben a figyelmet k√©t helyen haszn√°lj√°k:

* Mint√°k megragad√°s√°ra a bemeneti sz√∂vegen bel√ºl √∂nfigyelem seg√≠ts√©g√©vel
* Sorozatford√≠t√°s v√©grehajt√°s√°ra - ez az enk√≥der √©s dek√≥der k√∂z√∂tti figyelem r√©teg.

Az enk√≥der-dek√≥der figyelem nagyon hasonl√≥ az RNN-ekben haszn√°lt figyelem mechanizmushoz, ahogy a szakasz elej√©n le√≠rtuk. Ez az anim√°lt diagram magyar√°zza az enk√≥der-dek√≥der figyelem szerep√©t.

![Anim√°lt GIF, amely bemutatja, hogyan t√∂rt√©nnek az √©rt√©kel√©sek a transzformer modellekben.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Mivel minden bemeneti poz√≠ci√≥t f√ºggetlen√ºl t√©rk√©pez√ºnk minden kimeneti poz√≠ci√≥hoz, a transzformerek jobban p√°rhuzamos√≠that√≥k, mint az RNN-ek, ami lehet≈ëv√© teszi sokkal nagyobb √©s kifejez≈ëbb nyelvi modellek l√©trehoz√°s√°t. Minden figyelem fej k√ºl√∂nb√∂z≈ë szavak k√∂z√∂tti kapcsolatok tanul√°s√°ra haszn√°lhat√≥, ami jav√≠tja az NLP feladatok eredm√©nyeit.

## BERT

A **BERT** (Bidirectional Encoder Representations from Transformers) egy nagyon nagy, t√∂bbr√©teg≈± transzformer h√°l√≥zat, amelynek 12 r√©tege van a *BERT-base*-hez, √©s 24 a *BERT-large*-hoz. A modellt el≈ësz√∂r egy nagy sz√∂vegkorpuszra (WikiPedia + k√∂nyvek) tan√≠tj√°k be fel√ºgyelet n√©lk√ºli tan√≠t√°ssal (maszkolt szavak el≈ërejelz√©se egy mondatban). Az el≈ëtan√≠t√°s sor√°n a modell jelent≈ës szint≈± nyelvi meg√©rt√©st sz√≠v mag√°ba, amelyet k√©s≈ëbb m√°s adathalmazokkal finomhangolva lehet hasznos√≠tani. Ezt a folyamatot **transzfer tanul√°snak** nevezz√ºk.

![k√©p a http://jalammar.github.io/illustrated-bert/ oldalr√≥l](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.hu.png)

> K√©p [forr√°sa](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Gyakorlatok: Transzformerek

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:

* [Transzformerek PyTorch-ban](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transzformerek TensorFlow-ban](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## √ñsszegz√©s

Ebben a leck√©ben megismerkedt√©l a Transzformerekkel √©s Figyelem Mechanizmusokkal, amelyek az NLP eszk√∂zt√°r√°nak alapvet≈ë elemei. Sz√°mos Transzformer architekt√∫ra l√©tezik, bele√©rtve a BERT-et, DistilBERT-et, BigBird-et, OpenGPT3-at √©s m√©g sok m√°st, amelyeket finomhangolni lehet. A [HuggingFace csomag](https://github.com/huggingface/) lehet≈ës√©get ny√∫jt sok ilyen architekt√∫ra tan√≠t√°s√°ra PyTorch √©s TensorFlow seg√≠ts√©g√©vel.

## üöÄ Kih√≠v√°s

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/36)

## √Åttekint√©s √©s √ñn√°ll√≥ Tanul√°s

* [Blogbejegyz√©s](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), amely magyar√°zza a klasszikus [Attention is all you need](https://arxiv.org/abs/1706.03762) cikket a transzformerekr≈ël.
* [Blogbejegyz√©s-sorozat](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) a transzformerekr≈ël, amely r√©szletesen magyar√°zza az architekt√∫r√°t.

## [Feladat](assignment.md)

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.