{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beágyazások\n",
    "\n",
    "Az előző példánkban nagy dimenziójú bag-of-words vektorokkal dolgoztunk, amelyek hossza `vocab_size`, és kifejezetten alacsony dimenziójú pozíciós reprezentációs vektorokból alakítottuk át őket ritka one-hot reprezentációvá. Ez a one-hot reprezentáció nem memóriahatékony, ráadásul minden szót egymástól függetlenül kezel, azaz a one-hot kódolt vektorok nem fejeznek ki semmilyen szemantikai hasonlóságot a szavak között.\n",
    "\n",
    "Ebben az egységben továbbra is a **News AG** adathalmazt fogjuk vizsgálni. Kezdjük azzal, hogy betöltjük az adatokat, és előhívunk néhány definíciót az előző jegyzetfüzetből.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\train.csv: 29.5MB [00:01, 18.8MB/s]                            \n",
      "d:\\WORK\\ai-for-beginners\\5-NLP\\14-Embeddings\\data\\test.csv: 1.86MB [00:00, 11.2MB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Vocab size =  95812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi az az embedding?\n",
    "\n",
    "Az **embedding** ötlete az, hogy a szavakat alacsonyabb dimenziós, sűrű vektorokkal reprezentáljuk, amelyek valamilyen módon tükrözik a szó szemantikai jelentését. Később megbeszéljük, hogyan lehet értelmes szóbeágyazásokat létrehozni, de egyelőre gondoljunk az embeddingre úgy, mint egy módszerre a szóvektor dimenziójának csökkentésére.\n",
    "\n",
    "Az embedding réteg tehát egy szót kap bemenetként, és egy meghatározott `embedding_size` méretű kimeneti vektort állít elő. Bizonyos értelemben nagyon hasonló a `Linear` réteghez, de ahelyett, hogy egy one-hot kódolt vektort venne, képes lesz egy szó számát bemenetként fogadni.\n",
    "\n",
    "Ha az embedding réteget használjuk hálózatunk első rétegeként, akkor átállhatunk a bag-of-words modellről az **embedding bag** modellre. Ebben először minden szót a szövegünkben a megfelelő embeddingre konvertálunk, majd valamilyen aggregáló függvényt számítunk ki az összes embedding felett, például `sum`, `average` vagy `max`.\n",
    "\n",
    "![Kép, amely egy embedding osztályozót mutat öt szekvencia szóra.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.hu.png)\n",
    "\n",
    "Az osztályozó neurális hálózatunk embedding réteggel kezdődik, majd egy aggregáló réteggel, és végül egy lineáris osztályozóval a tetején:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A változó szekvenciahossz kezelése\n",
    "\n",
    "Ennek az architektúrának az eredményeként a hálózatunkhoz tartozó minibatch-eket egy bizonyos módon kell létrehozni. Az előző egységben, amikor bag-of-words-t (BoW) használtunk, minden BoW tenzor a minibatch-ben azonos méretű volt, `vocab_size`, függetlenül a szövegszekvencia tényleges hosszától. Amint áttérünk a szóbeágyazásokra, minden szövegmintában változó számú szó lesz, és amikor ezeket a mintákat minibatch-ekbe kombináljuk, némi kitöltést (padding) kell alkalmaznunk.\n",
    "\n",
    "Ezt úgy érhetjük el, hogy a datasource-hoz egy `collate_fn` függvényt biztosítunk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beágyazási osztályozó tanítása\n",
    "\n",
    "Most, hogy megfelelő adatbetöltőt definiáltunk, a korábbi egységben meghatározott tanítási függvénnyel elkezdhetjük a modell tanítását:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6415625\n",
      "6400: acc=0.6865625\n",
      "9600: acc=0.7103125\n",
      "12800: acc=0.726953125\n",
      "16000: acc=0.739375\n",
      "19200: acc=0.75046875\n",
      "22400: acc=0.7572321428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.889799795315499, 0.7623160588611644)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Megjegyzés**: Itt csak 25 ezer rekordot tanítunk (kevesebb, mint egy teljes epoch) az időtakarékosság érdekében, de folytathatja a tanítást, írhat egy függvényt több epoch tanítására, és kísérletezhet a tanulási ráta paraméterével a nagyobb pontosság elérése érdekében. Körülbelül 90%-os pontosságot kell tudnia elérni.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBag réteg és változó hosszúságú szekvenciák reprezentációja\n",
    "\n",
    "Az előző architektúrában minden szekvenciát ugyanarra a hosszra kellett kiegészíteni, hogy illeszkedjenek egy minibatch-be. Ez nem a leghatékonyabb módja a változó hosszúságú szekvenciák reprezentálásának – egy másik megközelítés az **offset** vektor használata, amely egy nagy vektorban tárolt összes szekvencia eltolásait tartalmazza.\n",
    "\n",
    "![Kép, amely egy offset szekvencia reprezentációt mutat](../../../../../translated_images/offset-sequence-representation.eb73fcefb29b46eecfbe74466077cfeb7c0f93a4f254850538a2efbc63517479.hu.png)\n",
    "\n",
    "> **Note**: A fenti képen karakterek szekvenciáját mutatjuk, de példánkban szavak szekvenciáival dolgozunk. Azonban a szekvenciák offset vektorral történő reprezentálásának általános elve ugyanaz marad.\n",
    "\n",
    "Az offset reprezentációval való munkához az [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html) réteget használjuk. Ez hasonló az `Embedding`-hez, de tartalomvektort és offset vektort vesz bemenetként, és tartalmaz egy átlagoló réteget is, amely lehet `mean`, `sum` vagy `max`.\n",
    "\n",
    "Íme egy módosított hálózat, amely az `EmbeddingBag`-et használja:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tanulási adathalmaz előkészítéséhez egy átalakító függvényt kell biztosítanunk, amely előkészíti az eltolási vektort:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Megjegyzés: az eddigi példáktól eltérően a hálózatunk most két paramétert fogad: adatvektort és eltolásvektort, amelyek különböző méretűek. Hasonlóképpen, az adatbetöltőnk is 3 értéket ad vissza 2 helyett: mind az adat-, mind az eltolásvektorokat jellemzőként biztosítja. Ezért kissé módosítanunk kell a tanítási függvényünket, hogy ezt kezelni tudjuk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6153125\n",
      "6400: acc=0.6615625\n",
      "9600: acc=0.6932291666666667\n",
      "12800: acc=0.715078125\n",
      "16000: acc=0.7270625\n",
      "19200: acc=0.7382291666666667\n",
      "22400: acc=0.7486160714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22.771553103007037, 0.7551983365323096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Szemantikus Beágyazások: Word2Vec\n",
    "\n",
    "Az előző példánkban a modell beágyazási rétege megtanulta a szavakat vektoriális reprezentációvá alakítani, azonban ez a reprezentáció nem hordozott sok szemantikai jelentést. Jó lenne olyan vektoriális reprezentációt tanulni, amelyben a hasonló szavak vagy szinonimák olyan vektoroknak felelnek meg, amelyek valamilyen vektortávolság (pl. euklideszi távolság) szempontjából közel vannak egymáshoz.\n",
    "\n",
    "Ehhez először egy nagy szöveggyűjteményen kell előre betanítanunk a beágyazási modellünket egy specifikus módon. Az egyik első módszer a szemantikus beágyazások tanítására a [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ez két fő architektúrán alapul, amelyeket a szavak elosztott reprezentációjának előállítására használnak:\n",
    "\n",
    " - **Folytonos szótáska** (CBoW) — ebben az architektúrában a modellt arra tanítjuk, hogy egy szót megjósoljon a környező kontextusból. Az $(W_{-2},W_{-1},W_0,W_1,W_2)$ ngram esetén a modell célja, hogy $W_0$-t megjósolja $(W_{-2},W_{-1},W_1,W_2)$ alapján.\n",
    " - **Folytonos skip-gram** a CBoW ellentéte. A modell a környező kontextusszavak ablakát használja a jelenlegi szó megjóslására.\n",
    "\n",
    "A CBoW gyorsabb, míg a skip-gram lassabb, de jobban reprezentálja a ritkábban előforduló szavakat.\n",
    "\n",
    "![Kép, amely a CBoW és a Skip-Gram algoritmusokat mutatja be a szavak vektorokká alakításához.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.hu.png)\n",
    "\n",
    "Ahhoz, hogy kísérletezzünk a Google News adathalmazon előre betanított word2vec beágyazással, használhatjuk a **gensim** könyvtárat. Az alábbiakban megkeressük a 'neural' szóhoz leginkább hasonló szavakat.\n",
    "\n",
    "> **Note:** Amikor először hozunk létre szóvektorokat, a letöltésük eltarthat egy ideig!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.7804799675941467\n",
      "neurons -> 0.7326500415802002\n",
      "neural_circuits -> 0.7252851724624634\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923246383666992\n",
      "synaptic -> 0.6699118614196777\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314064025879\n",
      "neuronal_activity -> 0.6531826257705688\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Számíthatunk vektorábrázolásokat is a szóból, amelyeket osztályozási modell tanításához használhatunk (a vektor első 20 komponensét mutatjuk csak a tisztánlátás érdekében):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A szemantikai beágyazások nagyszerűsége abban rejlik, hogy a vektorkódolást manipulálva megváltoztathatjuk a szemantikát. Például kérhetjük, hogy találjunk egy szót, amelynek vektorképviselete a lehető legközelebb áll a *király* és *nő* szavakhoz, és a lehető legtávolabb a *férfi* szótól:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118192911148071)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind a CBoW, mind a Skip-Gram „prediktív” beágyazások, mivel csak a helyi kontextusokat veszik figyelembe. A Word2Vec nem használja ki a globális kontextust.\n",
    "\n",
    "A **FastText** a Word2Vec-re épít, azáltal, hogy minden szóhoz és az egyes szavakban található karakter n-gramokhoz vektorreprezentációkat tanul. Ezeket a reprezentációkat minden tanítási lépésnél átlagolják egyetlen vektorrá. Bár ez jelentősen megnöveli az előképzés számítási igényét, lehetővé teszi, hogy a szóbeágyazások kódolják az al-szó információkat.\n",
    "\n",
    "Egy másik módszer, a **GloVe**, a társ-előfordulási mátrix ötletét használja ki, és neurális módszerekkel bontja le a társ-előfordulási mátrixot kifejezőbb és nemlineáris szóvektorokra.\n",
    "\n",
    "Kipróbálhatod a példát azzal, hogy az embeddingeket FastText-re vagy GloVe-ra változtatod, mivel a gensim több különböző szóbeágyazási modellt is támogat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Előre betanított beágyazások használata PyTorch-ban\n",
    "\n",
    "Az előző példát módosíthatjuk úgy, hogy az embedding réteg mátrixát szemantikai beágyazásokkal, például Word2Vec-kel töltsük fel. Figyelembe kell vennünk, hogy az előre betanított beágyazás és a szövegkorpusz szókészlete valószínűleg nem fog egyezni, ezért a hiányzó szavak súlyait véletlenszerű értékekkel inicializáljuk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.get_itos()):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most már képezzük a modellünket. Vegyük figyelembe, hogy a modell betanításához szükséges idő jelentősen hosszabb, mint az előző példában, mivel a beágyazási réteg mérete nagyobb, és így a paraméterek száma is sokkal magasabb. Emiatt előfordulhat, hogy több példán kell betanítanunk a modellünket, ha el akarjuk kerülni a túltanulást.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6359375\n",
      "6400: acc=0.68109375\n",
      "9600: acc=0.7067708333333333\n",
      "12800: acc=0.723671875\n",
      "16000: acc=0.73625\n",
      "19200: acc=0.7463541666666667\n",
      "22400: acc=0.7560714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214.1013875559821, 0.7626759436980166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az esetünkben nem tapasztalunk jelentős növekedést a pontosságban, ami valószínűleg a nagyon eltérő szókészleteknek köszönhető.  \n",
    "A különböző szókészletek problémájának megoldására az alábbi megoldások egyikét alkalmazhatjuk:  \n",
    "* Újra betaníthatjuk a word2vec modellt a saját szókészletünkre  \n",
    "* Betölthetjük az adatállományunkat a már előre betanított word2vec modell szókészletével. Az adatállomány betöltéséhez használt szókészletet a betöltés során megadhatjuk.  \n",
    "\n",
    "Az utóbbi megközelítés egyszerűbbnek tűnik, különösen azért, mert a PyTorch `torchtext` keretrendszer beépített támogatást nyújt az embeddingekhez. Például, GloVe-alapú szókészletet a következő módon hozhatunk létre:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25411.14it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A betöltött szókincs a következő alapműveleteket tartalmazza:\n",
    "* A `vocab.stoi` szótár lehetővé teszi, hogy egy szót a szótári indexévé alakítsunk.\n",
    "* A `vocab.itos` az ellenkezőjét végzi - számot alakít vissza szóvá.\n",
    "* A `vocab.vectors` az embedding vektorok tömbje, így egy szó `s` embeddingjének megszerzéséhez a `vocab.vectors[vocab.stoi[s]]`-t kell használnunk.\n",
    "\n",
    "Íme egy példa az embeddingek manipulálására, amely bemutatja az egyenletet **kind-man+woman = queen** (kicsit módosítanom kellett az együtthatót, hogy működjön):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahhoz, hogy betanítsuk az osztályozót ezekkel a beágyazásokkal, először kódolnunk kell az adatainkat a GloVe szókészlet segítségével:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amint fentebb láttuk, az összes vektorbeágyazás a `vocab.vectors` mátrixban van tárolva. Ez rendkívül egyszerűvé teszi ezeknek a súlyoknak az átmásolását az embedding réteg súlyaiba egyszerű másolással:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most edzük a modellünket, és nézzük meg, hogy jobb eredményeket kapunk-e:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.6271875\n",
      "6400: acc=0.68078125\n",
      "9600: acc=0.7030208333333333\n",
      "12800: acc=0.71984375\n",
      "16000: acc=0.7346875\n",
      "19200: acc=0.7455729166666667\n",
      "22400: acc=0.7529464285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.53972978646833, 0.7575175943698017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az egyik oka annak, hogy nem tapasztalunk jelentős növekedést a pontosságban, az az, hogy az adatállományunkból néhány szó hiányzik az előre betanított GloVe szókészletből, és így lényegében figyelmen kívül maradnak. Ennek kiküszöbölésére saját beágyazásokat taníthatunk az adatállományunkon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontextuális Beágyazások\n",
    "\n",
    "A hagyományos, előre betanított beágyazási reprezentációk, mint például a Word2Vec, egyik fő korlátja a szavak jelentésének egyértelműsítése. Bár az előre betanított beágyazások képesek valamennyire megragadni a szavak jelentését a kontextusban, egy szó minden lehetséges jelentését ugyanabba a beágyazásba kódolják. Ez problémákat okozhat az utófeldolgozó modellekben, mivel sok szó, például a 'play' szó, különböző jelentéssel bírhat attól függően, hogy milyen kontextusban használják.\n",
    "\n",
    "Például a 'play' szó az alábbi két mondatban egészen eltérő jelentéssel bír:\n",
    "- Elmentem egy **színdarabra** a színházba.\n",
    "- John játszani szeretne a barátaival.\n",
    "\n",
    "A fenti előre betanított beágyazások mindkét jelentést ugyanabba a beágyazásba sűrítik. Ennek a korlátnak a leküzdéséhez olyan beágyazásokat kell létrehoznunk, amelyek a **nyelvi modellen** alapulnak. Ez a modell egy nagy szövegkorpuszra van betanítva, és *tudja*, hogyan illeszkednek a szavak különböző kontextusokban. A kontextuális beágyazások részletes tárgyalása nem része ennek az oktatóanyagnak, de visszatérünk rájuk, amikor a nyelvi modellekről beszélünk a következő egységben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Felelősségkizárás**:  \nEz a dokumentum az [Co-op Translator](https://github.com/Azure/co-op-translator) AI fordítási szolgáltatás segítségével készült. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt professzionális, emberi fordítást igénybe venni. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy téves értelmezésekért.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
  },
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "coopTranslator": {
   "original_hash": "f50b026abce5cf36783a560ea72cb9b1",
   "translation_date": "2025-08-29T16:34:28+00:00",
   "source_file": "lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}