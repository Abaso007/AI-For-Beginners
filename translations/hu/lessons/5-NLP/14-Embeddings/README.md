<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b708c9b85b833864c73c6281f1e6b96e",
  "translation_date": "2025-09-23T11:19:51+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "hu"
}
-->
# Be√°gyaz√°sok

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/27)

Amikor BoW vagy TF/IDF alap√∫ oszt√°lyoz√≥kat tan√≠tottunk, magas dimenzi√≥j√∫ szavak zs√°kja vektorokkal dolgoztunk, amelyek hossza `vocab_size`, √©s kifejezetten alacsony dimenzi√≥j√∫ poz√≠ci√≥s reprezent√°ci√≥s vektorokat alak√≠tottunk √°t ritka egyforr√≥ (one-hot) reprezent√°ci√≥v√°. Ez az egyforr√≥ reprezent√°ci√≥ azonban nem mem√≥riahat√©kony. Ezenk√≠v√ºl minden sz√≥t egym√°st√≥l f√ºggetlen√ºl kezel√ºnk, azaz az egyforr√≥ k√≥dolt vektorok nem fejeznek ki semmilyen szemantikai hasonl√≥s√°got a szavak k√∂z√∂tt.

A **be√°gyaz√°s** √∂tlete az, hogy a szavakat alacsonyabb dimenzi√≥j√∫, s≈±r≈± vektorokkal reprezent√°ljuk, amelyek valamilyen m√≥don t√ºkr√∂zik a sz√≥ szemantikai jelent√©s√©t. K√©s≈ëbb megbesz√©lj√ºk, hogyan lehet √©rtelmes sz√≥be√°gyaz√°sokat l√©trehozni, de egyel≈ëre gondoljunk a be√°gyaz√°sokra √∫gy, mint egy m√≥dra, hogy cs√∂kkents√ºk a sz√≥vektorok dimenzi√≥j√°t.

A be√°gyaz√°si r√©teg teh√°t egy sz√≥t vesz bemenetk√©nt, √©s egy meghat√°rozott `embedding_size` m√©ret≈± kimeneti vektort √°ll√≠t el≈ë. Bizonyos √©rtelemben nagyon hasonl√≥ egy `Linear` r√©teghez, de az egyforr√≥ k√≥dolt vektor helyett k√©pes egy sz√≥ sz√°m√°t bemenetk√©nt fogadni, √≠gy elker√ºlhetj√ºk a nagy egyforr√≥ k√≥dolt vektorok l√©trehoz√°s√°t.

Ha a be√°gyaz√°si r√©teget haszn√°ljuk oszt√°lyoz√≥ h√°l√≥zatunk els≈ë r√©tegek√©nt, akkor a szavak zs√°kja modellr≈ël √°tt√©rhet√ºnk az **embedding bag** modellre, ahol el≈ësz√∂r minden sz√≥t a megfelel≈ë be√°gyaz√°sra alak√≠tunk, majd valamilyen aggreg√°lt f√ºggv√©nyt sz√°m√≠tunk ki az √∂sszes be√°gyaz√°s felett, p√©ld√°ul `sum`, `average` vagy `max`.

![K√©p, amely egy be√°gyaz√°si oszt√°lyoz√≥t mutat √∂t szekvencia sz√≥ra.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.hu.png)

> K√©p a szerz≈ët≈ël

## ‚úçÔ∏è Gyakorlatok: Be√°gyaz√°sok

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:
* [Be√°gyaz√°sok PyTorch seg√≠ts√©g√©vel](EmbeddingsPyTorch.ipynb)
* [Be√°gyaz√°sok TensorFlow-val](EmbeddingsTF.ipynb)

## Szemantikai be√°gyaz√°sok: Word2Vec

M√≠g a be√°gyaz√°si r√©teg megtanulta a szavakat vektorreprezent√°ci√≥ra lek√©pezni, ez a reprezent√°ci√≥ nem felt√©tlen√ºl hordozott sok szemantikai jelent√©st. J√≥ lenne olyan vektorreprezent√°ci√≥t tanulni, amelyben a hasonl√≥ szavak vagy szinonim√°k olyan vektoroknak felelnek meg, amelyek k√∂zel vannak egym√°shoz valamilyen vektort√°vols√°g (pl. euklideszi t√°vols√°g) szempontj√°b√≥l.

Ehhez el≈ë kell tan√≠tanunk a be√°gyaz√°si modell√ºnket egy nagy sz√∂veggy≈±jtem√©nyen egy speci√°lis m√≥don. Az egyik m√≥dszer a szemantikai be√°gyaz√°sok tan√≠t√°s√°ra a [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ez k√©t f≈ë architekt√∫r√°n alapul, amelyek a szavak elosztott reprezent√°ci√≥j√°t √°ll√≠tj√°k el≈ë:

 - **Folytonos szavak zs√°kja** (CBoW) ‚Äî ebben az architekt√∫r√°ban a modellt arra tan√≠tjuk, hogy egy sz√≥t a k√∂rnyez≈ë kontextusb√≥l j√≥soljon meg. Az ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ eset√©n a modell c√©lja, hogy $W_0$-t megj√≥solja $(W_{-2},W_{-1},W_1,W_2)$ alapj√°n.
 - **Folytonos skip-gram** ‚Äî ez a CBoW ellent√©te. A modell a k√∂rnyez≈ë kontextusszavak ablak√°t haszn√°lja a jelenlegi sz√≥ megj√≥sl√°s√°ra.

A CBoW gyorsabb, m√≠g a skip-gram lassabb, de jobban reprezent√°lja a ritka szavakat.

![K√©p, amely a CBoW √©s Skip-Gram algoritmusokat mutatja a szavak vektorokk√° alak√≠t√°s√°hoz.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.hu.png)

> K√©p ebb≈ël a [tanulm√°nyb√≥l](https://arxiv.org/pdf/1301.3781.pdf)

A Word2Vec el≈ëtan√≠tott be√°gyaz√°sok (√©s m√°s hasonl√≥ modellek, mint p√©ld√°ul a GloVe) szint√©n haszn√°lhat√≥k a neur√°lis h√°l√≥zatok be√°gyaz√°si r√©tege helyett. Azonban foglalkoznunk kell a sz√≥k√©szletekkel, mivel a Word2Vec/GloVe el≈ëtan√≠t√°s√°hoz haszn√°lt sz√≥k√©szlet val√≥sz√≠n≈±leg elt√©r a sz√∂vegkorpuszunk sz√≥k√©szlet√©t≈ël. N√©zd meg a fenti jegyzetf√ºzeteket, hogy l√°sd, hogyan oldhat√≥ meg ez a probl√©ma.

## Kontextu√°lis be√°gyaz√°sok

A hagyom√°nyos el≈ëtan√≠tott be√°gyaz√°si reprezent√°ci√≥k, mint p√©ld√°ul a Word2Vec, egyik kulcsfontoss√°g√∫ korl√°tja a szavak jelent√©s√©nek egy√©rtelm≈±s√≠t√©se. B√°r az el≈ëtan√≠tott be√°gyaz√°sok k√©pesek bizonyos m√©rt√©kben megragadni a szavak jelent√©s√©t a kontextusban, minden sz√≥ lehets√©ges jelent√©s√©t ugyanabba a be√°gyaz√°sba k√≥dolj√°k. Ez probl√©m√°kat okozhat a k√©s≈ëbbi modellekben, mivel sok sz√≥, p√©ld√°ul a 'play', k√ºl√∂nb√∂z≈ë jelent√©sekkel b√≠r att√≥l f√ºgg≈ëen, hogy milyen kontextusban haszn√°lj√°k.

P√©ld√°ul a 'play' sz√≥ az al√°bbi k√©t mondatban eg√©szen elt√©r≈ë jelent√©ssel b√≠r:

- Elmentem egy **sz√≠ndarabra** a sz√≠nh√°zban.
- John j√°tszani szeretne a bar√°taival.

A fent eml√≠tett el≈ëtan√≠tott be√°gyaz√°sok mindk√©t jelent√©s√©t ugyanabba a be√°gyaz√°sba s≈±r√≠tik. Ennek a korl√°tnak a lek√ºzd√©s√©hez olyan be√°gyaz√°sokat kell √©p√≠ten√ºnk, amelyek a **nyelvi modellre** alapoznak, amelyet egy nagy sz√∂vegkorpuszra tan√≠tottak, √©s *tudja*, hogyan illeszkednek a szavak k√ºl√∂nb√∂z≈ë kontextusokban. A kontextu√°lis be√°gyaz√°sok t√°rgyal√°sa t√∫lmutat ennek az oktat√≥anyagnak a keretein, de k√©s≈ëbb, a nyelvi modellekr≈ël sz√≥l√≥ r√©szben visszat√©r√ºnk r√°juk.

## √ñsszegz√©s

Ebben a leck√©ben megtanultad, hogyan √©p√≠ts √©s haszn√°lj be√°gyaz√°si r√©tegeket TensorFlow-ban √©s PyTorch-ban, hogy jobban t√ºkr√∂zd a szavak szemantikai jelent√©s√©t.

## üöÄ Kih√≠v√°s

A Word2Vec-et n√©h√°ny √©rdekes alkalmaz√°sra is haszn√°lt√°k, p√©ld√°ul dalsz√∂vegek √©s k√∂lt√©szet gener√°l√°s√°ra. N√©zd meg [ezt a cikket](https://www.politetype.com/blog/word2vec-color-poems), amely bemutatja, hogyan haszn√°lta a szerz≈ë a Word2Vec-et k√∂lt√©szet gener√°l√°s√°ra. N√©zd meg [ezt a vide√≥t Dan Shiffmannt√≥l](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) is, hogy egy m√°sik magyar√°zatot kapj err≈ël a technik√°r√≥l. Ezut√°n pr√≥b√°ld meg alkalmazni ezeket a technik√°kat a saj√°t sz√∂vegkorpuszodra, p√©ld√°ul a Kaggle-r≈ël sz√°rmaz√≥ adatokra.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Olvasd el ezt a tanulm√°nyt a Word2Vec-r≈ël: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Feladat: Jegyzetf√ºzetek](assignment.md)

---

