<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-25T21:37:56+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "hu"
}
-->
# Be√°gyaz√°sok

## [El≈ëad√°s el≈ëtti kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Amikor BoW vagy TF/IDF alap√∫ oszt√°lyoz√≥kat tan√≠tottunk, magas dimenzi√≥s szavak zs√°kja vektorokkal dolgoztunk, amelyek hossza `vocab_size`, √©s kifejezetten alacsony dimenzi√≥s poz√≠ci√≥s reprezent√°ci√≥s vektorokat alak√≠tottunk √°t ritka egyhot reprezent√°ci√≥v√°. Ez az egyhot reprezent√°ci√≥ azonban nem mem√≥riahat√©kony. Ezenk√≠v√ºl minden sz√≥t egym√°st√≥l f√ºggetlen√ºl kezel√ºnk, azaz az egyhot k√≥dolt vektorok nem fejeznek ki semmilyen szemantikai hasonl√≥s√°got a szavak k√∂z√∂tt.

A **be√°gyaz√°s** √∂tlete az, hogy a szavakat alacsonyabb dimenzi√≥s, s≈±r≈± vektorokkal reprezent√°ljuk, amelyek valamilyen m√≥don t√ºkr√∂zik a sz√≥ szemantikai jelent√©s√©t. K√©s≈ëbb megbesz√©lj√ºk, hogyan lehet √©rtelmes sz√≥be√°gyaz√°sokat l√©trehozni, de egyel≈ëre gondoljunk a be√°gyaz√°sokra √∫gy, mint a sz√≥vektor dimenzi√≥j√°nak cs√∂kkent√©s√©re.

Teh√°t a be√°gyaz√°si r√©teg egy sz√≥t vesz bemenetk√©nt, √©s egy meghat√°rozott `embedding_size` m√©ret≈± kimeneti vektort √°ll√≠t el≈ë. Bizonyos √©rtelemben nagyon hasonl√≥ egy `Linear` r√©teghez, de egyhot k√≥dolt vektor helyett k√©pes lesz egy sz√≥ sz√°m√°t bemenetk√©nt venni, lehet≈ëv√© t√©ve sz√°munkra, hogy elker√ºlj√ºk a nagy egyhot k√≥dolt vektorok l√©trehoz√°s√°t.

Ha a be√°gyaz√°si r√©teget haszn√°ljuk oszt√°lyoz√≥ h√°l√≥zatunk els≈ë r√©tegek√©nt, akkor √°tt√©rhet√ºnk a szavak zs√°kj√°r√≥l az **embedding bag** modellre, ahol el≈ësz√∂r minden sz√≥t a sz√∂veg√ºnkben a megfelel≈ë be√°gyaz√°sra konvert√°lunk, majd valamilyen aggreg√°lt f√ºggv√©nyt sz√°m√≠tunk ki az √∂sszes be√°gyaz√°s felett, p√©ld√°ul `sum`, `average` vagy `max`.

![K√©p, amely egy be√°gyaz√°si oszt√°lyoz√≥t mutat √∂t szekvencia sz√≥ra.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.hu.png)

> K√©p a szerz≈ët≈ël

## ‚úçÔ∏è Gyakorlatok: Be√°gyaz√°sok

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:
* [Be√°gyaz√°sok PyTorch seg√≠ts√©g√©vel](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Be√°gyaz√°sok TensorFlow seg√≠ts√©g√©vel](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Szemantikai be√°gyaz√°sok: Word2Vec

M√≠g a be√°gyaz√°si r√©teg megtanulta a szavakat vektorreprezent√°ci√≥ra lek√©pezni, ez a reprezent√°ci√≥ nem felt√©tlen√ºl hordozott sok szemantikai jelent√©st. J√≥ lenne olyan vektorreprezent√°ci√≥t tanulni, amelyben hasonl√≥ szavak vagy szinonim√°k olyan vektoroknak felelnek meg, amelyek k√∂zel vannak egym√°shoz valamilyen vektort√°vols√°g (pl. euklideszi t√°vols√°g) szempontj√°b√≥l.

Ehhez el≈ësz√∂r el≈ë kell tan√≠tanunk a be√°gyaz√°si modellt egy nagy sz√∂veggy≈±jtem√©nyen egy specifikus m√≥don. Az egyik m√≥dszer a szemantikai be√°gyaz√°sok tan√≠t√°s√°ra a [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ez k√©t f≈ë architekt√∫r√°n alapul, amelyeket a szavak elosztott reprezent√°ci√≥j√°nak el≈ë√°ll√≠t√°s√°ra haszn√°lnak:

 - **Folytonos szavak zs√°kja** (CBoW) ‚Äî ebben az architekt√∫r√°ban a modellt arra tan√≠tjuk, hogy egy sz√≥t j√≥soljon meg a k√∂rnyez≈ë kontextusb√≥l. Az ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ eset√©n a modell c√©lja, hogy megj√≥solja $W_0$-t $(W_{-2},W_{-1},W_1,W_2)$ alapj√°n.
 - **Folytonos skip-gram** a CBoW ellent√©te. A modell a k√∂rnyez≈ë kontextus szavait haszn√°lja a jelenlegi sz√≥ megj√≥sl√°s√°ra.

A CBoW gyorsabb, m√≠g a skip-gram lassabb, de jobban reprezent√°lja a ritka szavakat.

![K√©p, amely a CBoW √©s Skip-Gram algoritmusokat mutatja a szavak vektorokk√° alak√≠t√°s√°ra.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.hu.png)

> K√©p [ebb≈ël a tanulm√°nyb√≥l](https://arxiv.org/pdf/1301.3781.pdf)

A Word2Vec el≈ëtan√≠tott be√°gyaz√°sok (valamint m√°s hasonl√≥ modellek, mint p√©ld√°ul GloVe) szint√©n haszn√°lhat√≥k a be√°gyaz√°si r√©teg helyett a neur√°lis h√°l√≥zatokban. Azonban foglalkoznunk kell a sz√≥k√©szletekkel, mivel a Word2Vec/GloVe el≈ëtan√≠t√°s√°hoz haszn√°lt sz√≥k√©szlet val√≥sz√≠n≈±leg elt√©r a sz√∂vegkorpuszunk sz√≥k√©szlet√©t≈ël. N√©zd meg a fenti jegyzetf√ºzeteket, hogy l√°sd, hogyan lehet ezt a probl√©m√°t megoldani.

## Kontextu√°lis be√°gyaz√°sok

A hagyom√°nyos el≈ëtan√≠tott be√°gyaz√°si reprezent√°ci√≥k, mint p√©ld√°ul a Word2Vec, egyik f≈ë korl√°tja a szavak jelent√©s√©nek diszambigu√°ci√≥ja. M√≠g az el≈ëtan√≠tott be√°gyaz√°sok k√©pesek bizonyos m√©rt√©kben megragadni a szavak jelent√©s√©t a kontextusban, minden sz√≥ lehets√©ges jelent√©s√©t ugyanabba a be√°gyaz√°sba k√≥dolj√°k. Ez probl√©m√°kat okozhat a k√©s≈ëbbi modellekben, mivel sok sz√≥, p√©ld√°ul a "play", k√ºl√∂nb√∂z≈ë jelent√©sekkel b√≠r att√≥l f√ºgg≈ëen, hogy milyen kontextusban haszn√°lj√°k.

P√©ld√°ul a "play" sz√≥ az al√°bbi k√©t mondatban teljesen elt√©r≈ë jelent√©ssel b√≠r:

- Elmentem egy **sz√≠ndarabra** a sz√≠nh√°zban.
- John j√°tszani szeretne a bar√°taival.

A fent eml√≠tett el≈ëtan√≠tott be√°gyaz√°sok mindk√©t jelent√©s√©t ugyanabba a be√°gyaz√°sba k√≥dolj√°k. Ennek a korl√°tnak a lek√ºzd√©s√©hez olyan be√°gyaz√°sokat kell √©p√≠ten√ºnk, amelyek a **nyelvi modellre** alapoznak, amelyet egy nagy sz√∂vegkorpuszra tan√≠tanak, √©s *tudja*, hogyan lehet a szavakat k√ºl√∂nb√∂z≈ë kontextusokban √∂sszekapcsolni. A kontextu√°lis be√°gyaz√°sok t√°rgyal√°sa t√∫lmutat ennek az oktat√≥anyagnak a keretein, de k√©s≈ëbb visszat√©r√ºnk r√°juk, amikor a nyelvi modellekr≈ël besz√©l√ºnk a kurzus sor√°n.

## √ñsszegz√©s

Ebben a leck√©ben felfedezted, hogyan lehet be√°gyaz√°si r√©tegeket √©p√≠teni √©s haszn√°lni TensorFlow-ban √©s Pytorch-ban, hogy jobban t√ºkr√∂zz√©k a szavak szemantikai jelent√©s√©t.

## üöÄ Kih√≠v√°s

A Word2Vec-et n√©h√°ny √©rdekes alkalmaz√°sra haszn√°lt√°k, p√©ld√°ul dalok sz√∂veg√©nek √©s k√∂lt√©szet gener√°l√°s√°ra. N√©zd meg [ezt a cikket](https://www.politetype.com/blog/word2vec-color-poems), amely bemutatja, hogyan haszn√°lta a szerz≈ë a Word2Vec-et k√∂lt√©szet gener√°l√°s√°ra. N√©zd meg [Dan Shiffmann vide√≥j√°t](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) is, hogy felfedezz egy m√°sik magyar√°zatot erre a technik√°ra. Ezut√°n pr√≥b√°ld meg alkalmazni ezeket a technik√°kat saj√°t sz√∂vegkorpuszodra, amelyet p√©ld√°ul Kaggle-r≈ël szerezhetsz be.

## [El≈ëad√°s ut√°ni kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Olvasd el ezt a tanulm√°nyt a Word2Vec-r≈ël: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [Feladat: Jegyzetf√ºzetek](assignment.md)

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.