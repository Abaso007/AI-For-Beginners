<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58bf4adb210aab53e8f78c8082040e7c",
  "translation_date": "2025-08-25T21:31:36+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "hu"
}
-->
# Rekurrens Neur√°lis H√°l√≥zatok

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/31)

Az el≈ëz≈ë szakaszokban gazdag szemantikai reprezent√°ci√≥kat haszn√°ltunk a sz√∂vegekhez, valamint egy egyszer≈± line√°ris oszt√°lyoz√≥t az embeddingek tetej√©n. Ez az architekt√∫ra a mondatokban szerepl≈ë szavak √∂sszes√≠tett jelent√©s√©t ragadja meg, de nem veszi figyelembe a szavak **sorrendj√©t**, mivel az embeddingek tetej√©n v√©gzett aggreg√°ci√≥s m≈±velet elt√°vol√≠totta ezt az inform√°ci√≥t az eredeti sz√∂vegb≈ël. Mivel ezek a modellek nem k√©pesek a szavak sorrendj√©t modellezni, nem tudnak megoldani √∂sszetettebb vagy k√©t√©rtelm≈± feladatokat, mint p√©ld√°ul sz√∂veg gener√°l√°sa vagy k√©rd√©s megv√°laszol√°sa.

Ahhoz, hogy a sz√∂vegszekvencia jelent√©s√©t megragadjuk, egy m√°sik neur√°lis h√°l√≥zati architekt√∫r√°t kell haszn√°lnunk, amelyet **rekurrens neur√°lis h√°l√≥zatnak** (RNN) nevez√ºnk. Az RNN-ben a mondatot egyes√©vel, szimb√≥lumonk√©nt adjuk √°t a h√°l√≥zaton, amely egy **√°llapotot** hoz l√©tre, amit azt√°n a k√∂vetkez≈ë szimb√≥lummal egy√ºtt √∫jra √°tadunk a h√°l√≥zatnak.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.hu.png)

> K√©p a szerz≈ët≈ël

Az X<sub>0</sub>,...,X<sub>n</sub> tokenek bemeneti szekvenci√°j√°t figyelembe v√©ve az RNN egy neur√°lis h√°l√≥zati blokkok sorozat√°t hozza l√©tre, √©s ezt a sorozatot v√©gpontt√≥l v√©gpontig tan√≠tja visszaterjeszt√©ssel. Minden h√°l√≥zati blokk egy (X<sub>i</sub>,S<sub>i</sub>) p√°rt kap bemenetk√©nt, √©s S<sub>i+1</sub>-et √°ll√≠t el≈ë eredm√©nyk√©nt. Az utols√≥ √°llapot S<sub>n</sub> vagy (kimenet Y<sub>n</sub>) egy line√°ris oszt√°lyoz√≥ba ker√ºl, hogy el≈ë√°ll√≠tsa az eredm√©nyt. Az √∂sszes h√°l√≥zati blokk ugyanazokat a s√∫lyokat osztja meg, √©s egyetlen visszaterjeszt√©si l√©p√©sben v√©gpontt√≥l v√©gpontig tanul.

Mivel az √°llapotvektorok S<sub>0</sub>,...,S<sub>n</sub> √°tmennek a h√°l√≥zaton, k√©pes megtanulni a szavak k√∂z√∂tti szekvenci√°lis f√ºgg≈ës√©geket. P√©ld√°ul, amikor a *nem* sz√≥ megjelenik valahol a szekvenci√°ban, megtanulhatja bizonyos elemek tagad√°s√°t az √°llapotvektoron bel√ºl, ami tagad√°st eredm√©nyez.

> ‚úÖ Mivel a fenti k√©pen l√°that√≥ √∂sszes RNN blokk s√∫lyai megosztottak, ugyanaz a k√©p egyetlen blokk√©nt is √°br√°zolhat√≥ (a jobb oldalon) egy rekurrens visszacsatol√°si hurokkal, amely visszaviszi a h√°l√≥zat kimeneti √°llapot√°t a bemenethez.

## Az RNN cella anat√≥mi√°ja

N√©zz√ºk meg, hogyan van fel√©p√≠tve egy egyszer≈± RNN cella. Elfogadja az el≈ëz≈ë √°llapotot S<sub>i-1</sub> √©s az aktu√°lis szimb√≥lumot X<sub>i</sub> bemenetk√©nt, √©s el≈ë kell √°ll√≠tania a kimeneti √°llapotot S<sub>i</sub> (√©s n√©ha √©rdekel minket egy m√°sik kimenet Y<sub>i</sub> is, mint p√©ld√°ul generat√≠v h√°l√≥zatok eset√©ben).

Egy egyszer≈± RNN cell√°nak k√©t s√∫lym√°trixa van: az egyik √°talak√≠tja a bemeneti szimb√≥lumot (nevezz√ºk W-nek), a m√°sik pedig az √°llapotot (H). Ebben az esetben a h√°l√≥zat kimenete œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b) form√°ban sz√°m√≠t√≥dik ki, ahol œÉ az aktiv√°ci√≥s f√ºggv√©ny, √©s b egy tov√°bbi bias.

<img alt="RNN Cell Anatomy" src="images/rnn-anatomy.png" width="50%"/>

> K√©p a szerz≈ët≈ël

Sok esetben a bemeneti tokenek az embedding r√©tegen kereszt√ºl jutnak az RNN-be, hogy cs√∂kkents√©k a dimenzi√≥t. Ebben az esetben, ha a bemeneti vektorok dimenzi√≥ja *emb_size*, √©s az √°llapotvektor *hid_size* - akkor W m√©rete *emb_size*√ó*hid_size*, √©s H m√©rete *hid_size*√ó*hid_size*.

## Hossz√∫-r√∂vid t√°v√∫ mem√≥ria (LSTM)

A klasszikus RNN-ek egyik f≈ë probl√©m√°ja az √∫gynevezett **elt≈±n≈ë gradiens** probl√©ma. Mivel az RNN-eket egyetlen visszaterjeszt√©si l√©p√©sben v√©gpontt√≥l v√©gpontig tan√≠tj√°k, neh√©zs√©get okoz a hiba propag√°l√°sa a h√°l√≥zat els≈ë r√©tegeihez, √≠gy a h√°l√≥zat nem tudja megtanulni a t√°voli tokenek k√∂z√∂tti kapcsolatokat. Ennek a probl√©m√°nak az elker√ºl√©s√©re az egyik megold√°s az **explicit √°llapotkezel√©s** bevezet√©se √∫gynevezett **kapuk** seg√≠ts√©g√©vel. K√©t j√≥l ismert architekt√∫ra l√©tezik ebben a kateg√≥ri√°ban: **Hossz√∫-r√∂vid t√°v√∫ mem√≥ria** (LSTM) √©s **Kapuzott rel√© egys√©g** (GRU).

![Image showing an example long short term memory cell](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> K√©p forr√°sa TBD

Az LSTM h√°l√≥zat fel√©p√≠t√©se hasonl√≥ az RNN-hez, de k√©t √°llapotot tov√°bb√≠t r√©tegr≈ël r√©tegre: az aktu√°lis √°llapotot C, √©s a rejtett vektort H. Minden egys√©gn√©l a rejtett vektor H<sub>i</sub> √∂ssze van kapcsolva a bemenettel X<sub>i</sub>, √©s ezek ir√°ny√≠tj√°k, hogy mi t√∂rt√©nik az √°llapottal C **kapuk** seg√≠ts√©g√©vel. Minden kapu egy neur√°lis h√°l√≥zat szigmoid aktiv√°ci√≥val (kimenet tartom√°nya [0,1]), amely bitmaszkk√©nt m≈±k√∂dik, amikor megszorozzuk az √°llapotvektorral. A k√∂vetkez≈ë kapuk l√©teznek (balr√≥l jobbra a fenti k√©pen):

* A **felejt≈ë kapu** egy rejtett vektort vesz, √©s meghat√°rozza, hogy az √°llapotvektor C mely komponenseit kell elfelejteni, √©s melyeket kell tov√°bb√≠tani.
* A **bemeneti kapu** inform√°ci√≥t vesz a bemeneti √©s rejtett vektorokb√≥l, √©s beilleszti az √°llapotba.
* A **kimeneti kapu** az √°llapotot egy line√°ris r√©tegen kereszt√ºl √°talak√≠tja *tanh* aktiv√°ci√≥val, majd kiv√°lasztja annak bizonyos komponenseit a rejtett vektor H<sub>i</sub> seg√≠ts√©g√©vel, hogy √∫j √°llapotot C<sub>i+1</sub> √°ll√≠tson el≈ë.

Az √°llapot C komponensei gondolhat√≥k √∫gy, mint bizonyos jelz≈ëk, amelyeket be- √©s kikapcsolhatunk. P√©ld√°ul, amikor a *Alice* n√©vvel tal√°lkozunk a szekvenci√°ban, felt√©telezhetj√ºk, hogy egy n≈ëi karakterre utal, √©s bekapcsolhatjuk az √°llapotban azt a jelz≈ët, hogy n≈ëi f≈ën√©v van a mondatban. Amikor k√©s≈ëbb tal√°lkozunk az *√©s Tom* kifejez√©ssel, bekapcsolhatjuk azt a jelz≈ët, hogy t√∂bbes sz√°m√∫ f≈ën√©v van. √çgy az √°llapot manipul√°l√°s√°val nyomon k√∂vethetj√ºk a mondatr√©szek nyelvtani tulajdons√°gait.

> ‚úÖ Egy kiv√°l√≥ forr√°s az LSTM bels≈ë m≈±k√∂d√©s√©nek meg√©rt√©s√©hez Christopher Olah [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) c√≠m≈± cikke.

## Bidirekcion√°lis √©s t√∂bbr√©teg≈± RNN-ek

Olyan rekurrens h√°l√≥zatokat t√°rgyaltunk, amelyek egy ir√°nyban m≈±k√∂dnek, a szekvencia elej√©t≈ël a v√©g√©ig. Ez term√©szetesnek t≈±nik, mivel hasonl√≠t arra, ahogyan olvasunk √©s hallgatunk besz√©det. Azonban mivel sok gyakorlati esetben v√©letlen hozz√°f√©r√©s√ºnk van a bemeneti szekvenci√°hoz, √©rdemes lehet rekurrens sz√°m√≠t√°st v√©gezni mindk√©t ir√°nyban. Az ilyen h√°l√≥zatokat **bidirekcion√°lis** RNN-eknek nevezz√ºk. Bidirekcion√°lis h√°l√≥zat eset√©n k√©t rejtett √°llapotvektorra van sz√ºks√©g√ºnk, egy-egy ir√°nyhoz.

Egy rekurrens h√°l√≥zat, legyen az egyir√°ny√∫ vagy bidirekcion√°lis, bizonyos mint√°kat ragad meg egy szekvenci√°n bel√ºl, √©s ezeket az √°llapotvektorba menti vagy a kimenetbe tov√°bb√≠tja. Ahogy a konvol√∫ci√≥s h√°l√≥zatokn√°l, itt is √©p√≠thet√ºnk egy m√°sik rekurrens r√©teget az els≈ë f√∂l√©, hogy magasabb szint≈± mint√°kat ragadjunk meg, √©s az els≈ë r√©teg √°ltal kinyert alacsony szint≈± mint√°kb√≥l √©p√≠tkezz√ºnk. Ez vezet minket a **t√∂bbr√©teg≈± RNN** fogalm√°hoz, amely k√©t vagy t√∂bb rekurrens h√°l√≥zatb√≥l √°ll, ahol az el≈ëz≈ë r√©teg kimenete bemenetk√©nt ker√ºl a k√∂vetkez≈ë r√©tegbe.

![Image showing a Multilayer long-short-term-memory- RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.hu.jpg)

*K√©p Fernando L√≥pez [ezen csod√°latos bejegyz√©s√©b≈ël](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3)*

## ‚úçÔ∏è Gyakorlatok: Embeddingek

Folytasd a tanul√°st az al√°bbi jegyzetekben:

* [RNN-ek PyTorch seg√≠ts√©g√©vel](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNN-ek TensorFlow seg√≠ts√©g√©vel](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## √ñsszegz√©s

Ebben az egys√©gben l√°ttuk, hogy az RNN-ek haszn√°lhat√≥k szekvencia oszt√°lyoz√°sra, de val√≥j√°ban sok m√°s feladatot is k√©pesek kezelni, mint p√©ld√°ul sz√∂veg gener√°l√°sa, g√©pi ford√≠t√°s √©s m√©g sok m√°s. Ezeket a feladatokat a k√∂vetkez≈ë egys√©gben fogjuk megvizsg√°lni.

## üöÄ Kih√≠v√°s

Olvass el n√©h√°ny irodalmat az LSTM-ekr≈ël, √©s gondold √°t az alkalmaz√°saikat:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christopher Olah-t√≥l.

## [Feladat: Jegyzetek](assignment.md)

**Felel≈ëss√©gkiz√°r√°s**:  
Ez a dokumentum az [Co-op Translator](https://github.com/Azure/co-op-translator) AI ford√≠t√°si szolg√°ltat√°s seg√≠ts√©g√©vel k√©sz√ºlt. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis, emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get a ford√≠t√°s haszn√°lat√°b√≥l ered≈ë f√©lre√©rt√©sek√©rt vagy t√©ves √©rtelmez√©sek√©rt.