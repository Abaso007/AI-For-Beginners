<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2273cc150380a5e191903cea858f021",
  "translation_date": "2025-09-23T11:19:18+00:00",
  "source_file": "lessons/5-NLP/16-RNN/README.md",
  "language_code": "hu"
}
-->
# Rekurrens Neur√°lis H√°l√≥zatok

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/31)

Az el≈ëz≈ë szekci√≥kban gazdag szemantikai reprezent√°ci√≥kat haszn√°ltunk a sz√∂vegekhez, √©s egy egyszer≈± line√°ris oszt√°lyoz√≥t az embeddingek tetej√©n. Ez az architekt√∫ra a mondatokban l√©v≈ë szavak √∂sszes√≠tett jelent√©s√©t ragadja meg, de nem veszi figyelembe a szavak **sorrendj√©t**, mivel az embeddingek tetej√©n v√©gzett aggreg√°ci√≥s m≈±velet elt√°vol√≠totta ezt az inform√°ci√≥t az eredeti sz√∂vegb≈ël. Mivel ezek a modellek nem k√©pesek a szavak sorrendj√©t modellezni, nem tudnak megoldani √∂sszetettebb vagy k√©t√©rtelm≈± feladatokat, mint p√©ld√°ul sz√∂vegalkot√°s vagy k√©rd√©s-v√°lasz.

Ahhoz, hogy a sz√∂vegszekvencia jelent√©s√©t megragadjuk, egy m√°sik neur√°lis h√°l√≥zati architekt√∫r√°t kell haszn√°lnunk, amelyet **rekurrens neur√°lis h√°l√≥zatnak** (RNN) nevez√ºnk. Az RNN-ben mondatunkat egy szimb√≥lumonk√©nt adjuk √°t a h√°l√≥zaton, √©s a h√°l√≥zat egy **√°llapotot** hoz l√©tre, amelyet azt√°n a k√∂vetkez≈ë szimb√≥lummal egy√ºtt √∫jra √°tadunk a h√°l√≥zatnak.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.hu.png)

> K√©p a szerz≈ët≈ël

Az X<sub>0</sub>,...,X<sub>n</sub> bemeneti tokenek szekvenci√°j√°t tekintve az RNN egy neur√°lis h√°l√≥zati blokkok sorozat√°t hozza l√©tre, √©s ezt a sorozatot v√©gpontt√≥l v√©gpontig tan√≠tja visszaterjeszt√©ssel. Minden h√°l√≥zati blokk egy (X<sub>i</sub>,S<sub>i</sub>) p√°rt fogad bemenetk√©nt, √©s S<sub>i+1</sub>-et √°ll√≠t el≈ë eredm√©nyk√©nt. A v√©gs≈ë √°llapot S<sub>n</sub> vagy (kimenet Y<sub>n</sub>) egy line√°ris oszt√°lyoz√≥ba ker√ºl, hogy el≈ë√°ll√≠tsa az eredm√©nyt. Az √∂sszes h√°l√≥zati blokk ugyanazokat a s√∫lyokat osztja meg, √©s egy visszaterjeszt√©si l√©p√©sben v√©gpontt√≥l v√©gpontig tan√≠tj√°k.

Mivel az √°llapotvektorok S<sub>0</sub>,...,S<sub>n</sub> √°tmennek a h√°l√≥zaton, k√©pes megtanulni a szavak k√∂z√∂tti szekvenci√°lis f√ºgg≈ës√©geket. P√©ld√°ul, amikor a *nem* sz√≥ megjelenik valahol a szekvenci√°ban, megtanulhatja bizonyos elemek tagad√°s√°t az √°llapotvektoron bel√ºl, ami tagad√°st eredm√©nyez.

> ‚úÖ Mivel a fenti k√©pen l√°that√≥ √∂sszes RNN blokk s√∫lyai megosztottak, ugyanaz a k√©p egyetlen blokk√©nt is √°br√°zolhat√≥ (a jobb oldalon) egy rekurrens visszacsatol√°si hurokkal, amely visszaviszi a h√°l√≥zat kimeneti √°llapot√°t a bemenethez.

## Az RNN cella anat√≥mi√°ja

N√©zz√ºk meg, hogyan van fel√©p√≠tve egy egyszer≈± RNN cella. Elfogadja az el≈ëz≈ë √°llapotot S<sub>i-1</sub> √©s az aktu√°lis szimb√≥lumot X<sub>i</sub> bemenetk√©nt, √©s el≈ë kell √°ll√≠tania a kimeneti √°llapotot S<sub>i</sub> (√©s n√©ha √©rdekel minket egy m√°sik kimenet Y<sub>i</sub> is, mint p√©ld√°ul generat√≠v h√°l√≥zatok eset√©ben).

Egy egyszer≈± RNN cell√°nak k√©t s√∫lym√°trixa van: az egyik √°talak√≠tja a bemeneti szimb√≥lumot (nevezz√ºk W-nek), a m√°sik pedig a bemeneti √°llapotot (H). Ebben az esetben a h√°l√≥zat kimenete &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b) form√°ban sz√°m√≠t√≥dik ki, ahol &sigma; az aktiv√°ci√≥s f√ºggv√©ny, √©s b egy tov√°bbi bias.

<img alt="RNN cella anat√≥mi√°ja" src="images/rnn-anatomy.png" width="50%"/>

> K√©p a szerz≈ët≈ël

Sok esetben a bemeneti tokeneket az embedding r√©tegen kereszt√ºl adj√°k √°t az RNN-nek, hogy cs√∂kkents√©k a dimenzi√≥t. Ebben az esetben, ha a bemeneti vektorok dimenzi√≥ja *emb_size*, √©s az √°llapotvektor *hid_size* - akkor W m√©rete *emb_size*&times;*hid_size*, √©s H m√©rete *hid_size*&times;*hid_size*.

## Hossz√∫-r√∂vid t√°v√∫ mem√≥ria (LSTM)

A klasszikus RNN-ek egyik f≈ë probl√©m√°ja az √∫gynevezett **elt≈±n≈ë gradiens** probl√©ma. Mivel az RNN-eket egy visszaterjeszt√©si l√©p√©sben v√©gpontt√≥l v√©gpontig tan√≠tj√°k, neh√©zs√©get okoz a hiba propag√°l√°sa a h√°l√≥zat els≈ë r√©tegeihez, √≠gy a h√°l√≥zat nem tudja megtanulni a t√°voli tokenek k√∂z√∂tti kapcsolatokat. Ennek a probl√©m√°nak az elker√ºl√©s√©re az egyik m√≥d az **explicit √°llapotkezel√©s** bevezet√©se √∫gynevezett **kapuk** seg√≠ts√©g√©vel. K√©t j√≥l ismert architekt√∫ra l√©tezik ebben a kateg√≥ri√°ban: **Hossz√∫-r√∂vid t√°v√∫ mem√≥ria** (LSTM) √©s **Kapuzott rel√© egys√©g** (GRU).

![P√©lda egy hossz√∫-r√∂vid t√°v√∫ mem√≥ria cell√°ra](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> K√©p forr√°sa TBD

Az LSTM h√°l√≥zat hasonl√≥ m√≥don van szervezve, mint az RNN, de k√©t √°llapotot ad √°t r√©tegr≈ël r√©tegre: az aktu√°lis √°llapotot C, √©s a rejtett vektort H. Minden egys√©gn√©l a rejtett vektor H<sub>i</sub> √∂ssze van kapcsolva a bemenettel X<sub>i</sub>, √©s ezek ir√°ny√≠tj√°k, hogy mi t√∂rt√©nik az √°llapottal C **kapuk** seg√≠ts√©g√©vel. Minden kapu egy neur√°lis h√°l√≥zat szigmoid aktiv√°ci√≥val (kimenet tartom√°nya [0,1]), amely bitmaszkk√©nt √©rtelmezhet≈ë, amikor az √°llapotvektorral szorozzuk. Az al√°bbi kapuk l√©teznek (balr√≥l jobbra a fenti k√©pen):

* A **felejt≈ë kapu** elfogad egy rejtett vektort, √©s meghat√°rozza, hogy az √°llapotvektor C mely komponenseit kell elfelejteni, √©s melyeket kell tov√°bbadni.
* A **bemeneti kapu** inform√°ci√≥t vesz a bemeneti √©s rejtett vektorokb√≥l, √©s beilleszti az √°llapotba.
* A **kimeneti kapu** line√°ris r√©tegen kereszt√ºl √°talak√≠tja az √°llapotot *tanh* aktiv√°ci√≥val, majd kiv√°lasztja annak bizonyos komponenseit a rejtett vektor H<sub>i</sub> seg√≠ts√©g√©vel, hogy √∫j √°llapotot C<sub>i+1</sub> √°ll√≠tson el≈ë.

Az √°llapot C komponensei bizonyos jelz≈ëk√©nt √©rtelmezhet≈ëk, amelyeket be- √©s kikapcsolhatunk. P√©ld√°ul, amikor egy *Alice* nev≈± sz√≥t tal√°lunk a szekvenci√°ban, felt√©telezhetj√ºk, hogy egy n≈ëi karakterre utal, √©s bekapcsolhatjuk az √°llapotban azt a jelz≈ët, hogy n≈ëi f≈ën√©v van a mondatban. Amikor k√©s≈ëbb tal√°lkozunk az *√©s Tom* kifejez√©ssel, bekapcsolhatjuk azt a jelz≈ët, hogy t√∂bbes sz√°m√∫ f≈ën√©v van. √çgy az √°llapot manipul√°l√°s√°val nyomon k√∂vethetj√ºk a mondatr√©szek nyelvtani tulajdons√°gait.

> ‚úÖ Egy kiv√°l√≥ forr√°s az LSTM bels≈ë m≈±k√∂d√©s√©nek meg√©rt√©s√©hez Christopher Olah [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) c√≠m≈± cikke.

## Ir√°ny√≠tott √©s t√∂bbr√©teg≈± RNN-ek

Olyan rekurrens h√°l√≥zatokat t√°rgyaltunk, amelyek egy ir√°nyban m≈±k√∂dnek, a szekvencia elej√©t≈ël a v√©g√©ig. Ez term√©szetesnek t≈±nik, mivel hasonl√≠t arra, ahogyan olvasunk √©s hallgatjuk a besz√©det. Azonban mivel sok gyakorlati esetben v√©letlen hozz√°f√©r√©s√ºnk van a bemeneti szekvenci√°hoz, √©rdemes lehet rekurrens sz√°m√≠t√°st v√©gezni mindk√©t ir√°nyban. Az ilyen h√°l√≥zatokat **k√©tir√°ny√∫** RNN-eknek nevezz√ºk. K√©tir√°ny√∫ h√°l√≥zat eset√©n k√©t rejtett √°llapotvektorra van sz√ºks√©g√ºnk, egy-egy ir√°nyhoz.

Egy rekurrens h√°l√≥zat, ak√°r egyir√°ny√∫, ak√°r k√©tir√°ny√∫, bizonyos mint√°kat ragad meg egy szekvenci√°ban, √©s ezeket az √°llapotvektorba menti vagy a kimenetbe tov√°bb√≠tja. Ak√°rcsak a konvol√∫ci√≥s h√°l√≥zatok eset√©ben, egy m√°sik rekurrens r√©teget √©p√≠thet√ºnk az els≈ë f√∂l√©, hogy magasabb szint≈± mint√°kat ragadjunk meg, √©s az els≈ë r√©teg √°ltal kinyert alacsony szint≈± mint√°kb√≥l √©p√≠tkezz√ºnk. Ez vezet minket a **t√∂bbr√©teg≈± RNN** fogalm√°hoz, amely k√©t vagy t√∂bb rekurrens h√°l√≥zatb√≥l √°ll, ahol az el≈ëz≈ë r√©teg kimenete bemenetk√©nt ker√ºl a k√∂vetkez≈ë r√©tegbe.

![T√∂bbr√©teg≈± hossz√∫-r√∂vid t√°v√∫ mem√≥ria RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.hu.jpg)

*K√©p Fernando L√≥pez [ezen csod√°latos bejegyz√©s√©b≈ël](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3)*

## ‚úçÔ∏è Gyakorlatok: Embeddingek

Folytasd a tanul√°st az al√°bbi notebookokban:

* [RNN-ek PyTorch seg√≠ts√©g√©vel](RNNPyTorch.ipynb)
* [RNN-ek TensorFlow seg√≠ts√©g√©vel](RNNTF.ipynb)

## √ñsszegz√©s

Ebben az egys√©gben l√°ttuk, hogy az RNN-ek haszn√°lhat√≥k szekvencia oszt√°lyoz√°sra, de val√≥j√°ban sok m√°s feladatot is k√©pesek kezelni, mint p√©ld√°ul sz√∂vegalkot√°s, g√©pi ford√≠t√°s √©s m√©g sok m√°s. Ezeket a feladatokat a k√∂vetkez≈ë egys√©gben fogjuk megvizsg√°lni.

## üöÄ Kih√≠v√°s

Olvass el n√©h√°ny irodalmat az LSTM-ekr≈ël, √©s gondold √°t az alkalmaz√°saikat:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Christopher Olah-t√≥l.

## [Feladat: Notebookok](assignment.md)

---

