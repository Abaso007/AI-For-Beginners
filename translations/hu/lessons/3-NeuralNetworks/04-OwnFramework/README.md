<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "186bf7eeab776b36f557357ea56d4751",
  "translation_date": "2025-08-25T23:44:27+00:00",
  "source_file": "lessons/3-NeuralNetworks/04-OwnFramework/README.md",
  "language_code": "hu"
}
-->
# Bevezet√©s a neur√°lis h√°l√≥zatokba. T√∂bbr√©teg≈± perceptron

Az el≈ëz≈ë r√©szben megismerkedt√©l a legegyszer≈±bb neur√°lis h√°l√≥zati modellel ‚Äì az egyr√©teg≈± perceptronnal, amely egy line√°ris k√©toszt√°lyos oszt√°lyoz√°si modell.

Ebben a r√©szben kiterjesztj√ºk ezt a modellt egy rugalmasabb keretrendszerr√©, amely lehet≈ëv√© teszi sz√°munkra, hogy:

* **t√∂bboszt√°lyos oszt√°lyoz√°st** v√©gezz√ºnk a k√©toszt√°lyos oszt√°lyoz√°s mellett
* **regresszi√≥s probl√©m√°kat** oldjunk meg az oszt√°lyoz√°s mellett
* olyan oszt√°lyokat k√ºl√∂n√≠ts√ºnk el, amelyek nem line√°risan elv√°laszthat√≥k

Ezenk√≠v√ºl kifejleszt√ºnk egy saj√°t modul√°ris keretrendszert Pythonban, amely lehet≈ëv√© teszi sz√°munkra k√ºl√∂nb√∂z≈ë neur√°lis h√°l√≥zati architekt√∫r√°k l√©trehoz√°s√°t.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/7)

## A g√©pi tanul√°s formaliz√°l√°sa

Kezdj√ºk a g√©pi tanul√°si probl√©ma formaliz√°l√°s√°val. Tegy√ºk fel, hogy van egy **X** edz√©si adathalmazunk c√≠mk√©kkel **Y**, √©s egy olyan modellt kell √©p√≠ten√ºnk, *f*, amely a lehet≈ë legpontosabb el≈ërejelz√©seket adja. Az el≈ërejelz√©sek min≈ës√©g√©t a **vesztes√©gf√ºggv√©ny** ‚Ñí m√©ri. Az al√°bbi vesztes√©gf√ºggv√©nyeket gyakran haszn√°lj√°k:

* Regresszi√≥s probl√©m√°k eset√©n, amikor egy sz√°mot kell megj√≥solnunk, haszn√°lhatjuk az **abszol√∫t hib√°t** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, vagy a **n√©gyzetes hib√°t** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Oszt√°lyoz√°s eset√©n haszn√°ljuk a **0-1 vesztes√©get** (ami l√©nyeg√©ben a modell **pontoss√°g√°val** egyen√©rt√©k≈±), vagy a **logisztikus vesztes√©get**.

Az egyszint≈± perceptron eset√©ben az *f* f√ºggv√©nyt line√°ris f√ºggv√©nyk√©nt defini√°ltuk: *f(x)=wx+b* (ahol *w* a s√∫lym√°trix, *x* a bemeneti jellemz≈ëk vektora, √©s *b* az eltol√°si vektor). K√ºl√∂nb√∂z≈ë neur√°lis h√°l√≥zati architekt√∫r√°k eset√©n ez a f√ºggv√©ny bonyolultabb form√°t √∂lthet.

> Oszt√°lyoz√°s eset√©n gyakran k√≠v√°natos, hogy a h√°l√≥zat kimenete az oszt√°lyok val√≥sz√≠n≈±s√©geit adja meg. Az √∂nk√©nyes sz√°mok val√≥sz√≠n≈±s√©gekk√© alak√≠t√°s√°hoz (pl. a kimenet normaliz√°l√°s√°hoz) gyakran haszn√°ljuk a **softmax** f√ºggv√©nyt œÉ, √©s az *f* f√ºggv√©ny *f(x)=œÉ(wx+b)* form√°t √∂lt.

Az *f* fenti defin√≠ci√≥j√°ban *w* √©s *b* az √∫gynevezett **param√©terek**, Œ∏=‚ü®*w,b*‚ü©. Az ‚ü®**X**,**Y**‚ü© adathalmaz alapj√°n kisz√°m√≠thatjuk az eg√©sz adathalmazra vonatkoz√≥ √∂sszes√≠tett hib√°t a param√©terek Œ∏ f√ºggv√©ny√©ben.

> ‚úÖ **A neur√°lis h√°l√≥zat tan√≠t√°s√°nak c√©lja a hiba minimaliz√°l√°sa a param√©terek Œ∏ v√°ltoztat√°s√°val**

## Gradiensalap√∫ optimaliz√°l√°s

L√©tezik egy j√≥l ismert f√ºggv√©nyoptimaliz√°l√°si m√≥dszer, az √∫gynevezett **gradiensm√≥dszer**. Az √∂tlet az, hogy kisz√°m√≠thatjuk a vesztes√©gf√ºggv√©ny deriv√°ltj√°t (t√∂bbdimenzi√≥s esetben **gradiensnek** nevezz√ºk) a param√©terekre vonatkoz√≥an, √©s √∫gy v√°ltoztathatjuk a param√©tereket, hogy a hiba cs√∂kkenjen. Ez a k√∂vetkez≈ëk√©ppen formaliz√°lhat√≥:

* Inicializ√°ljuk a param√©tereket v√©letlenszer≈± √©rt√©kekkel: w<sup>(0)</sup>, b<sup>(0)</sup>
* Ism√©telj√ºk meg az al√°bbi l√©p√©st sokszor:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

A tan√≠t√°s sor√°n az optimaliz√°l√°si l√©p√©seket az eg√©sz adathalmaz figyelembev√©tel√©vel kellene kisz√°m√≠tani (eml√©kezz√ºnk, hogy a vesztes√©get az √∂sszes edz√©si minta √∂sszegz√©sek√©nt sz√°m√≠tjuk ki). Azonban a val√≥s√°gban az adathalmaz kis r√©szeit, √∫gynevezett **minibatch-eket** vessz√ºk, √©s a gradienseket az adatok egy r√©szhalmaz√°n alapulva sz√°m√≠tjuk ki. Mivel a r√©szhalmazt minden alkalommal v√©letlenszer≈±en v√°lasztjuk ki, ezt a m√≥dszert **sztochasztikus gradiensm√≥dszernek** (SGD) nevezz√ºk.

## T√∂bbr√©teg≈± perceptronok √©s visszaterjeszt√©s

Az egyr√©teg≈± h√°l√≥zat, ahogy fentebb l√°ttuk, k√©pes line√°risan elv√°laszthat√≥ oszt√°lyokat oszt√°lyozni. Gazdagabb modell l√©trehoz√°s√°hoz t√∂bb r√©teget kombin√°lhatunk a h√°l√≥zatban. Matematikailag ez azt jelenti, hogy az *f* f√ºggv√©ny bonyolultabb form√°t √∂lt, √©s t√∂bb l√©p√©sben sz√°m√≠t√≥dik ki:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

Itt Œ± egy **nemline√°ris aktiv√°ci√≥s f√ºggv√©ny**, œÉ egy softmax f√ºggv√©ny, √©s a param√©terek Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

A gradiensm√≥dszer algoritmusa v√°ltozatlan marad, de a gradiens kisz√°m√≠t√°sa bonyolultabb√° v√°lik. A l√°ncszab√°ly alapj√°n a deriv√°ltak a k√∂vetkez≈ëk√©ppen sz√°m√≠that√≥k:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ A l√°ncszab√°lyt haszn√°ljuk a vesztes√©gf√ºggv√©ny param√©terekre vonatkoz√≥ deriv√°ltjainak kisz√°m√≠t√°s√°hoz.

Figyelj√ºk meg, hogy ezeknek a kifejez√©seknek a bal sz√©ls≈ë r√©sze ugyanaz, √≠gy hat√©konyan kisz√°m√≠thatjuk a deriv√°ltakat a vesztes√©gf√ºggv√©nyt≈ël kiindulva, √©s "visszafel√©" haladva a sz√°m√≠t√°si gr√°fon. Ez√©rt a t√∂bbr√©teg≈± perceptron tan√≠t√°s√°nak m√≥dszer√©t **visszaterjeszt√©snek** (backpropagation), vagy r√∂viden 'backprop'-nak nevezz√ºk.

<img alt="sz√°m√≠t√°si gr√°f" src="images/ComputeGraphGrad.png"/>

> TODO: k√©p forr√°smegjel√∂l√©s

> ‚úÖ A visszaterjeszt√©st sokkal r√©szletesebben t√°rgyaljuk a notebook p√©ld√°nkban.  

## √ñsszegz√©s

Ebben a leck√©ben l√©trehoztuk saj√°t neur√°lis h√°l√≥zati k√∂nyvt√°runkat, √©s egy egyszer≈± k√©tdimenzi√≥s oszt√°lyoz√°si feladatra haszn√°ltuk.

## üöÄ Kih√≠v√°s

A mell√©kelt notebookban megval√≥s√≠tod saj√°t keretrendszeredet t√∂bbr√©teg≈± perceptronok √©p√≠t√©s√©re √©s tan√≠t√°s√°ra. R√©szletesen megismerheted, hogyan m≈±k√∂dnek a modern neur√°lis h√°l√≥zatok.

Haladj tov√°bb az [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) notebookhoz, √©s dolgozd √°t.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/8)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

A visszaterjeszt√©s egy gyakori algoritmus az AI √©s ML ter√ºlet√©n, √©rdemes [r√©szletesebben tanulm√°nyozni](https://wikipedia.org/wiki/Backpropagation).

## [Feladat](lab/README.md)

Ebben a laborban arra k√©r√ºnk, hogy haszn√°ld a leck√©ben l√©trehozott keretrendszert az MNIST k√©zzel √≠rt sz√°mjegyek oszt√°lyoz√°s√°nak megold√°s√°ra.

* [√ötmutat√≥](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.