<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b544f20b796402507fb05a0df893323",
  "translation_date": "2025-08-25T23:49:51+00:00",
  "source_file": "lessons/3-NeuralNetworks/05-Frameworks/README.md",
  "language_code": "hu"
}
-->
# Neur√°lis h√°l√≥zat keretrendszerek

Ahogy m√°r megtanultuk, a neur√°lis h√°l√≥zatok hat√©kony tan√≠t√°s√°hoz k√©t dolgot kell megtenn√ºnk:

* Tensorokon kell m≈±veleteket v√©gezni, p√©ld√°ul szorz√°s, √∂sszead√°s, valamint bizonyos f√ºggv√©nyek, mint a sigmoid vagy softmax kisz√°m√≠t√°sa
* Az √∂sszes kifejez√©s gradiens√©t ki kell sz√°m√≠tani, hogy gradient descent optimaliz√°ci√≥t v√©gezhess√ºnk

## [El≈ëad√°s el≈ëtti kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

M√≠g a `numpy` k√∂nyvt√°r k√©pes az els≈ë r√©szre, sz√ºks√©g√ºnk van egy mechanizmusra a gradiens sz√°m√≠t√°s√°hoz. Az [√°ltalunk fejlesztett keretrendszerben](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), amelyet az el≈ëz≈ë szekci√≥ban k√©sz√≠tett√ºnk, manu√°lisan kellett programoznunk az √∂sszes deriv√°lt f√ºggv√©nyt a `backward` met√≥dusban, amely a visszaterjeszt√©st v√©gzi. Ide√°lis esetben egy keretrendszer lehet≈ës√©get kellene adjon arra, hogy *b√°rmilyen kifejez√©s* gradiens√©t kisz√°m√≠tsuk, amit defini√°lni tudunk.

Egy m√°sik fontos dolog, hogy k√©pesek legy√ºnk sz√°m√≠t√°sokat v√©gezni GPU-n vagy m√°s speci√°lis sz√°m√≠t√°si egys√©geken, mint p√©ld√°ul [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). A m√©ly neur√°lis h√°l√≥zatok tan√≠t√°sa *rengeteg* sz√°m√≠t√°st ig√©nyel, √©s nagyon fontos, hogy ezeket a sz√°m√≠t√°sokat p√°rhuzamos√≠tsuk GPU-kon.

> ‚úÖ A 'p√°rhuzamos√≠t√°s' kifejez√©s azt jelenti, hogy a sz√°m√≠t√°sokat t√∂bb eszk√∂z k√∂z√∂tt osztjuk el.

Jelenleg a k√©t legn√©pszer≈±bb neur√°lis keretrendszer: [TensorFlow](http://TensorFlow.org) √©s [PyTorch](https://pytorch.org/). Mindkett≈ë alacsony szint≈± API-t biztos√≠t tensorokkal val√≥ m≈±veletekhez CPU-n √©s GPU-n egyar√°nt. Az alacsony szint≈± API mellett van egy magasabb szint≈± API is, amelyet [Keras](https://keras.io/) √©s [PyTorch Lightning](https://pytorchlightning.ai/) n√©ven ismer√ºnk.

Alacsony szint≈± API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------------|-------------------------------------|--------------------------------
Magas szint≈± API    | [Keras](https://keras.io/)         | [PyTorch Lightning](https://pytorchlightning.ai/)

**Az alacsony szint≈± API-k** mindk√©t keretrendszerben lehet≈ëv√© teszik √∫gynevezett **sz√°m√≠t√°si gr√°fok** l√©trehoz√°s√°t. Ez a gr√°f meghat√°rozza, hogyan kell kisz√°m√≠tani a kimenetet (√°ltal√°ban a vesztes√©gf√ºggv√©nyt) adott bemeneti param√©terekkel, √©s GPU-ra k√ºldhet≈ë sz√°m√≠t√°sra, ha el√©rhet≈ë. Vannak funkci√≥k, amelyekkel differenci√°lhatjuk ezt a sz√°m√≠t√°si gr√°fot √©s kisz√°m√≠thatjuk a gradiens √©rt√©keket, amelyeket azt√°n a modell param√©tereinek optimaliz√°l√°s√°ra haszn√°lhatunk.

**A magas szint≈± API-k** a neur√°lis h√°l√≥zatokat **r√©tegek sorozatak√©nt** kezelik, √©s a legt√∂bb neur√°lis h√°l√≥zat fel√©p√≠t√©s√©t jelent≈ësen leegyszer≈±s√≠tik. A modell tan√≠t√°sa √°ltal√°ban az adatok el≈ëk√©sz√≠t√©s√©t √©s egy `fit` f√ºggv√©ny megh√≠v√°s√°t ig√©nyli.

A magas szint≈± API lehet≈ëv√© teszi tipikus neur√°lis h√°l√≥zatok gyors fel√©p√≠t√©s√©t an√©lk√ºl, hogy sok r√©szlettel kellene foglalkozni. Ugyanakkor az alacsony szint≈± API sokkal nagyobb kontrollt biztos√≠t a tan√≠t√°si folyamat felett, ez√©rt gyakran haszn√°lj√°k kutat√°sok sor√°n, amikor √∫j neur√°lis h√°l√≥zat architekt√∫r√°kkal dolgozunk.

Fontos meg√©rteni, hogy mindk√©t API-t egy√ºtt is haszn√°lhatjuk, p√©ld√°ul saj√°t h√°l√≥zati r√©teg architekt√∫r√°t fejleszthet√ºnk alacsony szint≈± API-val, majd haszn√°lhatjuk azt egy nagyobb h√°l√≥zatban, amelyet magas szint≈± API-val √©p√≠tett√ºnk √©s tan√≠tottunk. Vagy defini√°lhatunk egy h√°l√≥zatot magas szint≈± API-val r√©tegek sorozatak√©nt, majd saj√°t alacsony szint≈± tan√≠t√°si ciklust haszn√°lhatunk az optimaliz√°ci√≥hoz. Mindk√©t API ugyanazokat az alapvet≈ë koncepci√≥kat haszn√°lja, √©s √∫gy tervezt√©k ≈ëket, hogy j√≥l m≈±k√∂djenek egy√ºtt.

## Tanul√°s

Ebben a kurzusban a legt√∂bb tartalmat PyTorch √©s TensorFlow keretrendszerekhez is k√≠n√°ljuk. Kiv√°laszthatod a prefer√°lt keretrendszert, √©s csak a megfelel≈ë jegyzetf√ºzeteket tanulm√°nyozhatod. Ha nem vagy biztos benne, melyik keretrendszert v√°laszd, olvass el n√©h√°ny vit√°t az interneten a **PyTorch vs. TensorFlow** t√©m√°ban. Megn√©zheted mindk√©t keretrendszert, hogy jobban meg√©rtsd ≈ëket.

Ahol lehets√©ges, egyszer≈±s√©g kedv√©√©rt magas szint≈± API-kat fogunk haszn√°lni. Ugyanakkor fontosnak tartjuk, hogy meg√©rts√ºk, hogyan m≈±k√∂dnek a neur√°lis h√°l√≥zatok az alapokt√≥l kezdve, ez√©rt az elej√©n alacsony szint≈± API-val √©s tensorokkal dolgozunk. Ha azonban gyorsan szeretn√©l haladni, √©s nem akarsz sok id≈ët t√∂lteni ezeknek a r√©szleteknek a tanul√°s√°val, kihagyhatod ezeket, √©s k√∂zvetlen√ºl a magas szint≈± API jegyzetf√ºzetekbe l√©phetsz.

## ‚úçÔ∏è Gyakorlatok: Keretrendszerek

Folytasd a tanul√°st az al√°bbi jegyzetf√ºzetekben:

Alacsony szint≈± API | [TensorFlow+Keras Jegyzetf√ºzet](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------------|-------------------------------------|--------------------------------
Magas szint≈± API    | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb)         | *PyTorch Lightning*

Miut√°n elsaj√°t√≠tottad a keretrendszereket, tekints√ºk √°t az overfitting fogalm√°t.

# Overfitting

Az overfitting rendk√≠v√ºl fontos fogalom a g√©pi tanul√°sban, √©s nagyon fontos, hogy j√≥l meg√©rts√ºk!

Vegy√ºk p√©ld√°nak az al√°bbi probl√©m√°t, amelyben 5 pontot pr√≥b√°lunk k√∂zel√≠teni (a grafikonokon `x`-szel jel√∂lve):

![line√°ris](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.hu.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.hu.jpg)
-------------------------|--------------------------
**Line√°ris modell, 2 param√©ter** | **Nem-line√°ris modell, 7 param√©ter**
Tan√≠t√°si hiba = 5.3 | Tan√≠t√°si hiba = 0
Valid√°ci√≥s hiba = 5.1 | Valid√°ci√≥s hiba = 20

* A bal oldalon egy j√≥ egyenes vonal k√∂zel√≠t√©st l√°tunk. Mivel a param√©terek sz√°ma megfelel≈ë, a modell helyesen √©rti a pontok eloszl√°s√°nak l√©nyeg√©t.
* A jobb oldalon a modell t√∫l er≈ës. Mivel csak 5 pontunk van, √©s a modellnek 7 param√©tere van, √∫gy tudja be√°ll√≠tani mag√°t, hogy minden ponton √°thaladjon, √≠gy a tan√≠t√°si hiba 0 lesz. Ez azonban megakad√°lyozza, hogy a modell meg√©rtse az adatok m√∂g√∂tti helyes mint√°zatot, √≠gy a valid√°ci√≥s hiba nagyon magas.

Nagyon fontos megtal√°lni a megfelel≈ë egyens√∫lyt a modell gazdags√°ga (param√©terek sz√°ma) √©s a tan√≠t√°si mint√°k sz√°ma k√∂z√∂tt.

## Mi√©rt fordul el≈ë overfitting?

  * Nem el√©g tan√≠t√°si adat
  * T√∫l er≈ës modell
  * T√∫l sok zaj a bemeneti adatokban

## Hogyan √©szlelhet≈ë az overfitting?

Ahogy a fenti grafikonon l√°that√≥, az overfittinget nagyon alacsony tan√≠t√°si hiba √©s magas valid√°ci√≥s hiba jelezheti. √Åltal√°ban a tan√≠t√°s sor√°n mind a tan√≠t√°si, mind a valid√°ci√≥s hib√°k cs√∂kkenni kezdenek, majd egy ponton a valid√°ci√≥s hiba meg√°llhat a cs√∂kken√©sben, √©s n√∂vekedni kezdhet. Ez az overfitting jele, √©s annak indik√°tora, hogy val√≥sz√≠n≈±leg abba kell hagynunk a tan√≠t√°st (vagy legal√°bbis k√©sz√≠ten√ºnk kell egy pillanatk√©pet a modellr≈ël).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.hu.png)

## Hogyan el≈ëzhet≈ë meg az overfitting?

Ha l√°tod, hogy overfitting t√∂rt√©nik, az al√°bbiakat teheted:

 * N√∂veld a tan√≠t√°si adatok mennyis√©g√©t
 * Cs√∂kkentsd a modell komplexit√°s√°t
 * Haszn√°lj valamilyen [regulariz√°ci√≥s technik√°t](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), p√©ld√°ul [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), amelyet k√©s≈ëbb megvizsg√°lunk.

## Overfitting √©s Bias-Variance Tradeoff

Az overfitting val√≥j√°ban egy √°ltal√°nosabb statisztikai probl√©ma, amelyet [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) n√©ven ismer√ºnk. Ha megvizsg√°ljuk a modell√ºnk hib√°inak lehets√©ges forr√°sait, k√©tf√©le hib√°t l√°thatunk:

* **Bias hib√°k**, amelyeket az algoritmusunk okoz, mert nem k√©pes helyesen megragadni a tan√≠t√°si adatok k√∂z√∂tti kapcsolatot. Ez abb√≥l ad√≥dhat, hogy a modell√ºnk nem el√©g er≈ës (**underfitting**).
* **Variance hib√°k**, amelyeket az okoz, hogy a modell a bemeneti adatok zaj√°t k√∂zel√≠ti ahelyett, hogy a jelent≈ës kapcsolatot ragadn√° meg (**overfitting**).

A tan√≠t√°s sor√°n a bias hiba cs√∂kken (mivel a modell megtanulja k√∂zel√≠teni az adatokat), √©s a variance hiba n√∂vekszik. Fontos, hogy meg√°ll√≠tsuk a tan√≠t√°st - ak√°r manu√°lisan (amikor √©szlelj√ºk az overfittinget), ak√°r automatikusan (regulariz√°ci√≥ bevezet√©s√©vel) -, hogy megel≈ëzz√ºk az overfittinget.

## √ñsszegz√©s

Ebben a leck√©ben megtanultad a k√©t legn√©pszer≈±bb AI keretrendszer, a TensorFlow √©s a PyTorch k√ºl√∂nb√∂z≈ë API-jainak k√∂z√∂tti k√ºl√∂nbs√©geket. Emellett megismerkedt√©l egy nagyon fontos t√©m√°val, az overfittinggel.

## üöÄ Kih√≠v√°s

Az accompanying jegyzetf√ºzetekben 'feladatokat' tal√°lsz az alj√°n; dolgozd √°t a jegyzetf√ºzeteket, √©s v√©gezd el a feladatokat.

## [El≈ëad√°s ut√°ni kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

V√©gezz kutat√°st az al√°bbi t√©m√°kban:

- TensorFlow
- PyTorch
- Overfitting

Tedd fel magadnak a k√∂vetkez≈ë k√©rd√©seket:

- Mi a k√ºl√∂nbs√©g a TensorFlow √©s a PyTorch k√∂z√∂tt?
- Mi a k√ºl√∂nbs√©g az overfitting √©s az underfitting k√∂z√∂tt?

## [Feladat](lab/README.md)

Ebben a laborban k√©t oszt√°lyoz√°si probl√©m√°t kell megoldanod egy- √©s t√∂bbr√©teg≈± teljesen √∂sszekapcsolt h√°l√≥zatokkal PyTorch vagy TensorFlow haszn√°lat√°val.

* [√ötmutat√≥](lab/README.md)
* [Jegyzetf√ºzet](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.