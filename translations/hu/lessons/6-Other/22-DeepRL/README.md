<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "dbacf9b1915612981d76059678e563e5",
  "translation_date": "2025-08-25T23:29:53+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "hu"
}
-->
# M√©ly meger≈ës√≠t√©ses tanul√°s

A meger≈ës√≠t√©ses tanul√°s (RL) az egyik alapvet≈ë g√©pi tanul√°si paradigma, a fel√ºgyelt tanul√°s √©s a nem fel√ºgyelt tanul√°s mellett. M√≠g a fel√ºgyelt tanul√°sn√°l ismert kimenetekkel rendelkez≈ë adathalmazra t√°maszkodunk, az RL a **cselekv√©s √°ltali tanul√°son** alapul. P√©ld√°ul, amikor el≈ësz√∂r l√°tunk egy sz√°m√≠t√≥g√©pes j√°t√©kot, elkezd√ºnk j√°tszani, m√©g akkor is, ha nem ismerj√ºk a szab√°lyokat, √©s hamarosan k√©pesek vagyunk jav√≠tani a k√©pess√©geinken puszt√°n az√°ltal, hogy j√°tszunk √©s alkalmazkodunk a viselked√©s√ºnkh√∂z.

## [El≈ëad√°s el≈ëtti kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Az RL v√©grehajt√°s√°hoz sz√ºks√©g√ºnk van:

* Egy **k√∂rnyezetre** vagy **szimul√°torra**, amely meghat√°rozza a j√°t√©k szab√°lyait. K√©pesnek kell lenn√ºnk k√≠s√©rleteket futtatni a szimul√°torban, √©s megfigyelni az eredm√©nyeket.
* Valamilyen **jutalomf√ºggv√©nyre**, amely jelzi, mennyire volt sikeres a k√≠s√©rlet√ºnk. Sz√°m√≠t√≥g√©pes j√°t√©k tanul√°sa eset√©n a jutalom a v√©gs≈ë pontsz√°munk lenne.

A jutalomf√ºggv√©ny alapj√°n k√©pesnek kell lenn√ºnk m√≥dos√≠tani a viselked√©s√ºnket √©s jav√≠tani a k√©pess√©geinken, hogy legk√∂zelebb jobban teljes√≠ts√ºnk. A f≈ë k√ºl√∂nbs√©g az RL √©s m√°s g√©pi tanul√°si t√≠pusok k√∂z√∂tt az, hogy az RL-ben √°ltal√°ban nem tudjuk, hogy nyert√ºnk-e vagy vesztett√ºnk-e, am√≠g be nem fejezz√ºk a j√°t√©kot. Ez√©rt nem mondhatjuk meg, hogy egy adott l√©p√©s √∂nmag√°ban j√≥-e vagy sem - csak a j√°t√©k v√©g√©n kapunk jutalmat.

Az RL sor√°n √°ltal√°ban sok k√≠s√©rletet hajtunk v√©gre. Minden k√≠s√©rlet sor√°n egyens√∫lyozni kell az eddig tanult optim√°lis strat√©gia k√∂vet√©se (**kihaszn√°l√°s**) √©s az √∫j lehets√©ges √°llapotok felfedez√©se (**felfedez√©s**) k√∂z√∂tt.

## OpenAI Gym

Az RL-hez nagyszer≈± eszk√∂z az [OpenAI Gym](https://gym.openai.com/) - egy **szimul√°ci√≥s k√∂rnyezet**, amely sz√°mos k√ºl√∂nb√∂z≈ë k√∂rnyezetet k√©pes szimul√°lni, az Atari j√°t√©kokt√≥l kezdve a fizikai egyens√∫lyoz√°si probl√©m√°kig. Ez az egyik legn√©pszer≈±bb szimul√°ci√≥s k√∂rnyezet a meger≈ës√≠t√©ses tanul√°si algoritmusok k√©pz√©s√©hez, √©s a [OpenAI](https://openai.com/) tartja karban.

> **Note**: Az OpenAI Gym √∂sszes el√©rhet≈ë k√∂rnyezet√©t [itt](https://gym.openai.com/envs/#classic_control) tekintheti meg.

## CartPole egyens√∫lyoz√°s

Val√≥sz√≠n≈±leg mindannyian l√°ttatok m√°r modern egyens√∫lyoz√≥ eszk√∂z√∂ket, mint p√©ld√°ul a *Segway* vagy *Gyroscooter*. Ezek k√©pesek automatikusan egyens√∫lyozni, az√°ltal, hogy a kerekeiket az accelerom√©ter vagy giroszk√≥p jeleire reag√°lva √°ll√≠tj√°k be. Ebben a r√©szben megtanuljuk, hogyan oldjunk meg egy hasonl√≥ probl√©m√°t - egy r√∫d egyens√∫lyoz√°s√°t. Ez hasonl√≥ ahhoz a helyzethez, amikor egy cirkuszi el≈ëad√≥ egy rudat pr√≥b√°l egyens√∫lyozni a kez√©n - de ez az egyens√∫lyoz√°s csak 1D-ben t√∂rt√©nik.

Az egyens√∫lyoz√°s egyszer≈±s√≠tett v√°ltozata **CartPole** probl√©mak√©nt ismert. A CartPole vil√°gban van egy v√≠zszintes cs√∫szka, amely balra vagy jobbra mozoghat, √©s a c√©l az, hogy egy f√ºgg≈ëleges rudat egyens√∫lyozzunk a cs√∫szka tetej√©n, mik√∂zben az mozog.

<img alt="egy cartpole" src="images/cartpole.png" width="200"/>

Ennek a k√∂rnyezetnek a l√©trehoz√°s√°hoz √©s haszn√°lat√°hoz n√©h√°ny Python k√≥dsorra van sz√ºks√©g:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Minden k√∂rnyezet pontosan ugyan√∫gy √©rhet≈ë el:
* `env.reset` √∫j k√≠s√©rletet ind√≠t
* `env.step` szimul√°ci√≥s l√©p√©st hajt v√©gre. Egy **akci√≥t** kap az **akci√≥t√©rb≈ël**, √©s visszaad egy **megfigyel√©st** (a megfigyel√©si t√©rb≈ël), valamint egy jutalmat √©s egy befejez√©si jelz≈ët.

A fenti p√©ld√°ban minden l√©p√©sn√©l v√©letlenszer≈± akci√≥t hajtunk v√©gre, ez√©rt a k√≠s√©rlet √©lettartama nagyon r√∂vid:

![nem egyens√∫lyoz√≥ cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Az RL algoritmus c√©lja egy modell - az √∫gynevezett **politika** œÄ - k√©pz√©se, amely az adott √°llapothoz tartoz√≥ akci√≥t adja vissza. A politik√°t tekinthetj√ºk val√≥sz√≠n≈±s√©gi modellnek is, p√©ld√°ul b√°rmely √°llapot *s* √©s akci√≥ *a* eset√©n visszaadja a val√≥sz√≠n≈±s√©get œÄ(*a*|*s*), hogy az *s* √°llapotban az *a* akci√≥t kellene v√©grehajtanunk.

## Policy Gradients algoritmus

A politika modellez√©s√©nek legnyilv√°nval√≥bb m√≥dja egy neur√°lis h√°l√≥zat l√©trehoz√°sa, amely bemenetk√©nt az √°llapotokat kapja, √©s visszaadja a megfelel≈ë akci√≥kat (pontosabban az √∂sszes akci√≥ val√≥sz√≠n≈±s√©geit). Bizonyos √©rtelemben ez hasonl√≥ lenne egy norm√°l oszt√°lyoz√°si feladathoz, egy jelent≈ës k√ºl√∂nbs√©ggel - el≈ëre nem tudjuk, hogy mely akci√≥kat kellene v√©grehajtanunk az egyes l√©p√©sekben.

Az √∂tlet itt az, hogy becs√ºlj√ºk meg ezeket a val√≥sz√≠n≈±s√©geket. L√©trehozunk egy **kumulat√≠v jutalmak** vektort, amely megmutatja a k√≠s√©rlet minden l√©p√©s√©n√©l el√©rt teljes jutalmat. Emellett alkalmazunk **jutalom diszkont√°l√°st**, azaz a kor√°bbi jutalmakat egy Œ≥=0.99 egy√ºtthat√≥val szorozzuk meg, hogy cs√∂kkents√ºk a kor√°bbi jutalmak szerep√©t. Ezut√°n meger≈ës√≠tj√ºk azokat a l√©p√©seket a k√≠s√©rleti √∫tvonal ment√©n, amelyek nagyobb jutalmakat eredm√©nyeznek.

> Tudjon meg t√∂bbet a Policy Gradient algoritmusr√≥l, √©s n√©zze meg m≈±k√∂d√©s k√∂zben a [p√©lda notebookban](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Actor-Critic algoritmus

A Policy Gradients megk√∂zel√≠t√©s tov√°bbfejlesztett v√°ltozata az **Actor-Critic**. A m√∂g√∂ttes f≈ë √∂tlet az, hogy a neur√°lis h√°l√≥zatot k√©t dolog visszaad√°s√°ra k√©pezz√ºk:

* A politika, amely meghat√°rozza, hogy melyik akci√≥t kell v√©grehajtani. Ezt a r√©szt **sz√≠n√©sznek** (actor) nevezz√ºk.
* Az √°llapotban v√°rhat√≥ teljes jutalom becsl√©se - ezt a r√©szt **kritikusnak** (critic) nevezz√ºk.

Bizonyos √©rtelemben ez az architekt√∫ra hasonl√≠t egy [GAN](../../4-ComputerVision/10-GANs/README.md)-ra, ahol k√©t h√°l√≥zatot egym√°s ellen k√©pez√ºnk. Az actor-critic modellben a sz√≠n√©sz javasolja a v√©grehajtand√≥ akci√≥t, m√≠g a kritikus megpr√≥b√°l kritikus lenni, √©s megbecs√ºlni az eredm√©nyt. Azonban a c√©lunk az, hogy ezeket a h√°l√≥zatokat √∂sszhangban k√©pezz√ºk.

Mivel ismerj√ºk mind a val√≥s kumulat√≠v jutalmakat, mind a kritikus √°ltal a k√≠s√©rlet sor√°n visszaadott eredm√©nyeket, viszonylag k√∂nny≈± olyan vesztes√©gf√ºggv√©nyt l√©trehozni, amely minimaliz√°lja a kett≈ë k√∂z√∂tti k√ºl√∂nbs√©get. Ez adja a **kritikus vesztes√©get**. Az **sz√≠n√©sz vesztes√©get** ugyanazzal a megk√∂zel√≠t√©ssel sz√°m√≠thatjuk ki, mint a Policy Gradient algoritmusban.

Egy ilyen algoritmus futtat√°sa ut√°n elv√°rhatjuk, hogy a CartPole √≠gy viselkedjen:

![egyens√∫lyoz√≥ cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Gyakorlatok: Policy Gradients √©s Actor-Critic RL

Folytassa a tanul√°st az al√°bbi notebookokban:

* [RL TensorFlow-ban](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL PyTorch-ban](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Egy√©b RL feladatok

A meger≈ës√≠t√©ses tanul√°s manaps√°g gyorsan n√∂vekv≈ë kutat√°si ter√ºlet. N√©h√°ny √©rdekes p√©lda a meger≈ës√≠t√©ses tanul√°sra:

* Sz√°m√≠t√≥g√©p tan√≠t√°sa **Atari j√°t√©kok** j√°tsz√°s√°ra. Ennek a probl√©m√°nak a kih√≠v√°sa, hogy nem egyszer≈± √°llapotot kapunk vektork√©nt, hanem egy k√©perny≈ëk√©pet - √©s CNN-t kell haszn√°lnunk, hogy ezt a k√©perny≈ëk√©pet vektorr√° alak√≠tsuk, vagy hogy jutalominform√°ci√≥t nyerj√ºnk ki. Az Atari j√°t√©kok el√©rhet≈ëk a Gym-ben.
* Sz√°m√≠t√≥g√©p tan√≠t√°sa t√°bl√°s j√°t√©kok, p√©ld√°ul sakk √©s go j√°tsz√°s√°ra. A leg√∫jabb cs√∫cstechnol√≥gi√°s programok, mint p√©ld√°ul **Alpha Zero**, k√©t √ºgyn√∂k egym√°s elleni j√°t√©k√°val, √©s minden l√©p√©sn√©l val√≥ fejl≈ëd√©ssel lettek kik√©pezve.
* Az iparban az RL-t szimul√°ci√≥b√≥l sz√°rmaz√≥ vez√©rl≈ërendszerek l√©trehoz√°s√°ra haszn√°lj√°k. Egy [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) nev≈± szolg√°ltat√°s kifejezetten erre lett tervezve.

## √ñsszegz√©s

Most megtanultuk, hogyan k√©pezz√ºnk √ºgyn√∂k√∂ket, hogy j√≥ eredm√©nyeket √©rjenek el puszt√°n az√°ltal, hogy egy jutalomf√ºggv√©nyt biztos√≠tunk nekik, amely meghat√°rozza a j√°t√©k k√≠v√°nt √°llapot√°t, √©s lehet≈ës√©get adunk nekik az intelligens keres√©si t√©r felfedez√©s√©re. Sikeresen kipr√≥b√°ltunk k√©t algoritmust, √©s viszonylag r√∂vid id≈ë alatt j√≥ eredm√©nyt √©rt√ºnk el. Ez azonban csak a kezdete az RL-be val√≥ utaz√°s√°nak, √©s mindenk√©ppen √©rdemes egy k√ºl√∂n tanfolyamot elv√©gezni, ha m√©lyebben szeretne belemer√ºlni.

## üöÄ Kih√≠v√°s

Fedezze fel az 'Egy√©b RL feladatok' szakaszban felsorolt alkalmaz√°sokat, √©s pr√≥b√°ljon meg egyet megval√≥s√≠tani!

## [El≈ëad√°s ut√°ni kv√≠z](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Tudjon meg t√∂bbet a klasszikus meger≈ës√≠t√©ses tanul√°sr√≥l a [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) seg√≠ts√©g√©vel.

N√©zze meg [ezt a nagyszer≈± vide√≥t](https://www.youtube.com/watch?v=qv6UVOQ0F44), amely arr√≥l sz√≥l, hogyan tanulhat meg egy sz√°m√≠t√≥g√©p Super Mario-t j√°tszani.

## Feladat: [Train a Mountain Car](lab/README.md)

A feladat sor√°n az lenne a c√©lja, hogy egy m√°sik Gym k√∂rnyezetet - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) - k√©pezzen ki.

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.