<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "04395657fc01648f8f70484d0e55ab67",
  "translation_date": "2025-09-23T11:09:36+00:00",
  "source_file": "lessons/6-Other/22-DeepRL/README.md",
  "language_code": "hu"
}
-->
# M√©ly meger≈ës√≠t√©ses tanul√°s

A meger≈ës√≠t√©ses tanul√°s (RL) az egyik alapvet≈ë g√©pi tanul√°si paradigma, a fel√ºgyelt tanul√°s √©s a nem fel√ºgyelt tanul√°s mellett. M√≠g a fel√ºgyelt tanul√°s sor√°n ismert eredm√©nyekkel rendelkez≈ë adathalmazra t√°maszkodunk, az RL a **cselekv√©s √°ltali tanul√°son** alapul. P√©ld√°ul, amikor el≈ësz√∂r l√°tunk egy sz√°m√≠t√≥g√©pes j√°t√©kot, elkezd√ºnk j√°tszani, m√©g akkor is, ha nem ismerj√ºk a szab√°lyokat, √©s hamarosan k√©pesek vagyunk jav√≠tani a k√©pess√©geinket puszt√°n a j√°t√©k √©s a viselked√©s√ºnk m√≥dos√≠t√°sa r√©v√©n.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/43)

Az RL v√©grehajt√°s√°hoz sz√ºks√©g√ºnk van:

* Egy **k√∂rnyezetre** vagy **szimul√°torra**, amely meghat√°rozza a j√°t√©k szab√°lyait. K√©pesnek kell lenn√ºnk k√≠s√©rleteket futtatni a szimul√°torban, √©s megfigyelni az eredm√©nyeket.
* Valamilyen **jutalomf√ºggv√©nyre**, amely jelzi, mennyire volt sikeres a k√≠s√©rlet√ºnk. Sz√°m√≠t√≥g√©pes j√°t√©k tanul√°sa eset√©n a jutalom a v√©gs≈ë pontsz√°munk lenne.

A jutalomf√ºggv√©ny alapj√°n k√©pesnek kell lenn√ºnk m√≥dos√≠tani a viselked√©s√ºnket √©s jav√≠tani a k√©pess√©geinket, hogy legk√∂zelebb jobban teljes√≠ts√ºnk. A f≈ë k√ºl√∂nbs√©g az RL √©s m√°s g√©pi tanul√°si t√≠pusok k√∂z√∂tt az, hogy az RL-ben √°ltal√°ban nem tudjuk, hogy nyert√ºnk vagy vesztett√ºnk, am√≠g be nem fejezz√ºk a j√°t√©kot. Ez√©rt nem mondhatjuk meg, hogy egy adott l√©p√©s √∂nmag√°ban j√≥-e vagy sem - csak a j√°t√©k v√©g√©n kapunk jutalmat.

Az RL sor√°n √°ltal√°ban sok k√≠s√©rletet hajtunk v√©gre. Minden k√≠s√©rlet sor√°n egyens√∫lyozni kell az eddig megtanult optim√°lis strat√©gia k√∂vet√©se (**kihaszn√°l√°s**) √©s az √∫j lehets√©ges √°llapotok felfedez√©se (**felfedez√©s**) k√∂z√∂tt.

## OpenAI Gym

Az RL-hez kiv√°l√≥ eszk√∂z az [OpenAI Gym](https://gym.openai.com/) - egy **szimul√°ci√≥s k√∂rnyezet**, amely sz√°mos k√ºl√∂nb√∂z≈ë k√∂rnyezetet k√©pes szimul√°lni, az Atari j√°t√©kokt√≥l kezdve a fizikai egyens√∫lyoz√°si probl√©m√°kig. Ez az egyik legn√©pszer≈±bb szimul√°ci√≥s k√∂rnyezet a meger≈ës√≠t√©ses tanul√°si algoritmusok k√©pz√©s√©hez, √©s a [OpenAI](https://openai.com/) tartja karban.

> **Note**: Az OpenAI Gym √∂sszes el√©rhet≈ë k√∂rnyezet√©t [itt](https://gym.openai.com/envs/#classic_control) tekintheti meg.

## CartPole egyens√∫lyoz√°s

Val√≥sz√≠n≈±leg mindannyian l√°ttatok m√°r modern egyens√∫lyoz√≥ eszk√∂z√∂ket, mint p√©ld√°ul a *Segway* vagy *Gyroscooter*. Ezek k√©pesek automatikusan egyens√∫lyozni, az√°ltal hogy a kerekeiket az accelerom√©ter vagy giroszk√≥p jeleire reag√°lva √°ll√≠tj√°k be. Ebben a r√©szben megtanuljuk, hogyan oldjunk meg egy hasonl√≥ probl√©m√°t - egy r√∫d egyens√∫lyoz√°s√°t. Ez hasonl√≥ ahhoz a helyzethez, amikor egy cirkuszi el≈ëad√≥ egy rudat egyens√∫lyoz a kez√©n - de ez az egyens√∫lyoz√°s csak 1D-ben t√∂rt√©nik.

Az egyens√∫lyoz√°s egyszer≈±s√≠tett v√°ltozata **CartPole** probl√©mak√©nt ismert. A CartPole vil√°gban van egy v√≠zszintes cs√∫szka, amely balra vagy jobbra mozoghat, √©s a c√©l az, hogy egy f√ºgg≈ëleges rudat egyens√∫lyozzunk a cs√∫szka tetej√©n, mik√∂zben az mozog.

<img alt="egy cartpole" src="images/cartpole.png" width="200"/>

Ennek a k√∂rnyezetnek a l√©trehoz√°s√°hoz √©s haszn√°lat√°hoz n√©h√°ny Python k√≥dsorra van sz√ºks√©g:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Minden k√∂rnyezet pontosan ugyan√∫gy √©rhet≈ë el:
* `env.reset` √∫j k√≠s√©rletet ind√≠t
* `env.step` egy szimul√°ci√≥s l√©p√©st hajt v√©gre. Egy **akci√≥t** kap az **akci√≥t√©rb≈ël**, √©s visszaad egy **megfigyel√©st** (a megfigyel√©si t√©rb≈ël), valamint egy jutalmat √©s egy befejez√©si jelz≈ët.

A fenti p√©ld√°ban minden l√©p√©sn√©l v√©letlenszer≈± akci√≥t hajtunk v√©gre, ez√©rt a k√≠s√©rlet √©lettartama nagyon r√∂vid:

![nem egyens√∫lyoz√≥ cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Az RL algoritmus c√©lja egy modell - az √∫gynevezett **policy** &pi; - k√©pz√©se, amely az adott √°llapotnak megfelel≈ë akci√≥t ad vissza. A policy-t tekinthetj√ºk val√≥sz√≠n≈±s√©gi modellnek is, p√©ld√°ul b√°rmely √°llapot *s* √©s akci√≥ *a* eset√©n visszaadja a val√≥sz√≠n≈±s√©get &pi;(*a*|*s*), hogy *a*-t kellene v√°lasztanunk *s* √°llapotban.

## Policy Gradients algoritmus

A policy modellez√©s√©nek legnyilv√°nval√≥bb m√≥dja egy neur√°lis h√°l√≥zat l√©trehoz√°sa, amely bemenetk√©nt az √°llapotokat veszi, √©s visszaadja a megfelel≈ë akci√≥kat (pontosabban az √∂sszes akci√≥ val√≥sz√≠n≈±s√©geit). Bizonyos √©rtelemben ez hasonl√≥ lenne egy norm√°l oszt√°lyoz√°si feladathoz, egy jelent≈ës k√ºl√∂nbs√©ggel - el≈ëre nem tudjuk, hogy mely akci√≥kat kellene v√©grehajtanunk az egyes l√©p√©sekben.

Az √∂tlet itt az, hogy becs√ºlj√ºk meg ezeket a val√≥sz√≠n≈±s√©geket. L√©trehozunk egy **kumulat√≠v jutalmak** vektort, amely megmutatja a teljes jutalmunkat az egyes k√≠s√©rleti l√©p√©sek sor√°n. Emellett alkalmazunk **jutalom diszkont√°l√°st**, azaz a kor√°bbi jutalmakat egy &gamma;=0.99 egy√ºtthat√≥val szorozzuk meg, hogy cs√∂kkents√ºk a kor√°bbi jutalmak szerep√©t. Ezut√°n meger≈ës√≠tj√ºk azokat a l√©p√©seket a k√≠s√©rleti √∫ton, amelyek nagyobb jutalmakat eredm√©nyeznek.

> Tudjon meg t√∂bbet a Policy Gradient algoritmusr√≥l, √©s n√©zze meg m≈±k√∂d√©s k√∂zben a [p√©lda notebookban](CartPole-RL-TF.ipynb).

## Actor-Critic algoritmus

A Policy Gradients megk√∂zel√≠t√©s tov√°bbfejlesztett v√°ltozata az **Actor-Critic**. Az alap√∂tlet az, hogy a neur√°lis h√°l√≥zatot k√©t dolog visszaad√°s√°ra k√©pezz√ºk ki:

* A policy, amely meghat√°rozza, hogy melyik akci√≥t kell v√©grehajtani. Ezt a r√©szt **actor**-nak nevezz√ºk.
* Az √∂sszes jutalom becsl√©se, amelyet az adott √°llapotban v√°rhatunk - ezt a r√©szt **critic**-nak nevezz√ºk.

Bizonyos √©rtelemben ez az architekt√∫ra hasonl√≠t egy [GAN](../../4-ComputerVision/10-GANs/README.md)-ra, ahol k√©t h√°l√≥zatot egym√°s ellen k√©pez√ºnk ki. Az actor-critic modellben az actor javasolja az akci√≥t, amelyet v√©gre kell hajtanunk, m√≠g a critic kritikus szerepet t√∂lt be, √©s becs√ºli az eredm√©nyt. Azonban a c√©lunk az, hogy ezeket a h√°l√≥zatokat √∂sszhangban k√©pezz√ºk ki.

Mivel ismerj√ºk mind a val√≥s kumulat√≠v jutalmakat, mind a critic √°ltal a k√≠s√©rlet sor√°n visszaadott eredm√©nyeket, viszonylag k√∂nny≈± olyan vesztes√©gf√ºggv√©nyt l√©trehozni, amely minimaliz√°lja a kett≈ë k√∂z√∂tti k√ºl√∂nbs√©get. Ez adja a **critic vesztes√©get**. Az **actor vesztes√©get** ugyanazzal a megk√∂zel√≠t√©ssel sz√°m√≠thatjuk ki, mint a Policy Gradient algoritmusban.

Egy ilyen algoritmus futtat√°sa ut√°n elv√°rhatjuk, hogy a CartPole √≠gy viselkedjen:

![egyens√∫lyoz√≥ cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Gyakorlatok: Policy Gradients √©s Actor-Critic RL

Folytassa a tanul√°st az al√°bbi notebookokban:

* [RL TensorFlow-ban](CartPole-RL-TF.ipynb)
* [RL PyTorch-ban](CartPole-RL-PyTorch.ipynb)

## Egy√©b RL feladatok

A meger≈ës√≠t√©ses tanul√°s napjainkban gyorsan n√∂vekv≈ë kutat√°si ter√ºlet. N√©h√°ny √©rdekes p√©lda a meger≈ës√≠t√©ses tanul√°sra:

* Sz√°m√≠t√≥g√©p tan√≠t√°sa **Atari j√°t√©kok** j√°tsz√°s√°ra. A kih√≠v√°s ebben a probl√©m√°ban az, hogy nincs egyszer≈± √°llapot, amely vektork√©nt van √°br√°zolva, hanem egy k√©perny≈ëk√©p - √©s a CNN-t kell haszn√°lni, hogy ezt a k√©perny≈ëk√©pet vektorr√° alak√≠tsuk, vagy hogy jutalominform√°ci√≥t nyerj√ºnk ki. Az Atari j√°t√©kok el√©rhet≈ëk a Gym-ben.
* Sz√°m√≠t√≥g√©p tan√≠t√°sa t√°bl√°s j√°t√©kok, p√©ld√°ul sakk √©s go j√°tsz√°s√°ra. Nemr√©giben olyan cs√∫cstechnol√≥gi√°s programokat, mint az **Alpha Zero**, k√©t √ºgyn√∂k egym√°s elleni j√°t√©k√°val k√©pezt√©k ki, amelyek minden l√©p√©sn√©l javultak.
* Az iparban az RL-t szimul√°ci√≥b√≥l sz√°rmaz√≥ vez√©rl≈ërendszerek l√©trehoz√°s√°ra haszn√°lj√°k. Egy szolg√°ltat√°s, a [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste), kifejezetten erre lett tervezve.

## √ñsszegz√©s

Most megtanultuk, hogyan k√©pezz√ºnk ki √ºgyn√∂k√∂ket, hogy j√≥ eredm√©nyeket √©rjenek el puszt√°n az√°ltal, hogy egy jutalomf√ºggv√©nyt biztos√≠tunk sz√°mukra, amely meghat√°rozza a j√°t√©k k√≠v√°nt √°llapot√°t, √©s lehet≈ës√©get adunk nekik az intelligens keres√©si t√©r felfedez√©s√©re. Sikeresen kipr√≥b√°ltunk k√©t algoritmust, √©s viszonylag r√∂vid id≈ë alatt j√≥ eredm√©nyt √©rt√ºnk el. Ez azonban csak az RL-be val√≥ utaz√°s kezdete, √©s √©rdemes lehet egy k√ºl√∂n tanfolyamot elv√©gezni, ha m√©lyebben szeretne elmer√ºlni benne.

## üöÄ Kih√≠v√°s

Fedezze fel az 'Egy√©b RL feladatok' szakaszban felsorolt alkalmaz√°sokat, √©s pr√≥b√°ljon meg egyet megval√≥s√≠tani!

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ai/quiz/44)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Tudjon meg t√∂bbet a klasszikus meger≈ës√≠t√©ses tanul√°sr√≥l a [Machine Learning for Beginners Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) seg√≠ts√©g√©vel.

N√©zze meg [ezt a remek vide√≥t](https://www.youtube.com/watch?v=qv6UVOQ0F44), amely arr√≥l sz√≥l, hogyan tanulhat meg egy sz√°m√≠t√≥g√©p Super Mario-t j√°tszani.

## Feladat: [Train a Mountain Car](lab/README.md)

A feladat sor√°n az lenne a c√©lja, hogy egy m√°sik Gym k√∂rnyezetet - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) - k√©pezzen ki.

---

