<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f07c85bbf05a1f67505da98f4ecc124c",
  "translation_date": "2025-08-25T22:38:16+00:00",
  "source_file": "lessons/4-ComputerVision/10-GANs/README.md",
  "language_code": "hu"
}
-->
# Generat√≠v Adverz√°lis H√°l√≥zatok

Az el≈ëz≈ë r√©szben megismerkedt√ºnk a **generat√≠v modellekkel**: olyan modellekkel, amelyek k√©pesek √∫j k√©peket gener√°lni, amelyek hasonl√≥ak a tan√≠t√≥ adathalmazban l√©v≈ëkh√∂z. A VAE egy j√≥ p√©lda volt a generat√≠v modellre.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/19)

Azonban, ha valami igaz√°n jelent≈ës dolgot szeretn√©nk gener√°lni, p√©ld√°ul egy festm√©nyt megfelel≈ë felbont√°sban, a VAE-vel azt tapasztaljuk, hogy a tan√≠t√°s nem konverg√°l j√≥l. Ehhez az esethez meg kell ismerkedn√ºnk egy m√°sik architekt√∫r√°val, amelyet kifejezetten generat√≠v modellekhez terveztek - **Generat√≠v Adverz√°lis H√°l√≥zatokkal**, vagy GAN-ekkel.

A GAN f≈ë √∂tlete, hogy k√©t neur√°lis h√°l√≥zatot tan√≠tunk egym√°s ellen:

<img src="images/gan_architecture.png" width="70%"/>

> K√©p: [Dmitry Soshnikov](http://soshnikov.com)

> ‚úÖ Egy kis sz√≥szedet:
> * **Gener√°tor**: egy h√°l√≥zat, amely egy v√©letlenszer≈± vektort vesz, √©s eredm√©nyk√©nt k√©pet √°ll√≠t el≈ë.
> * **Diszkrimin√°tor**: egy h√°l√≥zat, amely egy k√©pet vesz, √©s meg kell mondania, hogy az val√≥di k√©p-e (a tan√≠t√≥ adathalmazb√≥l), vagy a gener√°tor √°ltal gener√°lt. L√©nyeg√©ben egy k√©poszt√°lyoz√≥.

### Diszkrimin√°tor

A diszkrimin√°tor architekt√∫r√°ja nem k√ºl√∂nb√∂zik egy szokv√°nyos k√©poszt√°lyoz√≥ h√°l√≥zatt√≥l. A legegyszer≈±bb esetben lehet egy teljesen √∂sszekapcsolt oszt√°lyoz√≥, de val√≥sz√≠n≈±leg egy [konvol√∫ci√≥s h√°l√≥zat](../07-ConvNets/README.md) lesz.

> ‚úÖ A konvol√∫ci√≥s h√°l√≥zatokon alapul√≥ GAN-t [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)-nek h√≠vj√°k.

Egy CNN diszkrimin√°tor a k√∂vetkez≈ë r√©tegekb≈ël √°ll: t√∂bb konvol√∫ci√≥+pooling (cs√∂kken≈ë t√©rbeli m√©rettel), √©s egy vagy t√∂bb teljesen √∂sszekapcsolt r√©teg, hogy "jellemz≈ë vektort" kapjunk, v√©g√ºl egy bin√°ris oszt√°lyoz√≥.

> ‚úÖ A 'pooling' ebben az √∂sszef√ºgg√©sben egy olyan technika, amely cs√∂kkenti a k√©p m√©ret√©t. "A pooling r√©tegek cs√∂kkentik az adatok dimenzi√≥it az√°ltal, hogy az egyik r√©teg neuroncsoportjainak kimeneteit egyetlen neuronba kombin√°lj√°k a k√∂vetkez≈ë r√©tegben." - [forr√°s](https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers)

### Gener√°tor

A gener√°tor valamivel bonyolultabb. √ögy tekinthetj√ºk, mint egy ford√≠tott diszkrimin√°tort. Egy l√°tens vektorb√≥l kiindulva (a jellemz≈ë vektor helyett) van egy teljesen √∂sszekapcsolt r√©tege, amely √°talak√≠tja a sz√ºks√©ges m√©retre/alakra, majd dekonvol√∫ci√≥+felbont√°sn√∂vel√©s k√∂vetkezik. Ez hasonl√≥ az [autoencoder](../09-Autoencoders/README.md) *dek√≥der* r√©sz√©hez.

> ‚úÖ Mivel a konvol√∫ci√≥s r√©teg line√°ris sz≈±r≈ëk√©nt m≈±k√∂dik, amely v√©gigmegy a k√©pen, a dekonvol√∫ci√≥ l√©nyeg√©ben hasonl√≥ a konvol√∫ci√≥hoz, √©s ugyanazzal a r√©teglogik√°val megval√≥s√≠that√≥.

<img src="images/gan_arch_detail.png" width="70%"/>

> K√©p: [Dmitry Soshnikov](http://soshnikov.com)

### A GAN tan√≠t√°sa

A GAN-eket **adverz√°lisnak** nevezik, mert folyamatos verseny zajlik a gener√°tor √©s a diszkrimin√°tor k√∂z√∂tt. E verseny sor√°n mind a gener√°tor, mind a diszkrimin√°tor javul, √≠gy a h√°l√≥zat egyre jobb k√©pek el≈ë√°ll√≠t√°s√°t tanulja meg.

A tan√≠t√°s k√©t szakaszban t√∂rt√©nik:

* **A diszkrimin√°tor tan√≠t√°sa**. Ez a feladat viszonylag egyszer≈±: gener√°lunk egy k√©pcsomagot a gener√°torral, 0-val c√≠mk√©zve ≈ëket, ami hamis k√©pet jelent, √©s vesz√ºnk egy k√©pcsomagot a bemeneti adathalmazb√≥l (1-es c√≠mk√©vel, val√≥di k√©p). Kapunk egy *diszkrimin√°tor vesztes√©get*, √©s v√©grehajtjuk a backpropot.
* **A gener√°tor tan√≠t√°sa**. Ez valamivel bonyolultabb, mert nem ismerj√ºk k√∂zvetlen√ºl a gener√°tor v√°rt kimenet√©t. Az eg√©sz GAN h√°l√≥zatot, amely gener√°torb√≥l √©s diszkrimin√°torb√≥l √°ll, v√©letlenszer≈± vektorokkal t√°pl√°ljuk, √©s azt v√°rjuk, hogy az eredm√©ny 1 legyen (ami val√≥di k√©peket jelent). Ekkor befagyasztjuk a diszkrimin√°tor param√©tereit (nem akarjuk, hogy ebben a l√©p√©sben tanuljon), √©s v√©grehajtjuk a backpropot.

E folyamat sor√°n a gener√°tor √©s a diszkrimin√°tor vesztes√©gei nem cs√∂kkennek jelent≈ësen. Ide√°lis esetben oszcill√°lniuk kellene, ami azt jelzi, hogy mindk√©t h√°l√≥zat jav√≠tja teljes√≠tm√©ny√©t.

## ‚úçÔ∏è Feladatok: GAN-ek

* [GAN Notebook TensorFlow/Keras-ban](../../../../../lessons/4-ComputerVision/10-GANs/GANTF.ipynb)
* [GAN Notebook PyTorch-ban](../../../../../lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb)

### Probl√©m√°k a GAN-ek tan√≠t√°s√°val

A GAN-ekr≈ël k√∂ztudott, hogy k√ºl√∂n√∂sen neh√©z ≈ëket tan√≠tani. √çme n√©h√°ny probl√©ma:

* **M√≥dus√∂sszeoml√°s**. Ez azt jelenti, hogy a gener√°tor megtanul egy sikeres k√©pet el≈ë√°ll√≠tani, amely becsapja a diszkrimin√°tort, de nem k√©pes k√ºl√∂nf√©le k√©peket gener√°lni.
* **√ârz√©kenys√©g a hiperparam√©terekre**. Gyakran el≈ëfordulhat, hogy egy GAN egy√°ltal√°n nem konverg√°l, majd hirtelen a tanul√°si r√°ta cs√∂kken√©se konvergenci√°t eredm√©nyez.
* **Egyens√∫ly fenntart√°sa** a gener√°tor √©s a diszkrimin√°tor k√∂z√∂tt. Sok esetben a diszkrimin√°tor vesztes√©ge viszonylag gyorsan null√°ra cs√∂kkenhet, ami azt eredm√©nyezi, hogy a gener√°tor nem k√©pes tov√°bb tanulni. Ennek lek√ºzd√©s√©re megpr√≥b√°lhatunk k√ºl√∂nb√∂z≈ë tanul√°si r√°t√°kat be√°ll√≠tani a gener√°tor √©s a diszkrimin√°tor sz√°m√°ra, vagy kihagyhatjuk a diszkrimin√°tor tan√≠t√°s√°t, ha a vesztes√©g m√°r t√∫l alacsony.
* **Magas felbont√°s√∫ tan√≠t√°s**. Ugyanaz a probl√©ma, mint az autoencoderekn√©l, ez a probl√©ma akkor jelentkezik, amikor t√∫l sok konvol√∫ci√≥s h√°l√≥zati r√©teget kell rekonstru√°lni, ami artefaktumokat eredm√©nyez. Ezt a probl√©m√°t √°ltal√°ban √∫gy oldj√°k meg, hogy √∫gynevezett **progressz√≠v n√∂veked√©st** alkalmaznak, amikor el≈ësz√∂r n√©h√°ny r√©teget alacsony felbont√°s√∫ k√©peken tan√≠tanak, majd a r√©tegeket "feloldj√°k" vagy hozz√°adj√°k. Egy m√°sik megold√°s az lenne, hogy extra kapcsolatokat adunk a r√©tegek k√∂z√∂tt, √©s egyszerre t√∂bb felbont√°st tan√≠tunk - r√©szletek√©rt l√°sd ezt a [Multi-Scale Gradient GANs tanulm√°nyt](https://arxiv.org/abs/1903.06048).

## St√≠lus√°tvitel

A GAN-ek nagyszer≈± m√≥dot k√≠n√°lnak m≈±v√©szi k√©pek gener√°l√°s√°ra. Egy m√°sik √©rdekes technika az √∫gynevezett **st√≠lus√°tvitel**, amely egy **tartalomk√©pet** vesz, √©s egy m√°sik st√≠lusban √∫jrarajzolja, sz≈±r≈ëket alkalmazva egy **st√≠lusk√©pb≈ël**.

A m≈±k√∂d√©se a k√∂vetkez≈ë:
* V√©letlenszer≈± zajk√©ppel kezd√ºnk (vagy egy tartalomk√©ppel, de a meg√©rt√©s kedv√©√©rt egyszer≈±bb zajk√©ppel kezdeni)
* C√©lunk egy olyan k√©p l√©trehoz√°sa, amely k√∂zel √°ll mind a tartalomk√©phez, mind a st√≠lusk√©phez. Ezt k√©t vesztes√©gf√ºggv√©ny hat√°rozza meg:
   - **Tartalomvesztes√©g**, amelyet a CNN √°ltal bizonyos r√©tegeken kinyert jellemz≈ëk alapj√°n sz√°m√≠tunk ki az aktu√°lis k√©p √©s a tartalomk√©p k√∂z√∂tt
   - **St√≠lusvesztes√©g**, amelyet az aktu√°lis k√©p √©s a st√≠lusk√©p k√∂z√∂tt sz√°m√≠tunk ki egy okos m√≥dszerrel Gram-m√°trixok seg√≠ts√©g√©vel (tov√°bbi r√©szletek az [p√©lda notebookban](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb))
* A k√©p sim√°bb√° t√©tele √©s a zaj elt√°vol√≠t√°sa √©rdek√©ben bevezetj√ºk a **Vari√°ci√≥s vesztes√©get**, amely a szomsz√©dos pixelek k√∂z√∂tti √°tlagos t√°vols√°got sz√°m√≠tja ki
* A f≈ë optimaliz√°ci√≥s ciklus az aktu√°lis k√©pet gradient descent (vagy m√°s optimaliz√°ci√≥s algoritmus) seg√≠ts√©g√©vel m√≥dos√≠tja, hogy minimaliz√°lja a teljes vesztes√©get, amely az √∂sszes vesztes√©g s√∫lyozott √∂sszege.

## ‚úçÔ∏è P√©lda: [St√≠lus√°tvitel](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb)

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/20)

## √ñsszegz√©s

Ebben a leck√©ben megismerkedt√©l a GAN-ekkel √©s azok tan√≠t√°s√°val. Megtanultad azokat a k√ºl√∂nleges kih√≠v√°sokat is, amelyekkel ez a neur√°lis h√°l√≥zat t√≠pus szembes√ºlhet, valamint n√©h√°ny strat√©gi√°t, hogyan lehet ezeket lek√ºzdeni.

## üöÄ Kih√≠v√°s

Futtasd v√©gig a [St√≠lus√°tvitel notebookot](../../../../../lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb) saj√°t k√©peiddel.

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Tov√°bbi inform√°ci√≥√©rt olvass t√∂bbet a GAN-ekr≈ël az al√°bbi forr√°sokban:

* Marco Pasini, [10 tanuls√°g, amit egy √©v alatt GAN-ek tan√≠t√°s√°b√≥l tanultam](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)
* [StyleGAN](https://en.wikipedia.org/wiki/StyleGAN), egy *de facto* GAN architekt√∫ra, amit √©rdemes megfontolni
* [Generat√≠v m≈±v√©szet l√©trehoz√°sa GAN-ekkel az Azure ML-en](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)

## Feladat

N√©zd √°t a leck√©hez kapcsol√≥d√≥ k√©t notebook egyik√©t, √©s tan√≠tsd √∫jra a GAN-t saj√°t k√©peiden. Mit tudsz l√©trehozni?

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.