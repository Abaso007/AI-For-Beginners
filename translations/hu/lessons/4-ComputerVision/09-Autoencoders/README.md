<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b306c04f5337b6e7430e5c0b16bb5c0",
  "translation_date": "2025-08-25T22:28:38+00:00",
  "source_file": "lessons/4-ComputerVision/09-Autoencoders/README.md",
  "language_code": "hu"
}
-->
# Autoenk√≥derek

Amikor CNN-eket tan√≠tunk, az egyik probl√©ma az, hogy sok c√≠mk√©zett adatra van sz√ºks√©g√ºnk. K√©pklasszifik√°ci√≥ eset√©n p√©ld√°ul manu√°lisan kell az egyes k√©peket k√ºl√∂nb√∂z≈ë oszt√°lyokba sorolni.

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/17)

Azonban el≈ëfordulhat, hogy nyers (c√≠mk√©zetlen) adatokat szeretn√©nk haszn√°lni a CNN jellemz≈ëk kinyer√©s√©re, amit **√∂nfel√ºgyelt tanul√°snak** nevez√ºnk. C√≠mk√©k helyett a tan√≠t√≥k√©peket haszn√°ljuk h√°l√≥zati bemenetk√©nt √©s kimenetk√©nt is. Az **autoenk√≥der** f≈ë √∂tlete az, hogy lesz egy **enk√≥der h√°l√≥zat**, amely a bemeneti k√©pet valamilyen **rejtett t√©rbe** alak√≠tja (√°ltal√°ban egy kisebb m√©ret≈± vektor), majd egy **dek√≥der h√°l√≥zat**, amelynek c√©lja az eredeti k√©p rekonstru√°l√°sa.

> ‚úÖ Az [autoenk√≥der](https://wikipedia.org/wiki/Autoencoder) "egy mesters√©ges neur√°lis h√°l√≥zat t√≠pusa, amelyet c√≠mk√©zetlen adatok hat√©kony k√≥dol√°s√°nak megtanul√°s√°ra haszn√°lnak."

Mivel az autoenk√≥dert arra tan√≠tjuk, hogy az eredeti k√©pb≈ël min√©l t√∂bb inform√°ci√≥t megragadjon a pontos rekonstrukci√≥ √©rdek√©ben, a h√°l√≥zat megpr√≥b√°lja megtal√°lni a bemeneti k√©pek legjobb **be√°gyaz√°s√°t**, hogy megragadja azok jelent√©s√©t.

![AutoEncoder Diagram](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.hu.jpg)

> K√©p a [Keras blogb√≥l](https://blog.keras.io/building-autoencoders-in-keras.html)

## Autoenk√≥derek haszn√°lati forgat√≥k√∂nyvei

B√°r az eredeti k√©pek rekonstru√°l√°sa √∂nmag√°ban nem t≈±nik hasznosnak, van n√©h√°ny forgat√≥k√∂nyv, ahol az autoenk√≥derek k√ºl√∂n√∂sen hasznosak:

* **K√©pek dimenzi√≥j√°nak cs√∂kkent√©se vizualiz√°ci√≥hoz** vagy **k√©pbe√°gyaz√°sok tan√≠t√°sa**. Az autoenk√≥derek √°ltal√°ban jobb eredm√©nyeket adnak, mint a PCA, mivel figyelembe veszik a k√©pek t√©rbeli jelleg√©t √©s hierarchikus jellemz≈ëit.
* **Zajcs√∂kkent√©s**, azaz zaj elt√°vol√≠t√°sa a k√©pr≈ël. Mivel a zaj sok haszontalan inform√°ci√≥t hordoz, az autoenk√≥der nem tudja mindet beilleszteni a viszonylag kis rejtett t√©rbe, √≠gy csak a k√©p fontos r√©sz√©t ragadja meg. Zajcs√∂kkent≈ëk tan√≠t√°sakor az eredeti k√©pekkel kezd√ºnk, √©s mesters√©gesen zajt adunk hozz√°juk, amelyeket az autoenk√≥der bemenetek√©nt haszn√°lunk.
* **Szuperfelbont√°s**, a k√©p felbont√°s√°nak n√∂vel√©se. Magas felbont√°s√∫ k√©pekkel kezd√ºnk, √©s az alacsonyabb felbont√°s√∫ k√©pet haszn√°ljuk az autoenk√≥der bemenetek√©nt.
* **Generat√≠v modellek**. Miut√°n az autoenk√≥dert betan√≠tottuk, a dek√≥der r√©szt felhaszn√°lhatjuk √∫j objektumok l√©trehoz√°s√°ra v√©letlenszer≈± rejtett vektorokb√≥l kiindulva.

## Vari√°ci√≥s Autoenk√≥derek (VAE)

A hagyom√°nyos autoenk√≥derek valamilyen m√≥don cs√∂kkentik a bemeneti adatok dimenzi√≥j√°t, √©s megpr√≥b√°lj√°k azonos√≠tani a bemeneti k√©pek fontos jellemz≈ëit. Azonban a rejtett vektorok gyakran nem √©rtelmezhet≈ëk. M√°s sz√≥val, ha p√©ld√°ul az MNIST adathalmazt vessz√ºk, nem k√∂nny≈± meghat√°rozni, hogy mely sz√°mjegyek felelnek meg a k√ºl√∂nb√∂z≈ë rejtett vektoroknak, mivel a k√∂zeli rejtett vektorok nem felt√©tlen√ºl ugyanazokat a sz√°mjegyeket jelentik.

Ezzel szemben generat√≠v modellek tan√≠t√°s√°hoz jobb, ha van n√©mi meg√©rt√©s√ºnk a rejtett t√©rr≈ël. Ez az √∂tlet vezet el minket a **vari√°ci√≥s autoenk√≥derhez** (VAE).

A VAE egy olyan autoenk√≥der, amely megtanulja el≈ëre jelezni a rejtett param√©terek *statisztikai eloszl√°s√°t*, az √∫gynevezett **rejtett eloszl√°st**. P√©ld√°ul azt szeretn√©nk, hogy a rejtett vektorok norm√°lisan legyenek elosztva egy z<sub>mean</sub> √°tlaggal √©s z<sub>sigma</sub> sz√≥r√°ssal (mind az √°tlag, mind a sz√≥r√°s egy d dimenzi√≥s vektor). A VAE enk√≥der megtanulja ezeket a param√©tereket el≈ëre jelezni, majd a dek√≥der v√©letlenszer≈± vektort vesz ebb≈ël az eloszl√°sb√≥l, hogy rekonstru√°lja az objektumot.

√ñsszefoglalva:

 * A bemeneti vektorb√≥l el≈ëre jelezz√ºk `z_mean` √©s `z_log_sigma` √©rt√©keket (a sz√≥r√°s helyett annak logaritmus√°t jelezz√ºk el≈ëre)
 * Mint√°t vesz√ºnk egy `sample` vektorb√≥l az N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>)) eloszl√°sb√≥l
 * A dek√≥der megpr√≥b√°lja dek√≥dolni az eredeti k√©pet a `sample` vektort bemenetk√©nt haszn√°lva

 <img src="images/vae.png" width="50%">

> K√©p [Isaak Dykeman blogbejegyz√©s√©b≈ël](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

A vari√°ci√≥s autoenk√≥derek egy √∂sszetett vesztes√©gf√ºggv√©nyt haszn√°lnak, amely k√©t r√©szb≈ël √°ll:

* **Rekonstrukci√≥s vesztes√©g**, amely azt mutatja, hogy a rekonstru√°lt k√©p mennyire k√∂zel √°ll a c√©lhoz (ez lehet p√©ld√°ul a Mean Squared Error, vagy MSE). Ez ugyanaz a vesztes√©gf√ºggv√©ny, mint a norm√°l autoenk√≥derekn√©l.
* **KL vesztes√©g**, amely biztos√≠tja, hogy a rejtett v√°ltoz√≥ eloszl√°sa k√∂zel maradjon a norm√°l eloszl√°shoz. Ez a [Kullback-Leibler divergencia](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) fogalm√°n alapul - egy metrika, amely k√©t statisztikai eloszl√°s hasonl√≥s√°g√°t m√©ri.

A VAE-k egyik fontos el≈ënye, hogy viszonylag k√∂nnyen lehet √∫j k√©peket gener√°lni, mivel tudjuk, mely eloszl√°sb√≥l kell mint√°t venni a rejtett vektorokhoz. P√©ld√°ul, ha egy VAE-t tan√≠tunk 2D rejtett vektorral az MNIST adathalmazon, akkor a rejtett vektor komponenseit v√°ltoztatva k√ºl√∂nb√∂z≈ë sz√°mjegyeket kapunk:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> K√©p [Dmitry Soshnikov](http://soshnikov.com) √°ltal

Figyelj√ºk meg, hogyan olvadnak √∂ssze a k√©pek, ahogy a rejtett param√©tert√©r k√ºl√∂nb√∂z≈ë r√©szeib≈ël kezd√ºnk mint√°t venni. Ezt a teret 2D-ben is vizualiz√°lhatjuk:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> K√©p [Dmitry Soshnikov](http://soshnikov.com) √°ltal

## ‚úçÔ∏è Gyakorlatok: Autoenk√≥derek

Tov√°bbi inform√°ci√≥ az autoenk√≥derekr≈ël az al√°bbi jegyzetekben tal√°lhat√≥:

* [Autoenk√≥derek TensorFlow-ban](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [Autoenk√≥derek PyTorch-ban](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Az autoenk√≥derek tulajdons√°gai

* **Adatspecifikus** - csak azon k√©pt√≠pusokkal m≈±k√∂dnek j√≥l, amelyeken tan√≠tott√°k ≈ëket. P√©ld√°ul, ha egy szuperfelbont√°s√∫ h√°l√≥zatot vir√°gokon tan√≠tunk, az nem fog j√≥l m≈±k√∂dni portr√©kon. Ennek oka, hogy a h√°l√≥zat a magasabb felbont√°s√∫ k√©pet √∫gy √°ll√≠tja el≈ë, hogy finom r√©szleteket vesz a tan√≠t√≥ adathalmazb√≥l tanult jellemz≈ëkb≈ël.
* **Vesztes√©ges** - a rekonstru√°lt k√©p nem ugyanaz, mint az eredeti k√©p. A vesztes√©g jellege a tan√≠t√°s sor√°n haszn√°lt *vesztes√©gf√ºggv√©nyt≈ël* f√ºgg.
* **C√≠mk√©zetlen adatokkal m≈±k√∂dik**

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/18)

## √ñsszegz√©s

Ebben a leck√©ben megismerkedt√©l az autoenk√≥derek k√ºl√∂nb√∂z≈ë t√≠pusaival, amelyeket az AI kutat√≥k haszn√°lhatnak. Megtanultad, hogyan √©p√≠tsd fel ≈ëket, √©s hogyan haszn√°ld ≈ëket k√©pek rekonstru√°l√°s√°ra. Emellett megismerkedt√©l a VAE-vel √©s azzal, hogyan lehet √∫j k√©peket gener√°lni vele.

## üöÄ Kih√≠v√°s

Ebben a leck√©ben megtanultad, hogyan haszn√°lhat√≥k az autoenk√≥derek k√©pekhez. De zen√©hez is haszn√°lhat√≥k! N√©zd meg a Magenta projekt [MusicVAE](https://magenta.tensorflow.org/music-vae) projektj√©t, amely autoenk√≥dereket haszn√°l a zene rekonstru√°l√°s√°nak megtanul√°s√°ra. V√©gezzen n√©h√°ny [k√≠s√©rletet](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) ezzel a k√∂nyvt√°rral, hogy l√°ssa, mit tud l√©trehozni.

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ai/quiz/16)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Tov√°bbi inform√°ci√≥ az autoenk√≥derekr≈ël az al√°bbi forr√°sokban tal√°lhat√≥:

* [Autoenk√≥derek √©p√≠t√©se Keras-ban](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Blogbejegyz√©s a NeuroHive-on](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Vari√°ci√≥s autoenk√≥derek magyar√°zata](https://kvfrans.com/variational-autoencoders-explained/)
* [Felt√©teles vari√°ci√≥s autoenk√≥derek](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Feladat

A [TensorFlow jegyzet](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb) v√©g√©n tal√°lhat√≥ egy "feladat" - ezt haszn√°ld h√°zi feladatk√©nt.

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.