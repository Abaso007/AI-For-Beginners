<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e40b47ac3fd48f71304ede1474e66293",
  "translation_date": "2025-08-25T21:38:37+00:00",
  "source_file": "lessons/5-NLP/14-Embeddings/README.md",
  "language_code": "cs"
}
-->
# Vno≈ôen√≠

## [Kv√≠z p≈ôed p≈ôedn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/27)

P≈ôi tr√©nov√°n√≠ klasifik√°tor≈Ø zalo≈æen√Ωch na BoW nebo TF/IDF jsme pracovali s vysoce dimenzion√°ln√≠mi vektory bag-of-words o d√©lce `vocab_size` a explicitnƒõ jsme p≈ôev√°dƒõli n√≠zko-dimenzion√°ln√≠ poziƒçn√≠ reprezentace na ≈ô√≠dk√© jednohotov√© reprezentace. Tato jednohotov√° reprezentace v≈°ak nen√≠ pamƒõ≈•ovƒõ efektivn√≠. Nav√≠c je ka≈æd√© slovo pova≈æov√°no za nez√°visl√© na ostatn√≠ch, tj. jednohotov√© vektory nevyjad≈ôuj√≠ ≈æ√°dnou s√©mantickou podobnost mezi slovy.

My≈°lenka **vno≈ôen√≠** spoƒç√≠v√° v reprezentaci slov pomoc√≠ n√≠zko-dimenzion√°ln√≠ch hust√Ωch vektor≈Ø, kter√© nƒõjak√Ωm zp≈Øsobem odr√°≈æej√≠ s√©mantick√Ω v√Ωznam slova. Pozdƒõji si pov√≠me, jak vytvo≈ôit smyslupln√° vno≈ôen√≠ slov, ale prozat√≠m si vno≈ôen√≠ p≈ôedstavme jako zp≈Øsob sn√≠≈æen√≠ dimenzionality vektoru slova.

Vrstva vno≈ôen√≠ tedy p≈ôijme slovo jako vstup a vytvo≈ô√≠ v√Ωstupn√≠ vektor o specifikovan√© velikosti `embedding_size`. Sv√Ωm zp≈Øsobem je velmi podobn√° vrstvƒõ `Linear`, ale m√≠sto jednohotov√©ho vektoru bude schopna p≈ôijmout ƒç√≠slo slova jako vstup, co≈æ n√°m umo≈æn√≠ vyhnout se vytv√°≈ôen√≠ velk√Ωch jednohotov√Ωch vektor≈Ø.

Pou≈æit√≠m vrstvy vno≈ôen√≠ jako prvn√≠ vrstvy v na≈°√≠ s√≠ti klasifik√°toru m≈Ø≈æeme p≈ôej√≠t od modelu bag-of-words k modelu **embedding bag**, kde nejprve p≈ôevedeme ka≈æd√© slovo v na≈°em textu na odpov√≠daj√≠c√≠ vno≈ôen√≠ a pot√© vypoƒç√≠t√°me nƒõjakou agregaƒçn√≠ funkci p≈ôes v≈°echna tato vno≈ôen√≠, jako je `sum`, `average` nebo `max`.  

![Obr√°zek ukazuj√≠c√≠ klasifik√°tor s vno≈ôen√≠m pro pƒõt slov v sekvenci.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.cs.png)

> Obr√°zek od autora

## ‚úçÔ∏è Cviƒçen√≠: Vno≈ôen√≠

Pokraƒçujte ve sv√©m uƒçen√≠ v n√°sleduj√≠c√≠ch notebooc√≠ch:
* [Vno≈ôen√≠ s PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Vno≈ôen√≠ s TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## S√©mantick√° vno≈ôen√≠: Word2Vec

Zat√≠mco vrstva vno≈ôen√≠ se nauƒçila mapovat slova na vektorovou reprezentaci, tato reprezentace nemusela nutnƒõ m√≠t velk√Ω s√©mantick√Ω v√Ωznam. Bylo by skvƒõl√© nauƒçit se vektorovou reprezentaci takovou, ≈æe podobn√° slova nebo synonyma odpov√≠daj√≠ vektor≈Øm, kter√© jsou si bl√≠zk√© z hlediska nƒõjak√© vektorov√© vzd√°lenosti (nap≈ô. Euklidovsk√© vzd√°lenosti).

K tomu pot≈ôebujeme p≈ôedtr√©novat n√°≈° model vno≈ôen√≠ na velk√© sb√≠rce text≈Ø specifick√Ωm zp≈Øsobem. Jedn√≠m ze zp≈Øsob≈Ø tr√©nov√°n√≠ s√©mantick√Ωch vno≈ôen√≠ je metoda [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Ta je zalo≈æena na dvou hlavn√≠ch architektur√°ch, kter√© se pou≈æ√≠vaj√≠ k vytvo≈ôen√≠ distribuovan√© reprezentace slov:

 - **Kontinu√°ln√≠ bag-of-words** (CBoW) ‚Äî v t√©to architektu≈ôe tr√©nujeme model, aby p≈ôedpovƒõdƒõl slovo na z√°kladƒõ okoln√≠ho kontextu. Dan√Ω ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ m√° za c√≠l model p≈ôedpovƒõdƒõt $W_0$ z $(W_{-2},W_{-1},W_1,W_2)$.
 - **Kontinu√°ln√≠ skip-gram** je opakem CBoW. Model pou≈æ√≠v√° okoln√≠ okno kontextov√Ωch slov k p≈ôedpovƒõdi aktu√°ln√≠ho slova.

CBoW je rychlej≈°√≠, zat√≠mco skip-gram je pomalej≈°√≠, ale l√©pe reprezentuje m√©nƒõ ƒçast√° slova.

![Obr√°zek ukazuj√≠c√≠ algoritmy CBoW a Skip-Gram pro p≈ôevod slov na vektory.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.cs.png)

> Obr√°zek z [tohoto ƒçl√°nku](https://arxiv.org/pdf/1301.3781.pdf)

P≈ôedtr√©novan√° vno≈ôen√≠ Word2Vec (stejnƒõ jako jin√© podobn√© modely, nap≈ô√≠klad GloVe) mohou b√Ωt tak√© pou≈æita m√≠sto vrstvy vno≈ôen√≠ v neuronov√Ωch s√≠t√≠ch. Mus√≠me se v≈°ak vypo≈ô√°dat se slovn√≠ky, proto≈æe slovn√≠k pou≈æit√Ω k p≈ôedtr√©nov√°n√≠ Word2Vec/GloVe se pravdƒõpodobnƒõ li≈°√≠ od slovn√≠ku v na≈°em textov√©m korpusu. Pod√≠vejte se na v√Ω≈°e uveden√© notebooky, abyste zjistili, jak tento probl√©m vy≈ôe≈°it.

## Kontextov√° vno≈ôen√≠

Jedn√≠m z kl√≠ƒçov√Ωch omezen√≠ tradiƒçn√≠ch p≈ôedtr√©novan√Ωch reprezentac√≠ vno≈ôen√≠, jako je Word2Vec, je probl√©m rozli≈°en√≠ v√Ωznamu slov. Zat√≠mco p≈ôedtr√©novan√° vno≈ôen√≠ mohou zachytit ƒç√°st v√Ωznamu slov v kontextu, ka≈æd√Ω mo≈æn√Ω v√Ωznam slova je zak√≥dov√°n do stejn√©ho vno≈ôen√≠. To m≈Ø≈æe zp≈Øsobit probl√©my v n√°sledn√Ωch modelech, proto≈æe mnoho slov, jako nap≈ô√≠klad slovo 'play', m√° r≈Øzn√© v√Ωznamy v z√°vislosti na kontextu, ve kter√©m jsou pou≈æity.

Nap≈ô√≠klad slovo 'play' v tƒõchto dvou vƒõt√°ch m√° zcela odli≈°n√Ω v√Ωznam:

- ≈†el jsem na **hru** do divadla.
- John si chce **hr√°t** se sv√Ωmi p≈ô√°teli.

P≈ôedtr√©novan√° vno≈ôen√≠ v√Ω≈°e reprezentuj√≠ oba tyto v√Ωznamy slova 'play' ve stejn√©m vno≈ôen√≠. Abychom toto omezen√≠ p≈ôekonali, mus√≠me vytvo≈ôit vno≈ôen√≠ zalo≈æen√° na **jazykov√©m modelu**, kter√Ω je tr√©nov√°n na velk√©m korpusu text≈Ø a *v√≠*, jak mohou b√Ωt slova spojov√°na v r≈Øzn√Ωch kontextech. Diskuse o kontextov√Ωch vno≈ôen√≠ch p≈ôesahuje r√°mec tohoto tutori√°lu, ale vr√°t√≠me se k nim p≈ôi prob√≠r√°n√≠ jazykov√Ωch model≈Ø pozdƒõji v kurzu.

## Z√°vƒõr

V t√©to lekci jste se nauƒçili, jak vytvo≈ôit a pou≈æ√≠vat vrstvy vno≈ôen√≠ v TensorFlow a Pytorch, aby l√©pe odr√°≈æely s√©mantick√© v√Ωznamy slov.

## üöÄ V√Ωzva

Word2Vec byl pou≈æit pro nƒõkter√© zaj√≠mav√© aplikace, vƒçetnƒõ generov√°n√≠ text≈Ø p√≠sn√≠ a poezie. Pod√≠vejte se na [tento ƒçl√°nek](https://www.politetype.com/blog/word2vec-color-poems), kter√Ω popisuje, jak autor pou≈æil Word2Vec k generov√°n√≠ poezie. Pod√≠vejte se tak√© na [toto video od Dana Shiffmanna](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain), kde najdete jin√Ω v√Ωklad t√©to techniky. Pot√© zkuste tyto techniky aplikovat na sv≈Øj vlastn√≠ textov√Ω korpus, mo≈æn√° z√≠skan√Ω z Kaggle.

## [Kv√≠z po p≈ôedn√°≈°ce](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## P≈ôehled & Samostudium

Projdƒõte si tento ƒçl√°nek o Word2Vec: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

## [√ökol: Notebooky](assignment.md)

**Prohl√°≈°en√≠:**  
Tento dokument byl p≈ôelo≈æen pomoc√≠ slu≈æby pro automatick√Ω p≈ôeklad [Co-op Translator](https://github.com/Azure/co-op-translator). Aƒçkoli se sna≈æ√≠me o p≈ôesnost, mƒõjte na pamƒõti, ≈æe automatick√© p≈ôeklady mohou obsahovat chyby nebo nep≈ôesnosti. P≈Øvodn√≠ dokument v jeho p≈Øvodn√≠m jazyce by mƒõl b√Ωt pova≈æov√°n za autoritativn√≠ zdroj. Pro d≈Øle≈æit√© informace se doporuƒçuje profesion√°ln√≠ lidsk√Ω p≈ôeklad. Neodpov√≠d√°me za ≈æ√°dn√° nedorozumƒõn√≠ nebo nespr√°vn√© interpretace vypl√Ωvaj√≠c√≠ z pou≈æit√≠ tohoto p≈ôekladu.