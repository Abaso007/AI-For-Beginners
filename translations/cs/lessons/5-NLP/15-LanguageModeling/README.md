<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7ba20f54a5bfcd6521018cdfb17c7c57",
  "translation_date": "2025-09-23T11:29:23+00:00",
  "source_file": "lessons/5-NLP/15-LanguageModeling/README.md",
  "language_code": "cs"
}
-->
# Jazykov√© modelov√°n√≠

S√©mantick√© vektory, jako Word2Vec a GloVe, jsou ve skuteƒçnosti prvn√≠m krokem k **jazykov√©mu modelov√°n√≠** ‚Äì vytv√°≈ôen√≠ model≈Ø, kter√© nƒõjak√Ωm zp≈Øsobem *rozum√≠* (nebo *reprezentuj√≠*) podstatu jazyka.

## [Kv√≠z p≈ôed p≈ôedn√°≈°kou](https://ff-quizzes.netlify.app/en/ai/quiz/29)

Hlavn√≠ my≈°lenkou jazykov√©ho modelov√°n√≠ je jejich tr√©nov√°n√≠ na neoznaƒçen√Ωch datov√Ωch sad√°ch ne≈ô√≠zen√Ωm zp≈Øsobem. To je d≈Øle≈æit√©, proto≈æe m√°me k dispozici obrovsk√© mno≈æstv√≠ neoznaƒçen√©ho textu, zat√≠mco mno≈æstv√≠ oznaƒçen√©ho textu bude v≈ædy omezeno √∫sil√≠m, kter√© m≈Ø≈æeme vƒõnovat jeho oznaƒçov√°n√≠. Nejƒçastƒõji m≈Ø≈æeme vytv√°≈ôet jazykov√© modely, kter√© dok√°≈æou **p≈ôedpov√≠dat chybƒõj√≠c√≠ slova** v textu, proto≈æe je snadn√© n√°hodnƒõ zakr√Ωt slovo v textu a pou≈æ√≠t ho jako tr√©novac√≠ vzorek.

## Tr√©nov√°n√≠ vektor≈Ø

V na≈°ich p≈ôedchoz√≠ch p≈ô√≠kladech jsme pou≈æ√≠vali p≈ôedtr√©novan√© s√©mantick√© vektory, ale je zaj√≠mav√© vidƒõt, jak lze tyto vektory tr√©novat. Existuje nƒõkolik mo≈æn√Ωch p≈ô√≠stup≈Ø, kter√© lze pou≈æ√≠t:

* **Jazykov√© modelov√°n√≠ pomoc√≠ N-Gram≈Ø**, kdy p≈ôedpov√≠d√°me token na z√°kladƒõ N p≈ôedchoz√≠ch token≈Ø (N-gram).
* **Continuous Bag-of-Words** (CBoW), kdy p≈ôedpov√≠d√°me prost≈ôedn√≠ token $W_0$ v sekvenci token≈Ø $W_{-N}$, ..., $W_N$.
* **Skip-gram**, kde p≈ôedpov√≠d√°me sadu sousedn√≠ch token≈Ø {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} na z√°kladƒõ prost≈ôedn√≠ho tokenu $W_0$.

![obr√°zek z ƒçl√°nku o p≈ôevodu slov na vektory](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.cs.png)

> Obr√°zek z [tohoto ƒçl√°nku](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Uk√°zkov√© notebooky: Tr√©nov√°n√≠ modelu CBoW

Pokraƒçujte ve sv√©m uƒçen√≠ v n√°sleduj√≠c√≠ch notebooc√≠ch:

* [Tr√©nov√°n√≠ CBoW Word2Vec pomoc√≠ TensorFlow](CBoW-TF.ipynb)
* [Tr√©nov√°n√≠ CBoW Word2Vec pomoc√≠ PyTorch](CBoW-PyTorch.ipynb)

## Z√°vƒõr

V p≈ôedchoz√≠ lekci jsme vidƒõli, ≈æe vektory slov funguj√≠ jako kouzlo! Nyn√≠ v√≠me, ≈æe tr√©nov√°n√≠ vektor≈Ø slov nen√≠ p≈ô√≠li≈° slo≈æit√Ω √∫kol a mƒõli bychom b√Ωt schopni tr√©novat vlastn√≠ vektory slov pro texty specifick√© pro danou oblast, pokud to bude pot≈ôeba.

## [Kv√≠z po p≈ôedn√°≈°ce](https://ff-quizzes.netlify.app/en/ai/quiz/30)

## P≈ôehled & Samostudium

* [Ofici√°ln√≠ tutori√°l PyTorch o jazykov√©m modelov√°n√≠](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Ofici√°ln√≠ tutori√°l TensorFlow o tr√©nov√°n√≠ modelu Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Pou≈æit√≠ frameworku **gensim** k tr√©nov√°n√≠ nejbƒõ≈ænƒõji pou≈æ√≠van√Ωch vektor≈Ø v nƒõkolika ≈ô√°dc√≠ch k√≥du je pops√°no [v t√©to dokumentaci](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [√ökol: Tr√©nov√°n√≠ modelu Skip-Gram](lab/README.md)

V laborato≈ôi v√°s vyz√Ωv√°me, abyste upravili k√≥d z t√©to lekce a tr√©novali model Skip-Gram m√≠sto CBoW. [P≈ôeƒçtƒõte si podrobnosti](lab/README.md)

---

